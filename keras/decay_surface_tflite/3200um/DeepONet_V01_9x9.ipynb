{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-c80a796891d4>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.is_gpu_available())\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "import re\n",
    "path = os.getcwd()\n",
    "\n",
    "trial = 1\n",
    "dest_fpath = path + f\"/DeepONet_v0{trial}/\"\n",
    "os.makedirs(os.path.dirname(dest_fpath), exist_ok=True, mode=0o777)\n",
    "ckpt_fpath = path + f\"/DeepONet_v0{trial}/Checkpoint/\"\n",
    "os.makedirs(os.path.dirname(ckpt_fpath), exist_ok=True, mode=0o777)\n",
    "fig_fpath = path + f\"/DeepONet_v0{trial}/fig/\"\n",
    "os.makedirs(os.path.dirname(fig_fpath), exist_ok=True, mode=0o777)\n",
    "\n",
    "os.chmod(path + f\"/DeepONet_v0{trial}\", 0o777)\n",
    "for root, dirs, files in os.walk(dest_fpath):\n",
    "    for d in dirs:\n",
    "        os.chmod(os.path.join(root, d), 0o777)\n",
    "    for f in files:\n",
    "        os.chmod(os.path.join(root, f), 0o777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s max is:  26.323999404907227 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.2_tIns_0.028_Kd_0.02684_Tile_9\\\n",
      "s max is:  39.180999755859375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.7_tIns_0.023_Kd_0.11678_Tile_9\\\n",
      "s max is:  68.50700378417969 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_4.5_tIns_0.018_Kd_0.04162_Tile_9\\\n",
      "s max is:  26.826000213623047 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.8_tIns_0.017_Kd_0.02939_Tile_9\\\n",
      "s max is:  62.11000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_6.5_tIns_0.012_Kd_0.01223_Tile_9\\\n",
      "s max is:  207.19000244140625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-07_xy_16000_z_100_tDiel_10.0_tIns_0.035_Kd_0.11196_Tile_9\\\n",
      "s max is:  26.336999893188477 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.8_tIns_0.05_Kd_0.09657_Tile_9\\\n",
      "s max is:  47.321998596191406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_3.0_tIns_0.011_Kd_0.02636_Tile_9\\\n",
      "s max is:  111.20999908447266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-07_xy_16000_z_100_tDiel_5.8_tIns_0.038_Kd_0.04_Tile_9\\\n",
      "s max is:  39.20800018310547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.1_tIns_0.023_Kd_0.04018_Tile_9\\\n",
      "s max is:  26.30699920654297 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.8_tIns_0.011_Kd_0.06912_Tile_9\\\n",
      "s max is:  26.7810001373291 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.2_tIns_0.031_Kd_0.03929_Tile_9\\\n",
      "s max is:  39.869998931884766 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.2_tIns_0.038_Kd_0.05219_Tile_9\\\n",
      "s max is:  67.99299621582031 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_5.4_tIns_0.044_Kd_0.08643_Tile_9\\\n",
      "s max is:  130.22000122070312 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-07_xy_16000_z_100_tDiel_4.2_tIns_0.026_Kd_0.01927_Tile_9\\\n",
      "s max is:  60.23400115966797 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_4.1_tIns_0.035_Kd_0.10577_Tile_9\\\n",
      "s max is:  97.39600372314453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_9.9_tIns_0.034_Kd_0.12752_Tile_9\\\n",
      "s max is:  39.53200149536133 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.3_tIns_0.028_Kd_0.0887_Tile_9\\\n",
      "s max is:  39.8650016784668 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.8_tIns_0.05_Kd_0.07281_Tile_9\\\n",
      "s max is:  51.505001068115234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_5.5_tIns_0.042_Kd_0.02469_Tile_9\\\n",
      "s max is:  120.27999877929688 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-07_xy_16000_z_100_tDiel_4.7_tIns_0.018_Kd_0.08653_Tile_9\\\n",
      "s max is:  26.229999542236328 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.1_tIns_0.033_Kd_0.09392_Tile_9\\\n",
      "s max is:  46.77899932861328 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_1.6_tIns_0.032_Kd_0.12043_Tile_9\\\n",
      "s max is:  26.53499984741211 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.4_tIns_0.037_Kd_0.01993_Tile_9\\\n",
      "s max is:  68.0739974975586 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_8.9_tIns_0.031_Kd_0.09954_Tile_9\\\n",
      "s max is:  50.24399948120117 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_9.4_tIns_0.011_Kd_0.12776_Tile_9\\\n",
      "s max is:  26.847999572753906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.3_tIns_0.048_Kd_0.03326_Tile_9\\\n",
      "s max is:  39.24700164794922 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.1_tIns_0.018_Kd_0.08624_Tile_9\\\n",
      "s max is:  40.26100158691406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.6_tIns_0.026_Kd_0.04369_Tile_9\\\n",
      "s max is:  26.22599983215332 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.7_tIns_0.037_Kd_0.10128_Tile_9\\\n",
      "s max is:  54.691001892089844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_7.3_tIns_0.016_Kd_0.11882_Tile_9\\\n",
      "s max is:  26.506000518798828 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.8_tIns_0.014_Kd_0.06545_Tile_9\\\n",
      "s max is:  26.07200050354004 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_10.0_tIns_0.024_Kd_0.13699_Tile_9\\\n",
      "s max is:  55.92599868774414 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_7.2_tIns_0.024_Kd_0.04913_Tile_9\\\n",
      "s max is:  78.0459976196289 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_9.0_tIns_0.037_Kd_0.12551_Tile_9\\\n",
      "s max is:  39.12099838256836 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.3_tIns_0.025_Kd_0.12033_Tile_9\\\n",
      "s max is:  60.6870002746582 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_2.1_tIns_0.019_Kd_0.00638_Tile_9\\\n",
      "s max is:  26.711999893188477 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.3_tIns_0.033_Kd_0.00898_Tile_9\\\n",
      "s max is:  47.130001068115234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_2.2_tIns_0.048_Kd_0.04203_Tile_9\\\n",
      "s max is:  112.25 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-07_xy_16000_z_100_tDiel_7.1_tIns_0.05_Kd_0.03394_Tile_9\\\n",
      "s max is:  26.535999298095703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.6_tIns_0.019_Kd_0.03702_Tile_9\\\n",
      "s max is:  26.836000442504883 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.5_tIns_0.021_Kd_0.01632_Tile_9\\\n",
      "s max is:  39.45500183105469 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.8_tIns_0.032_Kd_0.03402_Tile_9\\\n",
      "s max is:  38.94300079345703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.3_tIns_0.043_Kd_0.13768_Tile_9\\\n",
      "s max is:  26.413999557495117 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.4_tIns_0.038_Kd_0.0499_Tile_9\\\n",
      "s max is:  26.253000259399414 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.4_tIns_0.025_Kd_0.08604_Tile_9\\\n",
      "s max is:  41.861000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_4.5_tIns_0.037_Kd_0.04589_Tile_9\\\n",
      "s max is:  54.494998931884766 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_2.7_tIns_0.024_Kd_0.13576_Tile_9\\\n",
      "s max is:  39.2859992980957 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.8_tIns_0.013_Kd_0.07127_Tile_9\\\n",
      "s max is:  46.69599914550781 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_3.5_tIns_0.042_Kd_0.13809_Tile_9\\\n",
      "s max is:  39.20199966430664 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.8_tIns_0.042_Kd_0.10515_Tile_9\\\n",
      "s max is:  39.46500015258789 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.5_tIns_0.038_Kd_0.07847_Tile_9\\\n",
      "s max is:  26.559999465942383 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.3_tIns_0.049_Kd_0.06359_Tile_9\\\n",
      "s max is:  26.552000045776367 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.8_tIns_0.025_Kd_0.05894_Tile_9\\\n",
      "s max is:  81.33499908447266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_9.1_tIns_0.034_Kd_0.03809_Tile_9\\\n",
      "s max is:  26.20800018310547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.1_tIns_0.02_Kd_0.05407_Tile_9\\\n",
      "s max is:  45.40399932861328 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_8.9_tIns_0.013_Kd_0.02632_Tile_9\\\n",
      "s max is:  97.19999694824219 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_9.8_tIns_0.014_Kd_0.13075_Tile_9\\\n",
      "s max is:  39.28200149536133 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.6_tIns_0.042_Kd_0.0885_Tile_9\\\n",
      "s max is:  39.29600143432617 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.3_tIns_0.012_Kd_0.08895_Tile_9\\\n",
      "s max is:  39.19200134277344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.1_tIns_0.049_Kd_0.11276_Tile_9\\\n",
      "s max is:  255.5800018310547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-08_xy_16000_z_100_tDiel_6.9_tIns_0.032_Kd_0.04499_Tile_9\\\n",
      "s max is:  39.11399841308594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.6_tIns_0.036_Kd_0.11191_Tile_9\\\n",
      "s max is:  26.27400016784668 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.6_tIns_0.028_Kd_0.02862_Tile_9\\\n",
      "s max is:  26.36199951171875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.3_tIns_0.044_Kd_0.09395_Tile_9\\\n",
      "s max is:  50.84600067138672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_6.3_tIns_0.037_Kd_0.08457_Tile_9\\\n",
      "s max is:  47.11399841308594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_1.8_tIns_0.025_Kd_0.02241_Tile_9\\\n",
      "s max is:  40.097999572753906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.7_tIns_0.046_Kd_0.05632_Tile_9\\\n",
      "s max is:  98.5 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_2.8_tIns_0.051_Kd_0.05221_Tile_9\\\n",
      "s max is:  40.02399826049805 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.1_tIns_0.023_Kd_0.02228_Tile_9\\\n",
      "s max is:  39.49399948120117 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.1_tIns_0.046_Kd_0.03668_Tile_9\\\n",
      "s max is:  39.624000549316406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.7_tIns_0.041_Kd_0.04789_Tile_9\\\n",
      "s max is:  60.459999084472656 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_3.9_tIns_0.022_Kd_0.08224_Tile_9\\\n",
      "s max is:  44.45100021362305 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_6.1_tIns_0.036_Kd_0.06665_Tile_9\\\n",
      "s max is:  50.34199905395508 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_3.9_tIns_0.028_Kd_0.11797_Tile_9\\\n",
      "s max is:  47.60499954223633 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_6.4_tIns_0.05_Kd_0.05969_Tile_9\\\n",
      "s max is:  55.25199890136719 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_2.8_tIns_0.038_Kd_0.02535_Tile_9\\\n",
      "s max is:  42.23400115966797 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_5.1_tIns_0.021_Kd_0.01234_Tile_9\\\n",
      "s max is:  67.26899719238281 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_3.7_tIns_0.038_Kd_0.12717_Tile_9\\\n",
      "s max is:  41.36000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_1.5_tIns_0.039_Kd_0.09465_Tile_9\\\n",
      "s max is:  39.36899948120117 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.2_tIns_0.012_Kd_0.08583_Tile_9\\\n",
      "s max is:  39.23400115966797 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.6_tIns_0.029_Kd_0.10996_Tile_9\\\n",
      "s max is:  39.73699951171875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.2_tIns_0.05_Kd_0.06525_Tile_9\\\n",
      "s max is:  60.029998779296875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_6.5_tIns_0.017_Kd_0.12432_Tile_9\\\n",
      "s max is:  100.5199966430664 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_10.0_tIns_0.043_Kd_0.07051_Tile_9\\\n",
      "s max is:  39.112998962402344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.3_tIns_0.042_Kd_0.12074_Tile_9\\\n",
      "s max is:  40.183998107910156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.0_tIns_0.019_Kd_0.04334_Tile_9\\\n",
      "s max is:  41.512001037597656 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_2.0_tIns_0.014_Kd_0.04828_Tile_9\\\n",
      "s max is:  70.31600189208984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_9.3_tIns_0.029_Kd_0.03039_Tile_9\\\n",
      "s max is:  51.26900100708008 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_4.5_tIns_0.035_Kd_0.02475_Tile_9\\\n",
      "s max is:  39.665000915527344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.1_tIns_0.048_Kd_0.07096_Tile_9\\\n",
      "s max is:  26.297000885009766 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.7_tIns_0.044_Kd_0.07724_Tile_9\\\n",
      "s max is:  39.18299865722656 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.0_tIns_0.039_Kd_0.10318_Tile_9\\\n",
      "s max is:  67.72599792480469 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_2.4_tIns_0.048_Kd_0.06932_Tile_9\\\n",
      "s max is:  26.298999786376953 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.8_tIns_0.025_Kd_0.08501_Tile_9\\\n",
      "s max is:  41.518001556396484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_3.5_tIns_0.015_Kd_0.0807_Tile_9\\\n",
      "s max is:  40.415000915527344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.2_tIns_0.031_Kd_0.03008_Tile_9\\\n",
      "s max is:  26.246999740600586 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.6_tIns_0.04_Kd_0.10497_Tile_9\\\n",
      "s max is:  55.375999450683594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_8.8_tIns_0.047_Kd_0.08952_Tile_9\\\n",
      "s max is:  104.98999786376953 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-07_xy_16000_z_100_tDiel_9.8_tIns_0.015_Kd_0.10985_Tile_9\\\n",
      "s max is:  122.22000122070312 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-07_xy_16000_z_100_tDiel_6.0_tIns_0.019_Kd_0.04839_Tile_9\\\n",
      "s max is:  26.183000564575195 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.1_tIns_0.047_Kd_0.11292_Tile_9\\\n",
      "s max is:  26.246000289916992 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.2_tIns_0.015_Kd_0.06548_Tile_9\\\n",
      "s max is:  54.70800018310547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_1.1_tIns_0.015_Kd_0.06277_Tile_9\\\n",
      "s max is:  39.71799850463867 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.4_tIns_0.02_Kd_0.07938_Tile_9\\\n",
      "s max is:  39.26499938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.8_tIns_0.021_Kd_0.0779_Tile_9\\\n",
      "s max is:  50.569000244140625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_7.6_tIns_0.026_Kd_0.10834_Tile_9\\\n",
      "s max is:  39.48500061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.5_tIns_0.044_Kd_0.0655_Tile_9\\\n",
      "s max is:  26.565000534057617 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.6_tIns_0.041_Kd_0.05645_Tile_9\\\n",
      "s max is:  41.83000183105469 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_3.2_tIns_0.039_Kd_0.02068_Tile_9\\\n",
      "s max is:  79.22899627685547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_3.4_tIns_0.025_Kd_0.03301_Tile_9\\\n",
      "s max is:  39.07099914550781 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.5_tIns_0.029_Kd_0.12752_Tile_9\\\n",
      "s max is:  39.49800109863281 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.5_tIns_0.021_Kd_0.04232_Tile_9\\\n",
      "s max is:  41.948001861572266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_5.4_tIns_0.035_Kd_0.04925_Tile_9\\\n",
      "s max is:  103.02999877929688 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_9.4_tIns_0.047_Kd_0.02113_Tile_9\\\n",
      "s max is:  39.167999267578125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.4_tIns_0.019_Kd_0.07667_Tile_9\\\n",
      "s max is:  26.270000457763672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.9_tIns_0.024_Kd_0.10302_Tile_9\\\n",
      "s max is:  26.17300033569336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.8_tIns_0.03_Kd_0.11172_Tile_9\\\n",
      "s max is:  26.680999755859375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.5_tIns_0.042_Kd_0.01374_Tile_9\\\n",
      "s max is:  26.75200080871582 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.1_tIns_0.034_Kd_0.01223_Tile_9\\\n",
      "s max is:  26.733999252319336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.7_tIns_0.015_Kd_0.03974_Tile_9\\\n",
      "s max is:  140.5 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-07_xy_16000_z_100_tDiel_8.5_tIns_0.016_Kd_0.09693_Tile_9\\\n",
      "s max is:  60.29199981689453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_1.2_tIns_0.044_Kd_0.03828_Tile_9\\\n",
      "s max is:  39.516998291015625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.3_tIns_0.046_Kd_0.09486_Tile_9\\\n",
      "s max is:  41.43299865722656 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_6.5_tIns_0.021_Kd_0.11088_Tile_9\\\n",
      "s max is:  47.29999923706055 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_8.8_tIns_0.027_Kd_0.09476_Tile_9\\\n",
      "s max is:  78.3010025024414 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_10.0_tIns_0.017_Kd_0.11875_Tile_9\\\n",
      "s max is:  42.332000732421875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_8.6_tIns_0.011_Kd_0.04963_Tile_9\\\n",
      "s max is:  60.18299865722656 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_5.7_tIns_0.028_Kd_0.11469_Tile_9\\\n",
      "s max is:  39.07400131225586 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.5_tIns_0.021_Kd_0.12607_Tile_9\\\n",
      "s max is:  26.617000579833984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.2_tIns_0.044_Kd_0.06146_Tile_9\\\n",
      "s max is:  79.70800018310547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_8.7_tIns_0.017_Kd_0.0774_Tile_9\\\n",
      "s max is:  39.01599884033203 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.1_tIns_0.033_Kd_0.13397_Tile_9\\\n",
      "s max is:  50.42300033569336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_1.6_tIns_0.018_Kd_0.07768_Tile_9\\\n",
      "s max is:  26.636999130249023 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.2_tIns_0.044_Kd_0.04104_Tile_9\\\n",
      "s max is:  41.60900115966797 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_8.0_tIns_0.046_Kd_0.10151_Tile_9\\\n",
      "s max is:  26.160999298095703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.7_tIns_0.014_Kd_0.11844_Tile_9\\\n",
      "s max is:  39.1609992980957 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.1_tIns_0.022_Kd_0.09634_Tile_9\\\n",
      "s max is:  41.597999572753906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_6.0_tIns_0.045_Kd_0.09409_Tile_9\\\n",
      "s max is:  47.17499923706055 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_7.0_tIns_0.045_Kd_0.09673_Tile_9\\\n",
      "s max is:  26.909000396728516 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.4_tIns_0.045_Kd_0.01092_Tile_9\\\n",
      "s max is:  55.27000045776367 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_5.7_tIns_0.011_Kd_0.07384_Tile_9\\\n",
      "s max is:  39.22600173950195 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.3_tIns_0.017_Kd_0.11346_Tile_9\\\n",
      "s max is:  50.42900085449219 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_3.1_tIns_0.032_Kd_0.10281_Tile_9\\\n",
      "s max is:  39.41600036621094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.1_tIns_0.031_Kd_0.01724_Tile_9\\\n",
      "s max is:  39.04499816894531 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.7_tIns_0.02_Kd_0.13277_Tile_9\\\n",
      "s max is:  39.111000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.9_tIns_0.038_Kd_0.12325_Tile_9\\\n",
      "s max is:  60.22600173950195 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_1.1_tIns_0.046_Kd_0.05286_Tile_9\\\n",
      "s max is:  39.81700134277344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.0_tIns_0.024_Kd_0.07018_Tile_9\\\n",
      "s max is:  26.099000930786133 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_10.1_tIns_0.011_Kd_0.13225_Tile_9\\\n",
      "s max is:  45.02399826049805 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_5.1_tIns_0.028_Kd_0.00369_Tile_9\\\n",
      "s max is:  39.020999908447266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.7_tIns_0.034_Kd_0.13288_Tile_9\\\n",
      "s max is:  26.280000686645508 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.8_tIns_0.035_Kd_0.03652_Tile_9\\\n",
      "s max is:  39.10200119018555 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.7_tIns_0.029_Kd_0.12404_Tile_9\\\n",
      "s max is:  68.1780014038086 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_4.7_tIns_0.031_Kd_0.06731_Tile_9\\\n",
      "s max is:  55.400001525878906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_4.4_tIns_0.016_Kd_0.04647_Tile_9\\\n",
      "s max is:  26.242000579833984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.3_tIns_0.045_Kd_0.10096_Tile_9\\\n",
      "s max is:  26.559999465942383 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.4_tIns_0.017_Kd_0.0534_Tile_9\\\n",
      "s max is:  26.858999252319336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.7_tIns_0.021_Kd_0.025_Tile_9\\\n",
      "s max is:  51.09199905395508 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_8.8_tIns_0.041_Kd_0.08408_Tile_9\\\n",
      "s max is:  26.18899917602539 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.2_tIns_0.024_Kd_0.08361_Tile_9\\\n",
      "s max is:  50.90999984741211 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_4.6_tIns_0.045_Kd_0.06313_Tile_9\\\n",
      "s max is:  56.698001861572266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_8.1_tIns_0.03_Kd_0.02138_Tile_9\\\n",
      "s max is:  39.676998138427734 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.5_tIns_0.017_Kd_0.08264_Tile_9\\\n",
      "s max is:  39.125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.2_tIns_0.044_Kd_0.12027_Tile_9\\\n",
      "s max is:  39.2869987487793 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.5_tIns_0.041_Kd_0.08621_Tile_9\\\n",
      "s max is:  44.540000915527344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_4.5_tIns_0.036_Kd_0.03439_Tile_9\\\n",
      "s max is:  55.04499816894531 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_8.8_tIns_0.041_Kd_0.1047_Tile_9\\\n",
      "s max is:  47.9119987487793 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_9.6_tIns_0.041_Kd_0.06568_Tile_9\\\n",
      "s max is:  39.17300033569336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.6_tIns_0.04_Kd_0.11554_Tile_9\\\n",
      "s max is:  26.23200035095215 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.1_tIns_0.049_Kd_0.11102_Tile_9\\\n",
      "s max is:  56.14500045776367 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_4.9_tIns_0.032_Kd_0.00293_Tile_9\\\n",
      "s max is:  40.14400100708008 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.6_tIns_0.021_Kd_0.05121_Tile_9\\\n",
      "s max is:  39.042999267578125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.9_tIns_0.036_Kd_0.13123_Tile_9\\\n",
      "s max is:  39.255001068115234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.8_tIns_0.025_Kd_0.10481_Tile_9\\\n",
      "s max is:  98.04100036621094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_5.7_tIns_0.028_Kd_0.10489_Tile_9\\\n",
      "s max is:  40.2140007019043 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.7_tIns_0.05_Kd_0.00183_Tile_9\\\n",
      "s max is:  44.06800079345703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_1.9_tIns_0.05_Kd_0.05218_Tile_9\\\n",
      "s max is:  26.552000045776367 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.5_tIns_0.012_Kd_0.01613_Tile_9\\\n",
      "s max is:  60.45800018310547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_6.8_tIns_0.021_Kd_0.10177_Tile_9\\\n",
      "s max is:  69.61499786376953 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_6.4_tIns_0.048_Kd_0.01848_Tile_9\\\n",
      "s max is:  39.24700164794922 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.4_tIns_0.028_Kd_0.10446_Tile_9\\\n",
      "s max is:  26.371999740600586 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.9_tIns_0.019_Kd_0.05049_Tile_9\\\n",
      "s max is:  56.145999908447266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_7.7_tIns_0.016_Kd_0.04255_Tile_9\\\n",
      "s max is:  26.448999404907227 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.9_tIns_0.025_Kd_0.00934_Tile_9\\\n",
      "s max is:  46.75600051879883 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_4.3_tIns_0.051_Kd_0.12953_Tile_9\\\n",
      "s max is:  39.22999954223633 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.7_tIns_0.039_Kd_0.10476_Tile_9\\\n",
      "s max is:  39.733001708984375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.5_tIns_0.01_Kd_0.07262_Tile_9\\\n",
      "s max is:  40.39099884033203 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.3_tIns_0.046_Kd_0.03337_Tile_9\\\n",
      "s max is:  39.46200180053711 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.1_tIns_0.031_Kd_0.04106_Tile_9\\\n",
      "s max is:  44.45800018310547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_7.8_tIns_0.037_Kd_0.07935_Tile_9\\\n",
      "s max is:  67.8030014038086 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_2.4_tIns_0.013_Kd_0.05397_Tile_9\\\n",
      "s max is:  39.0989990234375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.0_tIns_0.047_Kd_0.11931_Tile_9\\\n",
      "s max is:  39.19300079345703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.0_tIns_0.026_Kd_0.11755_Tile_9\\\n",
      "s max is:  39.65399932861328 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.7_tIns_0.048_Kd_0.02325_Tile_9\\\n",
      "s max is:  59.992000579833984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_1.9_tIns_0.03_Kd_0.12589_Tile_9\\\n",
      "s max is:  39.28499984741211 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.7_tIns_0.013_Kd_0.08487_Tile_9\\\n",
      "s max is:  39.290000915527344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.5_tIns_0.017_Kd_0.10275_Tile_9\\\n",
      "s max is:  26.600000381469727 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.3_tIns_0.035_Kd_0.03584_Tile_9\\\n",
      "s max is:  26.513999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.7_tIns_0.025_Kd_0.0281_Tile_9\\\n",
      "s max is:  26.356000900268555 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.7_tIns_0.031_Kd_0.03019_Tile_9\\\n",
      "s max is:  81.2760009765625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_8.3_tIns_0.041_Kd_0.03134_Tile_9\\\n",
      "s max is:  26.645999908447266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.7_tIns_0.019_Kd_0.03173_Tile_9\\\n",
      "s max is:  158.88999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-07_xy_16000_z_100_tDiel_8.4_tIns_0.035_Kd_0.11932_Tile_9\\\n",
      "s max is:  103.69999694824219 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-07_xy_16000_z_100_tDiel_9.5_tIns_0.018_Kd_0.06743_Tile_9\\\n",
      "s max is:  39.75299835205078 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.7_tIns_0.046_Kd_0.06755_Tile_9\\\n",
      "s max is:  42.36800003051758 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_8.7_tIns_0.039_Kd_0.04956_Tile_9\\\n",
      "s max is:  50.474998474121094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_1.5_tIns_0.034_Kd_0.06271_Tile_9\\\n",
      "s max is:  26.32200050354004 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.9_tIns_0.036_Kd_0.01817_Tile_9\\\n",
      "s max is:  39.375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.9_tIns_0.033_Kd_0.0739_Tile_9\\\n",
      "s max is:  108.33000183105469 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-07_xy_16000_z_100_tDiel_1.5_tIns_0.029_Kd_0.0959_Tile_9\\\n",
      "s max is:  26.184999465942383 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.5_tIns_0.024_Kd_0.11587_Tile_9\\\n",
      "s max is:  26.292999267578125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.7_tIns_0.039_Kd_0.02532_Tile_9\\\n",
      "s max is:  26.21500015258789 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.4_tIns_0.035_Kd_0.07077_Tile_9\\\n",
      "s max is:  59.99800109863281 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_3.8_tIns_0.029_Kd_0.12617_Tile_9\\\n",
      "s max is:  26.687999725341797 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.3_tIns_0.016_Kd_0.04237_Tile_9\\\n",
      "s max is:  26.621999740600586 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.1_tIns_0.026_Kd_0.0409_Tile_9\\\n",
      "s max is:  44.66899871826172 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_7.2_tIns_0.021_Kd_0.0579_Tile_9\\\n",
      "s max is:  26.259000778198242 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.8_tIns_0.037_Kd_0.1087_Tile_9\\\n",
      "s max is:  68.6500015258789 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_8.2_tIns_0.014_Kd_0.07523_Tile_9\\\n",
      "s max is:  43.79100036621094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_1.3_tIns_0.018_Kd_0.1299_Tile_9\\\n",
      "s max is:  55.00600051879883 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_7.7_tIns_0.018_Kd_0.10231_Tile_9\\\n",
      "s max is:  26.201000213623047 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.0_tIns_0.045_Kd_0.09997_Tile_9\\\n",
      "s max is:  26.224000930786133 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.7_tIns_0.042_Kd_0.11099_Tile_9\\\n",
      "s max is:  39.33599853515625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.7_tIns_0.015_Kd_0.01805_Tile_9\\\n",
      "s max is:  26.243000030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.7_tIns_0.045_Kd_0.06259_Tile_9\\\n",
      "s max is:  50.41999816894531 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_5.5_tIns_0.043_Kd_0.11515_Tile_9\\\n",
      "s max is:  39.229000091552734 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.9_tIns_0.038_Kd_0.07269_Tile_9\\\n",
      "s max is:  97.47100067138672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_4.1_tIns_0.023_Kd_0.12149_Tile_9\\\n",
      "s max is:  143.89999389648438 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-07_xy_16000_z_100_tDiel_8.8_tIns_0.011_Kd_0.04855_Tile_9\\\n",
      "s max is:  78.16100311279297 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_6.6_tIns_0.044_Kd_0.12039_Tile_9\\\n",
      "s max is:  97.19499969482422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_8.4_tIns_0.018_Kd_0.1313_Tile_9\\\n",
      "s max is:  97.88200378417969 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_1.0_tIns_0.02_Kd_0.0208_Tile_9\\\n",
      "s max is:  44.810001373291016 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_5.8_tIns_0.034_Kd_0.02931_Tile_9\\\n",
      "s max is:  50.63999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_2.0_tIns_0.03_Kd_0.03949_Tile_9\\\n",
      "s max is:  26.499000549316406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.7_tIns_0.047_Kd_0.06029_Tile_9\\\n",
      "s max is:  80.0780029296875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_9.5_tIns_0.027_Kd_0.07267_Tile_9\\\n",
      "s max is:  56.42599868774414 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_7.0_tIns_0.023_Kd_0.01936_Tile_9\\\n",
      "s max is:  42.3849983215332 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_7.7_tIns_0.035_Kd_0.03817_Tile_9\\\n",
      "s max is:  26.233999252319336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.2_tIns_0.02_Kd_0.10751_Tile_9\\\n",
      "s max is:  26.16900062561035 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.4_tIns_0.048_Kd_0.12101_Tile_9\\\n",
      "s max is:  26.410999298095703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.3_tIns_0.031_Kd_0.00674_Tile_9\\\n",
      "s max is:  48.5620002746582 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_8.4_tIns_0.026_Kd_0.01918_Tile_9\\\n",
      "s max is:  26.243999481201172 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.2_tIns_0.049_Kd_0.11441_Tile_9\\\n",
      "s max is:  39.297000885009766 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.1_tIns_0.023_Kd_0.00339_Tile_9\\\n",
      "s max is:  50.37300109863281 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_2.8_tIns_0.044_Kd_0.11151_Tile_9\\\n",
      "s max is:  38.99100112915039 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.4_tIns_0.049_Kd_0.1343_Tile_9\\\n",
      "s max is:  39.15399932861328 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.7_tIns_0.018_Kd_0.09075_Tile_9\\\n",
      "s max is:  39.4739990234375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.4_tIns_0.014_Kd_0.04365_Tile_9\\\n",
      "s max is:  97.1729965209961 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_4.4_tIns_0.015_Kd_0.13435_Tile_9\\\n",
      "s max is:  39.08000183105469 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.3_tIns_0.039_Kd_0.12796_Tile_9\\\n",
      "s max is:  26.1560001373291 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.8_tIns_0.01_Kd_0.12243_Tile_9\\\n",
      "s max is:  26.30500030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.3_tIns_0.033_Kd_0.07976_Tile_9\\\n",
      "s max is:  47.70600128173828 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_7.2_tIns_0.044_Kd_0.05973_Tile_9\\\n",
      "s max is:  67.28199768066406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_1.0_tIns_0.033_Kd_0.11908_Tile_9\\\n",
      "s max is:  40.50299835205078 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.7_tIns_0.013_Kd_0.01835_Tile_9\\\n",
      "s max is:  41.650001525878906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_7.5_tIns_0.034_Kd_0.09556_Tile_9\\\n",
      "s max is:  26.27199935913086 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.1_tIns_0.042_Kd_0.0967_Tile_9\\\n",
      "s max is:  26.305999755859375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.2_tIns_0.05_Kd_0.04163_Tile_9\\\n",
      "s max is:  26.304000854492188 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.9_tIns_0.03_Kd_0.02562_Tile_9\\\n",
      "s max is:  47.26900100708008 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_3.5_tIns_0.026_Kd_0.04998_Tile_9\\\n",
      "s max is:  27.277999877929688 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.0_tIns_0.011_Kd_0.00488_Tile_9\\\n",
      "s max is:  26.378000259399414 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.9_tIns_0.034_Kd_0.09289_Tile_9\\\n",
      "s max is:  78.4800033569336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_1.1_tIns_0.031_Kd_0.0055_Tile_9\\\n",
      "s max is:  39.03300094604492 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.4_tIns_0.045_Kd_0.13272_Tile_9\\\n",
      "s max is:  26.652000427246094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.0_tIns_0.038_Kd_0.03597_Tile_9\\\n",
      "s max is:  39.053001403808594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.7_tIns_0.039_Kd_0.12992_Tile_9\\\n",
      "s max is:  67.9020004272461 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_8.5_tIns_0.042_Kd_0.10479_Tile_9\\\n",
      "s max is:  44.48500061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_6.3_tIns_0.049_Kd_0.06641_Tile_9\\\n",
      "s max is:  26.406999588012695 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.9_tIns_0.034_Kd_0.06987_Tile_9\\\n",
      "s max is:  43.882999420166016 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_1.1_tIns_0.014_Kd_0.07927_Tile_9\\\n",
      "s max is:  39.500999450683594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.5_tIns_0.046_Kd_0.09675_Tile_9\\\n",
      "s max is:  39.7760009765625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.6_tIns_0.05_Kd_0.06498_Tile_9\\\n",
      "s max is:  39.13199996948242 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.2_tIns_0.013_Kd_0.12014_Tile_9\\\n",
      "s max is:  54.882999420166016 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_1.2_tIns_0.035_Kd_0.01519_Tile_9\\\n",
      "s max is:  40.0 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.1_tIns_0.032_Kd_0.02524_Tile_9\\\n",
      "s max is:  139.72999572753906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-07_xy_16000_z_100_tDiel_8.6_tIns_0.037_Kd_0.10974_Tile_9\\\n",
      "s max is:  60.279998779296875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_3.2_tIns_0.032_Kd_0.09402_Tile_9\\\n",
      "s max is:  39.18899917602539 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.3_tIns_0.033_Kd_0.06708_Tile_9\\\n",
      "s max is:  50.332000732421875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_2.5_tIns_0.039_Kd_0.11666_Tile_9\\\n",
      "s max is:  60.35599899291992 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_4.6_tIns_0.044_Kd_0.09929_Tile_9\\\n",
      "s max is:  26.219999313354492 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.6_tIns_0.024_Kd_0.06791_Tile_9\\\n",
      "s max is:  39.702999114990234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.8_tIns_0.042_Kd_0.03885_Tile_9\\\n",
      "s max is:  26.25 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.9_tIns_0.015_Kd_0.05358_Tile_9\\\n",
      "s max is:  39.79100036621094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.6_tIns_0.036_Kd_0.07026_Tile_9\\\n",
      "s max is:  39.387001037597656 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.1_tIns_0.043_Kd_0.09728_Tile_9\\\n",
      "s max is:  56.4119987487793 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_7.3_tIns_0.036_Kd_0.02504_Tile_9\\\n",
      "s max is:  26.554000854492188 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.7_tIns_0.037_Kd_0.008_Tile_9\\\n",
      "s max is:  40.111000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.6_tIns_0.04_Kd_0.00881_Tile_9\\\n",
      "s max is:  41.715999603271484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_6.0_tIns_0.015_Kd_0.07919_Tile_9\\\n",
      "s max is:  39.35100173950195 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.8_tIns_0.05_Kd_0.02767_Tile_9\\\n",
      "s max is:  41.452999114990234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_7.6_tIns_0.039_Kd_0.11274_Tile_9\\\n",
      "s max is:  67.40699768066406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_8.3_tIns_0.028_Kd_0.12202_Tile_9\\\n",
      "s max is:  26.45400047302246 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.4_tIns_0.028_Kd_0.03768_Tile_9\\\n",
      "s max is:  26.339000701904297 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.1_tIns_0.023_Kd_0.07679_Tile_9\\\n",
      "s max is:  67.38099670410156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_9.7_tIns_0.026_Kd_0.1237_Tile_9\\\n",
      "s max is:  26.260000228881836 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.0_tIns_0.041_Kd_0.05957_Tile_9\\\n",
      "s max is:  44.03799819946289 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_1.4_tIns_0.022_Kd_0.02856_Tile_9\\\n",
      "s max is:  79.77100372314453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_5.9_tIns_0.029_Kd_0.05126_Tile_9\\\n",
      "s max is:  54.4739990234375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_4.3_tIns_0.02_Kd_0.13441_Tile_9\\\n",
      "s max is:  26.2450008392334 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.3_tIns_0.044_Kd_0.07742_Tile_9\\\n",
      "s max is:  101.8499984741211 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-07_xy_16000_z_100_tDiel_6.3_tIns_0.047_Kd_0.08859_Tile_9\\\n",
      "s max is:  26.43600082397461 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.6_tIns_0.031_Kd_0.05995_Tile_9\\\n",
      "s max is:  55.5260009765625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_9.1_tIns_0.044_Kd_0.08398_Tile_9\\\n",
      "s max is:  26.349000930786133 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.6_tIns_0.027_Kd_0.02887_Tile_9\\\n",
      "s max is:  41.356998443603516 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_1.7_tIns_0.024_Kd_0.09495_Tile_9\\\n",
      "s max is:  43.12699890136719 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_10.0_tIns_0.025_Kd_0.01642_Tile_9\\\n",
      "s max is:  69.9530029296875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_9.3_tIns_0.051_Kd_0.04218_Tile_9\\\n",
      "s max is:  26.138999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.8_tIns_0.02_Kd_0.1294_Tile_9\\\n",
      "s max is:  50.763999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_6.0_tIns_0.039_Kd_0.08891_Tile_9\\\n",
      "s max is:  39.2130012512207 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.9_tIns_0.051_Kd_0.10945_Tile_9\\\n",
      "s max is:  26.322999954223633 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.7_tIns_0.04_Kd_0.0795_Tile_9\\\n",
      "s max is:  26.672000885009766 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.3_tIns_0.038_Kd_0.03625_Tile_9\\\n",
      "s max is:  39.60300064086914 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.9_tIns_0.043_Kd_0.03594_Tile_9\\\n",
      "s max is:  26.49799919128418 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.6_tIns_0.047_Kd_0.03266_Tile_9\\\n",
      "s max is:  126.62000274658203 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-07_xy_16000_z_100_tDiel_7.5_tIns_0.032_Kd_0.13547_Tile_9\\\n",
      "s max is:  26.312000274658203 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.3_tIns_0.029_Kd_0.07678_Tile_9\\\n",
      "s max is:  39.81399917602539 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.2_tIns_0.048_Kd_0.01271_Tile_9\\\n",
      "s max is:  78.56600189208984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_5.8_tIns_0.033_Kd_0.10112_Tile_9\\\n",
      "s max is:  128.2899932861328 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-07_xy_16000_z_100_tDiel_1.5_tIns_0.04_Kd_0.00257_Tile_9\\\n",
      "s max is:  55.189998626708984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_2.5_tIns_0.047_Kd_0.0247_Tile_9\\\n",
      "s max is:  248.6999969482422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-08_xy_16000_z_100_tDiel_3.6_tIns_0.012_Kd_0.12903_Tile_9\\\n",
      "s max is:  40.35599899291992 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.8_tIns_0.044_Kd_0.0028_Tile_9\\\n",
      "s max is:  105.0 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-07_xy_16000_z_100_tDiel_7.6_tIns_0.016_Kd_0.10418_Tile_9\\\n",
      "s max is:  50.880001068115234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_9.6_tIns_0.046_Kd_0.09809_Tile_9\\\n",
      "s max is:  26.26099967956543 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.3_tIns_0.03_Kd_0.06428_Tile_9\\\n",
      "s max is:  51.422000885009766 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_6.9_tIns_0.03_Kd_0.04976_Tile_9\\\n",
      "s max is:  26.215999603271484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.7_tIns_0.042_Kd_0.11747_Tile_9\\\n",
      "s max is:  39.66299819946289 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.3_tIns_0.048_Kd_0.07924_Tile_9\\\n",
      "s max is:  26.697999954223633 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.7_tIns_0.049_Kd_0.01443_Tile_9\\\n",
      "s max is:  39.3120002746582 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.9_tIns_0.011_Kd_0.10479_Tile_9\\\n",
      "s max is:  39.08100128173828 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.7_tIns_0.011_Kd_0.12444_Tile_9\\\n",
      "s max is:  40.18000030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.9_tIns_0.049_Kd_0.0217_Tile_9\\\n",
      "s max is:  39.92499923706055 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.7_tIns_0.02_Kd_0.05112_Tile_9\\\n",
      "s max is:  68.375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_7.8_tIns_0.014_Kd_0.08306_Tile_9\\\n",
      "s max is:  39.85499954223633 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.4_tIns_0.019_Kd_0.00999_Tile_9\\\n",
      "s max is:  54.334999084472656 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_9.8_tIns_0.024_Kd_0.13685_Tile_9\\\n",
      "s max is:  68.4219970703125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_5.8_tIns_0.048_Kd_0.06719_Tile_9\\\n",
      "s max is:  42.86000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_6.9_tIns_0.016_Kd_0.00305_Tile_9\\\n",
      "s max is:  26.327999114990234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.3_tIns_0.041_Kd_0.05856_Tile_9\\\n",
      "s max is:  26.204999923706055 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.1_tIns_0.018_Kd_0.10723_Tile_9\\\n",
      "s max is:  39.32099914550781 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.6_tIns_0.02_Kd_0.07794_Tile_9\\\n",
      "s max is:  47.321998596191406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_3.8_tIns_0.049_Kd_0.05166_Tile_9\\\n",
      "s max is:  26.743999481201172 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.7_tIns_0.021_Kd_0.029_Tile_9\\\n",
      "s max is:  26.52899932861328 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.9_tIns_0.028_Kd_0.05494_Tile_9\\\n",
      "s max is:  26.339000701904297 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.9_tIns_0.019_Kd_0.04028_Tile_9\\\n",
      "s max is:  27.04400062561035 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_10.0_tIns_0.027_Kd_0.01885_Tile_9\\\n",
      "s max is:  80.33899688720703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_6.7_tIns_0.022_Kd_0.03989_Tile_9\\\n",
      "s max is:  68.86000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_7.9_tIns_0.026_Kd_0.06588_Tile_9\\\n",
      "s max is:  26.981000900268555 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.4_tIns_0.04_Kd_0.00747_Tile_9\\\n",
      "s max is:  62.10599899291992 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_6.7_tIns_0.018_Kd_0.01592_Tile_9\\\n",
      "s max is:  46.91999816894531 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_1.7_tIns_0.02_Kd_0.07356_Tile_9\\\n",
      "s max is:  206.50999450683594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-07_xy_16000_z_100_tDiel_6.2_tIns_0.028_Kd_0.10265_Tile_9\\\n",
      "s max is:  39.30500030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.5_tIns_0.024_Kd_0.10177_Tile_9\\\n",
      "s max is:  26.46500015258789 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.2_tIns_0.033_Kd_0.07619_Tile_9\\\n",
      "s max is:  44.520999908447266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_4.9_tIns_0.026_Kd_0.04302_Tile_9\\\n",
      "s max is:  26.249000549316406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.1_tIns_0.021_Kd_0.01492_Tile_9\\\n",
      "s max is:  40.141998291015625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.2_tIns_0.029_Kd_0.0138_Tile_9\\\n",
      "s max is:  60.33000183105469 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_5.3_tIns_0.036_Kd_0.10419_Tile_9\\\n",
      "s max is:  47.49300003051758 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_5.5_tIns_0.048_Kd_0.05891_Tile_9\\\n",
      "s max is:  115.20999908447266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-07_xy_16000_z_100_tDiel_8.8_tIns_0.015_Kd_0.0875_Tile_9\\\n",
      "s max is:  26.225000381469727 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.9_tIns_0.027_Kd_0.10601_Tile_9\\\n",
      "s max is:  80.18900299072266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_8.2_tIns_0.051_Kd_0.06155_Tile_9\\\n",
      "s max is:  44.70100021362305 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_4.8_tIns_0.028_Kd_0.02138_Tile_9\\\n",
      "s max is:  54.61600112915039 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_5.6_tIns_0.029_Kd_0.12271_Tile_9\\\n",
      "s max is:  39.525001525878906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.0_tIns_0.05_Kd_0.09346_Tile_9\\\n",
      "s max is:  39.68000030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.3_tIns_0.027_Kd_0.06975_Tile_9\\\n",
      "s max is:  40.08599853515625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.3_tIns_0.013_Kd_0.03258_Tile_9\\\n",
      "s max is:  47.106998443603516 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_6.4_tIns_0.037_Kd_0.09882_Tile_9\\\n",
      "s max is:  101.33999633789062 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_9.7_tIns_0.026_Kd_0.05341_Tile_9\\\n",
      "s max is:  26.176000595092773 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.9_tIns_0.043_Kd_0.12177_Tile_9\\\n",
      "s max is:  41.33300018310547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_1.2_tIns_0.045_Kd_0.10379_Tile_9\\\n",
      "s max is:  26.461999893188477 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.8_tIns_0.028_Kd_0.06625_Tile_9\\\n",
      "s max is:  39.06700134277344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.4_tIns_0.022_Kd_0.12799_Tile_9\\\n",
      "s max is:  26.097999572753906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.8_tIns_0.05_Kd_0.13766_Tile_9\\\n",
      "s max is:  98.13200378417969 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_2.2_tIns_0.016_Kd_0.05775_Tile_9\\\n",
      "s max is:  39.729000091552734 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.2_tIns_0.048_Kd_0.07906_Tile_9\\\n",
      "s max is:  67.58899688720703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_1.1_tIns_0.037_Kd_0.0349_Tile_9\\\n",
      "s max is:  43.86600112915039 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_4.7_tIns_0.032_Kd_0.1199_Tile_9\\\n",
      "s max is:  60.31399917602539 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_3.3_tIns_0.045_Kd_0.09283_Tile_9\\\n",
      "s max is:  54.70100021362305 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_4.3_tIns_0.021_Kd_0.11215_Tile_9\\\n",
      "s max is:  26.79199981689453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.9_tIns_0.043_Kd_0.04443_Tile_9\\\n",
      "s max is:  80.18399810791016 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_7.2_tIns_0.034_Kd_0.05168_Tile_9\\\n",
      "s max is:  39.707000732421875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.2_tIns_0.039_Kd_0.06728_Tile_9\\\n",
      "s max is:  50.4370002746582 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_3.2_tIns_0.025_Kd_0.10135_Tile_9\\\n",
      "s max is:  39.03799819946289 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.9_tIns_0.039_Kd_0.13079_Tile_9\\\n",
      "s max is:  39.99700164794922 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.6_tIns_0.044_Kd_0.05492_Tile_9\\\n",
      "s max is:  61.5359992980957 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_9.4_tIns_0.023_Kd_0.06854_Tile_9\\\n",
      "s max is:  43.96900177001953 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_1.4_tIns_0.033_Kd_0.06053_Tile_9\\\n",
      "s max is:  46.731998443603516 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_3.1_tIns_0.05_Kd_0.13448_Tile_9\\\n",
      "s max is:  109.37999725341797 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-07_xy_16000_z_100_tDiel_3.3_tIns_0.049_Kd_0.0628_Tile_9\\\n",
      "s max is:  39.625999450683594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.3_tIns_0.026_Kd_0.01461_Tile_9\\\n",
      "s max is:  27.19700050354004 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.1_tIns_0.02_Kd_0.0021_Tile_9\\\n",
      "s max is:  39.2239990234375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.6_tIns_0.024_Kd_0.08555_Tile_9\\\n",
      "s max is:  50.86199951171875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_5.3_tIns_0.011_Kd_0.07299_Tile_9\\\n",
      "s max is:  26.825000762939453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.9_tIns_0.022_Kd_0.02126_Tile_9\\\n",
      "s max is:  26.523000717163086 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.8_tIns_0.044_Kd_0.01306_Tile_9\\\n",
      "s max is:  398.489990234375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-08_xy_16000_z_100_tDiel_8.2_tIns_0.03_Kd_0.05664_Tile_9\\\n",
      "s max is:  39.19300079345703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.9_tIns_0.021_Kd_0.11718_Tile_9\\\n",
      "s max is:  47.1879997253418 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_6.2_tIns_0.031_Kd_0.09066_Tile_9\\\n",
      "s max is:  26.601999282836914 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.3_tIns_0.023_Kd_0.03449_Tile_9\\\n",
      "s max is:  81.84700012207031 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_7.2_tIns_0.019_Kd_0.00149_Tile_9\\\n",
      "s max is:  47.189998626708984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_4.9_tIns_0.024_Kd_0.08017_Tile_9\\\n",
      "s max is:  26.243999481201172 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.9_tIns_0.013_Kd_0.09192_Tile_9\\\n",
      "s max is:  288.29998779296875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-08_xy_16000_z_100_tDiel_6.8_tIns_0.049_Kd_0.13123_Tile_9\\\n",
      "s max is:  50.44499969482422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_4.6_tIns_0.043_Kd_0.1102_Tile_9\\\n",
      "s max is:  41.507999420166016 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_2.1_tIns_0.01_Kd_0.05221_Tile_9\\\n",
      "s max is:  41.56999969482422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_1.8_tIns_0.043_Kd_0.02776_Tile_9\\\n",
      "s max is:  39.500999450683594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.0_tIns_0.038_Kd_0.09458_Tile_9\\\n",
      "s max is:  40.50299835205078 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.3_tIns_0.05_Kd_0.02634_Tile_9\\\n",
      "s max is:  40.21500015258789 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.3_tIns_0.04_Kd_0.03531_Tile_9\\\n",
      "s max is:  43.8489990234375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_1.1_tIns_0.038_Kd_0.1082_Tile_9\\\n",
      "s max is:  26.320999145507812 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.9_tIns_0.044_Kd_0.09513_Tile_9\\\n",
      "s max is:  26.356000900268555 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.4_tIns_0.046_Kd_0.05101_Tile_9\\\n",
      "s max is:  40.28799819946289 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.8_tIns_0.025_Kd_0.04357_Tile_9\\\n",
      "s max is:  26.79400062561035 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.6_tIns_0.038_Kd_0.00667_Tile_9\\\n",
      "s max is:  55.678001403808594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_4.0_tIns_0.015_Kd_0.01164_Tile_9\\\n",
      "s max is:  56.00899887084961 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_9.3_tIns_0.042_Kd_0.06404_Tile_9\\\n",
      "s max is:  39.220001220703125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.2_tIns_0.014_Kd_0.03672_Tile_9\\\n",
      "s max is:  140.27999877929688 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-07_xy_16000_z_100_tDiel_8.1_tIns_0.03_Kd_0.09923_Tile_9\\\n",
      "s max is:  55.30699920654297 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_9.6_tIns_0.024_Kd_0.0947_Tile_9\\\n",
      "s max is:  26.143999099731445 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_10.0_tIns_0.034_Kd_0.1278_Tile_9\\\n",
      "s max is:  113.62999725341797 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-07_xy_16000_z_100_tDiel_9.8_tIns_0.035_Kd_0.11752_Tile_9\\\n",
      "s max is:  47.28200149536133 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_9.4_tIns_0.015_Kd_0.09727_Tile_9\\\n",
      "s max is:  98.97100067138672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_3.3_tIns_0.029_Kd_0.03552_Tile_9\\\n",
      "s max is:  68.99199676513672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_8.7_tIns_0.024_Kd_0.06708_Tile_9\\\n",
      "s max is:  39.94499969482422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.7_tIns_0.036_Kd_0.0661_Tile_9\\\n",
      "s max is:  60.393001556396484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_3.5_tIns_0.021_Kd_0.08387_Tile_9\\\n",
      "s max is:  39.65599822998047 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.1_tIns_0.027_Kd_0.08278_Tile_9\\\n",
      "s max is:  26.179000854492188 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.5_tIns_0.031_Kd_0.12225_Tile_9\\\n",
      "s max is:  26.677000045776367 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.9_tIns_0.026_Kd_0.02982_Tile_9\\\n",
      "s max is:  56.8489990234375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_8.5_tIns_0.046_Kd_0.02041_Tile_9\\\n",
      "s max is:  78.5989990234375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_2.5_tIns_0.026_Kd_0.06107_Tile_9\\\n",
      "s max is:  26.54400062561035 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.2_tIns_0.029_Kd_0.0553_Tile_9\\\n",
      "s max is:  39.64899826049805 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.3_tIns_0.04_Kd_0.0799_Tile_9\\\n",
      "s max is:  44.24800109863281 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_4.5_tIns_0.016_Kd_0.06866_Tile_9\\\n",
      "s max is:  39.03900146484375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.4_tIns_0.018_Kd_0.12953_Tile_9\\\n",
      "s max is:  81.68699645996094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_9.7_tIns_0.042_Kd_0.03565_Tile_9\\\n",
      "s max is:  60.91699981689453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_7.1_tIns_0.013_Kd_0.0793_Tile_9\\\n",
      "s max is:  39.926998138427734 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.2_tIns_0.039_Kd_0.0567_Tile_9\\\n",
      "s max is:  39.00600051879883 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.4_tIns_0.012_Kd_0.13166_Tile_9\\\n",
      "s max is:  50.70899963378906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_8.8_tIns_0.016_Kd_0.10305_Tile_9\\\n",
      "s max is:  56.53300094604492 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_6.7_tIns_0.021_Kd_0.00942_Tile_9\\\n",
      "s max is:  68.30400085449219 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_9.3_tIns_0.035_Kd_0.09345_Tile_9\\\n",
      "s max is:  26.420000076293945 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.8_tIns_0.035_Kd_0.01352_Tile_9\\\n",
      "s max is:  26.39299964904785 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.2_tIns_0.03_Kd_0.0873_Tile_9\\\n",
      "s max is:  39.36199951171875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.7_tIns_0.02_Kd_0.10029_Tile_9\\\n",
      "s max is:  61.854000091552734 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_5.4_tIns_0.037_Kd_0.00729_Tile_9\\\n",
      "s max is:  39.79899978637695 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.5_tIns_0.024_Kd_0.07442_Tile_9\\\n",
      "s max is:  26.23900032043457 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.1_tIns_0.035_Kd_0.11043_Tile_9\\\n",
      "s max is:  39.349998474121094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.5_tIns_0.037_Kd_0.00893_Tile_9\\\n",
      "s max is:  42.20000076293945 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_7.5_tIns_0.013_Kd_0.04903_Tile_9\\\n",
      "s max is:  40.50400161743164 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.8_tIns_0.044_Kd_0.03084_Tile_9\\\n",
      "s max is:  55.26100158691406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_9.4_tIns_0.012_Kd_0.09552_Tile_9\\\n",
      "s max is:  77.70899963378906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_6.4_tIns_0.018_Kd_0.13683_Tile_9\\\n",
      "s max is:  50.63600158691406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_9.5_tIns_0.017_Kd_0.10851_Tile_9\\\n",
      "s max is:  39.040000915527344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.3_tIns_0.027_Kd_0.13225_Tile_9\\\n",
      "s max is:  41.71799850463867 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_3.6_tIns_0.047_Kd_0.0516_Tile_9\\\n",
      "s max is:  39.54800033569336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.7_tIns_0.048_Kd_0.01336_Tile_9\\\n",
      "s max is:  40.45399856567383 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.8_tIns_0.021_Kd_0.00441_Tile_9\\\n",
      "s max is:  159.33999633789062 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-07_xy_16000_z_100_tDiel_3.4_tIns_0.044_Kd_0.08559_Tile_9\\\n",
      "s max is:  41.507999420166016 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_1.9_tIns_0.027_Kd_0.04866_Tile_9\\\n",
      "s max is:  26.21500015258789 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.4_tIns_0.01_Kd_0.10417_Tile_9\\\n",
      "s max is:  39.189998626708984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.0_tIns_0.03_Kd_0.04762_Tile_9\\\n",
      "s max is:  70.36000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_8.8_tIns_0.019_Kd_0.02314_Tile_9\\\n",
      "s max is:  77.89399719238281 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_4.9_tIns_0.02_Kd_0.12988_Tile_9\\\n",
      "s max is:  39.26300048828125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.5_tIns_0.037_Kd_0.07604_Tile_9\\\n",
      "s max is:  81.76100158691406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_8.0_tIns_0.021_Kd_0.01327_Tile_9\\\n",
      "s max is:  40.04800033569336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.8_tIns_0.035_Kd_0.05981_Tile_9\\\n",
      "s max is:  26.200000762939453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.4_tIns_0.044_Kd_0.11077_Tile_9\\\n",
      "s max is:  39.55699920654297 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.4_tIns_0.039_Kd_0.05247_Tile_9\\\n",
      "s max is:  26.28499984741211 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.1_tIns_0.015_Kd_0.00437_Tile_9\\\n",
      "s max is:  47.2400016784668 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_8.2_tIns_0.028_Kd_0.09618_Tile_9\\\n",
      "s max is:  50.78799819946289 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_5.7_tIns_0.047_Kd_0.08553_Tile_9\\\n",
      "s max is:  48.43000030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_9.3_tIns_0.051_Kd_0.03694_Tile_9\\\n",
      "s max is:  47.446998596191406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_5.4_tIns_0.046_Kd_0.06183_Tile_9\\\n",
      "s max is:  26.121999740600586 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.1_tIns_0.027_Kd_0.13297_Tile_9\\\n",
      "s max is:  55.17399978637695 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_2.6_tIns_0.027_Kd_0.02797_Tile_9\\\n",
      "s max is:  39.36000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.9_tIns_0.021_Kd_0.022_Tile_9\\\n",
      "s max is:  61.13399887084961 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_4.9_tIns_0.03_Kd_0.04329_Tile_9\\\n",
      "s max is:  39.11600112915039 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.1_tIns_0.011_Kd_0.11745_Tile_9\\\n",
      "s max is:  79.1780014038086 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_2.8_tIns_0.028_Kd_0.01763_Tile_9\\\n",
      "s max is:  41.39899826049805 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_7.1_tIns_0.022_Kd_0.11541_Tile_9\\\n",
      "s max is:  97.4010009765625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_6.0_tIns_0.049_Kd_0.12726_Tile_9\\\n",
      "s max is:  47.143001556396484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_7.1_tIns_0.021_Kd_0.098_Tile_9\\\n",
      "s max is:  39.034000396728516 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.3_tIns_0.041_Kd_0.13494_Tile_9\\\n",
      "s max is:  54.91400146484375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_2.4_tIns_0.012_Kd_0.06136_Tile_9\\\n",
      "s max is:  40.04199981689453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.3_tIns_0.022_Kd_0.02348_Tile_9\\\n",
      "s max is:  61.04899978637695 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_3.3_tIns_0.04_Kd_0.01343_Tile_9\\\n",
      "s max is:  98.49299621582031 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_3.5_tIns_0.048_Kd_0.06759_Tile_9\\\n",
      "s max is:  54.78099822998047 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_10.0_tIns_0.024_Kd_0.11781_Tile_9\\\n",
      "s max is:  39.84199905395508 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.3_tIns_0.027_Kd_0.06361_Tile_9\\\n",
      "s max is:  50.32899856567383 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_1.6_tIns_0.027_Kd_0.10812_Tile_9\\\n",
      "s max is:  26.663000106811523 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.3_tIns_0.03_Kd_0.04688_Tile_9\\\n",
      "s max is:  67.71700286865234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_2.2_tIns_0.047_Kd_0.06503_Tile_9\\\n",
      "s max is:  50.37099838256836 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_1.4_tIns_0.015_Kd_0.08739_Tile_9\\\n",
      "s max is:  39.124000549316406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.0_tIns_0.024_Kd_0.12282_Tile_9\\\n",
      "s max is:  50.30099868774414 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_6.3_tIns_0.042_Kd_0.12565_Tile_9\\\n",
      "s max is:  61.10300064086914 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_7.7_tIns_0.03_Kd_0.07538_Tile_9\\\n",
      "s max is:  80.63099670410156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_9.0_tIns_0.012_Kd_0.05449_Tile_9\\\n",
      "s max is:  26.222999572753906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_10.1_tIns_0.018_Kd_0.11624_Tile_9\\\n",
      "s max is:  39.35200119018555 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.0_tIns_0.044_Kd_0.0651_Tile_9\\\n",
      "s max is:  39.13600158691406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.4_tIns_0.014_Kd_0.09069_Tile_9\\\n",
      "s max is:  46.89500045776367 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_3.4_tIns_0.044_Kd_0.10755_Tile_9\\\n",
      "s max is:  26.145000457763672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.1_tIns_0.049_Kd_0.12897_Tile_9\\\n",
      "s max is:  39.05500030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.3_tIns_0.022_Kd_0.13469_Tile_9\\\n",
      "s max is:  26.308000564575195 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.5_tIns_0.032_Kd_0.04569_Tile_9\\\n",
      "s max is:  39.76599884033203 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.4_tIns_0.035_Kd_0.05343_Tile_9\\\n",
      "s max is:  26.43000030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.2_tIns_0.048_Kd_0.05854_Tile_9\\\n",
      "s max is:  142.5800018310547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-07_xy_16000_z_100_tDiel_9.7_tIns_0.034_Kd_0.07368_Tile_9\\\n",
      "s max is:  41.12200164794922 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.1_tIns_0.015_Kd_0.00155_Tile_9\\\n",
      "s max is:  39.7400016784668 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.3_tIns_0.026_Kd_0.05428_Tile_9\\\n",
      "s max is:  51.27799987792969 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_4.9_tIns_0.045_Kd_0.03292_Tile_9\\\n",
      "s max is:  50.39099884033203 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_2.6_tIns_0.044_Kd_0.10708_Tile_9\\\n",
      "s max is:  39.05400085449219 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.9_tIns_0.041_Kd_0.129_Tile_9\\\n",
      "s max is:  55.27199935913086 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_9.9_tIns_0.046_Kd_0.09806_Tile_9\\\n",
      "s max is:  80.37999725341797 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_7.6_tIns_0.024_Kd_0.04918_Tile_9\\\n",
      "s max is:  26.878999710083008 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.2_tIns_0.034_Kd_0.01909_Tile_9\\\n",
      "s max is:  26.718000411987305 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.4_tIns_0.038_Kd_0.04928_Tile_9\\\n",
      "s max is:  26.18600082397461 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.4_tIns_0.019_Kd_0.12012_Tile_9\\\n",
      "s max is:  67.09700012207031 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_5.8_tIns_0.039_Kd_0.13681_Tile_9\\\n",
      "s max is:  62.279998779296875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_8.5_tIns_0.025_Kd_0.03268_Tile_9\\\n",
      "s max is:  26.233999252319336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.6_tIns_0.013_Kd_0.08009_Tile_9\\\n",
      "s max is:  47.32600021362305 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_4.0_tIns_0.044_Kd_0.05434_Tile_9\\\n",
      "s max is:  69.35199737548828 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_7.7_tIns_0.019_Kd_0.04527_Tile_9\\\n",
      "s max is:  39.702999114990234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.0_tIns_0.033_Kd_0.04127_Tile_9\\\n",
      "s max is:  26.160999298095703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.0_tIns_0.04_Kd_0.12585_Tile_9\\\n",
      "s max is:  39.77899932861328 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.2_tIns_0.022_Kd_0.07406_Tile_9\\\n",
      "s max is:  26.33799934387207 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.7_tIns_0.032_Kd_0.09443_Tile_9\\\n",
      "s max is:  68.10299682617188 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_3.2_tIns_0.018_Kd_0.04453_Tile_9\\\n",
      "s max is:  39.124000549316406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.2_tIns_0.032_Kd_0.12046_Tile_9\\\n",
      "s max is:  26.367000579833984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.8_tIns_0.012_Kd_0.06476_Tile_9\\\n",
      "s max is:  54.77299880981445 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_4.9_tIns_0.022_Kd_0.10795_Tile_9\\\n",
      "s max is:  39.099998474121094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.2_tIns_0.017_Kd_0.11587_Tile_9\\\n",
      "s max is:  40.643001556396484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.5_tIns_0.041_Kd_0.00994_Tile_9\\\n",
      "s max is:  26.777999877929688 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.3_tIns_0.017_Kd_0.03042_Tile_9\\\n",
      "s max is:  104.30999755859375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-07_xy_16000_z_100_tDiel_2.2_tIns_0.041_Kd_0.10108_Tile_9\\\n",
      "s max is:  39.8489990234375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.9_tIns_0.029_Kd_0.03765_Tile_9\\\n",
      "s max is:  27.288000106811523 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.3_tIns_0.014_Kd_0.0037_Tile_9\\\n",
      "s max is:  39.58399963378906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.4_tIns_0.01_Kd_0.02239_Tile_9\\\n",
      "s max is:  26.174999237060547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.3_tIns_0.028_Kd_0.1032_Tile_9\\\n",
      "s max is:  26.139999389648438 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.4_tIns_0.015_Kd_0.1271_Tile_9\\\n",
      "s max is:  39.111000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.8_tIns_0.034_Kd_0.11867_Tile_9\\\n",
      "s max is:  50.21900177001953 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_4.9_tIns_0.048_Kd_0.13368_Tile_9\\\n",
      "s max is:  26.22800064086914 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.0_tIns_0.032_Kd_0.10979_Tile_9\\\n",
      "s max is:  68.19100189208984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_2.6_tIns_0.043_Kd_0.0189_Tile_9\\\n",
      "s max is:  46.69499969482422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_9.4_tIns_0.016_Kd_0.13015_Tile_9\\\n",
      "s max is:  68.10900115966797 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_8.8_tIns_0.012_Kd_0.09726_Tile_9\\\n",
      "s max is:  26.280000686645508 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.5_tIns_0.047_Kd_0.09106_Tile_9\\\n",
      "s max is:  50.62099838256836 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_8.5_tIns_0.028_Kd_0.10763_Tile_9\\\n",
      "s max is:  26.70800018310547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.7_tIns_0.04_Kd_0.04492_Tile_9\\\n",
      "s max is:  41.97100067138672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_4.5_tIns_0.028_Kd_0.03038_Tile_9\\\n",
      "s max is:  313.2300109863281 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-08_xy_16000_z_100_tDiel_3.9_tIns_0.015_Kd_0.06466_Tile_9\\\n",
      "s max is:  111.23999786376953 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-07_xy_16000_z_100_tDiel_6.5_tIns_0.011_Kd_0.04765_Tile_9\\\n",
      "s max is:  26.128000259399414 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.0_tIns_0.022_Kd_0.1304_Tile_9\\\n",
      "s max is:  26.273000717163086 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.3_tIns_0.044_Kd_0.06154_Tile_9\\\n",
      "s max is:  50.50899887084961 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_4.9_tIns_0.049_Kd_0.10557_Tile_9\\\n",
      "s max is:  40.69499969482422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.6_tIns_0.019_Kd_0.00799_Tile_9\\\n",
      "s max is:  40.38999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.9_tIns_0.039_Kd_0.00264_Tile_9\\\n",
      "s max is:  40.42300033569336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.7_tIns_0.019_Kd_0.02374_Tile_9\\\n",
      "s max is:  39.50299835205078 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.6_tIns_0.039_Kd_0.08273_Tile_9\\\n",
      "s max is:  55.09299850463867 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_6.2_tIns_0.048_Kd_0.09235_Tile_9\\\n",
      "s max is:  128.00999450683594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-07_xy_16000_z_100_tDiel_2.3_tIns_0.031_Kd_0.06294_Tile_9\\\n",
      "s max is:  26.236000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.1_tIns_0.023_Kd_0.11357_Tile_9\\\n",
      "s max is:  55.52899932861328 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_5.4_tIns_0.035_Kd_0.05324_Tile_9\\\n",
      "s max is:  44.316001892089844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_2.3_tIns_0.036_Kd_0.00666_Tile_9\\\n",
      "s max is:  39.36800003051758 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.2_tIns_0.019_Kd_0.08658_Tile_9\\\n",
      "s max is:  26.24799919128418 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.6_tIns_0.038_Kd_0.05174_Tile_9\\\n",
      "s max is:  63.06100082397461 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_8.6_tIns_0.033_Kd_0.00621_Tile_9\\\n",
      "s max is:  39.65299987792969 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.9_tIns_0.019_Kd_0.07582_Tile_9\\\n",
      "s max is:  26.167999267578125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.9_tIns_0.01_Kd_0.11453_Tile_9\\\n",
      "s max is:  26.202999114990234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.1_tIns_0.017_Kd_0.11695_Tile_9\\\n",
      "s max is:  26.833999633789062 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.2_tIns_0.019_Kd_0.03219_Tile_9\\\n",
      "s max is:  67.96499633789062 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_5.0_tIns_0.049_Kd_0.08521_Tile_9\\\n",
      "s max is:  44.37099838256836 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_3.4_tIns_0.011_Kd_0.02915_Tile_9\\\n",
      "s max is:  40.178001403808594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.1_tIns_0.024_Kd_0.03509_Tile_9\\\n",
      "s max is:  39.808998107910156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.6_tIns_0.017_Kd_0.06776_Tile_9\\\n",
      "s max is:  39.2859992980957 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.0_tIns_0.019_Kd_0.07587_Tile_9\\\n",
      "s max is:  54.6510009765625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_9.2_tIns_0.045_Kd_0.1238_Tile_9\\\n",
      "s max is:  26.538000106811523 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.5_tIns_0.036_Kd_0.03702_Tile_9\\\n",
      "s max is:  39.619998931884766 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.5_tIns_0.045_Kd_0.04542_Tile_9\\\n",
      "s max is:  54.69599914550781 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_9.8_tIns_0.029_Kd_0.12144_Tile_9\\\n",
      "s max is:  39.67599868774414 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.7_tIns_0.048_Kd_0.07441_Tile_9\\\n",
      "s max is:  67.3499984741211 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_5.5_tIns_0.019_Kd_0.1216_Tile_9\\\n",
      "s max is:  40.652000427246094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.6_tIns_0.011_Kd_0.00184_Tile_9\\\n",
      "s max is:  48.053001403808594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_8.5_tIns_0.039_Kd_0.0494_Tile_9\\\n",
      "s max is:  39.72800064086914 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.3_tIns_0.039_Kd_0.0792_Tile_9\\\n",
      "s max is:  47.779998779296875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_4.7_tIns_0.012_Kd_0.01465_Tile_9\\\n",
      "s max is:  26.652000427246094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.0_tIns_0.023_Kd_0.02225_Tile_9\\\n",
      "s max is:  51.720001220703125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_8.2_tIns_0.038_Kd_0.04628_Tile_9\\\n",
      "s max is:  50.4109992980957 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_1.5_tIns_0.033_Kd_0.08265_Tile_9\\\n",
      "s max is:  39.0629997253418 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.6_tIns_0.015_Kd_0.12701_Tile_9\\\n",
      "s max is:  26.547000885009766 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.8_tIns_0.018_Kd_0.02225_Tile_9\\\n",
      "s max is:  55.34199905395508 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_2.5_tIns_0.044_Kd_0.00425_Tile_9\\\n",
      "s max is:  26.30699920654297 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.1_tIns_0.044_Kd_0.0038_Tile_9\\\n",
      "s max is:  121.63999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-07_xy_16000_z_100_tDiel_8.7_tIns_0.023_Kd_0.08316_Tile_9\\\n",
      "s max is:  48.527000427246094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_7.9_tIns_0.032_Kd_0.01524_Tile_9\\\n",
      "s max is:  51.840999603271484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_6.6_tIns_0.023_Kd_0.01837_Tile_9\\\n",
      "s max is:  39.231998443603516 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.7_tIns_0.017_Kd_0.0838_Tile_9\\\n",
      "s max is:  26.35700035095215 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.4_tIns_0.032_Kd_0.07633_Tile_9\\\n",
      "s max is:  43.81800079345703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_5.7_tIns_0.01_Kd_0.12424_Tile_9\\\n",
      "s max is:  52.36399841308594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_9.8_tIns_0.035_Kd_0.03094_Tile_9\\\n",
      "s max is:  39.3849983215332 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.8_tIns_0.043_Kd_0.10314_Tile_9\\\n",
      "s max is:  40.138999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.0_tIns_0.042_Kd_0.04776_Tile_9\\\n",
      "s max is:  60.99700164794922 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_9.2_tIns_0.026_Kd_0.08784_Tile_9\\\n",
      "s max is:  26.093000411987305 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.2_tIns_0.044_Kd_0.13629_Tile_9\\\n",
      "s max is:  40.112998962402344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.1_tIns_0.049_Kd_0.01599_Tile_9\\\n",
      "s max is:  39.090999603271484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.3_tIns_0.043_Kd_0.12598_Tile_9\\\n",
      "s max is:  39.22100067138672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.6_tIns_0.048_Kd_0.06954_Tile_9\\\n",
      "s max is:  26.398000717163086 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.2_tIns_0.035_Kd_0.00711_Tile_9\\\n",
      "s max is:  39.32699966430664 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.3_tIns_0.046_Kd_0.09534_Tile_9\\\n",
      "s max is:  39.75299835205078 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.4_tIns_0.048_Kd_0.07835_Tile_9\\\n",
      "s max is:  39.20199966430664 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.2_tIns_0.022_Kd_0.04995_Tile_9\\\n",
      "s max is:  26.336999893188477 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.7_tIns_0.033_Kd_0.08986_Tile_9\\\n",
      "s max is:  41.512001037597656 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_3.5_tIns_0.02_Kd_0.0825_Tile_9\\\n",
      "s max is:  78.33399963378906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_5.0_tIns_0.038_Kd_0.10877_Tile_9\\\n",
      "s max is:  39.24599838256836 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.6_tIns_0.04_Kd_0.05595_Tile_9\\\n",
      "s max is:  42.13999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_6.6_tIns_0.033_Kd_0.04545_Tile_9\\\n",
      "s max is:  67.43199920654297 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_1.0_tIns_0.026_Kd_0.07001_Tile_9\\\n",
      "s max is:  39.12699890136719 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.3_tIns_0.025_Kd_0.0969_Tile_9\\\n",
      "s max is:  26.20400047302246 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.5_tIns_0.01_Kd_0.07233_Tile_9\\\n",
      "s max is:  44.80699920654297 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_10.1_tIns_0.031_Kd_0.07061_Tile_9\\\n",
      "s max is:  59.928001403808594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_5.6_tIns_0.021_Kd_0.13074_Tile_9\\\n",
      "s max is:  44.02299880981445 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_3.4_tIns_0.03_Kd_0.0899_Tile_9\\\n",
      "s max is:  26.290000915527344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.7_tIns_0.018_Kd_0.1066_Tile_9\\\n",
      "s max is:  39.14899826049805 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.3_tIns_0.01_Kd_0.1081_Tile_9\\\n",
      "s max is:  55.29999923706055 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_3.6_tIns_0.035_Kd_0.04206_Tile_9\\\n",
      "s max is:  26.42300033569336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.7_tIns_0.042_Kd_0.09023_Tile_9\\\n",
      "s max is:  26.56999969482422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.8_tIns_0.02_Kd_0.03327_Tile_9\\\n",
      "s max is:  40.393001556396484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.4_tIns_0.023_Kd_0.01134_Tile_9\\\n",
      "s max is:  50.61899948120117 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_10.1_tIns_0.044_Kd_0.11151_Tile_9\\\n",
      "s max is:  39.367000579833984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.9_tIns_0.026_Kd_0.05637_Tile_9\\\n",
      "s max is:  68.94400024414062 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_4.1_tIns_0.015_Kd_0.00393_Tile_9\\\n",
      "s max is:  39.13100051879883 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.4_tIns_0.011_Kd_0.09194_Tile_9\\\n",
      "s max is:  39.310001373291016 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.5_tIns_0.012_Kd_0.10385_Tile_9\\\n",
      "s max is:  55.4010009765625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_9.3_tIns_0.02_Kd_0.08937_Tile_9\\\n",
      "s max is:  98.69200134277344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_4.1_tIns_0.03_Kd_0.06628_Tile_9\\\n",
      "s max is:  50.32400131225586 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_8.9_tIns_0.044_Kd_0.12504_Tile_9\\\n",
      "s max is:  113.69999694824219 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-07_xy_16000_z_100_tDiel_7.2_tIns_0.018_Kd_0.11114_Tile_9\\\n",
      "s max is:  26.493999481201172 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.5_tIns_0.03_Kd_0.0664_Tile_9\\\n",
      "s max is:  60.13600158691406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_7.3_tIns_0.014_Kd_0.11924_Tile_9\\\n",
      "s max is:  206.9199981689453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-07_xy_16000_z_100_tDiel_3.9_tIns_0.021_Kd_0.06342_Tile_9\\\n",
      "s max is:  40.03300094604492 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.9_tIns_0.027_Kd_0.01854_Tile_9\\\n",
      "s max is:  106.45999908447266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-07_xy_16000_z_100_tDiel_9.1_tIns_0.046_Kd_0.08186_Tile_9\\\n",
      "s max is:  39.41400146484375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.0_tIns_0.028_Kd_0.04844_Tile_9\\\n",
      "s max is:  39.415000915527344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.0_tIns_0.024_Kd_0.04755_Tile_9\\\n",
      "s max is:  39.935001373291016 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.5_tIns_0.023_Kd_0.0367_Tile_9\\\n",
      "s max is:  60.332000732421875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_3.0_tIns_0.04_Kd_0.08648_Tile_9\\\n",
      "s max is:  47.00199890136719 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_4.1_tIns_0.033_Kd_0.09587_Tile_9\\\n",
      "s max is:  47.26499938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_8.1_tIns_0.045_Kd_0.09509_Tile_9\\\n",
      "s max is:  67.16799926757812 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_3.3_tIns_0.027_Kd_0.13538_Tile_9\\\n",
      "s max is:  26.492000579833984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.7_tIns_0.028_Kd_0.07473_Tile_9\\\n",
      "s max is:  39.571998596191406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.4_tIns_0.033_Kd_0.07349_Tile_9\\\n",
      "s max is:  26.617000579833984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.7_tIns_0.038_Kd_0.02502_Tile_9\\\n",
      "s max is:  55.0620002746582 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_8.7_tIns_0.031_Kd_0.10318_Tile_9\\\n",
      "s max is:  39.402000427246094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.9_tIns_0.03_Kd_0.09396_Tile_9\\\n",
      "s max is:  61.11600112915039 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_3.1_tIns_0.051_Kd_0.00353_Tile_9\\\n",
      "s max is:  45.582000732421875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_7.5_tIns_0.033_Kd_0.00457_Tile_9\\\n",
      "s max is:  67.62300109863281 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_5.1_tIns_0.039_Kd_0.10591_Tile_9\\\n",
      "s max is:  39.590999603271484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.5_tIns_0.04_Kd_0.08008_Tile_9\\\n",
      "s max is:  26.374000549316406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.9_tIns_0.049_Kd_0.07884_Tile_9\\\n",
      "s max is:  39.27299880981445 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.2_tIns_0.037_Kd_0.10819_Tile_9\\\n",
      "s max is:  39.058998107910156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.0_tIns_0.037_Kd_0.13065_Tile_9\\\n",
      "s max is:  70.93000030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_9.1_tIns_0.019_Kd_0.00983_Tile_9\\\n",
      "s max is:  47.81399917602539 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_5.9_tIns_0.023_Kd_0.03437_Tile_9\\\n",
      "s max is:  39.28300094604492 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.0_tIns_0.028_Kd_0.00316_Tile_9\\\n",
      "s max is:  39.834999084472656 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.0_tIns_0.036_Kd_0.06237_Tile_9\\\n",
      "s max is:  39.34199905395508 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.6_tIns_0.028_Kd_0.08627_Tile_9\\\n",
      "s max is:  80.09700012207031 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_5.9_tIns_0.039_Kd_0.03859_Tile_9\\\n",
      "s max is:  129.39999389648438 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-07_xy_16000_z_100_tDiel_5.5_tIns_0.05_Kd_0.06671_Tile_9\\\n",
      "s max is:  61.5629997253418 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_4.8_tIns_0.017_Kd_0.01053_Tile_9\\\n",
      "s max is:  80.052001953125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_6.2_tIns_0.018_Kd_0.04357_Tile_9\\\n",
      "s max is:  40.220001220703125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_10.0_tIns_0.022_Kd_0.04941_Tile_9\\\n",
      "s max is:  79.4000015258789 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_4.5_tIns_0.032_Kd_0.04751_Tile_9\\\n",
      "s max is:  39.21699905395508 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.4_tIns_0.046_Kd_0.06267_Tile_9\\\n",
      "s max is:  50.895999908447266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_2.3_tIns_0.049_Kd_0.00668_Tile_9\\\n",
      "s max is:  26.21299934387207 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.4_tIns_0.018_Kd_0.08916_Tile_9\\\n",
      "s max is:  99.85399627685547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_6.8_tIns_0.041_Kd_0.06044_Tile_9\\\n",
      "s max is:  42.42100143432617 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_7.8_tIns_0.022_Kd_0.03586_Tile_9\\\n",
      "s max is:  39.926998138427734 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.6_tIns_0.015_Kd_0.04964_Tile_9\\\n",
      "s max is:  26.565000534057617 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.9_tIns_0.021_Kd_0.0357_Tile_9\\\n",
      "s max is:  77.70500183105469 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_7.1_tIns_0.018_Kd_0.13612_Tile_9\\\n",
      "s max is:  61.308998107910156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_7.5_tIns_0.02_Kd_0.06387_Tile_9\\\n",
      "s max is:  26.357999801635742 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.8_tIns_0.048_Kd_0.0879_Tile_9\\\n",
      "s max is:  39.486000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.8_tIns_0.033_Kd_0.07809_Tile_9\\\n",
      "s max is:  39.61000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.4_tIns_0.02_Kd_0.06848_Tile_9\\\n",
      "s max is:  26.45599937438965 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.6_tIns_0.044_Kd_0.00179_Tile_9\\\n",
      "s max is:  39.43899917602539 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.3_tIns_0.03_Kd_0.08679_Tile_9\\\n",
      "s max is:  39.32099914550781 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.8_tIns_0.042_Kd_0.10816_Tile_9\\\n",
      "s max is:  26.84600067138672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.9_tIns_0.01_Kd_0.01835_Tile_9\\\n",
      "s max is:  39.803001403808594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.4_tIns_0.029_Kd_0.04909_Tile_9\\\n",
      "s max is:  47.55400085449219 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_3.4_tIns_0.019_Kd_0.00791_Tile_9\\\n",
      "s max is:  110.20999908447266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-07_xy_16000_z_100_tDiel_10.0_tIns_0.041_Kd_0.02606_Tile_9\\\n",
      "s max is:  40.222999572753906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.1_tIns_0.031_Kd_0.01979_Tile_9\\\n",
      "s max is:  55.48899841308594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_4.4_tIns_0.029_Kd_0.03951_Tile_9\\\n",
      "s max is:  78.72699737548828 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_2.5_tIns_0.014_Kd_0.04675_Tile_9\\\n",
      "s max is:  127.44000244140625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-07_xy_16000_z_100_tDiel_4.0_tIns_0.036_Kd_0.11172_Tile_9\\\n",
      "s max is:  42.319000244140625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_5.9_tIns_0.047_Kd_0.01997_Tile_9\\\n",
      "s max is:  39.3650016784668 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.3_tIns_0.027_Kd_0.03931_Tile_9\\\n",
      "s max is:  158.9600067138672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-07_xy_16000_z_100_tDiel_1.2_tIns_0.05_Kd_0.04283_Tile_9\\\n",
      "s max is:  26.291000366210938 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.5_tIns_0.04_Kd_0.09386_Tile_9\\\n",
      "s max is:  39.77299880981445 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.0_tIns_0.012_Kd_0.04591_Tile_9\\\n",
      "s max is:  41.2239990234375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_4.0_tIns_0.029_Kd_0.13546_Tile_9\\\n",
      "s max is:  69.197998046875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_4.8_tIns_0.045_Kd_0.00773_Tile_9\\\n",
      "s max is:  41.24100112915039 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_4.3_tIns_0.013_Kd_0.13041_Tile_9\\\n",
      "s max is:  26.777000427246094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.4_tIns_0.013_Kd_0.03121_Tile_9\\\n",
      "s max is:  39.25199890136719 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.2_tIns_0.048_Kd_0.07528_Tile_9\\\n",
      "s max is:  26.215999603271484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.5_tIns_0.041_Kd_0.07582_Tile_9\\\n",
      "s max is:  44.327999114990234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_3.5_tIns_0.034_Kd_0.0417_Tile_9\\\n",
      "s max is:  39.73699951171875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.3_tIns_0.013_Kd_0.07717_Tile_9\\\n",
      "s max is:  61.16299819946289 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_4.0_tIns_0.044_Kd_0.02342_Tile_9\\\n",
      "s max is:  39.742000579833984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.5_tIns_0.029_Kd_0.07299_Tile_9\\\n",
      "s max is:  26.243000030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.5_tIns_0.038_Kd_0.10058_Tile_9\\\n",
      "s max is:  43.70100021362305 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_7.0_tIns_0.042_Kd_0.13681_Tile_9\\\n",
      "s max is:  69.802001953125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_8.1_tIns_0.029_Kd_0.03385_Tile_9\\\n",
      "s max is:  48.268001556396484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_7.7_tIns_0.045_Kd_0.02833_Tile_9\\\n",
      "s max is:  39.05400085449219 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.0_tIns_0.013_Kd_0.12977_Tile_9\\\n",
      "s max is:  26.18000030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.0_tIns_0.05_Kd_0.10492_Tile_9\\\n",
      "s max is:  38.99399948120117 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.2_tIns_0.015_Kd_0.13233_Tile_9\\\n",
      "s max is:  44.064998626708984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_3.5_tIns_0.015_Kd_0.08192_Tile_9\\\n",
      "s max is:  39.231998443603516 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.5_tIns_0.018_Kd_0.11151_Tile_9\\\n",
      "s max is:  39.108001708984375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.6_tIns_0.048_Kd_0.12392_Tile_9\\\n",
      "s max is:  26.822999954223633 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.8_tIns_0.044_Kd_0.04015_Tile_9\\\n",
      "s max is:  39.55400085449219 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.0_tIns_0.049_Kd_0.02183_Tile_9\\\n",
      "s max is:  39.779998779296875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.4_tIns_0.031_Kd_0.05162_Tile_9\\\n",
      "s max is:  26.2810001373291 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.3_tIns_0.048_Kd_0.07826_Tile_9\\\n",
      "s max is:  50.35100173950195 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_1.4_tIns_0.038_Kd_0.10153_Tile_9\\\n",
      "s max is:  78.84200286865234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_6.8_tIns_0.012_Kd_0.09403_Tile_9\\\n",
      "s max is:  26.165000915527344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.9_tIns_0.037_Kd_0.12247_Tile_9\\\n",
      "s max is:  39.26900100708008 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.7_tIns_0.033_Kd_0.04868_Tile_9\\\n",
      "s max is:  39.82699966430664 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.4_tIns_0.049_Kd_0.0668_Tile_9\\\n",
      "s max is:  39.178001403808594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.0_tIns_0.038_Kd_0.05987_Tile_9\\\n",
      "s max is:  50.17900085449219 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_9.1_tIns_0.017_Kd_0.13146_Tile_9\\\n",
      "s max is:  39.41299819946289 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.3_tIns_0.048_Kd_0.07455_Tile_9\\\n",
      "s max is:  113.02999877929688 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-07_xy_16000_z_100_tDiel_7.6_tIns_0.025_Kd_0.1264_Tile_9\\\n",
      "s max is:  40.665000915527344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.8_tIns_0.024_Kd_0.01094_Tile_9\\\n",
      "s max is:  39.97700119018555 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.3_tIns_0.044_Kd_0.04409_Tile_9\\\n",
      "s max is:  40.10300064086914 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.9_tIns_0.034_Kd_0.0394_Tile_9\\\n",
      "s max is:  67.10099792480469 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_7.5_tIns_0.048_Kd_0.13508_Tile_9\\\n",
      "s max is:  43.88399887084961 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_1.8_tIns_0.028_Kd_0.10123_Tile_9\\\n",
      "s max is:  39.066001892089844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.4_tIns_0.048_Kd_0.13147_Tile_9\\\n",
      "s max is:  39.12699890136719 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.4_tIns_0.01_Kd_0.11828_Tile_9\\\n",
      "s max is:  99.44499969482422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_8.1_tIns_0.039_Kd_0.08043_Tile_9\\\n",
      "s max is:  48.040000915527344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_7.1_tIns_0.019_Kd_0.03396_Tile_9\\\n",
      "s max is:  26.325000762939453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.0_tIns_0.049_Kd_0.05523_Tile_9\\\n",
      "s max is:  26.364999771118164 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.4_tIns_0.034_Kd_0.04542_Tile_9\\\n",
      "s max is:  50.595001220703125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_1.5_tIns_0.026_Kd_0.02354_Tile_9\\\n",
      "s max is:  50.875999450683594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_5.5_tIns_0.028_Kd_0.07509_Tile_9\\\n",
      "s max is:  60.404998779296875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_2.7_tIns_0.027_Kd_0.07026_Tile_9\\\n",
      "s max is:  26.7189998626709 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.7_tIns_0.018_Kd_0.05016_Tile_9\\\n",
      "s max is:  26.691999435424805 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.9_tIns_0.035_Kd_0.01639_Tile_9\\\n",
      "s max is:  50.82699966430664 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_5.6_tIns_0.043_Kd_0.08125_Tile_9\\\n",
      "s max is:  43.78499984741211 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_3.8_tIns_0.034_Kd_0.13088_Tile_9\\\n",
      "s max is:  43.83599853515625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_2.6_tIns_0.024_Kd_0.11922_Tile_9\\\n",
      "s max is:  104.73999786376953 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-07_xy_16000_z_100_tDiel_7.0_tIns_0.021_Kd_0.02078_Tile_9\\\n",
      "s max is:  63.63999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_9.8_tIns_0.03_Kd_0.00388_Tile_9\\\n",
      "s max is:  54.9640007019043 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_9.7_tIns_0.049_Kd_0.11067_Tile_9\\\n",
      "s max is:  44.00899887084961 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_1.7_tIns_0.015_Kd_0.05337_Tile_9\\\n",
      "s max is:  50.236000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_1.0_tIns_0.019_Kd_0.13756_Tile_9\\\n",
      "s max is:  26.53700065612793 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.4_tIns_0.034_Kd_0.05881_Tile_9\\\n",
      "s max is:  103.62000274658203 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-07_xy_16000_z_100_tDiel_7.9_tIns_0.018_Kd_0.05671_Tile_9\\\n",
      "s max is:  67.35099792480469 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_1.3_tIns_0.03_Kd_0.10371_Tile_9\\\n",
      "s max is:  50.242000579833984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_3.1_tIns_0.044_Kd_0.13322_Tile_9\\\n",
      "s max is:  71.03500366210938 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_8.8_tIns_0.017_Kd_0.00406_Tile_9\\\n",
      "s max is:  39.983001708984375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.3_tIns_0.029_Kd_0.04255_Tile_9\\\n",
      "s max is:  39.63199996948242 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.3_tIns_0.051_Kd_0.05662_Tile_9\\\n",
      "s max is:  26.341999053955078 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.4_tIns_0.036_Kd_0.07019_Tile_9\\\n",
      "s max is:  138.55999755859375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-07_xy_16000_z_100_tDiel_1.5_tIns_0.044_Kd_0.12256_Tile_9\\\n",
      "s max is:  55.36800003051758 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_3.5_tIns_0.033_Kd_0.03155_Tile_9\\\n",
      "s max is:  82.27999877929688 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_9.9_tIns_0.045_Kd_0.02414_Tile_9\\\n",
      "s max is:  26.277999877929688 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.0_tIns_0.019_Kd_0.0408_Tile_9\\\n",
      "s max is:  40.33000183105469 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.8_tIns_0.046_Kd_0.03255_Tile_9\\\n",
      "s max is:  26.35099983215332 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.7_tIns_0.042_Kd_0.05797_Tile_9\\\n",
      "s max is:  26.320999145507812 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.1_tIns_0.017_Kd_0.02167_Tile_9\\\n",
      "s max is:  26.17300033569336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.1_tIns_0.031_Kd_0.11983_Tile_9\\\n",
      "s max is:  55.305999755859375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_8.7_tIns_0.049_Kd_0.09248_Tile_9\\\n",
      "s max is:  40.63999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_10.1_tIns_0.043_Kd_0.02552_Tile_9\\\n",
      "s max is:  61.895999908447266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_9.5_tIns_0.046_Kd_0.05706_Tile_9\\\n",
      "s max is:  26.445999145507812 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.1_tIns_0.021_Kd_0.05001_Tile_9\\\n",
      "s max is:  50.4630012512207 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_1.7_tIns_0.024_Kd_0.071_Tile_9\\\n",
      "s max is:  129.16000366210938 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-07_xy_16000_z_100_tDiel_9.9_tIns_0.023_Kd_0.09725_Tile_9\\\n",
      "s max is:  52.41699981689453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_10.1_tIns_0.04_Kd_0.03173_Tile_9\\\n",
      "s max is:  39.058998107910156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.7_tIns_0.036_Kd_0.13_Tile_9\\\n",
      "s max is:  26.386999130249023 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.4_tIns_0.016_Kd_0.01025_Tile_9\\\n",
      "s max is:  60.4109992980957 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_4.3_tIns_0.042_Kd_0.09242_Tile_9\\\n",
      "s max is:  39.24300003051758 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.0_tIns_0.044_Kd_0.1085_Tile_9\\\n",
      "s max is:  40.34299850463867 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.8_tIns_0.012_Kd_0.01859_Tile_9\\\n",
      "s max is:  68.93499755859375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_6.2_tIns_0.037_Kd_0.04593_Tile_9\\\n",
      "s max is:  41.680999755859375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_3.5_tIns_0.016_Kd_0.05167_Tile_9\\\n",
      "s max is:  41.56700134277344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_3.2_tIns_0.03_Kd_0.06907_Tile_9\\\n",
      "s max is:  26.167999267578125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.6_tIns_0.045_Kd_0.12479_Tile_9\\\n",
      "s max is:  55.32600021362305 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_3.4_tIns_0.02_Kd_0.03218_Tile_9\\\n",
      "s max is:  45.23699951171875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_8.2_tIns_0.048_Kd_0.03031_Tile_9\\\n",
      "s max is:  77.96499633789062 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_5.2_tIns_0.015_Kd_0.12598_Tile_9\\\n",
      "s max is:  82.34300231933594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_8.6_tIns_0.048_Kd_0.00743_Tile_9\\\n",
      "s max is:  40.07400131225586 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.0_tIns_0.021_Kd_0.03033_Tile_9\\\n",
      "s max is:  39.41999816894531 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.5_tIns_0.032_Kd_0.09883_Tile_9\\\n",
      "s max is:  44.32099914550781 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_6.9_tIns_0.017_Kd_0.08299_Tile_9\\\n",
      "s max is:  39.763999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.6_tIns_0.029_Kd_0.07191_Tile_9\\\n",
      "s max is:  39.51300048828125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.1_tIns_0.048_Kd_0.08551_Tile_9\\\n",
      "s max is:  61.14799880981445 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_5.6_tIns_0.038_Kd_0.0535_Tile_9\\\n",
      "s max is:  26.155000686645508 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.1_tIns_0.015_Kd_0.11495_Tile_9\\\n",
      "s max is:  54.599998474121094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_1.4_tIns_0.048_Kd_0.11894_Tile_9\\\n",
      "s max is:  39.362998962402344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.7_tIns_0.031_Kd_0.0135_Tile_9\\\n",
      "s max is:  44.61800003051758 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_4.2_tIns_0.015_Kd_0.01647_Tile_9\\\n",
      "s max is:  39.638999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.2_tIns_0.029_Kd_0.035_Tile_9\\\n",
      "s max is:  39.470001220703125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.8_tIns_0.042_Kd_0.07122_Tile_9\\\n",
      "s max is:  39.10300064086914 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.7_tIns_0.043_Kd_0.12348_Tile_9\\\n",
      "s max is:  26.625 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.9_tIns_0.018_Kd_0.06329_Tile_9\\\n",
      "s max is:  60.12799835205078 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_1.5_tIns_0.042_Kd_0.09545_Tile_9\\\n",
      "s max is:  27.709999084472656 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_10.0_tIns_0.013_Kd_0.00268_Tile_9\\\n",
      "s max is:  56.25299835205078 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_10.1_tIns_0.028_Kd_0.05903_Tile_9\\\n",
      "s max is:  67.99500274658203 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_6.1_tIns_0.05_Kd_0.09145_Tile_9\\\n",
      "s max is:  67.83899688720703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_3.1_tIns_0.05_Kd_0.07156_Tile_9\\\n",
      "s max is:  44.40299987792969 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_5.1_tIns_0.037_Kd_0.06021_Tile_9\\\n",
      "s max is:  41.80500030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_3.7_tIns_0.02_Kd_0.03578_Tile_9\\\n",
      "s max is:  26.392000198364258 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.3_tIns_0.017_Kd_0.08151_Tile_9\\\n",
      "s max is:  26.608999252319336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.7_tIns_0.028_Kd_0.0123_Tile_9\\\n",
      "s max is:  50.560001373291016 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_1.3_tIns_0.042_Kd_0.02551_Tile_9\\\n",
      "s max is:  79.61000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_9.9_tIns_0.033_Kd_0.08649_Tile_9\\\n",
      "s max is:  26.67300033569336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.7_tIns_0.027_Kd_0.01598_Tile_9\\\n",
      "s max is:  54.652000427246094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_1.3_tIns_0.015_Kd_0.09077_Tile_9\\\n",
      "s max is:  42.2869987487793 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_7.5_tIns_0.023_Kd_0.04279_Tile_9\\\n",
      "s max is:  41.9900016784668 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_8.1_tIns_0.042_Kd_0.07223_Tile_9\\\n",
      "s max is:  51.23099899291992 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_4.9_tIns_0.031_Kd_0.03596_Tile_9\\\n",
      "s max is:  26.952999114990234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.8_tIns_0.016_Kd_0.01667_Tile_9\\\n",
      "s max is:  39.10499954223633 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.9_tIns_0.05_Kd_0.12412_Tile_9\\\n",
      "s max is:  26.302000045776367 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.2_tIns_0.023_Kd_0.09793_Tile_9\\\n",
      "s max is:  26.29199981689453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.2_tIns_0.034_Kd_0.06964_Tile_9\\\n",
      "s max is:  47.11600112915039 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_3.7_tIns_0.049_Kd_0.07843_Tile_9\\\n",
      "s max is:  80.27100372314453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_4.6_tIns_0.033_Kd_0.00493_Tile_9\\\n",
      "s max is:  39.56700134277344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.9_tIns_0.05_Kd_0.09361_Tile_9\\\n",
      "s max is:  39.17499923706055 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.8_tIns_0.011_Kd_0.08253_Tile_9\\\n",
      "s max is:  26.216999053955078 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.9_tIns_0.041_Kd_0.08569_Tile_9\\\n",
      "s max is:  39.638999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.9_tIns_0.028_Kd_0.06099_Tile_9\\\n",
      "s max is:  26.34600067138672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.5_tIns_0.05_Kd_0.05768_Tile_9\\\n",
      "s max is:  26.150999069213867 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.2_tIns_0.025_Kd_0.12614_Tile_9\\\n",
      "s max is:  160.99000549316406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-07_xy_16000_z_100_tDiel_4.4_tIns_0.042_Kd_0.04956_Tile_9\\\n",
      "s max is:  26.243000030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.1_tIns_0.044_Kd_0.07435_Tile_9\\\n",
      "s max is:  67.52300262451172 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_7.3_tIns_0.012_Kd_0.11547_Tile_9\\\n",
      "s max is:  39.25 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.6_tIns_0.041_Kd_0.09462_Tile_9\\\n",
      "s max is:  39.09400177001953 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.7_tIns_0.035_Kd_0.12531_Tile_9\\\n",
      "s max is:  26.14699935913086 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.4_tIns_0.032_Kd_0.12737_Tile_9\\\n",
      "s max is:  39.354000091552734 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.0_tIns_0.028_Kd_0.06198_Tile_9\\\n",
      "s max is:  26.197999954223633 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.3_tIns_0.049_Kd_0.10589_Tile_9\\\n",
      "s max is:  79.5739974975586 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_8.8_tIns_0.012_Kd_0.08148_Tile_9\\\n",
      "s max is:  39.51599884033203 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.1_tIns_0.02_Kd_0.08862_Tile_9\\\n",
      "s max is:  55.082000732421875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_1.9_tIns_0.042_Kd_0.01508_Tile_9\\\n",
      "s max is:  41.874000549316406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_4.0_tIns_0.039_Kd_0.03433_Tile_9\\\n",
      "s max is:  77.58300018310547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_10.1_tIns_0.044_Kd_0.13777_Tile_9\\\n",
      "s max is:  40.4640007019043 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.9_tIns_0.024_Kd_0.02345_Tile_9\\\n",
      "s max is:  130.1199951171875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-07_xy_16000_z_100_tDiel_6.1_tIns_0.043_Kd_0.05634_Tile_9\\\n",
      "s max is:  39.30699920654297 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.0_tIns_0.014_Kd_0.07022_Tile_9\\\n",
      "s max is:  39.125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.8_tIns_0.016_Kd_0.12068_Tile_9\\\n",
      "s max is:  46.86000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_9.8_tIns_0.013_Kd_0.12097_Tile_9\\\n",
      "s max is:  39.053001403808594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.2_tIns_0.022_Kd_0.12913_Tile_9\\\n",
      "s max is:  26.356000900268555 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.4_tIns_0.026_Kd_0.08936_Tile_9\\\n",
      "s max is:  39.222999572753906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.9_tIns_0.024_Kd_0.09958_Tile_9\\\n",
      "s max is:  39.35300064086914 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.7_tIns_0.014_Kd_0.0725_Tile_9\\\n",
      "s max is:  26.44499969482422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.6_tIns_0.048_Kd_0.0076_Tile_9\\\n",
      "s max is:  26.375999450683594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.0_tIns_0.026_Kd_0.06678_Tile_9\\\n",
      "s max is:  41.40599822998047 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_1.1_tIns_0.011_Kd_0.04378_Tile_9\\\n",
      "s max is:  55.14400100708008 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_2.3_tIns_0.045_Kd_0.02374_Tile_9\\\n",
      "s max is:  56.56999969482422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_9.2_tIns_0.012_Kd_0.03835_Tile_9\\\n",
      "s max is:  67.50399780273438 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_1.1_tIns_0.019_Kd_0.05236_Tile_9\\\n",
      "s max is:  39.12300109863281 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.7_tIns_0.035_Kd_0.11845_Tile_9\\\n",
      "s max is:  44.34600067138672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_3.3_tIns_0.022_Kd_0.03187_Tile_9\\\n",
      "s max is:  100.19999694824219 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_7.2_tIns_0.051_Kd_0.05606_Tile_9\\\n",
      "s max is:  26.224000930786133 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.3_tIns_0.042_Kd_0.0886_Tile_9\\\n",
      "s max is:  39.270999908447266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.7_tIns_0.018_Kd_0.07398_Tile_9\\\n",
      "s max is:  39.345001220703125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.2_tIns_0.026_Kd_0.06757_Tile_9\\\n",
      "s max is:  39.117000579833984 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.2_tIns_0.029_Kd_0.11329_Tile_9\\\n",
      "s max is:  26.211000442504883 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.1_tIns_0.042_Kd_0.10931_Tile_9\\\n",
      "s max is:  26.375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.0_tIns_0.024_Kd_0.05246_Tile_9\\\n",
      "s max is:  39.499000549316406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.9_tIns_0.034_Kd_0.02762_Tile_9\\\n",
      "s max is:  26.246000289916992 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.8_tIns_0.011_Kd_0.07706_Tile_9\\\n",
      "s max is:  54.95000076293945 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_3.2_tIns_0.045_Kd_0.07756_Tile_9\\\n",
      "s max is:  39.46799850463867 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.7_tIns_0.029_Kd_0.02728_Tile_9\\\n",
      "s max is:  39.32400131225586 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.1_tIns_0.029_Kd_0.07094_Tile_9\\\n",
      "s max is:  39.31399917602539 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.7_tIns_0.035_Kd_0.11004_Tile_9\\\n",
      "s max is:  41.56100082397461 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_1.3_tIns_0.02_Kd_0.00303_Tile_9\\\n",
      "s max is:  55.62300109863281 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_5.4_tIns_0.023_Kd_0.04533_Tile_9\\\n",
      "s max is:  26.66200065612793 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.1_tIns_0.033_Kd_0.0112_Tile_9\\\n",
      "s max is:  26.4950008392334 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.8_tIns_0.034_Kd_0.06882_Tile_9\\\n",
      "s max is:  39.49800109863281 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.7_tIns_0.047_Kd_0.09761_Tile_9\\\n",
      "s max is:  39.005001068115234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.2_tIns_0.012_Kd_0.13637_Tile_9\\\n",
      "s max is:  68.5009994506836 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_3.4_tIns_0.044_Kd_0.01735_Tile_9\\\n",
      "s max is:  69.76300048828125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_7.7_tIns_0.043_Kd_0.03106_Tile_9\\\n",
      "s max is:  26.485000610351562 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.4_tIns_0.027_Kd_0.01157_Tile_9\\\n",
      "s max is:  40.051998138427734 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.9_tIns_0.038_Kd_0.05295_Tile_9\\\n",
      "s max is:  51.619998931884766 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_7.3_tIns_0.016_Kd_0.04115_Tile_9\\\n",
      "s max is:  39.165000915527344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.0_tIns_0.041_Kd_0.09823_Tile_9\\\n",
      "s max is:  26.54800033569336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.6_tIns_0.042_Kd_0.04965_Tile_9\\\n",
      "s max is:  78.94200134277344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_5.6_tIns_0.046_Kd_0.08425_Tile_9\\\n",
      "s max is:  44.56399917602539 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_5.3_tIns_0.049_Kd_0.04676_Tile_9\\\n",
      "s max is:  26.152000427246094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.2_tIns_0.031_Kd_0.12892_Tile_9\\\n",
      "s max is:  26.486000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.5_tIns_0.023_Kd_0.03063_Tile_9\\\n",
      "s max is:  39.073001861572266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.3_tIns_0.033_Kd_0.12786_Tile_9\\\n",
      "s max is:  26.263999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.2_tIns_0.033_Kd_0.0805_Tile_9\\\n",
      "s max is:  39.07500076293945 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.1_tIns_0.016_Kd_0.1261_Tile_9\\\n",
      "s max is:  39.43000030517578 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.2_tIns_0.024_Kd_0.07897_Tile_9\\\n",
      "s max is:  39.35900115966797 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.7_tIns_0.013_Kd_0.05069_Tile_9\\\n",
      "s max is:  39.08000183105469 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.6_tIns_0.02_Kd_0.12606_Tile_9\\\n",
      "s max is:  39.13600158691406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.3_tIns_0.045_Kd_0.12023_Tile_9\\\n",
      "s max is:  70.36000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_10.1_tIns_0.029_Kd_0.03687_Tile_9\\\n",
      "s max is:  39.48400115966797 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.2_tIns_0.038_Kd_0.00783_Tile_9\\\n",
      "s max is:  39.30699920654297 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.3_tIns_0.035_Kd_0.10771_Tile_9\\\n",
      "s max is:  60.11600112915039 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_1.3_tIns_0.04_Kd_0.09318_Tile_9\\\n",
      "s max is:  40.834999084472656 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.5_tIns_0.018_Kd_0.00933_Tile_9\\\n",
      "s max is:  51.51300048828125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_7.8_tIns_0.03_Kd_0.05348_Tile_9\\\n",
      "s max is:  26.3799991607666 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.5_tIns_0.021_Kd_0.07047_Tile_9\\\n",
      "s max is:  39.28799819946289 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.2_tIns_0.012_Kd_0.08926_Tile_9\\\n",
      "s max is:  97.43699645996094 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_1.0_tIns_0.014_Kd_0.10308_Tile_9\\\n",
      "s max is:  26.993999481201172 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.8_tIns_0.048_Kd_0.01484_Tile_9\\\n",
      "s max is:  46.869998931884766 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_9.6_tIns_0.043_Kd_0.12165_Tile_9\\\n",
      "s max is:  69.34700012207031 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_7.0_tIns_0.043_Kd_0.03844_Tile_9\\\n",
      "s max is:  44.3129997253418 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_5.0_tIns_0.048_Kd_0.0701_Tile_9\\\n",
      "s max is:  38.994998931884766 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.1_tIns_0.036_Kd_0.13347_Tile_9\\\n",
      "s max is:  39.50299835205078 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.8_tIns_0.044_Kd_0.09403_Tile_9\\\n",
      "s max is:  27.07900047302246 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.1_tIns_0.042_Kd_0.01131_Tile_9\\\n",
      "s max is:  26.16699981689453 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.4_tIns_0.048_Kd_0.12243_Tile_9\\\n",
      "s max is:  292.3900146484375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-08_xy_16000_z_100_tDiel_9.1_tIns_0.02_Kd_0.10416_Tile_9\\\n",
      "s max is:  26.73200035095215 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.6_tIns_0.03_Kd_0.04856_Tile_9\\\n",
      "s max is:  46.874000549316406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_6.1_tIns_0.024_Kd_0.11663_Tile_9\\\n",
      "s max is:  98.98799896240234 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_2.9_tIns_0.033_Kd_0.02204_Tile_9\\\n",
      "s max is:  26.981000900268555 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.3_tIns_0.031_Kd_0.01884_Tile_9\\\n",
      "s max is:  39.34299850463867 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.2_tIns_0.016_Kd_0.10332_Tile_9\\\n",
      "s max is:  39.362998962402344 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.6_tIns_0.047_Kd_0.09268_Tile_9\\\n",
      "s max is:  68.31400299072266 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_5.3_tIns_0.011_Kd_0.0651_Tile_9\\\n",
      "s max is:  39.40700149536133 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.0_tIns_0.022_Kd_0.06829_Tile_9\\\n",
      "s max is:  26.381999969482422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.0_tIns_0.012_Kd_0.04809_Tile_9\\\n",
      "s max is:  42.61399841308594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_6.6_tIns_0.033_Kd_0.00801_Tile_9\\\n",
      "s max is:  39.3390007019043 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_10.0_tIns_0.022_Kd_0.10821_Tile_9\\\n",
      "s max is:  26.535999298095703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.3_tIns_0.042_Kd_0.00637_Tile_9\\\n",
      "s max is:  26.323999404907227 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.4_tIns_0.024_Kd_0.09855_Tile_9\\\n",
      "s max is:  39.483001708984375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.8_tIns_0.034_Kd_0.05335_Tile_9\\\n",
      "s max is:  77.76499938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_5.5_tIns_0.01_Kd_0.1351_Tile_9\\\n",
      "s max is:  39.29899978637695 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.9_tIns_0.045_Kd_0.1119_Tile_9\\\n",
      "s max is:  39.21900177001953 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.3_tIns_0.05_Kd_0.09957_Tile_9\\\n",
      "s max is:  55.17300033569336 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_3.7_tIns_0.028_Kd_0.05755_Tile_9\\\n",
      "s max is:  55.486000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_3.4_tIns_0.038_Kd_0.01593_Tile_9\\\n",
      "s max is:  26.43400001525879 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.1_tIns_0.018_Kd_0.07937_Tile_9\\\n",
      "s max is:  26.277999877929688 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.1_tIns_0.042_Kd_0.10377_Tile_9\\\n",
      "s max is:  39.358001708984375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.5_tIns_0.03_Kd_0.0966_Tile_9\\\n",
      "s max is:  39.2400016784668 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.4_tIns_0.011_Kd_0.03819_Tile_9\\\n",
      "s max is:  39.89500045776367 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.0_tIns_0.029_Kd_0.06466_Tile_9\\\n",
      "s max is:  39.95899963378906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.0_tIns_0.012_Kd_0.05066_Tile_9\\\n",
      "s max is:  245.16000366210938 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-08_xy_16000_z_100_tDiel_8.4_tIns_0.048_Kd_0.01962_Tile_9\\\n",
      "s max is:  47.441001892089844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_8.3_tIns_0.042_Kd_0.08479_Tile_9\\\n",
      "s max is:  39.12300109863281 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.8_tIns_0.014_Kd_0.11968_Tile_9\\\n",
      "s max is:  230.42999267578125 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-08_xy_16000_z_100_tDiel_8.8_tIns_0.042_Kd_0.05583_Tile_9\\\n",
      "s max is:  39.85100173950195 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.8_tIns_0.044_Kd_0.03723_Tile_9\\\n",
      "s max is:  26.59000015258789 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_6.3_tIns_0.024_Kd_0.03666_Tile_9\\\n",
      "s max is:  26.270000457763672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.9_tIns_0.019_Kd_0.07002_Tile_9\\\n",
      "s max is:  39.35499954223633 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_8.7_tIns_0.027_Kd_0.10436_Tile_9\\\n",
      "s max is:  39.06800079345703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.1_tIns_0.02_Kd_0.12691_Tile_9\\\n",
      "s max is:  26.152999877929688 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.3_tIns_0.015_Kd_0.1184_Tile_9\\\n",
      "s max is:  67.7509994506836 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_9.2_tIns_0.017_Kd_0.11067_Tile_9\\\n",
      "s max is:  39.3129997253418 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.5_tIns_0.048_Kd_0.11043_Tile_9\\\n",
      "s max is:  26.566999435424805 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_8.2_tIns_0.05_Kd_0.06186_Tile_9\\\n",
      "s max is:  39.00299835205078 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_5.8_tIns_0.036_Kd_0.13622_Tile_9\\\n",
      "s max is:  79.05000305175781 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_7.1_tIns_0.025_Kd_0.08886_Tile_9\\\n",
      "s max is:  39.87300109863281 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.5_tIns_0.031_Kd_0.05438_Tile_9\\\n",
      "s max is:  41.28900146484375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_2.6_tIns_0.018_Kd_0.12174_Tile_9\\\n",
      "s max is:  61.7239990234375 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_4e-06_xy_16000_z_100_tDiel_4.7_tIns_0.045_Kd_0.00291_Tile_9\\\n",
      "s max is:  54.707000732421875 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_3.4_tIns_0.043_Kd_0.1101_Tile_9\\\n",
      "s max is:  39.07099914550781 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_3.9_tIns_0.011_Kd_0.1251_Tile_9\\\n",
      "s max is:  39.334999084472656 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.6_tIns_0.034_Kd_0.05839_Tile_9\\\n",
      "s max is:  26.26799964904785 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.6_tIns_0.028_Kd_0.03301_Tile_9\\\n",
      "s max is:  39.98099899291992 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.7_tIns_0.028_Kd_0.03528_Tile_9\\\n",
      "s max is:  26.19700050354004 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_9.2_tIns_0.013_Kd_0.1186_Tile_9\\\n",
      "s max is:  39.18199920654297 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.3_tIns_0.042_Kd_0.07434_Tile_9\\\n",
      "s max is:  39.23899841308594 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_1.6_tIns_0.018_Kd_0.05168_Tile_9\\\n",
      "s max is:  78.35800170898438 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_8.6_tIns_0.04_Kd_0.11634_Tile_9\\\n",
      "s max is:  40.321998596191406 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.4_tIns_0.018_Kd_0.01557_Tile_9\\\n",
      "s max is:  26.465999603271484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.9_tIns_0.051_Kd_0.02786_Tile_9\\\n",
      "s max is:  52.23500061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_6e-06_xy_16000_z_100_tDiel_9.3_tIns_0.047_Kd_0.03222_Tile_9\\\n",
      "s max is:  47.53799819946289 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_7e-06_xy_16000_z_100_tDiel_8.0_tIns_0.018_Kd_0.07571_Tile_9\\\n",
      "s max is:  39.763999938964844 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_6.1_tIns_0.031_Kd_0.04973_Tile_9\\\n",
      "s max is:  56.400001525878906 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_5e-06_xy_16000_z_100_tDiel_9.6_tIns_0.021_Kd_0.04924_Tile_9\\\n",
      "s max is:  98.9020004272461 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-06_xy_16000_z_100_tDiel_4.6_tIns_0.013_Kd_0.06356_Tile_9\\\n",
      "s max is:  41.619998931884766 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_6.4_tIns_0.023_Kd_0.09227_Tile_9\\\n",
      "s max is:  68.86699676513672 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_6.6_tIns_0.049_Kd_0.05459_Tile_9\\\n",
      "s max is:  41.22800064086914 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_9e-06_xy_16000_z_100_tDiel_2.5_tIns_0.021_Kd_0.1379_Tile_9\\\n",
      "s max is:  68.22899627685547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_3e-06_xy_16000_z_100_tDiel_6.7_tIns_0.024_Kd_0.08278_Tile_9\\\n",
      "s max is:  40.41999816894531 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_7.3_tIns_0.04_Kd_0.00959_Tile_9\\\n",
      "s max is:  26.215999603271484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_7.8_tIns_0.025_Kd_0.11499_Tile_9\\\n",
      "s max is:  26.197999954223633 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.0_tIns_0.042_Kd_0.1089_Tile_9\\\n",
      "s max is:  26.465999603271484 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_5.3_tIns_0.039_Kd_0.05018_Tile_9\\\n",
      "s max is:  81.04900360107422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_9.3_tIns_0.013_Kd_0.04642_Tile_9\\\n",
      "s max is:  78.23200225830078 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_7.5_tIns_0.034_Kd_0.11855_Tile_9\\\n",
      "s max is:  79.52899932861328 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-06_xy_16000_z_100_tDiel_6.2_tIns_0.042_Kd_0.06512_Tile_9\\\n",
      "s max is:  26.187999725341797 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_3.2_tIns_0.049_Kd_0.11577_Tile_9\\\n",
      "s max is:  45.56800079345703 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_8e-06_xy_16000_z_100_tDiel_9.6_tIns_0.036_Kd_0.0257_Tile_9\\\n",
      "s max is:  26.4060001373291 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_4.7_tIns_0.036_Kd_0.05639_Tile_9\\\n",
      "s max is:  26.187000274658203 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_2.8_tIns_0.039_Kd_0.11228_Tile_9\\\n",
      "s max is:  26.131999969482422 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_2e-05_xy_16000_z_100_tDiel_1.6_tIns_0.011_Kd_0.13644_Tile_9\\\n",
      "s max is:  39.4630012512207 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_2.6_tIns_0.036_Kd_0.02616_Tile_9\\\n",
      "s max is:  41.11000061035156 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_9.0_tIns_0.04_Kd_0.00267_Tile_9\\\n",
      "s max is:  39.20800018310547 folder name is:  /home/ansysai/HaiyangHe/ThermalArtist/DecaySurface/3200um_9x9/DeepONet_v01/One_tile_3D_data_npz/Q_5000.0_HTC_1e-05_xy_16000_z_100_tDiel_4.4_tIns_0.049_Kd_0.1079_Tile_9\\\n",
      "workbench_u shape is (70000, 4)\n",
      "workbench_y shape is (70000, 1)\n",
      "workbench_s shape is (70000, 9, 9)\n",
      "The time took to generate data is:  11.561010599136353\n",
      "workbench_y[:, 0] min is 0.0001\n",
      "workbench_y[:, 0] max is 100000.0\n",
      "workbench_u[:, 0] min is 1e-08\n",
      "workbench_u[:, 0] max is 2e-05\n",
      "workbench_u[:, 1] min is 1.0\n",
      "workbench_u[:, 1] max is 10.1\n",
      "workbench_u[:, 2] min is 0.01\n",
      "workbench_u[:, 2] max is 0.051\n",
      "workbench_u[:, 3] min is 0.00149\n",
      "workbench_u[:, 3] max is 0.13809\n",
      "workbench_s min is 0.0\n",
      "workbench_s max is 398.489990234375\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "\n",
    "\n",
    "def get_all_file_paths(directory):\n",
    "    file_paths = []\n",
    "    file_directories = []\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            file_paths.append(filepath)\n",
    "        file_directories.append(directories)\n",
    "    return file_paths, file_directories\n",
    "\n",
    "def get_DeepONet_data(train_data, train_label, Power, HTC, Die_xy, Die_z, t_Diel, t_Insulator, K_Diel):\n",
    "    start_time = time.time()\n",
    "\n",
    "#     y_train, s_train = train_data[:, :1], train_data[:, 1:2].reshape(-1, 1)\n",
    "    y_train = train_data[:, -1:]\n",
    "    s_train = np.clip(train_label, a_min=0, a_max =None)\n",
    "    \n",
    "\n",
    "    ## Try without local minmax scaling\n",
    "    ## Try without local minmax scaling\n",
    "#     s_train = s_train - s_train.min()\n",
    "#     max_temp_local = s_train.max()\n",
    "#     min_temp_local = s_train.min()\n",
    "       \n",
    "########################################## No need for scaling ##############################33\n",
    "    # power_mag_n = np.log(Power*10**2) * 0.1\n",
    "    # HTC_n = np.log(HTC*10**9) * 0.1\n",
    "    # Die_xy_n = (Die_xy - 4000.0) / (30000.0 - 4000.0)\n",
    "    # Die_z_n = (Die_z - 10.0) / (200.0 - 10.0)\n",
    "    \n",
    "    # At data generation:\n",
    "    # t_Diel = np.random.uniform(low=1.0, high=10.1, size=(num_cases, 1))\n",
    "    # t_Insulator = np.random.uniform(low=1.0, high=5.1, size=(num_cases, 1)) * 10**-2\n",
    "    # t_Diel_n = (t_Diel - 1.0) / (10.1 - 1.0)\n",
    "    # t_Insulator_n = (t_Insulator - 0.01) / (0.051 - 0.01)\n",
    "\n",
    "    # K_Diel_n = np.log(K_Diel*10**3) * 0.1\n",
    "########################################## No need for scaling ##############################33\n",
    "\n",
    "\n",
    "#     y_train[:, 0] = y_train[:, 0] / (Die_xy / 2.0)\n",
    "    # y_train = np.log(y_train*10**4)/21.0\n",
    "\n",
    "    ## Global scaling of s_train\n",
    "    # s_train = s_train/80.0\n",
    "    \n",
    "    #     u_train = np.asarray([power_mag_n, HTC_n, Die_xy_n, Die_z_n, t_Diel_n, t_Insulator_n, K_Diel_n])\n",
    "    # u_train = np.asarray([HTC_n, t_Diel_n, t_Insulator_n, K_Diel_n])\n",
    "    u_train = np.asarray([HTC, t_Diel, t_Insulator, K_Diel])\n",
    "    u_train = np.tile(u_train, reps=(y_train.shape[0], 1))\n",
    "    \n",
    "#     xy_coordinates = u_train[:, -2:]\n",
    "#     u_train = np.concatenate((u_train, xy_coordinates), axis=1)\n",
    "#     print('xy_coordinates shape is: ', xy_coordinates.shape)\n",
    "#     print('u_train shape is: ', u_train.shape)\n",
    "#     print('train_data shape is: ', train_data.shape)\n",
    "#     print('train_data[0] is: ', train_data[0])\n",
    "#     raise\n",
    "    \n",
    "    # print(\"y_train max is: \", y_train.max())\n",
    "    # print(\"y_train min is: \", y_train.min())\n",
    "    # print(\"s_train max is: \", s_train.max())\n",
    "    # print(\"s_train min is: \", s_train.min())\n",
    "    # print(\"u_train max is: \", u_train.max())\n",
    "    # print(\"u_train min is: \", u_train.min())\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     total_time = end_time - start_time\n",
    "    #     print(\"Total time consumed is: \", total_time)\n",
    "#     power_test_BC = u_train[0, :].reshape(-1, 1)\n",
    "\n",
    "    #     print(\"power_test_BC shape is: \", power_test_BC.shape)\n",
    "    #     print(\"power_test_BC max is: \", power_test_BC.max())\n",
    "    #     print(\"power_test_BC min is: \", power_test_BC.min())\n",
    "    return u_train, y_train, s_train\n",
    "\n",
    "\n",
    "def main():\n",
    "    # path to folder which needs to be zipped\n",
    "    directory = './Data_1000/One_tile_3D_data'\n",
    "    save_data_path = dest_fpath + 'One_tile_3D_data_npz/'\n",
    "    os.makedirs(os.path.dirname(save_data_path), exist_ok=True, mode=0o777)\n",
    "\n",
    "    file_paths, file_dir = get_all_file_paths(directory)\n",
    "    file_directories = file_dir[0]\n",
    "    counter = 1\n",
    "\n",
    "    data = 0\n",
    "\n",
    "    workbench_u = np.empty((0, 4))\n",
    "    # workbench_y = np.empty((0, 3))\n",
    "    workbench_y = np.empty((0, 1))\n",
    "#     workbench_s = np.empty((0, 1))\n",
    "    workbench_s = np.empty((0, 9, 9))\n",
    "    \n",
    "    for folder in file_directories:\n",
    "        source_dir = directory + '/' + folder + '/'\n",
    "        # os.chdir(temp_working_dir)\n",
    "        txt_name = \"commands_trans_xy_decay_3200um.txt\"\n",
    "        source_file_name = source_dir + txt_name\n",
    "        f = open(source_file_name, 'r')\n",
    "        lines = f.readlines()\n",
    "\n",
    "        Die_xy = int(re.findall(\"\\d+\", lines[22].split()[0])[0])\n",
    "        Die_z = int(re.findall(\"\\d+\", lines[24].split()[0])[0])\n",
    "        match_number = re.compile('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "        t_Insulator = float(re.findall(match_number, lines[25].split()[0])[0])\n",
    "        t_Diel = float(re.findall(match_number, lines[26].split()[0])[0])\n",
    "        tile_size = int(re.findall(match_number, lines[27].split()[0])[0])\n",
    "        K_Diel = float(re.findall(match_number, lines[108].split()[1])[0])\n",
    "        top_HTC = float(re.findall(match_number, lines[121].split()[0])[0])\n",
    "        bottom_HTC = float(re.findall(match_number, lines[123].split()[0])[0])\n",
    "        Power = float(re.findall(match_number, lines[131].split()[0])[0])\n",
    "        HTC = top_HTC\n",
    "#         print(f\"Q_{Power}_HTC_{HTC}_xy_{Die_xy}_z_{Die_z}_tDiel_{t_Diel}_tIns_{t_Insulator}_Kd_{K_Diel}_Tile_{tile_size}\\\\\")\n",
    "        target_folder_name = save_data_path + f\"Q_{Power}_HTC_{HTC}_xy_{Die_xy}_z_{Die_z}_tDiel_{t_Diel}_tIns_{t_Insulator}_Kd_{K_Diel}_Tile_{tile_size}\\\\\"\n",
    "\n",
    "\n",
    "        input_data_file_name = \"raw_input.npy\"\n",
    "        output_data_file_name = \"raw_output.npy\"\n",
    "        input_data = np.load(source_dir + input_data_file_name)\n",
    "        output_data = np.load(source_dir + output_data_file_name)\n",
    "        u, y, s = get_DeepONet_data(input_data, output_data, Power, HTC, Die_xy, Die_z, t_Diel, t_Insulator, K_Diel)\n",
    "        print('s max is: ', s.max(), 'folder name is: ', target_folder_name)\n",
    "#         num_pts_per_case = unit_size\n",
    "#         idx = np.random.choice(u.shape[0], num_pts_per_case)\n",
    "#         u = u[idx]\n",
    "#         y = y[idx]\n",
    "#         s = s[idx]\n",
    "#         print('u shape is: ', u.shape)\n",
    "#         print('y shape is: ', y.shape)\n",
    "#         print('s shape is: ', s.shape)\n",
    "#         raise\n",
    "        workbench_u = np.vstack((workbench_u, u))\n",
    "        workbench_y = np.vstack((workbench_y, y))\n",
    "        workbench_s = np.vstack((workbench_s, s))\n",
    "    print(f'workbench_u shape is {workbench_u.shape}')\n",
    "    print(f'workbench_y shape is {workbench_y.shape}')\n",
    "    print(f'workbench_s shape is {workbench_s.shape}')\n",
    "    np.savez_compressed(save_data_path + \"workbench_u\", workbench_u)\n",
    "    np.savez_compressed(save_data_path + \"workbench_y\", workbench_y)\n",
    "    np.savez_compressed(save_data_path + \"workbench_s\", workbench_s)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_data_path = dest_fpath + 'One_tile_3D_data_npz/'\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    print('The time took to generate data is: ', time.time() - start_time)\n",
    "    workbench_u_f = np.load(save_data_path + \"workbench_u.npz\")\n",
    "    workbench_y_f = np.load(save_data_path + \"workbench_y.npz\")\n",
    "    workbench_s_f = np.load(save_data_path + \"workbench_s.npz\")\n",
    "    workbench_u = workbench_u_f['arr_0']\n",
    "    workbench_y = workbench_y_f['arr_0']\n",
    "    workbench_s = workbench_s_f['arr_0']\n",
    "\n",
    "    print(f\"workbench_y[:, 0] min is {workbench_y[:, 0].min()}\")\n",
    "    print(f\"workbench_y[:, 0] max is {workbench_y[:, 0].max()}\")\n",
    "    print(f\"workbench_u[:, 0] min is {workbench_u[:, 0].min()}\")\n",
    "    print(f\"workbench_u[:, 0] max is {workbench_u[:, 0].max()}\")\n",
    "    print(f\"workbench_u[:, 1] min is {workbench_u[:, 1].min()}\")\n",
    "    print(f\"workbench_u[:, 1] max is {workbench_u[:, 1].max()}\")\n",
    "    print(f\"workbench_u[:, 2] min is {workbench_u[:, 2].min()}\")\n",
    "    print(f\"workbench_u[:, 2] max is {workbench_u[:, 2].max()}\")\n",
    "    print(f\"workbench_u[:, 3] min is {workbench_u[:, 3].min()}\")\n",
    "    print(f\"workbench_u[:, 3] max is {workbench_u[:, 3].max()}\")\n",
    "    # print(f\"workbench_u[:, 4] min is {workbench_u[:, 4].min()}\")\n",
    "    # print(f\"workbench_u[:, 4] max is {workbench_u[:, 4].max()}\")\n",
    "    # print(f\"workbench_u[:, 5] min is {workbench_u[:, 5].min()}\")\n",
    "    # print(f\"workbench_u[:, 5] max is {workbench_u[:, 5].max()}\")\n",
    "    # print(f\"workbench_u[:, 6] min is {workbench_u[:, 6].min()}\")\n",
    "    # print(f\"workbench_u[:, 6] max is {workbench_u[:, 6].max()}\")\n",
    "    print(f\"workbench_s min is {workbench_s.min()}\")\n",
    "    print(f\"workbench_s max is {workbench_s.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398.489990234375"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workbench_s.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.9903487552536128\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.03220834991691132 0.4927253685157205\n"
     ]
    }
   ],
   "source": [
    "# u_train = np.asarray([HTC_n, t_Diel_n, t_Insulator_n, K_Diel_n])\n",
    "# HTC: [1e-9-- 2e-5]\n",
    "# t_Diel: 1-10\n",
    "# t_Insulator: [0.01-0.05]\n",
    "# K_Diel: [0.00138--0.138]\n",
    "\n",
    "# HTC_n = np.log(HTC*10**9) * 0.1\n",
    "# t_Diel_n = (t_Diel - 1.0) / (10.0 - 1.0)\n",
    "# t_Insulator_n = (t_Insulator - 0.01) / (0.05 - 0.01)\n",
    "# K_Diel_n = np.log(K_Diel*10**3) * 0.1\n",
    "\n",
    "# HTC_n \n",
    "print(np.log(1e-9*10**9) * 0.1, np.log(2e-5*10**9) * 0.1)\n",
    "# t_Diel_n (1--10)\n",
    "print((1 - 1.0) / (10.0 - 1.0), (10 - 1.0) / (10.0 - 1.0))\n",
    "# t_Insulator_n (0.01--0.05)\n",
    "print((0.01 - 0.01) / (0.05 - 0.01), (0.05 - 0.01) / (0.05 - 0.01))\n",
    "# K diel\n",
    "print(np.log(0.00138*10**3) * 0.1, np.log(0.138*10**3) * 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_num_data is:  1000\n",
      "The shape of u_train is:  (63000, 4)\n",
      "The shape of y_train is:  (63000, 1)\n",
      "The shape of s_train is:  (63000, 9, 9)\n",
      "The shape of u_test is:  (7000, 4)\n",
      "The shape of y_test is:  (7000, 1)\n",
      "The shape of s_test is:  (7000, 9, 9)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "save_data_path = dest_fpath + 'One_tile_3D_data_npz/'\n",
    "workbench_u_f = np.load(save_data_path + \"workbench_u.npz\")\n",
    "workbench_y_f = np.load(save_data_path + \"workbench_y.npz\")\n",
    "workbench_s_f = np.load(save_data_path + \"workbench_s.npz\")\n",
    "workbench_u = workbench_u_f['arr_0']\n",
    "workbench_y = workbench_y_f['arr_0']\n",
    "workbench_s = workbench_s_f['arr_0']\n",
    "\n",
    "unit_size = 70\n",
    "total_num_data = int(workbench_u.shape[0] / unit_size)\n",
    "print(\"total_num_data is: \", total_num_data)\n",
    "\n",
    "train_idx = np.random.choice(total_num_data, size = 900, replace = False)\n",
    "test_idx = np.setxor1d(np.arange(total_num_data), train_idx)\n",
    "\n",
    "# train_idx = np.random.choice(total_num_data, size = total_num_data, replace = False)\n",
    "# test_idx = train_idx\n",
    "\n",
    "# train_idx = np.append(np.arange(25, 50), np.arange(175, 225))\n",
    "# test_idx = train_idx\n",
    "\n",
    "u_train = np.empty((0, workbench_u.shape[1]))\n",
    "y_train = np.empty((0, workbench_y.shape[1]))\n",
    "s_train = np.empty((0, workbench_s.shape[1], workbench_s.shape[1]))\n",
    "# sdf_train = np.empty((0, workbench_y.shape[1]))\n",
    "for idx in train_idx:\n",
    "    t_u = np.take(workbench_u, np.array(np.arange(idx*unit_size, (idx + 1)*unit_size)), axis = 0)\n",
    "    t_y = np.take(workbench_y, np.array(np.arange(idx*unit_size, (idx + 1)*unit_size)), axis = 0)\n",
    "    t_s = np.take(workbench_s, np.array(np.arange(idx*unit_size, (idx + 1)*unit_size)), axis = 0)\n",
    "    \n",
    "#     sdf = 1 - t_y\n",
    "#     sdf = 1 / (1 + np.exp(10*t_y-4))\n",
    "    \n",
    "    u_train = np.vstack((u_train, t_u))\n",
    "    y_train = np.vstack((y_train, t_y))\n",
    "    s_train = np.vstack((s_train, t_s))\n",
    "#     sdf_train = np.vstack((sdf_train, sdf))\n",
    "print(\"The shape of u_train is: \", u_train.shape)\n",
    "print(\"The shape of y_train is: \", y_train.shape)\n",
    "print(\"The shape of s_train is: \", s_train.shape)\n",
    "# print(\"The shape of sdf_train is: \", sdf_train.shape)\n",
    "\n",
    "u_test = np.empty((0, workbench_u.shape[1]))\n",
    "y_test = np.empty((0, workbench_y.shape[1]))\n",
    "s_test = np.empty((0, workbench_s.shape[1], workbench_s.shape[1]))\n",
    "\n",
    "for idx in test_idx:\n",
    "    t_u = np.take(workbench_u, np.array(np.arange(idx*unit_size, (idx + 1)*unit_size)), axis = 0)\n",
    "    t_y = np.take(workbench_y, np.array(np.arange(idx*unit_size, (idx + 1)*unit_size)), axis = 0)\n",
    "    t_s = np.take(workbench_s, np.array(np.arange(idx*unit_size, (idx + 1)*unit_size)), axis = 0)\n",
    "    \n",
    "    \n",
    "    u_test = np.vstack((u_test, t_u))\n",
    "    y_test = np.vstack((y_test, t_y))\n",
    "    s_test = np.vstack((s_test, t_s))\n",
    "print(\"The shape of u_test is: \", u_test.shape)\n",
    "print(\"The shape of y_test is: \", y_test.shape)\n",
    "print(\"The shape of s_test is: \", s_test.shape)\n",
    "\n",
    "power_test_BC = u_train[0, :].reshape(-1, 1)\n",
    "# print(\"The shape of temp_scaling_factor_train is: \", temp_scaling_factor_train.shape)\n",
    "# print(\"The shape of temp_scaling_factor_test is: \", temp_scaling_factor_test.shape)\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# plt.plot(y_train[:unit_size, :], sdf_train[:unit_size, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workbench_u max is:  10.1\n",
      "workbench_y max is:  100000.0\n",
      "workbench_s max is:  398.489990234375\n"
     ]
    }
   ],
   "source": [
    "workbench_u = workbench_u_f['arr_0']\n",
    "workbench_y = workbench_y_f['arr_0']\n",
    "workbench_s = workbench_s_f['arr_0']\n",
    "print('workbench_u max is: ', workbench_u.max())\n",
    "print('workbench_y max is: ', workbench_y.max())\n",
    "print('workbench_s max is: ', workbench_s.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale s_train and s_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_train max is: 0.9962249755859375, s_train max is: 0.0\n",
      "s_test max is: 0.32290000915527345, s_test max is: 0.0\n"
     ]
    }
   ],
   "source": [
    "## Temperature scaling for 210 um is 80.5\n",
    "## Temperature scaling for 3200 um is 400\n",
    "# s_train = s_train/80.5\n",
    "# s_test = s_test/80.5\n",
    "s_train = s_train/400.0\n",
    "s_test = s_test/400.0\n",
    "print(f's_train max is: {s_train.max()}, s_train max is: {s_train.min()}')\n",
    "print(f's_test max is: {s_test.max()}, s_test max is: {s_test.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_train = u_train[:unit_size, :]\n",
    "# y_train = y_train[:unit_size, :]\n",
    "# s_train = s_train[:unit_size, :]\n",
    "# sdf_train = sdf_train[:unit_size, :]\n",
    "# u_test = u_test[:unit_size, :]\n",
    "# y_test = y_test[:unit_size, :]\n",
    "# s_test = s_test[:unit_size, :]\n",
    "# print('u_train shape is: ', u_train.shape)\n",
    "# print('y_train shape is: ', y_train.shape)\n",
    "# print('s_train shape is: ', s_train.shape)\n",
    "# print('sdf_train shape is: ', sdf_train.shape)\n",
    "# print('u_test shape is: ', u_test.shape)\n",
    "# print('y_test shape is: ', y_test.shape)\n",
    "# print('s_test shape is: ', s_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following is just a template for custom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import math_ops\n",
    "class Rescaling(tf.keras.layers.Layer):\n",
    "#   \"\"\"Multiply inputs by `scale` and adds `offset`.\n",
    "#   For instance:\n",
    "#   1. To rescale an input in the `[0, 255]` range\n",
    "#   to be in the `[0, 1]` range, you would pass `scale=1./255`.\n",
    "#   2. To rescale an input in the `[0, 255]` range to be in the `[-1, 1]` range,\n",
    "#   you would pass `scale=1./127.5, offset=-1`.\n",
    "#   The rescaling is applied both during training and inference.\n",
    "#   Input shape:\n",
    "#     Arbitrary.\n",
    "#   Output shape:\n",
    "#     Same as input.\n",
    "#   Arguments:\n",
    "#     scale: Float, the scale to apply to the inputs.\n",
    "#     offset: Float, the offset to apply to the inputs.\n",
    "#     name: A string, the name of the layer.\n",
    "#   \"\"\"\n",
    "    def __init__(self, scale, offset=0., name=None, **kwargs):\n",
    "        self.scale = scale\n",
    "        self.offset = offset\n",
    "        super(Rescaling, self).__init__(name=name, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        dtype = self._compute_dtype\n",
    "        scale = math_ops.cast(self.scale, dtype)\n",
    "        offset = math_ops.cast(self.offset, dtype)\n",
    "        return math_ops.cast(inputs, dtype) * scale + offset\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'scale': self.scale,\n",
    "            'offset': self.offset,\n",
    "        }\n",
    "        base_config = super(Rescaling, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "# inputA = Input(shape=(7,))\n",
    "# inputB = Input(shape=(1,))\n",
    "# out = Rescaling(1)(inputA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layer for u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out shape is:  (None, 4)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "class Rescaling_u(tf.keras.layers.Layer):\n",
    "#     s_train = s_train - s_train.min()\n",
    "#     max_temp_local = s_train.max()\n",
    "#     min_temp_local = s_train.min()\n",
    "## Scale s_train and s_test\n",
    "#     s_train = (s_train - min_temp_local) / (max_temp_local - min_temp_local)\n",
    "\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        self.scale = 1\n",
    "        self.offset = 0.0\n",
    "        super(Rescaling_u, self).__init__(name=name, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        dtype = self._compute_dtype\n",
    "#         # Power = inputs[:, 0:1]\n",
    "#         HTC = inputs[:, 1:2]\n",
    "#         # Die_xy = inputs[:, 2:3]\n",
    "#         # Die_z = inputs[:, 3:4]\n",
    "#         t_Diel = inputs[:, 4:5]\n",
    "#         t_Insulator = inputs[:, 5:6]\n",
    "#         K_Diel = inputs[:, 6:7]\n",
    "        \n",
    "        HTC = inputs[:, 0:1]\n",
    "        t_Diel = inputs[:, 1:2]\n",
    "        t_Insulator = inputs[:, 2:3]\n",
    "        K_Diel = inputs[:, 3:4]\n",
    "#         x_coord = inputs[:, 4:5]\n",
    "#         y_coord = inputs[:, 5:6]\n",
    "    \n",
    "        # power_mag_n = tf.math.log(Power*10**2) * 0.1\n",
    "        HTC_n = tf.math.log(HTC*10**9) * 0.1\n",
    "        # Die_xy_n = (Die_xy - 4000.0) / (30000.0 - 4000.0)\n",
    "        # Die_z_n = (Die_z - 10.0) / (200.0 - 10.0)\n",
    "        # At data generation:\n",
    "        # t_Diel = np.random.uniform(low=1.0, high=10.1, size=(num_cases, 1))\n",
    "        # t_Insulator = np.random.uniform(low=1.0, high=5.1, size=(num_cases, 1)) * 10**-2\n",
    "        t_Diel_n = (t_Diel - 1.0) / (10.1 - 1.0)\n",
    "        t_Insulator_n = (t_Insulator - 0.01) / (0.051 - 0.01)\n",
    "        K_Diel_n = tf.math.log(K_Diel*10**3) * 0.1\n",
    "    \n",
    "#     y_train[:, 0] = y_train[:, 0] / (Die_xy / 2.0)\n",
    "        # inputs = tf.concat([power_mag_n, HTC_n, Die_xy_n, Die_z_n, t_Diel_n, t_Insulator_n, K_Diel_n], 1)\n",
    "#         inputs = tf.concat([HTC_n, t_Diel_n, t_Insulator_n, K_Diel_n, x_coord, y_coord], 1)\n",
    "        inputs = tf.concat([HTC_n, t_Diel_n, t_Insulator_n, K_Diel_n], 1)\n",
    "#         scale = math_ops.cast(self.scale, dtype)\n",
    "#         offset = math_ops.cast(self.offset, dtype)\n",
    "        return math_ops.cast(inputs, dtype)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'scale': self.scale,\n",
    "            'offset': self.offset,\n",
    "        }\n",
    "        base_config = super(Rescaling_u, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "inputA = Input(shape=(4,))\n",
    "# inputB = Input(shape=(1,))\n",
    "out = Rescaling_u()(inputA)\n",
    "print('out shape is: ', out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layer for y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out shape is:  (None, 1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# class Rescaling_y(tf.keras.layers.Layer):\n",
    "#     def __init__(self, Die_xy, name=None, **kwargs):\n",
    "#         self.Die_xy = Die_xy\n",
    "#         super(Rescaling_y, self).__init__(name=name, **kwargs)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         dtype = self._compute_dtype\n",
    "#         inputs = inputs[:, 0:1] / (self.Die_xy / 2.0)\n",
    "# #         scale = math_ops.cast(self.scale, dtype)\n",
    "# #         offset = math_ops.cast(self.offset, dtype)\n",
    "#         return math_ops.cast(inputs, dtype)\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return input_shape\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = {\n",
    "#             'Die_xy': self.Die_xy,\n",
    "#         }\n",
    "#         base_config = super(Rescaling_y, self).get_config()\n",
    "#         return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "# inputB = Input(shape=(1,))\n",
    "# Die_xy = u_train[0, 2:3]\n",
    "# inputB_scale = Rescaling_y(Die_xy)(inputB)\n",
    "\n",
    "# out = Rescaling_y(Die_xy)(inputB)\n",
    "############# For TTSS y is t and y scaling will be # y_train = np.log(y_train*10**4)/21.0\n",
    "class Rescaling_y(tf.keras.layers.Layer):\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        self.scale = 1\n",
    "        self.offset = 0.0\n",
    "        super(Rescaling_y, self).__init__(name=name, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        dtype = self._compute_dtype\n",
    "        # inputs = inputs[:, 0:1] / (self.Die_xy / 2.0)\n",
    "        inputs = tf.math.log(inputs[:, 0:1]*10**4)/21.0\n",
    "#         scale = math_ops.cast(self.scale, dtype)\n",
    "#         offset = math_ops.cast(self.offset, dtype)\n",
    "        return math_ops.cast(inputs, dtype)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'scale': self.scale,\n",
    "            'offset': self.offset,\n",
    "        }\n",
    "        base_config = super(Rescaling_y, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "inputB = Input(shape=(1,))\n",
    "inputB_scale = Rescaling_y()(inputB)\n",
    "\n",
    "out = Rescaling_y()(inputB)\n",
    "print('out shape is: ', out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9962249755859375, 0.0, (63000, 9, 9))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_train.max(), s_train.min(),s_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_train_max = np.max(s_train.reshape(-1, unit_size), axis=1)\n",
    "# s_train_max_v = np.empty((0, 1))\n",
    "# for i, s_max in enumerate(s_train_max):\n",
    "#     s_train_max_v = np.vstack((s_train_max_v, np.tile(s_max, (unit_size, 1))))\n",
    "    \n",
    "# s_train_min = np.min(s_train.reshape(-1, unit_size), axis=1)\n",
    "# s_train_min_v = np.empty((0, 1))\n",
    "# for i, s_min in enumerate(s_train_min):\n",
    "#     s_train_min_v = np.vstack((s_train_min_v, np.tile(s_min, (unit_size, 1))))\n",
    "    \n",
    "# s_train = (s_train - s_train_min_v) / (s_train_max_v - s_train_min_v)\n",
    "\n",
    "# s_test_max = np.max(s_test.reshape(-1, unit_size), axis=1)\n",
    "# s_test_max_v = np.empty((0, 1))\n",
    "# for i, s_max in enumerate(s_test_max):\n",
    "#     s_test_max_v = np.vstack((s_test_max_v, np.tile(s_max, (unit_size, 1))))\n",
    "    \n",
    "# s_test_min = np.min(s_test.reshape(-1, unit_size), axis=1)\n",
    "# s_test_min_v = np.empty((0, 1))\n",
    "# for i, s_min in enumerate(s_test_min):\n",
    "#     s_test_min_v = np.vstack((s_test_min_v, np.tile(s_min, (unit_size, 1))))\n",
    "    \n",
    "# s_test = (s_test - s_test_min_v) / (s_test_max_v - s_test_min_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63000, 9, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63000, 4), (63000, 1), 10.1, 1e-08)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_train.shape, y_train.shape, u_train.max(),  u_train.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the net (faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.keras.backend.set_floatx('float64')\n",
    "# import numpy as np\n",
    "from tensorflow.keras import Model, losses\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Multiply, Lambda\n",
    "import time\n",
    "# import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import types\n",
    "# import tempfile\n",
    "# import keras.models\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "# activation = tf.keras.activations.swish\n",
    "activation = tf.keras.activations.relu\n",
    "    \n",
    "# define two sets of inputs\n",
    "# inputA = Input(shape=(6,))\n",
    "inputA = Input(shape=(4,))\n",
    "inputB = Input(shape=(1,))\n",
    "\n",
    "\n",
    "inputA_scale = Rescaling_u()(inputA)\n",
    "# Die_xy = u_train[0, 2:3]\n",
    "# inputB_scale = Rescaling_y(Die_xy)(inputB)\n",
    "\n",
    "inputB_scale = Rescaling_y()(inputB)\n",
    "\n",
    "num_neuron = 32\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x = Dense(num_neuron, activation=activation)(inputA_scale)\n",
    "x = Dense(num_neuron, activation=activation)(x)\n",
    "# x = Dense(num_neuron, activation=activation)(x)\n",
    "# x = Dense(num_neuron, activation=activation)(x)\n",
    "# x = Dense(num_neuron, activation=activation)(x)\n",
    "x = Dense(num_neuron, activation=activation)(x)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "# the second branch opreates on the second input\n",
    "y = Dense(num_neuron, activation=activation)(inputB_scale)\n",
    "y = Dense(num_neuron, activation=activation)(y)\n",
    "# y = Dense(num_neuron, activation=activation)(y)\n",
    "# y = Dense(num_neuron, activation=activation)(y)\n",
    "# y = Dense(num_neuron, activation=activation)(y)\n",
    "y = Dense(num_neuron, activation=activation)(y)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "\n",
    "combined = Multiply()([x.output, y.output])\n",
    "# out = tf.math.reduce_sum(combined, axis = 1)\n",
    "# out = tf.keras.backend.reshape(out, (-1, 1))\n",
    "out = Dense(64, activation=activation)(combined)\n",
    "out = Dense(9*9)(out)\n",
    "out = tf.keras.backend.reshape(out, (-1, 9, 9))\n",
    "\n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = Model(inputs=[x.input, y.input], outputs=out)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "trainEpochVector = []\n",
    "trainLossVector = []\n",
    "testLossVector = []\n",
    "def plotLossHistory(trainEpochVector, trainLossVector, testLossVector):        \n",
    "    font_size = 12\n",
    "    fig_loss, ax_loss = plt.subplots()\n",
    "    plt.rcParams.update({'font.size': font_size})\n",
    "    plt.subplots_adjust(left=0.25, bottom=0.15, right=0.9, top=0.9, wspace=0.1, hspace=0.1)\n",
    "    ax_loss.plot(trainEpochVector, np.log10(trainLossVector), 'b-.')\n",
    "    ax_loss.plot(trainEpochVector, np.log10(testLossVector), 'r-*')\n",
    "    ax_loss.legend(['Train loss', 'Test loss'])\n",
    "    ax_loss.set_xlabel('Epoch')\n",
    "    ax_loss.set_ylabel(\"Log loss\")\n",
    "    ax_loss.title.set_text('Total loss history')\n",
    "    fig_loss_history = fig_fpath + \"LossHistory.png\"\n",
    "    plt.savefig(fig_loss_history)\n",
    "    plt.close()\n",
    "\n",
    "def test_and_save(model, epoch):        \n",
    "    font_size = 40\n",
    "    fig = plt.figure(figsize=(40, 12), dpi = 100, facecolor='white')\n",
    "    plt.rcParams.update({'font.size': font_size})\n",
    "    fig.suptitle('DeepONet on test set', y=0.99)\n",
    "    plt.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.85, wspace=0.25, hspace=0.1)\n",
    "    num_subplot = 1\n",
    "    idx = np.random.choice(u_test.shape[0] // unit_size)\n",
    "    for i in range(num_subplot):            \n",
    "        u_validate = u_test[idx * unit_size: (idx+1)*unit_size, :]\n",
    "        y_validate = y_test[idx * unit_size: (idx+1)*unit_size, :]\n",
    "        s_validate = s_test[idx * unit_size: (idx+1)*unit_size, :]\n",
    "\n",
    "        time_sample = np.random.choice(70)\n",
    "        u_validate = u_validate[time_sample: (time_sample+1), :]\n",
    "        y_validate = y_validate[time_sample: (time_sample+1), :]\n",
    "        s_validate = s_validate[time_sample: (time_sample+1), :, :]\n",
    "        \n",
    "        s_pred = model.predict([u_validate, y_validate])\n",
    "        axs = fig.add_subplot(131)\n",
    "#         im = axs.scatter(u_validate[:unit_size, -2:-1], u_validate[:unit_size, -1:], c = s_validate, cmap = 'jet', alpha = 0.9)\n",
    "#         im = axs.scatter(y_validate[:, 0] / (u_validate[:unit_size, 2:3].flatten() / 2), s_validate, cmap = 'jet', alpha = 0.9)\n",
    "        im = axs.imshow(s_validate[0, :, :], cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"Ground Truth\")\n",
    "        # plt.xlim(0, 0.02)\n",
    "        plt.colorbar(im)\n",
    "\n",
    "        axs = fig.add_subplot(132)\n",
    "#         im = axs.scatter(u_validate[:unit_size, -2:-1], u_validate[:unit_size, -1:], c = s_pred, cmap = 'jet', alpha = 0.9)\n",
    "        # im = axs.scatter(y_validate[:, 0] / (u_validate[:unit_size, 2:3].flatten() / 2), s_pred, cmap = 'jet', alpha = 0.9)\n",
    "        im = axs.imshow(s_pred[0, :, :], cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"DeepONet Prediction\")\n",
    "        # plt.xlim(0, 0.02)\n",
    "        plt.colorbar(im)\n",
    "\n",
    "        axs = fig.add_subplot(133)\n",
    "#         im = axs.scatter(u_validate[:unit_size, -2:-1], u_validate[:unit_size, -1:], c = np.abs(s_pred - s_validate), cmap = 'jet', alpha = 0.9)\n",
    "        # im = axs.scatter(y_validate[:, 0] / (u_validate[:unit_size, 2:3].flatten() / 2), np.abs(s_pred - s_validate), cmap = 'jet', alpha = 0.9)\n",
    "        im = axs.imshow(np.abs(s_pred[0, :, :] - s_validate[0, :, :]), cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"DeepONet Error\")\n",
    "        # plt.xlim(0, 0.02)\n",
    "        plt.colorbar(im)\n",
    "\n",
    "    path = f\"./DeepONet_v0{trial}/fig/\"\n",
    "    fig_test_name = path + \"Test_tf_pred_epoch_{0}.png\".format(epoch)\n",
    "    plt.savefig(fig_test_name)\n",
    "    plt.close()\n",
    "        \n",
    "def validate_and_save(model, epoch):        \n",
    "    font_size = 40\n",
    "    fig = plt.figure(figsize=(40, 12), dpi = 100, facecolor='white')\n",
    "    plt.rcParams.update({'font.size': font_size})\n",
    "\n",
    "    fig.suptitle('DeepONet on training set', y=0.99)\n",
    "    plt.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.85, wspace=0.25, hspace=0.1)\n",
    "\n",
    "    num_subplot = 1\n",
    "    idx = np.random.choice(u_train.shape[0] // unit_size)\n",
    "\n",
    "    for i in range(num_subplot):   \n",
    "        u_validate = u_train[idx * unit_size: (idx+1)*unit_size, :]\n",
    "        y_validate = y_train[idx * unit_size: (idx+1)*unit_size, :]\n",
    "        s_validate = s_train[idx * unit_size: (idx+1)*unit_size, :]\n",
    "        \n",
    "        time_sample = np.random.choice(70)\n",
    "        u_validate = u_validate[time_sample: (time_sample+1), :]\n",
    "        y_validate = y_validate[time_sample: (time_sample+1), :]\n",
    "        s_validate = s_validate[time_sample: (time_sample+1), :, :]\n",
    "        \n",
    "\n",
    "#         tf_dict = {self.X_func_tf: u_validate, self.X_loc_tf: y_validate} \n",
    "#         s_pred = self.sess.run(self.y_pred, tf_dict)\n",
    "        s_pred = model.predict([u_validate, y_validate])\n",
    "\n",
    "        axs = fig.add_subplot(131)\n",
    "        # im = axs.scatter(y_validate[:, 0] / (u_validate[:unit_size, 2:3].flatten() / 2), s_validate, cmap = 'jet', alpha = 0.9)\n",
    "#         im = axs.scatter(u_validate[:unit_size, -2:-1], u_validate[:unit_size, -1:], c = s_validate, cmap = 'jet', alpha = 0.9)\n",
    "        im = axs.imshow(s_validate[0, :, :], cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"Ground Truth\")\n",
    "        # plt.xlim(0, 0.02)\n",
    "        plt.colorbar(im)\n",
    "\n",
    "        axs = fig.add_subplot(132)\n",
    "#         im = axs.scatter(u_validate[:unit_size, -2:-1], u_validate[:unit_size, -1:], c = s_pred, cmap = 'jet', alpha = 0.9)\n",
    "        # im = axs.scatter(y_validate[:, 0] / (u_validate[:unit_size, 2:3].flatten() / 2), s_pred, cmap = 'jet', alpha = 0.9)\n",
    "        im = axs.imshow(s_pred[0, :, :], cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"DeepONet Prediction\")\n",
    "        # plt.xlim(0, 0.02)\n",
    "        plt.colorbar(im)\n",
    "\n",
    "        axs = fig.add_subplot(133)\n",
    "#         im = axs.scatter(u_validate[:unit_size, -2:-1], u_validate[:unit_size, -1:], c = np.abs(s_pred - s_validate), cmap = 'jet', alpha = 0.9)\n",
    "        # im = axs.scatter(y_validate[:, 0] / (u_validate[:unit_size, 2:3].flatten() / 2), np.abs(s_pred - s_validate), cmap = 'jet', alpha = 0.9)\n",
    "        im = axs.imshow(np.abs(s_pred[0, :, :] - s_validate[0, :, :]), cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"DeepONet Error\")\n",
    "        # plt.xlim(0, 0.02)\n",
    "        plt.colorbar(im)\n",
    "\n",
    "    path = f\"./DeepONet_v0{trial}/fig/\"\n",
    "    fig_test_name = path + \"Validation_tf_pred_epoch_{0}.png\".format(epoch)\n",
    "    plt.savefig(fig_test_name)\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    \n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "my_callbacks = [\n",
    "#     tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "#     tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_dir + 'model.{epoch:02d}-{val_loss:.2f}.h5',save_freq='epoch'),\\\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath = path + f\"/DeepONet_v0{trial}/Checkpoint/model.h5\", monitor='val_loss',\\\n",
    "                                       save_weights_only=False, save_best_only=True, save_freq='epoch'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir = path + f\"/DeepONet_v0{trial}/logs/\")\n",
    "    # lr_callback\n",
    "]\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "with tf.device('/device:GPU:1'):\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "    EPOCH = 25000\n",
    "    \n",
    "    load_model_plot = True\n",
    "    load_model_and_train = True\n",
    "    if load_model_plot:\n",
    "        model.built = True\n",
    "        model.load_weights(path + f\"/DeepONet_v0{trial}/Checkpoint/\" + 'model.h5')\n",
    "    elif load_model_and_train:\n",
    "        model.built = True\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        model.load_weights(path + f\"/DeepONet_v0{trial}/Checkpoint/\" + 'model.h5')\n",
    "#         history = model.fit([u_train, y_train], s_train, batch_size=u_train.shape[0],\\\n",
    "#                  validation_data=([u_test, y_test], s_test), callbacks=my_callbacks, shuffle=True, epochs=EPOCH, verbose=1)\n",
    "        history = model.fit([u_train, y_train], s_train, batch_size=256,\\\n",
    "                 validation_data=([u_test, y_test], s_test), callbacks=my_callbacks, shuffle=True, epochs=EPOCH, verbose=1)\n",
    "\n",
    "        # train_loss = history.history['loss']\n",
    "        # val_loss = history.history['val_loss']\n",
    "\n",
    "        # fig = plt.figure()\n",
    "        # ax = fig.add_subplot(111)\n",
    "        # ax.semilogy(train_loss)\n",
    "        # ax.semilogy(val_loss)\n",
    "        # ax.legend(['train loss', 'val loss'])\n",
    "        # plt.savefig(work_dir + 'loss')\n",
    "        \n",
    "        font_size = 12\n",
    "        plt.rcParams.update({'font.size': font_size})\n",
    "        train_loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.semilogy(train_loss)\n",
    "        ax.semilogy(val_loss)\n",
    "        ax.legend(['train', 'validation'])\n",
    "        plt.savefig(path + f\"/DeepONet_v0{trial}/\" + 'loss_history')\n",
    "    else:\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "#         history = model.fit([u_train, y_train], s_train, batch_size=u_train.shape[0],\\\n",
    "#                  validation_data=([u_test, y_test], s_test), callbacks=my_callbacks, shuffle=True, epochs=EPOCH, verbose=1)\n",
    "        history = model.fit([u_train, y_train], s_train, batch_size=256,\\\n",
    "                 validation_data=([u_test, y_test], s_test), callbacks=my_callbacks, shuffle=True, epochs=EPOCH, verbose=1)\n",
    "\n",
    "        train_loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.semilogy(train_loss)\n",
    "        ax.semilogy(val_loss)\n",
    "        ax.legend(['train loss', 'val loss'])\n",
    "        plt.savefig(path + f\"/DeepONet_v0{trial}/\" + 'loss')\n",
    "    \n",
    "    # plotLossHistory(trainEpochVector, trainLossVector, testLossVector)\n",
    "    test_and_save(model, EPOCH)\n",
    "    validate_and_save(model, EPOCH)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_and_save' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c968ef02e82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidate_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_and_save' is not defined"
     ]
    }
   ],
   "source": [
    "test_and_save(model, EPOCH)\n",
    "validate_and_save(model, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative_l2_test is:  0.006627443515074852\n",
      "relative_l2_train is:  0.009019062633910184\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "# load the model from disk\n",
    "loaded_model = model.load_weights(path + f\"/DeepONet_v0{trial}/Checkpoint/\" + 'model.h5')\n",
    "\n",
    "# X_train = params_train\n",
    "# Y_train = temp_max_train \n",
    "\n",
    "# result = loaded_model.score(X_train, Y_train)\n",
    "# print(result)\n",
    "\n",
    "pred_all = model.predict([u_train, y_train])\n",
    "ground_truth_all = s_train\n",
    "relative_l2_test = np.linalg.norm(ground_truth_all.flatten() - \\\n",
    "            pred_all.flatten()) / np.linalg.norm(ground_truth_all.flatten())\n",
    "print('relative_l2_test is: ', relative_l2_test)\n",
    "\n",
    "pred_all = model.predict([u_test, y_test])\n",
    "ground_truth_all = s_test\n",
    "relative_l2_train = np.linalg.norm(ground_truth_all.flatten() -\\\n",
    "            pred_all.flatten()) / np.linalg.norm(ground_truth_all.flatten())\n",
    "print('relative_l2_train is: ', relative_l2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_save(model, EPOCH)\n",
    "validate_and_save(model, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((180000, 6), (180000, 1), (180000, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_train.shape, y_train.shape, s_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.00e-06, 4.50e+00, 3.70e-02, 8.33e-02, 3.70e-02, 8.33e-02])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_test[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0, 1.0, 0.5179503067680027)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_test.max(), y_test.max(), s_test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25000\n",
      "  1/704 [..............................] - ETA: 0s - loss: 0.0700"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.keras.backend.set_floatx('float64')\n",
    "# import numpy as np\n",
    "from tensorflow.keras import Model, losses\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Multiply, Lambda\n",
    "import time\n",
    "# import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import types\n",
    "# import tempfile\n",
    "# import keras.models\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "# activation = tf.keras.activations.swish\n",
    "activation = tf.keras.activations.relu\n",
    "    \n",
    "# define two sets of inputs\n",
    "inputA = Input(shape=(6,))\n",
    "inputB = Input(shape=(1,))\n",
    "\n",
    "\n",
    "inputA_scale = Rescaling_u()(inputA)\n",
    "# Die_xy = u_train[0, 2:3]\n",
    "# inputB_scale = Rescaling_y(Die_xy)(inputB)\n",
    "\n",
    "inputB_scale = Rescaling_y()(inputB)\n",
    "\n",
    "num_neuron = 32\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x = Dense(num_neuron, activation=activation)(inputA_scale)\n",
    "# x = Dense(num_neuron, activation=activation)(x)\n",
    "# x = Dense(num_neuron, activation=activation)(x)\n",
    "# x = Dense(num_neuron, activation=activation)(x)\n",
    "# x = Dense(num_neuron, activation=activation)(x)\n",
    "x = Dense(num_neuron, activation=activation)(x)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "# the second branch opreates on the second input\n",
    "y = Dense(num_neuron, activation=activation)(inputB_scale)\n",
    "# y = Dense(num_neuron, activation=activation)(y)\n",
    "# y = Dense(num_neuron, activation=activation)(y)\n",
    "# y = Dense(num_neuron, activation=activation)(y)\n",
    "# y = Dense(num_neuron, activation=activation)(y)\n",
    "y = Dense(num_neuron, activation=activation)(y)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "\n",
    "combined = Multiply()([x.output, y.output])\n",
    "out = tf.math.reduce_sum(combined, axis = 1)\n",
    "out = tf.keras.backend.reshape(out, (-1, 1))\n",
    "# out = Dense(128, activation=activation)(combined)\n",
    "# out = Dense(21*21)(out)\n",
    "# out = tf.keras.backend.reshape(out, (-1, 1))\n",
    "\n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = Model(inputs=[x.input, y.input], outputs=out)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "trainEpochVector = []\n",
    "trainLossVector = []\n",
    "testLossVector = []\n",
    "def plotLossHistory(trainEpochVector, trainLossVector, testLossVector):        \n",
    "    font_size = 12\n",
    "    fig_loss, ax_loss = plt.subplots()\n",
    "    plt.rcParams.update({'font.size': font_size})\n",
    "    plt.subplots_adjust(left=0.25, bottom=0.15, right=0.9, top=0.9, wspace=0.1, hspace=0.1)\n",
    "    ax_loss.plot(trainEpochVector, np.log10(trainLossVector), 'b-.')\n",
    "    ax_loss.plot(trainEpochVector, np.log10(testLossVector), 'r-*')\n",
    "    ax_loss.legend(['Train loss', 'Test loss'])\n",
    "    ax_loss.set_xlabel('Epoch')\n",
    "    ax_loss.set_ylabel(\"Log loss\")\n",
    "    ax_loss.title.set_text('Total loss history')\n",
    "    fig_loss_history = fig_fpath + \"LossHistory.png\"\n",
    "    plt.savefig(fig_loss_history)\n",
    "    plt.close()\n",
    "\n",
    "def test_and_save(model, epoch):        \n",
    "    font_size = 40\n",
    "    fig = plt.figure(figsize=(40, 12), dpi = 100)\n",
    "    plt.rcParams.update({'font.size': font_size})\n",
    "    fig.suptitle('DeepONet on test set', y=0.99)\n",
    "    plt.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.85, wspace=0.25, hspace=0.1)\n",
    "    num_subplot = 1\n",
    "    idx = np.random.choice(u_test.shape[0] // unit_size)\n",
    "    for i in range(num_subplot):            \n",
    "        u_validate = u_test[idx * unit_size: (idx+1)*unit_size, :]\n",
    "        y_validate = y_test[idx * unit_size: (idx+1)*unit_size, :]\n",
    "        s_validate = s_test[idx * unit_size: (idx+1)*unit_size, :, :]\n",
    "\n",
    "#         time_sample = np.random.choice(70)\n",
    "#         u_validate = u_validate[time_sample: (time_sample+1), :]\n",
    "#         y_validate = y_validate[time_sample: (time_sample+1), :]\n",
    "#         s_validate = s_validate[time_sample: (time_sample+1), :, :]\n",
    "        \n",
    "        s_pred = model.predict([u_validate, y_validate])\n",
    "        axs = fig.add_subplot(131)\n",
    "        im = axs.scatter(u_validate[:unit_size, -2:-1], u_validate[:unit_size, -1:], c = s_validate, cmap = 'jet', alpha = 0.9)\n",
    "#         im = axs.scatter(y_validate[:, 0] / (u_validate[:unit_size, 2:3].flatten() / 2), s_validate, cmap = 'jet', alpha = 0.9)\n",
    "#         im = axs.imshow(s_validate[0, :, :], cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"Ground Truth\")\n",
    "        # plt.xlim(0, 0.02)\n",
    "        plt.colorbar(im)\n",
    "\n",
    "        axs = fig.add_subplot(132)\n",
    "        im = axs.scatter(u_validate[:unit_size, -2:-1], u_validate[:unit_size, -1:], c = s_pred, cmap = 'jet', alpha = 0.9)\n",
    "        # im = axs.scatter(y_validate[:, 0] / (u_validate[:unit_size, 2:3].flatten() / 2), s_pred, cmap = 'jet', alpha = 0.9)\n",
    "        im = axs.imshow(s_pred[0, :, :], cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"DeepONet Prediction\")\n",
    "        # plt.xlim(0, 0.02)\n",
    "        plt.colorbar(im)\n",
    "\n",
    "        axs = fig.add_subplot(133)\n",
    "        im = axs.scatter(u_validate[:unit_size, -2:-1], u_validate[:unit_size, -1:], c = np.abs(s_pred - s_validate), cmap = 'jet', alpha = 0.9)\n",
    "        # im = axs.scatter(y_validate[:, 0] / (u_validate[:unit_size, 2:3].flatten() / 2), np.abs(s_pred - s_validate), cmap = 'jet', alpha = 0.9)\n",
    "#         im = axs.imshow(np.abs(s_pred[0, :, :] - s_validate[0, :, :]), cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"DeepONet Error\")\n",
    "        # plt.xlim(0, 0.02)\n",
    "        plt.colorbar(im)\n",
    "\n",
    "    path = f\"./DeepONet_v0{trial}/fig/\"\n",
    "    fig_test_name = path + \"Test_tf_pred_epoch_{0}.png\".format(epoch)\n",
    "    plt.savefig(fig_test_name)\n",
    "    plt.close()\n",
    "        \n",
    "def validate_and_save(model, epoch):        \n",
    "    font_size = 40\n",
    "    fig = plt.figure(figsize=(40, 12), dpi = 100)\n",
    "    plt.rcParams.update({'font.size': font_size})\n",
    "\n",
    "    fig.suptitle('DeepONet on training set', y=0.99)\n",
    "    plt.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.85, wspace=0.25, hspace=0.1)\n",
    "\n",
    "    num_subplot = 1\n",
    "    idx = np.random.choice(u_train.shape[0] // unit_size)\n",
    "\n",
    "    for i in range(num_subplot):   \n",
    "        u_validate = u_train[idx * unit_size: (idx+1)*unit_size, :]\n",
    "        y_validate = y_train[idx * unit_size: (idx+1)*unit_size, :]\n",
    "        s_validate = s_train[idx * unit_size: (idx+1)*unit_size, :, :]\n",
    "        \n",
    "#         time_sample = np.random.choice(70)\n",
    "#         u_validate = u_validate[time_sample: (time_sample+1), :]\n",
    "#         y_validate = y_validate[time_sample: (time_sample+1), :]\n",
    "#         s_validate = s_validate[time_sample: (time_sample+1), :, :]\n",
    "        \n",
    "\n",
    "#         tf_dict = {self.X_func_tf: u_validate, self.X_loc_tf: y_validate} \n",
    "#         s_pred = self.sess.run(self.y_pred, tf_dict)\n",
    "        s_pred = model.predict([u_validate, y_validate])\n",
    "\n",
    "        axs = fig.add_subplot(131)\n",
    "        # im = axs.scatter(y_validate[:, 0] / (u_validate[:unit_size, 2:3].flatten() / 2), s_validate, cmap = 'jet', alpha = 0.9)\n",
    "        im = axs.scatter(u_validate[:unit_size, -2:-1], u_validate[:unit_size, -1:], c = s_validate, cmap = 'jet', alpha = 0.9)\n",
    "#         im = axs.imshow(s_validate[0, :, :], cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"Ground Truth\")\n",
    "        # plt.xlim(0, 0.02)\n",
    "        plt.colorbar(im)\n",
    "\n",
    "        axs = fig.add_subplot(132)\n",
    "        im = axs.scatter(u_validate[:unit_size, -2:-1], u_validate[:unit_size, -1:], c = s_pred, cmap = 'jet', alpha = 0.9)\n",
    "        # im = axs.scatter(y_validate[:, 0] / (u_validate[:unit_size, 2:3].flatten() / 2), s_pred, cmap = 'jet', alpha = 0.9)\n",
    "#         im = axs.imshow(s_pred[0, :, :], cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"DeepONet Prediction\")\n",
    "        # plt.xlim(0, 0.02)\n",
    "        plt.colorbar(im)\n",
    "\n",
    "        axs = fig.add_subplot(133)\n",
    "        im = axs.scatter(u_validate[:unit_size, -2:-1], u_validate[:unit_size, -1:], c = np.abs(s_pred - s_validate), cmap = 'jet', alpha = 0.9)\n",
    "        # im = axs.scatter(y_validate[:, 0] / (u_validate[:unit_size, 2:3].flatten() / 2), np.abs(s_pred - s_validate), cmap = 'jet', alpha = 0.9)\n",
    "#         im = axs.imshow(np.abs(s_pred[0, :, :] - s_validate[0, :, :]), cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"DeepONet Error\")\n",
    "        # plt.xlim(0, 0.02)\n",
    "        plt.colorbar(im)\n",
    "\n",
    "    path = f\"./DeepONet_v0{trial}/fig/\"\n",
    "    fig_test_name = path + \"Validation_tf_pred_epoch_{0}.png\".format(epoch)\n",
    "    plt.savefig(fig_test_name)\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    \n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "my_callbacks = [\n",
    "#     tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "#     tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_dir + 'model.{epoch:02d}-{val_loss:.2f}.h5',save_freq='epoch'),\\\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath = path + f\"/DeepONet_v0{trial}/Checkpoint/model.h5\", monitor='val_loss',\\\n",
    "                                       save_weights_only=False, save_best_only=True, save_freq='epoch'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir = path + f\"/DeepONet_v0{trial}/logs/\")\n",
    "    # lr_callback\n",
    "]\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "with tf.device('/device:GPU:1'):\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "    EPOCH = 25000\n",
    "    \n",
    "    load_model_plot = False\n",
    "    load_model_and_train = False\n",
    "    if load_model_plot:\n",
    "        model.built = True\n",
    "        model.load_weights(path + f\"/DeepONet_v0{trial}/Checkpoint/\" + 'model.h5')\n",
    "    elif load_model_and_train:\n",
    "        model.built = True\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        model.load_weights(path + f\"/DeepONet_v0{trial}/Checkpoint/\" + 'model.h5')\n",
    "#         history = model.fit([u_train, y_train], s_train, batch_size=u_train.shape[0],\\\n",
    "#                  validation_data=([u_test, y_test], s_test), callbacks=my_callbacks, shuffle=True, epochs=EPOCH, verbose=1)\n",
    "        history = model.fit([u_train, y_train], s_train, batch_size=256,\\\n",
    "                 validation_data=([u_test, y_test], s_test), callbacks=my_callbacks, shuffle=True, epochs=EPOCH, verbose=1)\n",
    "\n",
    "        # train_loss = history.history['loss']\n",
    "        # val_loss = history.history['val_loss']\n",
    "\n",
    "        # fig = plt.figure()\n",
    "        # ax = fig.add_subplot(111)\n",
    "        # ax.semilogy(train_loss)\n",
    "        # ax.semilogy(val_loss)\n",
    "        # ax.legend(['train loss', 'val loss'])\n",
    "        # plt.savefig(work_dir + 'loss')\n",
    "        \n",
    "        font_size = 12\n",
    "        plt.rcParams.update({'font.size': font_size})\n",
    "        train_loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.semilogy(train_loss)\n",
    "        ax.semilogy(val_loss)\n",
    "        ax.legend(['train', 'validation'])\n",
    "        plt.savefig(path + f\"/DeepONet_v0{trial}/\" + 'loss_history')\n",
    "    else:\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "#         history = model.fit([u_train, y_train], s_train, batch_size=u_train.shape[0],\\\n",
    "#                  validation_data=([u_test, y_test], s_test), callbacks=my_callbacks, shuffle=True, epochs=EPOCH, verbose=1)\n",
    "        history = model.fit([u_train, y_train], s_train, batch_size=256,\\\n",
    "                 validation_data=([u_test, y_test], s_test), callbacks=my_callbacks, shuffle=True, epochs=EPOCH, verbose=1)\n",
    "\n",
    "        train_loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.semilogy(train_loss)\n",
    "        ax.semilogy(val_loss)\n",
    "        ax.legend(['train loss', 'val loss'])\n",
    "        plt.savefig(work_dir + 'loss')\n",
    "    \n",
    "    # plotLossHistory(trainEpochVector, trainLossVector, testLossVector)\n",
    "    test_and_save(model, EPOCH)\n",
    "    validate_and_save(model, EPOCH)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_and_save' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a72a563fad5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidate_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_and_save' is not defined"
     ]
    }
   ],
   "source": [
    "test_and_save(model, EPOCH)\n",
    "validate_and_save(model, EPOCH)\n",
    "\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Relative L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative_l2_test is:  0.48124542039140467\n",
      "relative_l2_train is:  0.48025697728319217\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "# load the model from disk\n",
    "loaded_model = model.load_weights(path + f\"/DeepONet_v0{trial}/Checkpoint/\" + 'model.h5')\n",
    "\n",
    "# X_train = params_train\n",
    "# Y_train = temp_max_train \n",
    "\n",
    "# result = loaded_model.score(X_train, Y_train)\n",
    "# print(result)\n",
    "\n",
    "pred_all = model.predict([u_train, y_train])\n",
    "ground_truth_all = s_train\n",
    "relative_l2_test = np.linalg.norm(ground_truth_all.flatten() - \\\n",
    "            pred_all.flatten()) / np.linalg.norm(ground_truth_all.flatten())\n",
    "print('relative_l2_test is: ', relative_l2_test)\n",
    "\n",
    "pred_all = model.predict([u_test, y_test])\n",
    "ground_truth_all = s_test\n",
    "relative_l2_train = np.linalg.norm(ground_truth_all.flatten() -\\\n",
    "            pred_all.flatten()) / np.linalg.norm(ground_truth_all.flatten())\n",
    "print('relative_l2_train is: ', relative_l2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05289646931303482"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "_, axs = plt.subplots(1,2)\n",
    "im1 = axs[0].imshow(s_train[35, :, :])\n",
    "# plt.colorbar(im1)\n",
    "im2 = axs[1].imshow(pred_all[35, :, :])\n",
    "# plt.colorbar(im2)\n",
    "\n",
    "np.linalg.norm(s_train[35, :, :].flatten() -\\\n",
    "            pred_all[35, :, :].flatten()) / np.linalg.norm(s_train[35, :, :].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa4b04a9f40>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3087000,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_all.flatten() - pred_all.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3087000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_all.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63000, 21, 21), (63000, 21, 21))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_all.shape, pred_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First save it to a .pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /work/HaiyangHe/ThermalArtist/DecaySurface/210um/DeepONet_v01/Checkpoint/saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "# model.save(ckpt_fpath[:-8] + \"saved_model\")\n",
    "model.save(ckpt_fpath[:-8] + \"saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then save it to a tflite file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/HaiyangHe/ThermalArtist/DecaySurface/210um/DeepONet_v01/Checkpoint/'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_fpath[:-8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_183_layer_call_and_return_conditional_losses_133951) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_183_layer_call_and_return_conditional_losses_133951) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_188_layer_call_and_return_conditional_losses_133926) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_188_layer_call_and_return_conditional_losses_133926) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_185_layer_call_and_return_conditional_losses_132584) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_185_layer_call_and_return_conditional_losses_132584) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_188_layer_call_and_return_conditional_losses_132360) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_188_layer_call_and_return_conditional_losses_132360) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_195_layer_call_and_return_conditional_losses_132791) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_195_layer_call_and_return_conditional_losses_132791) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_193_layer_call_and_return_conditional_losses_134176) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_193_layer_call_and_return_conditional_losses_134176) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_182_layer_call_and_return_conditional_losses_132392) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_182_layer_call_and_return_conditional_losses_132392) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_132270) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_187_layer_call_and_return_conditional_losses_134151) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_187_layer_call_and_return_conditional_losses_134151) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_185_layer_call_and_return_conditional_losses_134051) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_185_layer_call_and_return_conditional_losses_134051) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_194_layer_call_and_return_conditional_losses_134213) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_194_layer_call_and_return_conditional_losses_134213) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_191_layer_call_and_return_conditional_losses_132552) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_191_layer_call_and_return_conditional_losses_132552) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_191_layer_call_and_return_conditional_losses_134076) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_191_layer_call_and_return_conditional_losses_134076) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_193_layer_call_and_return_conditional_losses_132712) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_193_layer_call_and_return_conditional_losses_132712) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_186_layer_call_and_return_conditional_losses_134101) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_186_layer_call_and_return_conditional_losses_134101) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_189_layer_call_and_return_conditional_losses_133976) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_189_layer_call_and_return_conditional_losses_133976) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_184_layer_call_and_return_conditional_losses_132520) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_184_layer_call_and_return_conditional_losses_132520) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133698) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_190_layer_call_and_return_conditional_losses_134026) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_190_layer_call_and_return_conditional_losses_134026) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_182_layer_call_and_return_conditional_losses_133901) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_182_layer_call_and_return_conditional_losses_133901) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_186_layer_call_and_return_conditional_losses_132648) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_186_layer_call_and_return_conditional_losses_132648) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_190_layer_call_and_return_conditional_losses_132488) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_190_layer_call_and_return_conditional_losses_132488) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_195_layer_call_and_return_conditional_losses_134238) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_195_layer_call_and_return_conditional_losses_134238) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_187_layer_call_and_return_conditional_losses_132680) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_187_layer_call_and_return_conditional_losses_132680) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_194_layer_call_and_return_conditional_losses_132759) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_194_layer_call_and_return_conditional_losses_132759) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_192_layer_call_and_return_conditional_losses_132616) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_192_layer_call_and_return_conditional_losses_132616) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_183_layer_call_and_return_conditional_losses_132456) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_183_layer_call_and_return_conditional_losses_132456) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_model_41_layer_call_and_return_conditional_losses_133477) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_184_layer_call_and_return_conditional_losses_134001) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_184_layer_call_and_return_conditional_losses_134001) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_189_layer_call_and_return_conditional_losses_132424) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_189_layer_call_and_return_conditional_losses_132424) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_192_layer_call_and_return_conditional_losses_134126) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_dense_192_layer_call_and_return_conditional_losses_134126) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert the model.\n",
    "# converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(ckpt_fpath)\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(ckpt_fpath[:-8] + 'saved_model')\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model(ckpt_fpath)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(ckpt_fpath[:-8] + 'DeepONet_model_full_data.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(601000, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "# interpreter = tf.lite.Interpreter(model_path=ckpt_fpath[:-8] + \"model.tflite\")\n",
    "# interpreter = tf.lite.Interpreter(model_path=ckpt_fpath + \"model.tflite\")\n",
    "interpreter = tf.lite.Interpreter(model_path=ckpt_fpath + \"DeepONet_model_full_data.tflite\")\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "pred = np.empty((0, 1))\n",
    "for i in range(u_train.shape[0]):\n",
    "# for i in range(unit_size):\n",
    "    u_tr = u_train[i:i+1, :]\n",
    "    y_tr = y_train[i:i+1, :]\n",
    "    # Test the model on random input data.\n",
    "    input_shape_1 = input_details[0]['shape']\n",
    "    input_data_1 = np.array(u_tr, dtype=np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data_1)\n",
    "\n",
    "    input_shape_2 = input_details[1]['shape']\n",
    "    input_data_2 = np.array(y_tr, dtype=np.float32)\n",
    "    interpreter.set_tensor(input_details[1]['index'], input_data_2)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # The function `get_tensor()` returns a copy of the tensor data.\n",
    "    # Use `tensor()` in order to get a pointer to the tensor.\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    pred = np.vstack((pred, output_data))\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0074985027313232"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.001994352787733078"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = ((s_train- s_train.min()) / (s_train.max() - s_train.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2518351718211433"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(aaa - pred)/np.linalg.norm(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2518351718211433"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(pred - (s_train- s_train.min()) / (s_train.max() - s_train.min())) / np.linalg.norm((s_train- s_train.min()) / (s_train.max() - s_train.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3e6022f320>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(y_train[i*unit_size:(i+1) * unit_size, :].flatten(), pred.flatten())\n",
    "ax.scatter(y_train[i*unit_size:(i+1) * unit_size, :].flatten(), s_train[i*unit_size:(i+1) * unit_size, :].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19477734]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=ckpt_fpath[:-8] + \"model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape_1 = input_details[0]['shape']\n",
    "input_data_1 = np.array(np.random.random_sample(input_shape_1), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data_1)\n",
    "\n",
    "input_shape_2 = input_details[1]['shape']\n",
    "input_data_2 = np.array(np.random.random_sample(input_shape_2), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[1]['index'], input_data_2)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, elapsed: 1.36e+00, train loss: 2.87828e-02, val loss: 2.86491e-02, min loss: 2.87828e-02\n",
      "Epoch: 100, elapsed: 1.17e+01, train loss: 1.78073e-02, val loss: 1.78074e-02, min loss: 1.78073e-02\n",
      "Epoch: 200, elapsed: 1.01e+01, train loss: 1.68342e-02, val loss: 1.69228e-02, min loss: 1.68342e-02\n",
      "Epoch: 300, elapsed: 9.62e+00, train loss: 1.59022e-02, val loss: 1.59875e-02, min loss: 1.59022e-02\n",
      "Epoch: 400, elapsed: 9.73e+00, train loss: 1.41949e-02, val loss: 1.41820e-02, min loss: 1.41949e-02\n",
      "Epoch: 500, elapsed: 9.61e+00, train loss: 8.85103e-03, val loss: 8.70470e-03, min loss: 8.85103e-03\n",
      "Epoch: 600, elapsed: 9.76e+00, train loss: 5.74908e-03, val loss: 5.64450e-03, min loss: 5.74908e-03\n",
      "Epoch: 700, elapsed: 9.82e+00, train loss: 3.83678e-03, val loss: 3.79904e-03, min loss: 3.83678e-03\n",
      "Epoch: 800, elapsed: 9.63e+00, train loss: 2.69628e-03, val loss: 2.67551e-03, min loss: 2.69628e-03\n",
      "Epoch: 900, elapsed: 9.70e+00, train loss: 1.90138e-03, val loss: 1.91621e-03, min loss: 1.90138e-03\n",
      "Epoch: 1000, elapsed: 9.61e+00, train loss: 1.53725e-03, val loss: 1.55500e-03, min loss: 1.53725e-03\n",
      "Epoch: 1100, elapsed: 9.79e+00, train loss: 1.28285e-03, val loss: 1.31285e-03, min loss: 1.28285e-03\n",
      "Epoch: 1200, elapsed: 9.61e+00, train loss: 1.72315e-03, val loss: 1.16952e-03, min loss: 1.28285e-03\n",
      "Epoch: 1300, elapsed: 1.02e+01, train loss: 1.05778e-03, val loss: 1.09920e-03, min loss: 1.05778e-03\n",
      "Epoch: 1400, elapsed: 1.00e+01, train loss: 1.29732e-03, val loss: 1.36818e-03, min loss: 1.05778e-03\n",
      "Epoch: 1500, elapsed: 1.04e+01, train loss: 9.10774e-04, val loss: 9.72835e-04, min loss: 9.10774e-04\n",
      "Epoch: 1600, elapsed: 1.03e+01, train loss: 8.72786e-04, val loss: 9.19789e-04, min loss: 8.72786e-04\n",
      "Epoch: 1700, elapsed: 1.01e+01, train loss: 7.94495e-04, val loss: 8.58953e-04, min loss: 7.94495e-04\n",
      "Epoch: 1800, elapsed: 1.03e+01, train loss: 7.81719e-04, val loss: 8.63201e-04, min loss: 7.81719e-04\n",
      "Epoch: 1900, elapsed: 1.02e+01, train loss: 7.43518e-04, val loss: 8.02868e-04, min loss: 7.43518e-04\n",
      "Epoch: 2000, elapsed: 1.02e+01, train loss: 7.29816e-04, val loss: 7.81851e-04, min loss: 7.29816e-04\n",
      "Epoch: 2100, elapsed: 1.01e+01, train loss: 7.40783e-04, val loss: 8.02294e-04, min loss: 7.29816e-04\n",
      "Epoch: 2200, elapsed: 1.04e+01, train loss: 6.91017e-04, val loss: 7.49088e-04, min loss: 6.91017e-04\n",
      "Epoch: 2300, elapsed: 9.96e+00, train loss: 6.78710e-04, val loss: 7.18243e-04, min loss: 6.78710e-04\n",
      "Epoch: 2400, elapsed: 1.04e+01, train loss: 6.62350e-04, val loss: 7.01033e-04, min loss: 6.62350e-04\n",
      "Epoch: 2500, elapsed: 1.02e+01, train loss: 6.46540e-04, val loss: 6.89565e-04, min loss: 6.46540e-04\n",
      "Epoch: 2600, elapsed: 1.03e+01, train loss: 6.39302e-04, val loss: 6.81645e-04, min loss: 6.39302e-04\n",
      "Epoch: 2700, elapsed: 1.05e+01, train loss: 6.15200e-04, val loss: 6.45526e-04, min loss: 6.15200e-04\n",
      "Epoch: 2800, elapsed: 1.03e+01, train loss: 6.01189e-04, val loss: 6.28340e-04, min loss: 6.01189e-04\n",
      "Epoch: 2900, elapsed: 1.05e+01, train loss: 6.12987e-04, val loss: 6.57028e-04, min loss: 6.01189e-04\n",
      "Epoch: 3000, elapsed: 1.03e+01, train loss: 7.13736e-04, val loss: 7.78181e-04, min loss: 6.01189e-04\n",
      "Epoch: 3100, elapsed: 1.06e+01, train loss: 5.98066e-04, val loss: 6.26127e-04, min loss: 5.98066e-04\n",
      "Epoch: 3200, elapsed: 1.02e+01, train loss: 5.68978e-04, val loss: 5.87264e-04, min loss: 5.68978e-04\n",
      "Epoch: 3300, elapsed: 1.05e+01, train loss: 5.57537e-04, val loss: 5.66321e-04, min loss: 5.57537e-04\n",
      "Epoch: 3400, elapsed: 1.03e+01, train loss: 5.49495e-04, val loss: 5.59513e-04, min loss: 5.49495e-04\n",
      "Epoch: 3500, elapsed: 1.05e+01, train loss: 5.39451e-04, val loss: 5.43762e-04, min loss: 5.39451e-04\n",
      "Epoch: 3600, elapsed: 1.05e+01, train loss: 5.50853e-04, val loss: 5.69308e-04, min loss: 5.39451e-04\n",
      "Epoch: 3700, elapsed: 1.05e+01, train loss: 5.38780e-04, val loss: 5.43056e-04, min loss: 5.38780e-04\n",
      "Epoch: 3800, elapsed: 1.05e+01, train loss: 5.13058e-04, val loss: 5.24189e-04, min loss: 5.13058e-04\n",
      "Epoch: 3900, elapsed: 1.05e+01, train loss: 5.14421e-04, val loss: 5.24865e-04, min loss: 5.13058e-04\n",
      "Epoch: 4000, elapsed: 1.06e+01, train loss: 4.94430e-04, val loss: 4.94097e-04, min loss: 4.94430e-04\n",
      "Epoch: 4100, elapsed: 1.03e+01, train loss: 5.01753e-04, val loss: 5.15853e-04, min loss: 4.94430e-04\n",
      "Epoch: 4200, elapsed: 1.06e+01, train loss: 4.86212e-04, val loss: 4.89570e-04, min loss: 4.86212e-04\n",
      "Epoch: 4300, elapsed: 1.04e+01, train loss: 4.77413e-04, val loss: 4.78492e-04, min loss: 4.77413e-04\n",
      "Epoch: 4400, elapsed: 1.06e+01, train loss: 4.91191e-04, val loss: 4.78852e-04, min loss: 4.77413e-04\n",
      "Epoch: 4500, elapsed: 1.04e+01, train loss: 4.64538e-04, val loss: 4.65643e-04, min loss: 4.64538e-04\n",
      "Epoch: 4600, elapsed: 1.05e+01, train loss: 4.66054e-04, val loss: 4.72643e-04, min loss: 4.64538e-04\n",
      "Epoch: 4700, elapsed: 1.06e+01, train loss: 4.63875e-04, val loss: 4.59503e-04, min loss: 4.63875e-04\n",
      "Epoch: 4800, elapsed: 1.04e+01, train loss: 4.78611e-04, val loss: 4.71511e-04, min loss: 4.63875e-04\n",
      "Epoch: 4900, elapsed: 1.04e+01, train loss: 4.95837e-04, val loss: 5.18992e-04, min loss: 4.63875e-04\n",
      "Epoch: 5000, elapsed: 1.03e+01, train loss: 4.61025e-04, val loss: 4.58065e-04, min loss: 4.61025e-04\n",
      "Epoch: 5100, elapsed: 1.23e+01, train loss: 4.27957e-04, val loss: 4.27840e-04, min loss: 4.27957e-04\n",
      "Epoch: 5200, elapsed: 1.02e+01, train loss: 5.29454e-04, val loss: 5.26195e-04, min loss: 4.27957e-04\n",
      "Epoch: 5300, elapsed: 1.06e+01, train loss: 4.17449e-04, val loss: 4.17952e-04, min loss: 4.17449e-04\n",
      "Epoch: 5400, elapsed: 1.06e+01, train loss: 4.61780e-04, val loss: 4.60449e-04, min loss: 4.17449e-04\n",
      "Epoch: 5500, elapsed: 1.05e+01, train loss: 4.18679e-04, val loss: 4.20385e-04, min loss: 4.17449e-04\n",
      "Epoch: 5600, elapsed: 1.06e+01, train loss: 4.28081e-04, val loss: 4.35687e-04, min loss: 4.17449e-04\n",
      "Epoch: 5700, elapsed: 1.05e+01, train loss: 4.09102e-04, val loss: 4.03742e-04, min loss: 4.09102e-04\n",
      "Epoch: 5800, elapsed: 1.05e+01, train loss: 3.90599e-04, val loss: 3.88723e-04, min loss: 3.90599e-04\n",
      "Epoch: 5900, elapsed: 1.03e+01, train loss: 4.76880e-04, val loss: 4.96129e-04, min loss: 3.90599e-04\n",
      "Epoch: 6000, elapsed: 1.05e+01, train loss: 3.87781e-04, val loss: 3.87689e-04, min loss: 3.87781e-04\n",
      "Epoch: 6100, elapsed: 1.05e+01, train loss: 3.76393e-04, val loss: 3.74833e-04, min loss: 3.76393e-04\n",
      "Epoch: 6200, elapsed: 1.06e+01, train loss: 4.06915e-04, val loss: 3.99480e-04, min loss: 3.76393e-04\n",
      "Epoch: 6300, elapsed: 1.03e+01, train loss: 3.83593e-04, val loss: 3.77014e-04, min loss: 3.76393e-04\n",
      "Epoch: 6400, elapsed: 1.05e+01, train loss: 4.05786e-04, val loss: 3.87337e-04, min loss: 3.76393e-04\n",
      "Epoch: 6500, elapsed: 1.03e+01, train loss: 3.73351e-04, val loss: 3.74154e-04, min loss: 3.73351e-04\n",
      "Epoch: 6600, elapsed: 1.05e+01, train loss: 3.60586e-04, val loss: 3.57125e-04, min loss: 3.60586e-04\n",
      "Epoch: 6700, elapsed: 1.05e+01, train loss: 3.74739e-04, val loss: 3.66104e-04, min loss: 3.60586e-04\n",
      "Epoch: 6800, elapsed: 1.06e+01, train loss: 3.66720e-04, val loss: 3.66724e-04, min loss: 3.60586e-04\n",
      "Epoch: 6900, elapsed: 1.03e+01, train loss: 3.48404e-04, val loss: 3.46450e-04, min loss: 3.48404e-04\n",
      "Epoch: 7000, elapsed: 1.05e+01, train loss: 3.61015e-04, val loss: 3.61052e-04, min loss: 3.48404e-04\n",
      "Epoch: 7100, elapsed: 1.03e+01, train loss: 3.37609e-04, val loss: 3.36376e-04, min loss: 3.37609e-04\n",
      "Epoch: 7200, elapsed: 1.06e+01, train loss: 3.32273e-04, val loss: 3.29989e-04, min loss: 3.32273e-04\n",
      "Epoch: 7300, elapsed: 1.03e+01, train loss: 3.29341e-04, val loss: 3.27118e-04, min loss: 3.29341e-04\n",
      "Epoch: 7400, elapsed: 1.04e+01, train loss: 3.39935e-04, val loss: 3.37911e-04, min loss: 3.29341e-04\n",
      "Epoch: 7500, elapsed: 1.04e+01, train loss: 3.26646e-04, val loss: 3.24157e-04, min loss: 3.26646e-04\n",
      "Epoch: 7600, elapsed: 1.04e+01, train loss: 3.16219e-04, val loss: 3.14859e-04, min loss: 3.16219e-04\n",
      "Epoch: 7700, elapsed: 1.05e+01, train loss: 3.23958e-04, val loss: 3.25846e-04, min loss: 3.16219e-04\n",
      "Epoch: 7800, elapsed: 1.04e+01, train loss: 3.18016e-04, val loss: 3.13762e-04, min loss: 3.16219e-04\n",
      "Epoch: 7900, elapsed: 1.05e+01, train loss: 3.34534e-04, val loss: 3.48246e-04, min loss: 3.16219e-04\n",
      "Epoch: 8000, elapsed: 1.03e+01, train loss: 3.02582e-04, val loss: 3.04616e-04, min loss: 3.02582e-04\n",
      "Epoch: 8100, elapsed: 1.05e+01, train loss: 2.99235e-04, val loss: 2.95445e-04, min loss: 2.99235e-04\n",
      "Epoch: 8200, elapsed: 1.03e+01, train loss: 2.85843e-04, val loss: 2.85019e-04, min loss: 2.85843e-04\n",
      "Epoch: 8300, elapsed: 1.05e+01, train loss: 2.89307e-04, val loss: 2.91715e-04, min loss: 2.85843e-04\n",
      "Epoch: 8400, elapsed: 1.02e+01, train loss: 2.89357e-04, val loss: 2.88121e-04, min loss: 2.85843e-04\n",
      "Epoch: 8500, elapsed: 1.05e+01, train loss: 2.89065e-04, val loss: 2.92472e-04, min loss: 2.85843e-04\n",
      "Epoch: 8600, elapsed: 1.01e+01, train loss: 2.75823e-04, val loss: 2.78978e-04, min loss: 2.75823e-04\n",
      "Epoch: 8700, elapsed: 1.06e+01, train loss: 2.61285e-04, val loss: 2.61095e-04, min loss: 2.61285e-04\n",
      "Epoch: 8800, elapsed: 1.02e+01, train loss: 2.56842e-04, val loss: 2.56717e-04, min loss: 2.56842e-04\n",
      "Epoch: 8900, elapsed: 1.07e+01, train loss: 2.57717e-04, val loss: 2.57078e-04, min loss: 2.56842e-04\n",
      "Epoch: 9000, elapsed: 1.03e+01, train loss: 2.87661e-04, val loss: 2.94764e-04, min loss: 2.56842e-04\n",
      "Epoch: 9100, elapsed: 1.06e+01, train loss: 2.54577e-04, val loss: 2.52069e-04, min loss: 2.54577e-04\n",
      "Epoch: 9200, elapsed: 1.02e+01, train loss: 2.41578e-04, val loss: 2.41440e-04, min loss: 2.41578e-04\n",
      "Epoch: 9300, elapsed: 1.06e+01, train loss: 2.36959e-04, val loss: 2.39167e-04, min loss: 2.36959e-04\n",
      "Epoch: 9400, elapsed: 1.03e+01, train loss: 2.43466e-04, val loss: 2.49661e-04, min loss: 2.36959e-04\n",
      "Epoch: 9500, elapsed: 1.03e+01, train loss: 2.28854e-04, val loss: 2.30376e-04, min loss: 2.28854e-04\n",
      "Epoch: 9600, elapsed: 1.05e+01, train loss: 2.24365e-04, val loss: 2.27497e-04, min loss: 2.24365e-04\n",
      "Epoch: 9700, elapsed: 1.02e+01, train loss: 2.23043e-04, val loss: 2.25439e-04, min loss: 2.23043e-04\n",
      "Epoch: 9800, elapsed: 1.05e+01, train loss: 2.18650e-04, val loss: 2.19360e-04, min loss: 2.18650e-04\n",
      "Epoch: 9900, elapsed: 1.03e+01, train loss: 2.08536e-04, val loss: 2.10917e-04, min loss: 2.08536e-04\n",
      "Epoch: 10000, elapsed: 1.04e+01, train loss: 2.14052e-04, val loss: 2.13939e-04, min loss: 2.08536e-04\n",
      "Epoch: 10100, elapsed: 1.20e+01, train loss: 2.03481e-04, val loss: 2.06122e-04, min loss: 2.03481e-04\n",
      "Epoch: 10200, elapsed: 1.06e+01, train loss: 2.03601e-04, val loss: 1.98203e-04, min loss: 2.03481e-04\n",
      "Epoch: 10300, elapsed: 1.02e+01, train loss: 2.33443e-04, val loss: 2.45374e-04, min loss: 2.03481e-04\n",
      "Epoch: 10400, elapsed: 1.05e+01, train loss: 1.87100e-04, val loss: 1.90997e-04, min loss: 1.87100e-04\n",
      "Epoch: 10500, elapsed: 1.02e+01, train loss: 2.09778e-04, val loss: 2.08875e-04, min loss: 1.87100e-04\n",
      "Epoch: 10600, elapsed: 1.05e+01, train loss: 1.75899e-04, val loss: 1.79931e-04, min loss: 1.75899e-04\n",
      "Epoch: 10700, elapsed: 1.01e+01, train loss: 1.81528e-04, val loss: 1.79319e-04, min loss: 1.75899e-04\n",
      "Epoch: 10800, elapsed: 1.05e+01, train loss: 1.72439e-04, val loss: 1.74935e-04, min loss: 1.72439e-04\n",
      "Epoch: 10900, elapsed: 1.01e+01, train loss: 1.57073e-04, val loss: 1.60576e-04, min loss: 1.57073e-04\n",
      "Epoch: 11000, elapsed: 1.06e+01, train loss: 1.51891e-04, val loss: 1.57614e-04, min loss: 1.51891e-04\n",
      "Epoch: 11100, elapsed: 1.02e+01, train loss: 1.46497e-04, val loss: 1.49492e-04, min loss: 1.46497e-04\n",
      "Epoch: 11200, elapsed: 1.04e+01, train loss: 1.41086e-04, val loss: 1.44152e-04, min loss: 1.41086e-04\n",
      "Epoch: 11300, elapsed: 1.02e+01, train loss: 1.65284e-04, val loss: 1.70344e-04, min loss: 1.41086e-04\n",
      "Epoch: 11400, elapsed: 1.07e+01, train loss: 1.27159e-04, val loss: 1.30537e-04, min loss: 1.27159e-04\n",
      "Epoch: 11500, elapsed: 1.01e+01, train loss: 1.21860e-04, val loss: 1.24502e-04, min loss: 1.21860e-04\n",
      "Epoch: 11600, elapsed: 1.05e+01, train loss: 1.17136e-04, val loss: 1.19510e-04, min loss: 1.17136e-04\n",
      "Epoch: 11700, elapsed: 1.02e+01, train loss: 1.12841e-04, val loss: 1.15033e-04, min loss: 1.12841e-04\n",
      "Epoch: 11800, elapsed: 1.06e+01, train loss: 1.09052e-04, val loss: 1.11211e-04, min loss: 1.09052e-04\n",
      "Epoch: 11900, elapsed: 1.05e+01, train loss: 1.05455e-04, val loss: 1.08151e-04, min loss: 1.05455e-04\n",
      "Epoch: 12000, elapsed: 1.04e+01, train loss: 1.03585e-04, val loss: 1.05255e-04, min loss: 1.03585e-04\n",
      "Epoch: 12100, elapsed: 1.04e+01, train loss: 9.97606e-05, val loss: 1.02416e-04, min loss: 9.97606e-05\n",
      "Epoch: 12200, elapsed: 1.03e+01, train loss: 9.68856e-05, val loss: 9.87521e-05, min loss: 9.68856e-05\n",
      "Epoch: 12300, elapsed: 1.04e+01, train loss: 9.43222e-05, val loss: 9.65447e-05, min loss: 9.43222e-05\n",
      "Epoch: 12400, elapsed: 1.04e+01, train loss: 9.19797e-05, val loss: 9.43557e-05, min loss: 9.19797e-05\n",
      "Epoch: 12500, elapsed: 1.04e+01, train loss: 9.00637e-05, val loss: 9.27052e-05, min loss: 9.00637e-05\n",
      "Epoch: 12600, elapsed: 1.03e+01, train loss: 8.85824e-05, val loss: 9.14074e-05, min loss: 8.85824e-05\n",
      "Epoch: 12700, elapsed: 1.04e+01, train loss: 8.66881e-05, val loss: 8.93248e-05, min loss: 8.66881e-05\n",
      "Epoch: 12800, elapsed: 1.01e+01, train loss: 1.43487e-04, val loss: 1.64502e-04, min loss: 8.66881e-05\n",
      "Epoch: 12900, elapsed: 1.04e+01, train loss: 8.38170e-05, val loss: 8.64924e-05, min loss: 8.38170e-05\n",
      "Epoch: 13000, elapsed: 1.03e+01, train loss: 8.86263e-05, val loss: 9.18314e-05, min loss: 8.38170e-05\n",
      "Epoch: 13100, elapsed: 1.06e+01, train loss: 8.12680e-05, val loss: 8.41274e-05, min loss: 8.12680e-05\n",
      "Epoch: 13200, elapsed: 1.03e+01, train loss: 8.06872e-05, val loss: 8.32464e-05, min loss: 8.06872e-05\n",
      "Epoch: 13300, elapsed: 1.04e+01, train loss: 7.89588e-05, val loss: 8.19454e-05, min loss: 7.89588e-05\n",
      "Epoch: 13400, elapsed: 1.04e+01, train loss: 7.78913e-05, val loss: 8.07848e-05, min loss: 7.78913e-05\n",
      "Epoch: 13500, elapsed: 1.03e+01, train loss: 8.69029e-05, val loss: 8.28792e-05, min loss: 7.78913e-05\n",
      "Epoch: 13600, elapsed: 1.01e+01, train loss: 7.57262e-05, val loss: 7.88157e-05, min loss: 7.57262e-05\n",
      "Epoch: 13700, elapsed: 1.04e+01, train loss: 7.47510e-05, val loss: 7.76896e-05, min loss: 7.47510e-05\n",
      "Epoch: 13800, elapsed: 1.02e+01, train loss: 7.47025e-05, val loss: 7.86206e-05, min loss: 7.47025e-05\n",
      "Epoch: 13900, elapsed: 1.05e+01, train loss: 7.26303e-05, val loss: 7.59429e-05, min loss: 7.26303e-05\n",
      "Epoch: 14000, elapsed: 1.04e+01, train loss: 7.40129e-05, val loss: 7.75575e-05, min loss: 7.26303e-05\n",
      "Epoch: 14100, elapsed: 1.03e+01, train loss: 7.05903e-05, val loss: 7.40239e-05, min loss: 7.05903e-05\n",
      "Epoch: 14200, elapsed: 1.03e+01, train loss: 6.95957e-05, val loss: 7.30714e-05, min loss: 6.95957e-05\n",
      "Epoch: 14300, elapsed: 1.05e+01, train loss: 9.47592e-05, val loss: 7.63717e-05, min loss: 6.95957e-05\n",
      "Epoch: 14400, elapsed: 1.03e+01, train loss: 6.75916e-05, val loss: 7.11228e-05, min loss: 6.75916e-05\n",
      "Epoch: 14500, elapsed: 1.05e+01, train loss: 6.69603e-05, val loss: 7.02085e-05, min loss: 6.69603e-05\n",
      "Epoch: 14600, elapsed: 1.03e+01, train loss: 8.01458e-05, val loss: 8.15607e-05, min loss: 6.69603e-05\n",
      "Epoch: 14700, elapsed: 1.06e+01, train loss: 6.47016e-05, val loss: 6.85367e-05, min loss: 6.47016e-05\n",
      "Epoch: 14800, elapsed: 1.03e+01, train loss: 6.38446e-05, val loss: 6.78609e-05, min loss: 6.38446e-05\n",
      "Epoch: 14900, elapsed: 1.03e+01, train loss: 6.29125e-05, val loss: 6.69285e-05, min loss: 6.29125e-05\n",
      "Epoch: 15000, elapsed: 1.03e+01, train loss: 6.18436e-05, val loss: 6.57409e-05, min loss: 6.18436e-05\n",
      "Epoch: 15100, elapsed: 1.22e+01, train loss: 6.08020e-05, val loss: 6.48560e-05, min loss: 6.08020e-05\n",
      "Epoch: 15200, elapsed: 1.03e+01, train loss: 6.01204e-05, val loss: 6.44597e-05, min loss: 6.01204e-05\n",
      "Epoch: 15300, elapsed: 1.05e+01, train loss: 6.55456e-05, val loss: 7.46773e-05, min loss: 6.01204e-05\n",
      "Epoch: 15400, elapsed: 1.02e+01, train loss: 5.80161e-05, val loss: 6.23055e-05, min loss: 5.80161e-05\n",
      "Epoch: 15500, elapsed: 1.06e+01, train loss: 5.73716e-05, val loss: 6.18002e-05, min loss: 5.73716e-05\n",
      "Epoch: 15600, elapsed: 1.03e+01, train loss: 5.62149e-05, val loss: 6.06259e-05, min loss: 5.62149e-05\n",
      "Epoch: 15700, elapsed: 1.03e+01, train loss: 5.63740e-05, val loss: 6.29109e-05, min loss: 5.62149e-05\n",
      "Epoch: 15800, elapsed: 1.02e+01, train loss: 5.48147e-05, val loss: 5.99142e-05, min loss: 5.48147e-05\n",
      "Epoch: 15900, elapsed: 1.04e+01, train loss: 5.38923e-05, val loss: 5.85595e-05, min loss: 5.38923e-05\n",
      "Epoch: 16000, elapsed: 1.02e+01, train loss: 5.27494e-05, val loss: 5.75225e-05, min loss: 5.27494e-05\n",
      "Epoch: 16100, elapsed: 1.06e+01, train loss: 5.22080e-05, val loss: 5.72745e-05, min loss: 5.22080e-05\n",
      "Epoch: 16200, elapsed: 1.02e+01, train loss: 5.13291e-05, val loss: 5.67698e-05, min loss: 5.13291e-05\n",
      "Epoch: 16300, elapsed: 1.04e+01, train loss: 6.87786e-05, val loss: 7.46764e-05, min loss: 5.13291e-05\n",
      "Epoch: 16400, elapsed: 1.03e+01, train loss: 5.18369e-05, val loss: 5.62957e-05, min loss: 5.13291e-05\n",
      "Epoch: 16500, elapsed: 1.04e+01, train loss: 5.07141e-05, val loss: 5.63796e-05, min loss: 5.07141e-05\n",
      "Epoch: 16600, elapsed: 1.02e+01, train loss: 4.85297e-05, val loss: 5.41088e-05, min loss: 4.85297e-05\n",
      "Epoch: 16700, elapsed: 1.05e+01, train loss: 4.76717e-05, val loss: 5.27084e-05, min loss: 4.76717e-05\n",
      "Epoch: 16800, elapsed: 1.04e+01, train loss: 4.71724e-05, val loss: 5.21032e-05, min loss: 4.71724e-05\n",
      "Epoch: 16900, elapsed: 1.01e+01, train loss: 4.76436e-05, val loss: 5.20218e-05, min loss: 4.71724e-05\n",
      "Epoch: 17000, elapsed: 1.05e+01, train loss: 6.47673e-05, val loss: 7.12664e-05, min loss: 4.71724e-05\n",
      "Epoch: 17100, elapsed: 1.02e+01, train loss: 6.29725e-05, val loss: 6.58984e-05, min loss: 4.71724e-05\n",
      "Epoch: 17200, elapsed: 1.07e+01, train loss: 1.23426e-04, val loss: 1.29487e-04, min loss: 4.71724e-05\n",
      "Epoch: 17300, elapsed: 1.01e+01, train loss: 4.44751e-05, val loss: 4.93029e-05, min loss: 4.44751e-05\n",
      "Epoch: 17400, elapsed: 1.04e+01, train loss: 4.40371e-05, val loss: 4.87818e-05, min loss: 4.40371e-05\n",
      "Epoch: 17500, elapsed: 1.01e+01, train loss: 4.36972e-05, val loss: 4.85308e-05, min loss: 4.36972e-05\n",
      "Epoch: 17600, elapsed: 1.04e+01, train loss: 4.88346e-05, val loss: 5.04018e-05, min loss: 4.36972e-05\n",
      "Epoch: 17700, elapsed: 1.03e+01, train loss: 4.28653e-05, val loss: 4.72765e-05, min loss: 4.28653e-05\n",
      "Epoch: 17800, elapsed: 1.05e+01, train loss: 4.23387e-05, val loss: 4.67888e-05, min loss: 4.23387e-05\n",
      "Epoch: 17900, elapsed: 1.02e+01, train loss: 4.66409e-05, val loss: 5.32277e-05, min loss: 4.23387e-05\n",
      "Epoch: 18000, elapsed: 1.05e+01, train loss: 4.94204e-05, val loss: 5.44174e-05, min loss: 4.23387e-05\n",
      "Epoch: 18100, elapsed: 1.03e+01, train loss: 4.14612e-05, val loss: 4.57933e-05, min loss: 4.14612e-05\n",
      "Epoch: 18200, elapsed: 1.06e+01, train loss: 4.21019e-05, val loss: 4.70035e-05, min loss: 4.14612e-05\n",
      "Epoch: 18300, elapsed: 1.02e+01, train loss: 4.42623e-05, val loss: 4.79730e-05, min loss: 4.14612e-05\n",
      "Epoch: 18400, elapsed: 1.05e+01, train loss: 4.08628e-05, val loss: 4.47678e-05, min loss: 4.08628e-05\n",
      "Epoch: 18500, elapsed: 1.04e+01, train loss: 3.99882e-05, val loss: 4.42794e-05, min loss: 3.99882e-05\n",
      "Epoch: 18600, elapsed: 1.06e+01, train loss: 3.95158e-05, val loss: 4.34887e-05, min loss: 3.95158e-05\n",
      "Epoch: 18700, elapsed: 1.03e+01, train loss: 3.92894e-05, val loss: 4.32850e-05, min loss: 3.92894e-05\n",
      "Epoch: 18800, elapsed: 1.05e+01, train loss: 4.79720e-05, val loss: 5.23116e-05, min loss: 3.92894e-05\n",
      "Epoch: 18900, elapsed: 1.03e+01, train loss: 3.93058e-05, val loss: 4.34310e-05, min loss: 3.92894e-05\n",
      "Epoch: 19000, elapsed: 1.05e+01, train loss: 6.05895e-05, val loss: 6.22216e-05, min loss: 3.92894e-05\n",
      "Epoch: 19100, elapsed: 1.02e+01, train loss: 3.97711e-05, val loss: 4.42344e-05, min loss: 3.92894e-05\n",
      "Epoch: 19200, elapsed: 1.04e+01, train loss: 4.12016e-05, val loss: 4.70112e-05, min loss: 3.92894e-05\n",
      "Epoch: 19300, elapsed: 1.03e+01, train loss: 4.10554e-05, val loss: 4.61482e-05, min loss: 3.92894e-05\n",
      "Epoch: 19400, elapsed: 1.05e+01, train loss: 3.71668e-05, val loss: 4.07991e-05, min loss: 3.71668e-05\n",
      "Epoch: 19500, elapsed: 1.02e+01, train loss: 5.14781e-05, val loss: 5.90708e-05, min loss: 3.71668e-05\n",
      "Epoch: 19600, elapsed: 1.03e+01, train loss: 3.69365e-05, val loss: 4.02422e-05, min loss: 3.69365e-05\n",
      "Epoch: 19700, elapsed: 1.02e+01, train loss: 3.66093e-05, val loss: 4.01545e-05, min loss: 3.66093e-05\n",
      "Epoch: 19800, elapsed: 1.04e+01, train loss: 5.54288e-05, val loss: 5.64816e-05, min loss: 3.66093e-05\n",
      "Epoch: 19900, elapsed: 1.03e+01, train loss: 3.60616e-05, val loss: 3.98237e-05, min loss: 3.60616e-05\n",
      "Epoch: 20000, elapsed: 1.01e+01, train loss: 3.59384e-05, val loss: 3.91264e-05, min loss: 3.59384e-05\n",
      "Epoch: 20100, elapsed: 1.24e+01, train loss: 3.53356e-05, val loss: 3.86798e-05, min loss: 3.53356e-05\n",
      "Epoch: 20200, elapsed: 1.04e+01, train loss: 3.55613e-05, val loss: 3.93434e-05, min loss: 3.53356e-05\n",
      "Epoch: 20300, elapsed: 1.03e+01, train loss: 4.05233e-05, val loss: 4.01058e-05, min loss: 3.53356e-05\n",
      "Epoch: 20400, elapsed: 1.04e+01, train loss: 3.62185e-05, val loss: 3.97509e-05, min loss: 3.53356e-05\n",
      "Epoch: 20500, elapsed: 1.02e+01, train loss: 3.46225e-05, val loss: 3.74071e-05, min loss: 3.46225e-05\n",
      "Epoch: 20600, elapsed: 1.02e+01, train loss: 3.77514e-05, val loss: 3.88271e-05, min loss: 3.46225e-05\n",
      "Epoch: 20700, elapsed: 1.05e+01, train loss: 3.39564e-05, val loss: 3.70716e-05, min loss: 3.39564e-05\n",
      "Epoch: 20800, elapsed: 1.01e+01, train loss: 3.46129e-05, val loss: 3.84318e-05, min loss: 3.39564e-05\n",
      "Epoch: 20900, elapsed: 1.05e+01, train loss: 6.25725e-05, val loss: 6.81249e-05, min loss: 3.39564e-05\n",
      "Epoch: 21000, elapsed: 1.02e+01, train loss: 3.38393e-05, val loss: 3.74132e-05, min loss: 3.38393e-05\n",
      "Epoch: 21100, elapsed: 1.05e+01, train loss: 4.88621e-05, val loss: 4.95901e-05, min loss: 3.38393e-05\n",
      "Epoch: 21200, elapsed: 1.01e+01, train loss: 3.34457e-05, val loss: 3.62575e-05, min loss: 3.34457e-05\n",
      "Epoch: 21300, elapsed: 1.05e+01, train loss: 6.06147e-05, val loss: 5.70739e-05, min loss: 3.34457e-05\n",
      "Epoch: 21400, elapsed: 1.03e+01, train loss: 3.33288e-05, val loss: 3.62622e-05, min loss: 3.33288e-05\n",
      "Epoch: 21500, elapsed: 1.05e+01, train loss: 3.22697e-05, val loss: 3.53139e-05, min loss: 3.22697e-05\n",
      "Epoch: 21600, elapsed: 1.03e+01, train loss: 3.22934e-05, val loss: 3.52130e-05, min loss: 3.22697e-05\n",
      "Epoch: 21700, elapsed: 1.04e+01, train loss: 3.32909e-05, val loss: 3.61866e-05, min loss: 3.22697e-05\n",
      "Epoch: 21800, elapsed: 1.01e+01, train loss: 3.24480e-05, val loss: 3.51890e-05, min loss: 3.22697e-05\n",
      "Epoch: 21900, elapsed: 1.02e+01, train loss: 3.92319e-05, val loss: 4.22798e-05, min loss: 3.22697e-05\n",
      "Epoch: 22000, elapsed: 1.04e+01, train loss: 3.31964e-05, val loss: 3.65138e-05, min loss: 3.22697e-05\n",
      "Epoch: 22100, elapsed: 1.02e+01, train loss: 3.16905e-05, val loss: 3.51405e-05, min loss: 3.16905e-05\n",
      "Epoch: 22200, elapsed: 1.06e+01, train loss: 3.42117e-05, val loss: 3.73697e-05, min loss: 3.16905e-05\n",
      "Epoch: 22300, elapsed: 1.04e+01, train loss: 3.17238e-05, val loss: 3.43298e-05, min loss: 3.16905e-05\n",
      "Epoch: 22400, elapsed: 1.04e+01, train loss: 3.10734e-05, val loss: 3.39422e-05, min loss: 3.10734e-05\n",
      "Epoch: 22500, elapsed: 1.03e+01, train loss: 5.13289e-05, val loss: 5.69691e-05, min loss: 3.10734e-05\n",
      "Epoch: 22600, elapsed: 1.05e+01, train loss: 3.11258e-05, val loss: 3.40970e-05, min loss: 3.10734e-05\n",
      "Epoch: 22700, elapsed: 1.03e+01, train loss: 3.07814e-05, val loss: 3.33412e-05, min loss: 3.07814e-05\n",
      "Epoch: 22800, elapsed: 1.05e+01, train loss: 3.11449e-05, val loss: 3.35815e-05, min loss: 3.07814e-05\n",
      "Epoch: 22900, elapsed: 1.03e+01, train loss: 3.06469e-05, val loss: 3.33500e-05, min loss: 3.06469e-05\n",
      "Epoch: 23000, elapsed: 1.05e+01, train loss: 3.40517e-05, val loss: 3.88213e-05, min loss: 3.06469e-05\n",
      "Epoch: 23100, elapsed: 1.04e+01, train loss: 3.00208e-05, val loss: 3.26960e-05, min loss: 3.00208e-05\n",
      "Epoch: 23200, elapsed: 1.02e+01, train loss: 3.04977e-05, val loss: 3.31967e-05, min loss: 3.00208e-05\n",
      "Epoch: 23300, elapsed: 1.05e+01, train loss: 3.57167e-05, val loss: 3.56336e-05, min loss: 3.00208e-05\n",
      "Epoch: 23400, elapsed: 1.01e+01, train loss: 2.92328e-05, val loss: 3.24703e-05, min loss: 2.92328e-05\n",
      "Epoch: 23500, elapsed: 1.07e+01, train loss: 4.67296e-05, val loss: 5.04114e-05, min loss: 2.92328e-05\n",
      "Epoch: 23600, elapsed: 1.02e+01, train loss: 2.89911e-05, val loss: 3.22030e-05, min loss: 2.89911e-05\n",
      "Epoch: 23700, elapsed: 1.06e+01, train loss: 2.87889e-05, val loss: 3.22195e-05, min loss: 2.87889e-05\n",
      "Epoch: 23800, elapsed: 1.03e+01, train loss: 2.84831e-05, val loss: 3.19772e-05, min loss: 2.84831e-05\n",
      "Epoch: 23900, elapsed: 1.05e+01, train loss: 2.84974e-05, val loss: 3.18143e-05, min loss: 2.84831e-05\n",
      "Epoch: 24000, elapsed: 1.02e+01, train loss: 2.83379e-05, val loss: 3.16681e-05, min loss: 2.83379e-05\n",
      "Epoch: 24100, elapsed: 1.05e+01, train loss: 2.84069e-05, val loss: 3.21291e-05, min loss: 2.83379e-05\n",
      "Epoch: 24200, elapsed: 1.03e+01, train loss: 3.18543e-05, val loss: 3.73168e-05, min loss: 2.83379e-05\n",
      "Epoch: 24300, elapsed: 1.05e+01, train loss: 3.43636e-05, val loss: 3.66341e-05, min loss: 2.83379e-05\n",
      "Epoch: 24400, elapsed: 1.02e+01, train loss: 2.85565e-05, val loss: 3.20426e-05, min loss: 2.83379e-05\n",
      "Epoch: 24500, elapsed: 1.05e+01, train loss: 2.77683e-05, val loss: 3.10021e-05, min loss: 2.77683e-05\n",
      "Epoch: 24600, elapsed: 1.02e+01, train loss: 2.98602e-05, val loss: 3.33163e-05, min loss: 2.77683e-05\n",
      "Epoch: 24700, elapsed: 1.00e+01, train loss: 2.97855e-05, val loss: 3.36276e-05, min loss: 2.77683e-05\n",
      "Epoch: 24800, elapsed: 1.04e+01, train loss: 3.92882e-05, val loss: 3.45861e-05, min loss: 2.77683e-05\n",
      "Epoch: 24900, elapsed: 1.00e+01, train loss: 2.70026e-05, val loss: 3.04744e-05, min loss: 2.70026e-05\n",
      "Epoch: 25000, elapsed: 1.04e+01, train loss: 2.78537e-05, val loss: 3.47475e-05, min loss: 2.70026e-05\n",
      "Epoch: 25100, elapsed: 1.20e+01, train loss: 2.89557e-05, val loss: 3.18843e-05, min loss: 2.70026e-05\n",
      "Epoch: 25200, elapsed: 1.06e+01, train loss: 2.82858e-05, val loss: 3.14736e-05, min loss: 2.70026e-05\n",
      "Epoch: 25300, elapsed: 1.02e+01, train loss: 2.70937e-05, val loss: 3.11001e-05, min loss: 2.70026e-05\n",
      "Epoch: 25400, elapsed: 1.06e+01, train loss: 2.91313e-05, val loss: 3.22826e-05, min loss: 2.70026e-05\n",
      "Epoch: 25500, elapsed: 1.03e+01, train loss: 2.84685e-05, val loss: 3.18772e-05, min loss: 2.70026e-05\n",
      "Epoch: 25600, elapsed: 1.09e+01, train loss: 3.79780e-05, val loss: 4.54894e-05, min loss: 2.70026e-05\n",
      "Epoch: 25700, elapsed: 1.01e+01, train loss: 2.63202e-05, val loss: 2.98582e-05, min loss: 2.63202e-05\n",
      "Epoch: 25800, elapsed: 1.07e+01, train loss: 2.64849e-05, val loss: 3.00343e-05, min loss: 2.63202e-05\n",
      "Epoch: 25900, elapsed: 1.04e+01, train loss: 3.03138e-05, val loss: 3.45554e-05, min loss: 2.63202e-05\n",
      "Epoch: 26000, elapsed: 1.05e+01, train loss: 2.77118e-05, val loss: 3.09979e-05, min loss: 2.63202e-05\n",
      "Epoch: 26100, elapsed: 1.03e+01, train loss: 4.13568e-05, val loss: 4.89791e-05, min loss: 2.63202e-05\n",
      "Epoch: 26200, elapsed: 1.01e+01, train loss: 2.64746e-05, val loss: 3.15555e-05, min loss: 2.63202e-05\n",
      "Epoch: 26300, elapsed: 1.05e+01, train loss: 2.53864e-05, val loss: 2.97596e-05, min loss: 2.53864e-05\n",
      "Epoch: 26400, elapsed: 1.01e+01, train loss: 2.59997e-05, val loss: 3.00817e-05, min loss: 2.53864e-05\n",
      "Epoch: 26500, elapsed: 1.05e+01, train loss: 2.96376e-05, val loss: 3.26642e-05, min loss: 2.53864e-05\n",
      "Epoch: 26600, elapsed: 1.02e+01, train loss: 2.49274e-05, val loss: 2.92985e-05, min loss: 2.49274e-05\n",
      "Epoch: 26700, elapsed: 1.05e+01, train loss: 2.64885e-05, val loss: 2.99819e-05, min loss: 2.49274e-05\n",
      "Epoch: 26800, elapsed: 1.03e+01, train loss: 2.49046e-05, val loss: 2.90721e-05, min loss: 2.49046e-05\n",
      "Epoch: 26900, elapsed: 1.05e+01, train loss: 3.04096e-05, val loss: 3.24243e-05, min loss: 2.49046e-05\n",
      "Epoch: 27000, elapsed: 1.03e+01, train loss: 2.45717e-05, val loss: 2.86880e-05, min loss: 2.45717e-05\n",
      "Epoch: 27100, elapsed: 1.03e+01, train loss: 2.41313e-05, val loss: 2.82409e-05, min loss: 2.41313e-05\n",
      "Epoch: 27200, elapsed: 1.06e+01, train loss: 2.63435e-05, val loss: 3.04495e-05, min loss: 2.41313e-05\n",
      "Epoch: 27300, elapsed: 1.02e+01, train loss: 3.01325e-05, val loss: 3.29437e-05, min loss: 2.41313e-05\n",
      "Epoch: 27400, elapsed: 1.05e+01, train loss: 3.81891e-05, val loss: 4.12728e-05, min loss: 2.41313e-05\n",
      "Epoch: 27500, elapsed: 1.02e+01, train loss: 2.39281e-05, val loss: 2.81517e-05, min loss: 2.39281e-05\n",
      "Epoch: 27600, elapsed: 1.05e+01, train loss: 2.38748e-05, val loss: 2.74810e-05, min loss: 2.38748e-05\n",
      "Epoch: 27700, elapsed: 1.03e+01, train loss: 2.43276e-05, val loss: 2.89690e-05, min loss: 2.38748e-05\n",
      "Epoch: 27800, elapsed: 1.06e+01, train loss: 2.92054e-05, val loss: 3.22468e-05, min loss: 2.38748e-05\n",
      "Epoch: 27900, elapsed: 1.03e+01, train loss: 2.42820e-05, val loss: 2.80845e-05, min loss: 2.38748e-05\n",
      "Epoch: 28000, elapsed: 1.02e+01, train loss: 2.47064e-05, val loss: 2.91706e-05, min loss: 2.38748e-05\n",
      "Epoch: 28100, elapsed: 1.06e+01, train loss: 2.38012e-05, val loss: 2.76846e-05, min loss: 2.38012e-05\n",
      "Epoch: 28200, elapsed: 1.01e+01, train loss: 2.44340e-05, val loss: 2.79996e-05, min loss: 2.38012e-05\n",
      "Epoch: 28300, elapsed: 1.07e+01, train loss: 2.40978e-05, val loss: 2.81629e-05, min loss: 2.38012e-05\n",
      "Epoch: 28400, elapsed: 1.02e+01, train loss: 2.52235e-05, val loss: 2.77495e-05, min loss: 2.38012e-05\n",
      "Epoch: 28500, elapsed: 1.06e+01, train loss: 2.97337e-05, val loss: 3.33021e-05, min loss: 2.38012e-05\n",
      "Epoch: 28600, elapsed: 1.02e+01, train loss: 2.55183e-05, val loss: 2.96495e-05, min loss: 2.38012e-05\n",
      "Epoch: 28700, elapsed: 1.06e+01, train loss: 2.78276e-05, val loss: 3.16757e-05, min loss: 2.38012e-05\n",
      "Epoch: 28800, elapsed: 1.04e+01, train loss: 3.72697e-05, val loss: 3.56009e-05, min loss: 2.38012e-05\n",
      "Epoch: 28900, elapsed: 1.01e+01, train loss: 2.36858e-05, val loss: 2.78926e-05, min loss: 2.36858e-05\n",
      "Epoch: 29000, elapsed: 1.05e+01, train loss: 2.62700e-05, val loss: 2.98106e-05, min loss: 2.36858e-05\n",
      "Epoch: 29100, elapsed: 1.01e+01, train loss: 2.25370e-05, val loss: 2.68356e-05, min loss: 2.25370e-05\n",
      "Epoch: 29200, elapsed: 1.06e+01, train loss: 4.64303e-05, val loss: 3.76566e-05, min loss: 2.25370e-05\n",
      "Epoch: 29300, elapsed: 1.02e+01, train loss: 2.40051e-05, val loss: 2.79526e-05, min loss: 2.25370e-05\n",
      "Epoch: 29400, elapsed: 1.05e+01, train loss: 2.24973e-05, val loss: 2.67839e-05, min loss: 2.24973e-05\n",
      "Epoch: 29500, elapsed: 1.03e+01, train loss: 2.72748e-05, val loss: 3.10371e-05, min loss: 2.24973e-05\n",
      "Epoch: 29600, elapsed: 1.06e+01, train loss: 2.65150e-05, val loss: 2.95436e-05, min loss: 2.24973e-05\n",
      "Epoch: 29700, elapsed: 1.02e+01, train loss: 2.21065e-05, val loss: 2.60179e-05, min loss: 2.21065e-05\n",
      "Epoch: 29800, elapsed: 1.01e+01, train loss: 2.23103e-05, val loss: 2.63699e-05, min loss: 2.21065e-05\n",
      "Epoch: 29900, elapsed: 1.06e+01, train loss: 2.19951e-05, val loss: 2.63894e-05, min loss: 2.19951e-05\n",
      "Epoch: 30000, elapsed: 1.02e+01, train loss: 2.44657e-05, val loss: 2.80931e-05, min loss: 2.19951e-05\n",
      "Epoch: 30100, elapsed: 1.23e+01, train loss: 2.30955e-05, val loss: 2.69354e-05, min loss: 2.19951e-05\n",
      "Epoch: 30200, elapsed: 1.01e+01, train loss: 3.12731e-05, val loss: 3.60024e-05, min loss: 2.19951e-05\n",
      "Epoch: 30300, elapsed: 1.05e+01, train loss: 2.17898e-05, val loss: 2.54568e-05, min loss: 2.17898e-05\n",
      "Epoch: 30400, elapsed: 1.03e+01, train loss: 2.15997e-05, val loss: 2.53534e-05, min loss: 2.15997e-05\n",
      "Epoch: 30500, elapsed: 1.04e+01, train loss: 2.17073e-05, val loss: 2.55163e-05, min loss: 2.15997e-05\n",
      "Epoch: 30600, elapsed: 1.03e+01, train loss: 2.14938e-05, val loss: 2.53728e-05, min loss: 2.14938e-05\n",
      "Epoch: 30700, elapsed: 1.05e+01, train loss: 3.71899e-05, val loss: 4.15091e-05, min loss: 2.14938e-05\n",
      "Epoch: 30800, elapsed: 1.04e+01, train loss: 2.73558e-05, val loss: 3.09853e-05, min loss: 2.14938e-05\n",
      "Epoch: 30900, elapsed: 1.03e+01, train loss: 2.18938e-05, val loss: 2.57242e-05, min loss: 2.14938e-05\n",
      "Epoch: 31000, elapsed: 1.05e+01, train loss: 2.99350e-05, val loss: 3.09544e-05, min loss: 2.14938e-05\n",
      "Epoch: 31100, elapsed: 9.93e+00, train loss: 2.55760e-05, val loss: 2.88303e-05, min loss: 2.14938e-05\n",
      "Epoch: 31200, elapsed: 1.04e+01, train loss: 2.12834e-05, val loss: 2.47499e-05, min loss: 2.12834e-05\n",
      "Epoch: 31300, elapsed: 1.01e+01, train loss: 2.11587e-05, val loss: 2.47519e-05, min loss: 2.11587e-05\n",
      "Epoch: 31400, elapsed: 1.02e+01, train loss: 4.02654e-05, val loss: 4.10586e-05, min loss: 2.11587e-05\n",
      "Epoch: 31500, elapsed: 1.02e+01, train loss: 2.18485e-05, val loss: 2.56315e-05, min loss: 2.11587e-05\n",
      "Epoch: 31600, elapsed: 9.97e+00, train loss: 2.15512e-05, val loss: 2.54031e-05, min loss: 2.11587e-05\n",
      "Epoch: 31700, elapsed: 1.02e+01, train loss: 2.77638e-05, val loss: 3.31707e-05, min loss: 2.11587e-05\n",
      "Epoch: 31800, elapsed: 9.99e+00, train loss: 2.40674e-05, val loss: 2.78040e-05, min loss: 2.11587e-05\n",
      "Epoch: 31900, elapsed: 1.03e+01, train loss: 2.18113e-05, val loss: 2.53295e-05, min loss: 2.11587e-05\n",
      "Epoch: 32000, elapsed: 1.02e+01, train loss: 2.26155e-05, val loss: 2.58663e-05, min loss: 2.11587e-05\n",
      "Epoch: 32100, elapsed: 1.05e+01, train loss: 2.07025e-05, val loss: 2.42503e-05, min loss: 2.07025e-05\n",
      "Epoch: 32200, elapsed: 1.04e+01, train loss: 2.18760e-05, val loss: 2.58759e-05, min loss: 2.07025e-05\n",
      "Epoch: 32300, elapsed: 9.95e+00, train loss: 2.27309e-05, val loss: 2.63959e-05, min loss: 2.07025e-05\n",
      "Epoch: 32400, elapsed: 1.04e+01, train loss: 2.54635e-05, val loss: 2.93204e-05, min loss: 2.07025e-05\n",
      "Epoch: 32500, elapsed: 1.01e+01, train loss: 2.50163e-05, val loss: 2.94025e-05, min loss: 2.07025e-05\n",
      "Epoch: 32600, elapsed: 1.04e+01, train loss: 2.15499e-05, val loss: 2.48423e-05, min loss: 2.07025e-05\n",
      "Epoch: 32700, elapsed: 1.00e+01, train loss: 2.10112e-05, val loss: 2.44848e-05, min loss: 2.07025e-05\n",
      "Epoch: 32800, elapsed: 1.03e+01, train loss: 6.55215e-05, val loss: 6.86250e-05, min loss: 2.07025e-05\n",
      "Epoch: 32900, elapsed: 1.03e+01, train loss: 4.52599e-05, val loss: 3.96324e-05, min loss: 2.07025e-05\n",
      "Epoch: 33000, elapsed: 9.92e+00, train loss: 2.29282e-05, val loss: 2.64140e-05, min loss: 2.07025e-05\n",
      "Epoch: 33100, elapsed: 1.03e+01, train loss: 2.11167e-05, val loss: 2.43537e-05, min loss: 2.07025e-05\n",
      "Epoch: 33200, elapsed: 1.01e+01, train loss: 2.12831e-05, val loss: 2.38729e-05, min loss: 2.07025e-05\n",
      "Epoch: 33300, elapsed: 1.03e+01, train loss: 2.02311e-05, val loss: 2.35884e-05, min loss: 2.02311e-05\n",
      "Epoch: 33400, elapsed: 1.00e+01, train loss: 3.49530e-05, val loss: 3.71117e-05, min loss: 2.02311e-05\n",
      "Epoch: 33500, elapsed: 1.02e+01, train loss: 2.03159e-05, val loss: 2.37408e-05, min loss: 2.02311e-05\n",
      "Epoch: 33600, elapsed: 9.97e+00, train loss: 2.26102e-05, val loss: 2.63721e-05, min loss: 2.02311e-05\n",
      "Epoch: 33700, elapsed: 1.00e+01, train loss: 1.97973e-05, val loss: 2.30644e-05, min loss: 1.97973e-05\n",
      "Epoch: 33800, elapsed: 1.03e+01, train loss: 1.99068e-05, val loss: 2.33588e-05, min loss: 1.97973e-05\n",
      "Epoch: 33900, elapsed: 9.93e+00, train loss: 1.99878e-05, val loss: 2.35555e-05, min loss: 1.97973e-05\n",
      "Epoch: 34000, elapsed: 1.03e+01, train loss: 4.04920e-05, val loss: 3.91824e-05, min loss: 1.97973e-05\n",
      "Epoch: 34100, elapsed: 1.01e+01, train loss: 1.98948e-05, val loss: 2.30528e-05, min loss: 1.97973e-05\n",
      "Epoch: 34200, elapsed: 1.02e+01, train loss: 2.60164e-05, val loss: 3.04952e-05, min loss: 1.97973e-05\n",
      "Epoch: 34300, elapsed: 1.02e+01, train loss: 2.08721e-05, val loss: 2.46996e-05, min loss: 1.97973e-05\n",
      "Epoch: 34400, elapsed: 9.86e+00, train loss: 1.95832e-05, val loss: 2.26675e-05, min loss: 1.95832e-05\n",
      "Epoch: 34500, elapsed: 1.04e+01, train loss: 2.10721e-05, val loss: 2.49113e-05, min loss: 1.95832e-05\n",
      "Epoch: 34600, elapsed: 9.98e+00, train loss: 2.98403e-05, val loss: 2.98103e-05, min loss: 1.95832e-05\n",
      "Epoch: 34700, elapsed: 1.05e+01, train loss: 2.01161e-05, val loss: 2.28020e-05, min loss: 1.95832e-05\n",
      "Epoch: 34800, elapsed: 1.01e+01, train loss: 2.02275e-05, val loss: 2.33042e-05, min loss: 1.95832e-05\n",
      "Epoch: 34900, elapsed: 1.03e+01, train loss: 2.03410e-05, val loss: 2.37062e-05, min loss: 1.95832e-05\n",
      "Epoch: 35000, elapsed: 1.01e+01, train loss: 2.06517e-05, val loss: 2.38184e-05, min loss: 1.95832e-05\n",
      "Epoch: 35100, elapsed: 1.21e+01, train loss: 2.02993e-05, val loss: 2.32709e-05, min loss: 1.95832e-05\n",
      "Epoch: 35200, elapsed: 1.01e+01, train loss: 1.90829e-05, val loss: 2.22205e-05, min loss: 1.90829e-05\n",
      "Epoch: 35300, elapsed: 1.01e+01, train loss: 3.17686e-05, val loss: 3.67783e-05, min loss: 1.90829e-05\n",
      "Epoch: 35400, elapsed: 1.06e+01, train loss: 2.14653e-05, val loss: 2.44380e-05, min loss: 1.90829e-05\n",
      "Epoch: 35500, elapsed: 1.01e+01, train loss: 1.92717e-05, val loss: 2.21928e-05, min loss: 1.90829e-05\n",
      "Epoch: 35600, elapsed: 1.04e+01, train loss: 1.98086e-05, val loss: 2.23306e-05, min loss: 1.90829e-05\n",
      "Epoch: 35700, elapsed: 1.02e+01, train loss: 2.13430e-05, val loss: 2.56975e-05, min loss: 1.90829e-05\n",
      "Epoch: 35800, elapsed: 1.01e+01, train loss: 1.88303e-05, val loss: 2.18961e-05, min loss: 1.88303e-05\n",
      "Epoch: 35900, elapsed: 1.05e+01, train loss: 1.92536e-05, val loss: 2.25037e-05, min loss: 1.88303e-05\n",
      "Epoch: 36000, elapsed: 1.02e+01, train loss: 1.87347e-05, val loss: 2.16409e-05, min loss: 1.87347e-05\n",
      "Epoch: 36100, elapsed: 1.04e+01, train loss: 1.87205e-05, val loss: 2.16629e-05, min loss: 1.87205e-05\n",
      "Epoch: 36200, elapsed: 1.00e+01, train loss: 1.93331e-05, val loss: 2.18419e-05, min loss: 1.87205e-05\n",
      "Epoch: 36300, elapsed: 1.04e+01, train loss: 1.86450e-05, val loss: 2.15860e-05, min loss: 1.86450e-05\n",
      "Epoch: 36400, elapsed: 1.01e+01, train loss: 2.40197e-05, val loss: 2.58919e-05, min loss: 1.86450e-05\n",
      "Epoch: 36500, elapsed: 1.00e+01, train loss: 1.89020e-05, val loss: 2.22567e-05, min loss: 1.86450e-05\n",
      "Epoch: 36600, elapsed: 1.04e+01, train loss: 2.01152e-05, val loss: 2.26568e-05, min loss: 1.86450e-05\n",
      "Epoch: 36700, elapsed: 9.91e+00, train loss: 1.86052e-05, val loss: 2.14315e-05, min loss: 1.86052e-05\n",
      "Epoch: 36800, elapsed: 1.03e+01, train loss: 2.32685e-05, val loss: 2.87155e-05, min loss: 1.86052e-05\n",
      "Epoch: 36900, elapsed: 9.96e+00, train loss: 1.99871e-05, val loss: 2.23712e-05, min loss: 1.86052e-05\n",
      "Epoch: 37000, elapsed: 9.98e+00, train loss: 2.94652e-05, val loss: 3.52974e-05, min loss: 1.86052e-05\n",
      "Epoch: 37100, elapsed: 1.04e+01, train loss: 4.77179e-05, val loss: 5.57008e-05, min loss: 1.86052e-05\n",
      "Epoch: 37200, elapsed: 1.01e+01, train loss: 2.02622e-05, val loss: 2.43590e-05, min loss: 1.86052e-05\n",
      "Epoch: 37300, elapsed: 1.03e+01, train loss: 2.25398e-05, val loss: 2.60764e-05, min loss: 1.86052e-05\n",
      "Epoch: 37400, elapsed: 1.01e+01, train loss: 2.03055e-05, val loss: 2.33351e-05, min loss: 1.86052e-05\n",
      "Epoch: 37500, elapsed: 9.99e+00, train loss: 1.83018e-05, val loss: 2.12390e-05, min loss: 1.83018e-05\n",
      "Epoch: 37600, elapsed: 1.04e+01, train loss: 2.61709e-05, val loss: 3.07904e-05, min loss: 1.83018e-05\n",
      "Epoch: 37700, elapsed: 9.96e+00, train loss: 1.93670e-05, val loss: 2.20841e-05, min loss: 1.83018e-05\n",
      "Epoch: 37800, elapsed: 1.03e+01, train loss: 2.29180e-05, val loss: 2.38875e-05, min loss: 1.83018e-05\n",
      "Epoch: 37900, elapsed: 1.02e+01, train loss: 1.99667e-05, val loss: 2.21575e-05, min loss: 1.83018e-05\n",
      "Epoch: 38000, elapsed: 1.03e+01, train loss: 2.30848e-05, val loss: 2.81889e-05, min loss: 1.83018e-05\n",
      "Epoch: 38100, elapsed: 1.01e+01, train loss: 1.91245e-05, val loss: 2.09479e-05, min loss: 1.83018e-05\n",
      "Epoch: 38200, elapsed: 1.01e+01, train loss: 2.09712e-05, val loss: 2.35415e-05, min loss: 1.83018e-05\n",
      "Epoch: 38300, elapsed: 1.04e+01, train loss: 2.17085e-05, val loss: 2.38568e-05, min loss: 1.83018e-05\n",
      "Epoch: 38400, elapsed: 1.02e+01, train loss: 3.55409e-05, val loss: 3.28944e-05, min loss: 1.83018e-05\n",
      "Epoch: 38500, elapsed: 1.03e+01, train loss: 1.79744e-05, val loss: 2.04848e-05, min loss: 1.79744e-05\n",
      "Epoch: 38600, elapsed: 1.03e+01, train loss: 1.79469e-05, val loss: 2.06262e-05, min loss: 1.79469e-05\n",
      "Epoch: 38700, elapsed: 1.01e+01, train loss: 1.88881e-05, val loss: 2.25798e-05, min loss: 1.79469e-05\n",
      "Epoch: 38800, elapsed: 1.05e+01, train loss: 2.22938e-05, val loss: 2.43962e-05, min loss: 1.79469e-05\n",
      "Epoch: 38900, elapsed: 1.01e+01, train loss: 1.77850e-05, val loss: 2.04521e-05, min loss: 1.77850e-05\n",
      "Epoch: 39000, elapsed: 1.02e+01, train loss: 1.77385e-05, val loss: 2.01932e-05, min loss: 1.77385e-05\n",
      "Epoch: 39100, elapsed: 1.01e+01, train loss: 1.84549e-05, val loss: 1.98739e-05, min loss: 1.77385e-05\n",
      "Epoch: 39200, elapsed: 1.05e+01, train loss: 1.75634e-05, val loss: 2.02404e-05, min loss: 1.75634e-05\n",
      "Epoch: 39300, elapsed: 1.02e+01, train loss: 1.83721e-05, val loss: 2.07416e-05, min loss: 1.75634e-05\n",
      "Epoch: 39400, elapsed: 1.01e+01, train loss: 2.01180e-05, val loss: 2.17986e-05, min loss: 1.75634e-05\n",
      "Epoch: 39500, elapsed: 1.04e+01, train loss: 2.93986e-05, val loss: 3.26524e-05, min loss: 1.75634e-05\n",
      "Epoch: 39600, elapsed: 1.01e+01, train loss: 1.77769e-05, val loss: 2.04594e-05, min loss: 1.75634e-05\n",
      "Epoch: 39700, elapsed: 1.02e+01, train loss: 2.27299e-05, val loss: 2.51025e-05, min loss: 1.75634e-05\n",
      "Epoch: 39800, elapsed: 1.02e+01, train loss: 2.29623e-05, val loss: 2.49227e-05, min loss: 1.75634e-05\n",
      "Epoch: 39900, elapsed: 9.89e+00, train loss: 1.76802e-05, val loss: 1.97004e-05, min loss: 1.75634e-05\n",
      "Epoch: 40000, elapsed: 1.05e+01, train loss: 2.82418e-05, val loss: 3.48946e-05, min loss: 1.75634e-05\n",
      "Epoch: 40100, elapsed: 1.19e+01, train loss: 2.12632e-05, val loss: 2.21743e-05, min loss: 1.75634e-05\n",
      "Epoch: 40200, elapsed: 1.04e+01, train loss: 2.18997e-05, val loss: 2.43200e-05, min loss: 1.75634e-05\n",
      "Epoch: 40300, elapsed: 1.01e+01, train loss: 2.20788e-05, val loss: 2.36360e-05, min loss: 1.75634e-05\n",
      "Epoch: 40400, elapsed: 1.03e+01, train loss: 1.85852e-05, val loss: 2.06992e-05, min loss: 1.75634e-05\n",
      "Epoch: 40500, elapsed: 1.01e+01, train loss: 2.34118e-05, val loss: 2.62673e-05, min loss: 1.75634e-05\n",
      "Epoch: 40600, elapsed: 9.96e+00, train loss: 2.02408e-05, val loss: 2.27452e-05, min loss: 1.75634e-05\n",
      "Epoch: 40700, elapsed: 1.04e+01, train loss: 1.77364e-05, val loss: 2.03132e-05, min loss: 1.75634e-05\n",
      "Epoch: 40800, elapsed: 1.01e+01, train loss: 1.96740e-05, val loss: 2.11958e-05, min loss: 1.75634e-05\n",
      "Epoch: 40900, elapsed: 1.03e+01, train loss: 1.71303e-05, val loss: 1.94045e-05, min loss: 1.71303e-05\n",
      "Epoch: 41000, elapsed: 1.02e+01, train loss: 1.68377e-05, val loss: 1.91666e-05, min loss: 1.68377e-05\n",
      "Epoch: 41100, elapsed: 9.95e+00, train loss: 2.07516e-05, val loss: 2.30222e-05, min loss: 1.68377e-05\n",
      "Epoch: 41200, elapsed: 1.04e+01, train loss: 2.78520e-05, val loss: 3.22819e-05, min loss: 1.68377e-05\n",
      "Epoch: 41300, elapsed: 1.00e+01, train loss: 1.74727e-05, val loss: 1.97815e-05, min loss: 1.68377e-05\n",
      "Epoch: 41400, elapsed: 1.05e+01, train loss: 1.68819e-05, val loss: 1.91169e-05, min loss: 1.68377e-05\n",
      "Epoch: 41500, elapsed: 1.03e+01, train loss: 2.15127e-05, val loss: 2.45170e-05, min loss: 1.68377e-05\n",
      "Epoch: 41600, elapsed: 9.93e+00, train loss: 1.68250e-05, val loss: 1.85484e-05, min loss: 1.68250e-05\n",
      "Epoch: 41700, elapsed: 1.05e+01, train loss: 1.93614e-05, val loss: 2.02722e-05, min loss: 1.68250e-05\n",
      "Epoch: 41800, elapsed: 1.01e+01, train loss: 1.75128e-05, val loss: 1.85188e-05, min loss: 1.68250e-05\n",
      "Epoch: 41900, elapsed: 1.05e+01, train loss: 1.64912e-05, val loss: 1.86199e-05, min loss: 1.64912e-05\n",
      "Epoch: 42000, elapsed: 1.01e+01, train loss: 1.90885e-05, val loss: 2.24596e-05, min loss: 1.64912e-05\n",
      "Epoch: 42100, elapsed: 1.01e+01, train loss: 1.65932e-05, val loss: 1.88291e-05, min loss: 1.64912e-05\n",
      "Epoch: 42200, elapsed: 1.05e+01, train loss: 1.87805e-05, val loss: 2.03021e-05, min loss: 1.64912e-05\n",
      "Epoch: 42300, elapsed: 1.00e+01, train loss: 2.03245e-05, val loss: 2.42634e-05, min loss: 1.64912e-05\n",
      "Epoch: 42400, elapsed: 1.03e+01, train loss: 1.78510e-05, val loss: 2.02171e-05, min loss: 1.64912e-05\n",
      "Epoch: 42500, elapsed: 9.93e+00, train loss: 1.92923e-05, val loss: 2.01420e-05, min loss: 1.64912e-05\n",
      "Epoch: 42600, elapsed: 1.03e+01, train loss: 1.65563e-05, val loss: 1.81777e-05, min loss: 1.64912e-05\n",
      "Epoch: 42700, elapsed: 1.01e+01, train loss: 1.96540e-05, val loss: 2.11789e-05, min loss: 1.64912e-05\n",
      "Epoch: 42800, elapsed: 1.01e+01, train loss: 1.70006e-05, val loss: 1.89226e-05, min loss: 1.64912e-05\n",
      "Epoch: 42900, elapsed: 1.04e+01, train loss: 2.20925e-05, val loss: 2.53205e-05, min loss: 1.64912e-05\n",
      "Epoch: 43000, elapsed: 9.96e+00, train loss: 2.41438e-05, val loss: 2.21812e-05, min loss: 1.64912e-05\n",
      "Epoch: 43100, elapsed: 1.04e+01, train loss: 1.72701e-05, val loss: 1.89246e-05, min loss: 1.64912e-05\n",
      "Epoch: 43200, elapsed: 1.01e+01, train loss: 1.63855e-05, val loss: 1.81090e-05, min loss: 1.63855e-05\n",
      "Epoch: 43300, elapsed: 9.91e+00, train loss: 2.19049e-05, val loss: 2.34529e-05, min loss: 1.63855e-05\n",
      "Epoch: 43400, elapsed: 1.04e+01, train loss: 1.60796e-05, val loss: 1.82493e-05, min loss: 1.60796e-05\n",
      "Epoch: 43500, elapsed: 9.98e+00, train loss: 2.30122e-05, val loss: 2.43351e-05, min loss: 1.60796e-05\n",
      "Epoch: 43600, elapsed: 1.02e+01, train loss: 1.59913e-05, val loss: 1.78366e-05, min loss: 1.59913e-05\n",
      "Epoch: 43700, elapsed: 1.02e+01, train loss: 1.57025e-05, val loss: 1.74911e-05, min loss: 1.57025e-05\n",
      "Epoch: 43800, elapsed: 1.01e+01, train loss: 1.57154e-05, val loss: 1.74307e-05, min loss: 1.57025e-05\n",
      "Epoch: 43900, elapsed: 1.05e+01, train loss: 1.56203e-05, val loss: 1.75020e-05, min loss: 1.56203e-05\n",
      "Epoch: 44000, elapsed: 1.01e+01, train loss: 1.55576e-05, val loss: 1.73201e-05, min loss: 1.55576e-05\n",
      "Epoch: 44100, elapsed: 1.04e+01, train loss: 1.55156e-05, val loss: 1.72859e-05, min loss: 1.55156e-05\n",
      "Epoch: 44200, elapsed: 1.01e+01, train loss: 2.39735e-05, val loss: 2.77995e-05, min loss: 1.55156e-05\n",
      "Epoch: 44300, elapsed: 1.00e+01, train loss: 1.82107e-05, val loss: 1.94176e-05, min loss: 1.55156e-05\n",
      "Epoch: 44400, elapsed: 1.04e+01, train loss: 1.93279e-05, val loss: 2.13185e-05, min loss: 1.55156e-05\n",
      "Epoch: 44500, elapsed: 1.00e+01, train loss: 2.71171e-05, val loss: 2.95209e-05, min loss: 1.55156e-05\n",
      "Epoch: 44600, elapsed: 1.03e+01, train loss: 1.71553e-05, val loss: 1.83484e-05, min loss: 1.55156e-05\n",
      "Epoch: 44700, elapsed: 1.01e+01, train loss: 1.53700e-05, val loss: 1.71287e-05, min loss: 1.53700e-05\n",
      "Epoch: 44800, elapsed: 9.92e+00, train loss: 1.53584e-05, val loss: 1.71281e-05, min loss: 1.53584e-05\n",
      "Epoch: 44900, elapsed: 1.03e+01, train loss: 1.54614e-05, val loss: 1.74003e-05, min loss: 1.53584e-05\n",
      "Epoch: 45000, elapsed: 1.00e+01, train loss: 1.58593e-05, val loss: 1.78969e-05, min loss: 1.53584e-05\n",
      "Epoch: 45100, elapsed: 1.21e+01, train loss: 1.77656e-05, val loss: 1.87951e-05, min loss: 1.53584e-05\n",
      "Epoch: 45200, elapsed: 1.02e+01, train loss: 1.52812e-05, val loss: 1.69875e-05, min loss: 1.52812e-05\n",
      "Epoch: 45300, elapsed: 9.93e+00, train loss: 2.17908e-05, val loss: 2.46920e-05, min loss: 1.52812e-05\n",
      "Epoch: 45400, elapsed: 1.05e+01, train loss: 1.77265e-05, val loss: 1.79801e-05, min loss: 1.52812e-05\n",
      "Epoch: 45500, elapsed: 9.96e+00, train loss: 2.19060e-05, val loss: 2.61441e-05, min loss: 1.52812e-05\n",
      "Epoch: 45600, elapsed: 1.04e+01, train loss: 1.75811e-05, val loss: 1.97869e-05, min loss: 1.52812e-05\n",
      "Epoch: 45700, elapsed: 1.02e+01, train loss: 1.51246e-05, val loss: 1.65849e-05, min loss: 1.51246e-05\n",
      "Epoch: 45800, elapsed: 1.00e+01, train loss: 1.66152e-05, val loss: 1.82007e-05, min loss: 1.51246e-05\n",
      "Epoch: 45900, elapsed: 1.03e+01, train loss: 1.74720e-05, val loss: 1.91564e-05, min loss: 1.51246e-05\n",
      "Epoch: 46000, elapsed: 1.01e+01, train loss: 1.69796e-05, val loss: 1.84175e-05, min loss: 1.51246e-05\n",
      "Epoch: 46100, elapsed: 1.04e+01, train loss: 1.90724e-05, val loss: 1.98321e-05, min loss: 1.51246e-05\n",
      "Epoch: 46200, elapsed: 1.01e+01, train loss: 1.50902e-05, val loss: 1.66182e-05, min loss: 1.50902e-05\n",
      "Epoch: 46300, elapsed: 9.91e+00, train loss: 1.83732e-05, val loss: 2.00062e-05, min loss: 1.50902e-05\n",
      "Epoch: 46400, elapsed: 1.05e+01, train loss: 1.88951e-05, val loss: 2.07199e-05, min loss: 1.50902e-05\n",
      "Epoch: 46500, elapsed: 1.00e+01, train loss: 1.66319e-05, val loss: 1.71599e-05, min loss: 1.50902e-05\n",
      "Epoch: 46600, elapsed: 1.04e+01, train loss: 1.50503e-05, val loss: 1.63862e-05, min loss: 1.50503e-05\n",
      "Epoch: 46700, elapsed: 1.00e+01, train loss: 2.01771e-05, val loss: 2.09368e-05, min loss: 1.50503e-05\n",
      "Epoch: 46800, elapsed: 1.00e+01, train loss: 2.16945e-05, val loss: 2.20140e-05, min loss: 1.50503e-05\n",
      "Epoch: 46900, elapsed: 1.03e+01, train loss: 1.64782e-05, val loss: 1.82959e-05, min loss: 1.50503e-05\n",
      "Epoch: 47000, elapsed: 9.97e+00, train loss: 1.55979e-05, val loss: 1.68699e-05, min loss: 1.50503e-05\n",
      "Epoch: 47100, elapsed: 1.02e+01, train loss: 1.73830e-05, val loss: 1.77185e-05, min loss: 1.50503e-05\n",
      "Epoch: 47200, elapsed: 1.02e+01, train loss: 1.58816e-05, val loss: 1.80356e-05, min loss: 1.50503e-05\n",
      "Epoch: 47300, elapsed: 9.90e+00, train loss: 1.68295e-05, val loss: 1.85386e-05, min loss: 1.50503e-05\n",
      "Epoch: 47400, elapsed: 1.03e+01, train loss: 1.58968e-05, val loss: 1.73679e-05, min loss: 1.50503e-05\n",
      "Epoch: 47500, elapsed: 9.95e+00, train loss: 1.90225e-05, val loss: 1.89455e-05, min loss: 1.50503e-05\n",
      "Epoch: 47600, elapsed: 9.93e+00, train loss: 1.47665e-05, val loss: 1.63839e-05, min loss: 1.47665e-05\n",
      "Epoch: 47700, elapsed: 1.05e+01, train loss: 2.13598e-05, val loss: 2.06964e-05, min loss: 1.47665e-05\n",
      "Epoch: 47800, elapsed: 9.92e+00, train loss: 1.54181e-05, val loss: 1.71427e-05, min loss: 1.47665e-05\n",
      "Epoch: 47900, elapsed: 1.04e+01, train loss: 1.87533e-05, val loss: 2.16381e-05, min loss: 1.47665e-05\n",
      "Epoch: 48000, elapsed: 1.03e+01, train loss: 1.43707e-05, val loss: 1.63193e-05, min loss: 1.43707e-05\n",
      "Epoch: 48100, elapsed: 1.00e+01, train loss: 1.63963e-05, val loss: 1.77795e-05, min loss: 1.43707e-05\n",
      "Epoch: 48200, elapsed: 1.04e+01, train loss: 2.15788e-05, val loss: 2.52215e-05, min loss: 1.43707e-05\n",
      "Epoch: 48300, elapsed: 9.95e+00, train loss: 1.79713e-05, val loss: 1.92567e-05, min loss: 1.43707e-05\n",
      "Epoch: 48400, elapsed: 1.03e+01, train loss: 1.45145e-05, val loss: 1.57786e-05, min loss: 1.43707e-05\n",
      "Epoch: 48500, elapsed: 1.01e+01, train loss: 2.25642e-05, val loss: 2.35167e-05, min loss: 1.43707e-05\n",
      "Epoch: 48600, elapsed: 1.00e+01, train loss: 1.77271e-05, val loss: 1.88312e-05, min loss: 1.43707e-05\n",
      "Epoch: 48700, elapsed: 1.04e+01, train loss: 2.57714e-05, val loss: 2.97309e-05, min loss: 1.43707e-05\n",
      "Epoch: 48800, elapsed: 1.01e+01, train loss: 1.62521e-05, val loss: 1.73526e-05, min loss: 1.43707e-05\n",
      "Epoch: 48900, elapsed: 1.04e+01, train loss: 1.60816e-05, val loss: 1.73930e-05, min loss: 1.43707e-05\n",
      "Epoch: 49000, elapsed: 1.02e+01, train loss: 1.50157e-05, val loss: 1.69170e-05, min loss: 1.43707e-05\n",
      "Epoch: 49100, elapsed: 9.92e+00, train loss: 1.49487e-05, val loss: 1.58061e-05, min loss: 1.43707e-05\n",
      "Epoch: 49200, elapsed: 1.03e+01, train loss: 2.44696e-05, val loss: 2.46677e-05, min loss: 1.43707e-05\n",
      "Epoch: 49300, elapsed: 1.01e+01, train loss: 1.43542e-05, val loss: 1.59829e-05, min loss: 1.43542e-05\n",
      "Epoch: 49400, elapsed: 9.95e+00, train loss: 1.49329e-05, val loss: 1.62106e-05, min loss: 1.43542e-05\n",
      "Epoch: 49500, elapsed: 1.03e+01, train loss: 2.01245e-05, val loss: 2.19384e-05, min loss: 1.43542e-05\n",
      "Epoch: 49600, elapsed: 9.95e+00, train loss: 1.48197e-05, val loss: 1.59229e-05, min loss: 1.43542e-05\n",
      "Epoch: 49700, elapsed: 1.04e+01, train loss: 2.36896e-05, val loss: 2.38049e-05, min loss: 1.43542e-05\n",
      "Epoch: 49800, elapsed: 1.02e+01, train loss: 2.17247e-05, val loss: 2.27439e-05, min loss: 1.43542e-05\n",
      "Epoch: 49900, elapsed: 9.94e+00, train loss: 1.97122e-05, val loss: 2.28379e-05, min loss: 1.43542e-05\n",
      "Epoch: 50000, elapsed: 1.04e+01, train loss: 1.85488e-05, val loss: 2.21417e-05, min loss: 1.43542e-05\n",
      "Epoch: 50100, elapsed: 1.19e+01, train loss: 1.84675e-05, val loss: 1.71805e-05, min loss: 1.43542e-05\n",
      "Epoch: 50200, elapsed: 1.03e+01, train loss: 1.84212e-05, val loss: 1.81492e-05, min loss: 1.43542e-05\n",
      "Epoch: 50300, elapsed: 1.01e+01, train loss: 1.49636e-05, val loss: 1.64752e-05, min loss: 1.43542e-05\n",
      "Epoch: 50400, elapsed: 1.00e+01, train loss: 2.71435e-05, val loss: 2.89301e-05, min loss: 1.43542e-05\n",
      "Epoch: 50500, elapsed: 1.04e+01, train loss: 1.51142e-05, val loss: 1.65088e-05, min loss: 1.43542e-05\n",
      "Epoch: 50600, elapsed: 9.99e+00, train loss: 1.68882e-05, val loss: 1.72525e-05, min loss: 1.43542e-05\n",
      "Epoch: 50700, elapsed: 1.05e+01, train loss: 1.37121e-05, val loss: 1.50785e-05, min loss: 1.37121e-05\n",
      "Epoch: 50800, elapsed: 1.01e+01, train loss: 1.65457e-05, val loss: 1.97930e-05, min loss: 1.37121e-05\n",
      "Epoch: 50900, elapsed: 1.00e+01, train loss: 1.36710e-05, val loss: 1.51034e-05, min loss: 1.36710e-05\n",
      "Epoch: 51000, elapsed: 1.03e+01, train loss: 1.38267e-05, val loss: 1.51508e-05, min loss: 1.36710e-05\n",
      "Epoch: 51100, elapsed: 1.00e+01, train loss: 1.63195e-05, val loss: 1.78983e-05, min loss: 1.36710e-05\n",
      "Epoch: 51200, elapsed: 1.04e+01, train loss: 2.00794e-05, val loss: 2.13629e-05, min loss: 1.36710e-05\n",
      "Epoch: 51300, elapsed: 1.01e+01, train loss: 1.40982e-05, val loss: 1.68098e-05, min loss: 1.36710e-05\n",
      "Epoch: 51400, elapsed: 1.00e+01, train loss: 1.93933e-05, val loss: 2.16272e-05, min loss: 1.36710e-05\n",
      "Epoch: 51500, elapsed: 1.03e+01, train loss: 1.88585e-05, val loss: 1.92462e-05, min loss: 1.36710e-05\n",
      "Epoch: 51600, elapsed: 1.01e+01, train loss: 1.33499e-05, val loss: 1.47052e-05, min loss: 1.33499e-05\n",
      "Epoch: 51700, elapsed: 9.94e+00, train loss: 2.21105e-05, val loss: 2.23786e-05, min loss: 1.33499e-05\n",
      "Epoch: 51800, elapsed: 1.03e+01, train loss: 1.77001e-05, val loss: 1.86131e-05, min loss: 1.33499e-05\n",
      "Epoch: 51900, elapsed: 9.90e+00, train loss: 1.47338e-05, val loss: 1.60428e-05, min loss: 1.33499e-05\n",
      "Epoch: 52000, elapsed: 1.03e+01, train loss: 1.37147e-05, val loss: 1.60981e-05, min loss: 1.33499e-05\n",
      "Epoch: 52100, elapsed: 1.01e+01, train loss: 1.39404e-05, val loss: 1.57130e-05, min loss: 1.33499e-05\n",
      "Epoch: 52200, elapsed: 1.01e+01, train loss: 1.43291e-05, val loss: 1.55161e-05, min loss: 1.33499e-05\n",
      "Epoch: 52300, elapsed: 1.04e+01, train loss: 1.43347e-05, val loss: 1.63374e-05, min loss: 1.33499e-05\n",
      "Epoch: 52400, elapsed: 1.01e+01, train loss: 2.35154e-05, val loss: 3.09095e-05, min loss: 1.33499e-05\n",
      "Epoch: 52500, elapsed: 9.97e+00, train loss: 1.31389e-05, val loss: 1.45637e-05, min loss: 1.31389e-05\n",
      "Epoch: 52600, elapsed: 1.04e+01, train loss: 1.34605e-05, val loss: 1.50959e-05, min loss: 1.31389e-05\n",
      "Epoch: 52700, elapsed: 1.02e+01, train loss: 1.54660e-05, val loss: 1.67670e-05, min loss: 1.31389e-05\n",
      "Epoch: 52800, elapsed: 1.03e+01, train loss: 1.42832e-05, val loss: 1.57626e-05, min loss: 1.31389e-05\n",
      "Epoch: 52900, elapsed: 1.01e+01, train loss: 1.32145e-05, val loss: 1.45125e-05, min loss: 1.31389e-05\n",
      "Epoch: 53000, elapsed: 9.88e+00, train loss: 2.44197e-05, val loss: 2.01022e-05, min loss: 1.31389e-05\n",
      "Epoch: 53100, elapsed: 1.03e+01, train loss: 2.00235e-05, val loss: 1.79988e-05, min loss: 1.31389e-05\n",
      "Epoch: 53200, elapsed: 1.01e+01, train loss: 1.40496e-05, val loss: 1.44823e-05, min loss: 1.31389e-05\n",
      "Epoch: 53300, elapsed: 1.01e+01, train loss: 2.21479e-05, val loss: 2.15736e-05, min loss: 1.31389e-05\n",
      "Epoch: 53400, elapsed: 1.04e+01, train loss: 1.76869e-05, val loss: 1.76105e-05, min loss: 1.31389e-05\n",
      "Epoch: 53500, elapsed: 9.89e+00, train loss: 1.30636e-05, val loss: 1.46047e-05, min loss: 1.30636e-05\n",
      "Epoch: 53600, elapsed: 1.02e+01, train loss: 3.12038e-05, val loss: 3.45411e-05, min loss: 1.30636e-05\n",
      "Epoch: 53700, elapsed: 9.97e+00, train loss: 1.29440e-05, val loss: 1.44849e-05, min loss: 1.29440e-05\n",
      "Epoch: 53800, elapsed: 9.96e+00, train loss: 1.37893e-05, val loss: 1.51499e-05, min loss: 1.29440e-05\n",
      "Epoch: 53900, elapsed: 1.04e+01, train loss: 1.30553e-05, val loss: 1.46485e-05, min loss: 1.29440e-05\n",
      "Epoch: 54000, elapsed: 1.00e+01, train loss: 2.35866e-05, val loss: 2.26241e-05, min loss: 1.29440e-05\n",
      "Epoch: 54100, elapsed: 9.99e+00, train loss: 1.45255e-05, val loss: 1.62677e-05, min loss: 1.29440e-05\n",
      "Epoch: 54200, elapsed: 1.05e+01, train loss: 1.94342e-05, val loss: 2.05202e-05, min loss: 1.29440e-05\n",
      "Epoch: 54300, elapsed: 1.00e+01, train loss: 1.32796e-05, val loss: 1.51118e-05, min loss: 1.29440e-05\n",
      "Epoch: 54400, elapsed: 1.03e+01, train loss: 1.32983e-05, val loss: 1.47824e-05, min loss: 1.29440e-05\n",
      "Epoch: 54500, elapsed: 1.02e+01, train loss: 1.33149e-05, val loss: 1.47394e-05, min loss: 1.29440e-05\n",
      "Epoch: 54600, elapsed: 9.94e+00, train loss: 1.29731e-05, val loss: 1.48050e-05, min loss: 1.29440e-05\n",
      "Epoch: 54700, elapsed: 1.02e+01, train loss: 1.74532e-05, val loss: 1.72410e-05, min loss: 1.29440e-05\n",
      "Epoch: 54800, elapsed: 9.95e+00, train loss: 1.35766e-05, val loss: 1.52676e-05, min loss: 1.29440e-05\n",
      "Epoch: 54900, elapsed: 9.90e+00, train loss: 1.45042e-05, val loss: 1.53293e-05, min loss: 1.29440e-05\n",
      "Epoch: 55000, elapsed: 1.04e+01, train loss: 1.30143e-05, val loss: 1.46643e-05, min loss: 1.29440e-05\n",
      "Epoch: 55100, elapsed: 1.17e+01, train loss: 1.90354e-05, val loss: 1.77515e-05, min loss: 1.29440e-05\n",
      "Epoch: 55200, elapsed: 1.04e+01, train loss: 1.36771e-05, val loss: 1.54134e-05, min loss: 1.29440e-05\n",
      "Epoch: 55300, elapsed: 1.01e+01, train loss: 1.95558e-05, val loss: 2.14428e-05, min loss: 1.29440e-05\n",
      "Epoch: 55400, elapsed: 1.00e+01, train loss: 1.30761e-05, val loss: 1.42736e-05, min loss: 1.29440e-05\n",
      "Epoch: 55500, elapsed: 1.04e+01, train loss: 1.28161e-05, val loss: 1.41562e-05, min loss: 1.28161e-05\n",
      "Epoch: 55600, elapsed: 1.01e+01, train loss: 1.45223e-05, val loss: 1.57478e-05, min loss: 1.28161e-05\n",
      "Epoch: 55700, elapsed: 1.03e+01, train loss: 1.26419e-05, val loss: 1.42530e-05, min loss: 1.26419e-05\n",
      "Epoch: 55800, elapsed: 1.02e+01, train loss: 1.32070e-05, val loss: 1.45033e-05, min loss: 1.26419e-05\n",
      "Epoch: 55900, elapsed: 9.93e+00, train loss: 1.32161e-05, val loss: 1.46815e-05, min loss: 1.26419e-05\n",
      "Epoch: 56000, elapsed: 1.04e+01, train loss: 1.34470e-05, val loss: 1.48413e-05, min loss: 1.26419e-05\n",
      "Epoch: 56100, elapsed: 1.02e+01, train loss: 1.65195e-05, val loss: 1.83070e-05, min loss: 1.26419e-05\n",
      "Epoch: 56200, elapsed: 1.02e+01, train loss: 1.55648e-05, val loss: 1.75416e-05, min loss: 1.26419e-05\n",
      "Epoch: 56300, elapsed: 1.03e+01, train loss: 1.31663e-05, val loss: 1.48470e-05, min loss: 1.26419e-05\n",
      "Epoch: 56400, elapsed: 9.99e+00, train loss: 1.61155e-05, val loss: 1.64333e-05, min loss: 1.26419e-05\n",
      "Epoch: 56500, elapsed: 1.00e+01, train loss: 1.26065e-05, val loss: 1.42313e-05, min loss: 1.26065e-05\n",
      "Epoch: 56600, elapsed: 1.04e+01, train loss: 2.62935e-05, val loss: 2.55632e-05, min loss: 1.26065e-05\n",
      "Epoch: 56700, elapsed: 1.00e+01, train loss: 1.85215e-05, val loss: 2.11093e-05, min loss: 1.26065e-05\n",
      "Epoch: 56800, elapsed: 1.04e+01, train loss: 1.52262e-05, val loss: 1.71040e-05, min loss: 1.26065e-05\n",
      "Epoch: 56900, elapsed: 1.02e+01, train loss: 2.21289e-05, val loss: 2.53193e-05, min loss: 1.26065e-05\n",
      "Epoch: 57000, elapsed: 1.01e+01, train loss: 1.28561e-05, val loss: 1.41701e-05, min loss: 1.26065e-05\n",
      "Epoch: 57100, elapsed: 1.04e+01, train loss: 2.81908e-05, val loss: 2.66332e-05, min loss: 1.26065e-05\n",
      "Epoch: 57200, elapsed: 1.01e+01, train loss: 1.24198e-05, val loss: 1.39468e-05, min loss: 1.24198e-05\n",
      "Epoch: 57300, elapsed: 1.00e+01, train loss: 1.34423e-05, val loss: 1.55815e-05, min loss: 1.24198e-05\n",
      "Epoch: 57400, elapsed: 1.04e+01, train loss: 1.56458e-05, val loss: 1.89642e-05, min loss: 1.24198e-05\n",
      "Epoch: 57500, elapsed: 1.01e+01, train loss: 1.24445e-05, val loss: 1.39152e-05, min loss: 1.24198e-05\n",
      "Epoch: 57600, elapsed: 1.00e+01, train loss: 1.29735e-05, val loss: 1.48402e-05, min loss: 1.24198e-05\n",
      "Epoch: 57700, elapsed: 1.04e+01, train loss: 2.78706e-05, val loss: 2.63048e-05, min loss: 1.24198e-05\n",
      "Epoch: 57800, elapsed: 1.00e+01, train loss: 1.34698e-05, val loss: 1.49216e-05, min loss: 1.24198e-05\n",
      "Epoch: 57900, elapsed: 1.05e+01, train loss: 1.27040e-05, val loss: 1.44094e-05, min loss: 1.24198e-05\n",
      "Epoch: 58000, elapsed: 1.01e+01, train loss: 1.56680e-05, val loss: 1.91674e-05, min loss: 1.24198e-05\n",
      "Epoch: 58100, elapsed: 1.00e+01, train loss: 1.27213e-05, val loss: 1.44993e-05, min loss: 1.24198e-05\n",
      "Epoch: 58200, elapsed: 1.05e+01, train loss: 2.32880e-05, val loss: 1.84831e-05, min loss: 1.24198e-05\n",
      "Epoch: 58300, elapsed: 9.91e+00, train loss: 1.71764e-05, val loss: 1.96081e-05, min loss: 1.24198e-05\n",
      "Epoch: 58400, elapsed: 9.89e+00, train loss: 1.53490e-05, val loss: 1.65498e-05, min loss: 1.24198e-05\n",
      "Epoch: 58500, elapsed: 1.05e+01, train loss: 1.46905e-05, val loss: 1.68621e-05, min loss: 1.24198e-05\n",
      "Epoch: 58600, elapsed: 9.97e+00, train loss: 1.66267e-05, val loss: 1.86423e-05, min loss: 1.24198e-05\n",
      "Epoch: 58700, elapsed: 1.00e+01, train loss: 1.49129e-05, val loss: 1.60947e-05, min loss: 1.24198e-05\n",
      "Epoch: 58800, elapsed: 1.04e+01, train loss: 1.22306e-05, val loss: 1.38860e-05, min loss: 1.22306e-05\n",
      "Epoch: 58900, elapsed: 1.00e+01, train loss: 1.21750e-05, val loss: 1.37784e-05, min loss: 1.21750e-05\n",
      "Epoch: 59000, elapsed: 1.05e+01, train loss: 1.21683e-05, val loss: 1.37949e-05, min loss: 1.21683e-05\n",
      "Epoch: 59100, elapsed: 1.01e+01, train loss: 1.22407e-05, val loss: 1.38342e-05, min loss: 1.21683e-05\n",
      "Epoch: 59200, elapsed: 1.00e+01, train loss: 1.28645e-05, val loss: 1.42651e-05, min loss: 1.21683e-05\n",
      "Epoch: 59300, elapsed: 1.04e+01, train loss: 1.27665e-05, val loss: 1.41106e-05, min loss: 1.21683e-05\n",
      "Epoch: 59400, elapsed: 1.01e+01, train loss: 1.80692e-05, val loss: 2.10622e-05, min loss: 1.21683e-05\n",
      "Epoch: 59500, elapsed: 1.01e+01, train loss: 1.26113e-05, val loss: 1.39951e-05, min loss: 1.21683e-05\n",
      "Epoch: 59600, elapsed: 1.05e+01, train loss: 1.22765e-05, val loss: 1.38684e-05, min loss: 1.21683e-05\n",
      "Epoch: 59700, elapsed: 1.01e+01, train loss: 1.52736e-05, val loss: 1.46460e-05, min loss: 1.21683e-05\n",
      "Epoch: 59800, elapsed: 1.00e+01, train loss: 1.33774e-05, val loss: 1.45801e-05, min loss: 1.21683e-05\n",
      "Epoch: 59900, elapsed: 1.04e+01, train loss: 2.36328e-05, val loss: 2.55846e-05, min loss: 1.21683e-05\n",
      "Epoch: 60000, elapsed: 9.96e+00, train loss: 1.21626e-05, val loss: 1.38585e-05, min loss: 1.21626e-05\n",
      "Epoch: 60100, elapsed: 1.22e+01, train loss: 1.33950e-05, val loss: 1.45348e-05, min loss: 1.21626e-05\n",
      "Epoch: 60200, elapsed: 1.01e+01, train loss: 1.27184e-05, val loss: 1.43007e-05, min loss: 1.21626e-05\n",
      "Epoch: 60300, elapsed: 1.00e+01, train loss: 1.43325e-05, val loss: 1.63483e-05, min loss: 1.21626e-05\n",
      "Epoch: 60400, elapsed: 1.06e+01, train loss: 2.58252e-05, val loss: 2.82490e-05, min loss: 1.21626e-05\n",
      "Epoch: 60500, elapsed: 1.01e+01, train loss: 1.48824e-05, val loss: 1.61388e-05, min loss: 1.21626e-05\n",
      "Epoch: 60600, elapsed: 9.92e+00, train loss: 1.31030e-05, val loss: 1.52202e-05, min loss: 1.21626e-05\n",
      "Epoch: 60700, elapsed: 1.05e+01, train loss: 1.26569e-05, val loss: 1.43740e-05, min loss: 1.21626e-05\n",
      "Epoch: 60800, elapsed: 9.97e+00, train loss: 2.53652e-05, val loss: 2.88014e-05, min loss: 1.21626e-05\n",
      "Epoch: 60900, elapsed: 9.99e+00, train loss: 1.24902e-05, val loss: 1.40059e-05, min loss: 1.21626e-05\n",
      "Epoch: 61000, elapsed: 1.06e+01, train loss: 1.72569e-05, val loss: 2.06837e-05, min loss: 1.21626e-05\n",
      "Epoch: 61100, elapsed: 1.01e+01, train loss: 1.24576e-05, val loss: 1.38335e-05, min loss: 1.21626e-05\n",
      "Epoch: 61200, elapsed: 1.05e+01, train loss: 1.34805e-05, val loss: 1.56605e-05, min loss: 1.21626e-05\n",
      "Epoch: 61300, elapsed: 1.01e+01, train loss: 1.22379e-05, val loss: 1.38295e-05, min loss: 1.21626e-05\n",
      "Epoch: 61400, elapsed: 1.02e+01, train loss: 1.27166e-05, val loss: 1.46389e-05, min loss: 1.21626e-05\n",
      "Epoch: 61500, elapsed: 1.05e+01, train loss: 1.42786e-05, val loss: 1.58088e-05, min loss: 1.21626e-05\n",
      "Epoch: 61600, elapsed: 1.01e+01, train loss: 2.40141e-05, val loss: 2.46375e-05, min loss: 1.21626e-05\n",
      "Epoch: 61700, elapsed: 1.02e+01, train loss: 1.21552e-05, val loss: 1.38033e-05, min loss: 1.21552e-05\n",
      "Epoch: 61800, elapsed: 1.04e+01, train loss: 1.18761e-05, val loss: 1.36159e-05, min loss: 1.18761e-05\n",
      "Epoch: 61900, elapsed: 1.01e+01, train loss: 1.18605e-05, val loss: 1.36186e-05, min loss: 1.18605e-05\n",
      "Epoch: 62000, elapsed: 9.90e+00, train loss: 1.18489e-05, val loss: 1.35314e-05, min loss: 1.18489e-05\n",
      "Epoch: 62100, elapsed: 1.08e+01, train loss: 1.22756e-05, val loss: 1.41931e-05, min loss: 1.18489e-05\n",
      "Epoch: 62200, elapsed: 1.00e+01, train loss: 2.25239e-05, val loss: 2.30447e-05, min loss: 1.18489e-05\n",
      "Epoch: 62300, elapsed: 1.00e+01, train loss: 1.19975e-05, val loss: 1.36500e-05, min loss: 1.18489e-05\n",
      "Epoch: 62400, elapsed: 1.04e+01, train loss: 1.52266e-05, val loss: 1.78147e-05, min loss: 1.18489e-05\n",
      "Epoch: 62500, elapsed: 1.01e+01, train loss: 1.19472e-05, val loss: 1.35449e-05, min loss: 1.18489e-05\n",
      "Epoch: 62600, elapsed: 1.00e+01, train loss: 1.37457e-05, val loss: 1.50257e-05, min loss: 1.18489e-05\n",
      "Epoch: 62700, elapsed: 1.05e+01, train loss: 1.18677e-05, val loss: 1.35677e-05, min loss: 1.18489e-05\n",
      "Epoch: 62800, elapsed: 1.01e+01, train loss: 1.60600e-05, val loss: 1.85263e-05, min loss: 1.18489e-05\n",
      "Epoch: 62900, elapsed: 1.04e+01, train loss: 1.28137e-05, val loss: 1.46758e-05, min loss: 1.18489e-05\n",
      "Epoch: 63000, elapsed: 1.02e+01, train loss: 1.23974e-05, val loss: 1.41745e-05, min loss: 1.18489e-05\n",
      "Epoch: 63100, elapsed: 1.00e+01, train loss: 1.19421e-05, val loss: 1.35679e-05, min loss: 1.18489e-05\n",
      "Epoch: 63200, elapsed: 1.04e+01, train loss: 1.17556e-05, val loss: 1.35601e-05, min loss: 1.17556e-05\n",
      "Epoch: 63300, elapsed: 1.03e+01, train loss: 1.17151e-05, val loss: 1.34841e-05, min loss: 1.17151e-05\n",
      "Epoch: 63400, elapsed: 9.94e+00, train loss: 1.20551e-05, val loss: 1.37161e-05, min loss: 1.17151e-05\n",
      "Epoch: 63500, elapsed: 1.04e+01, train loss: 1.18793e-05, val loss: 1.37905e-05, min loss: 1.17151e-05\n",
      "Epoch: 63600, elapsed: 1.02e+01, train loss: 1.20025e-05, val loss: 1.34672e-05, min loss: 1.17151e-05\n",
      "Epoch: 63700, elapsed: 1.01e+01, train loss: 2.65095e-05, val loss: 2.40645e-05, min loss: 1.17151e-05\n",
      "Epoch: 63800, elapsed: 1.04e+01, train loss: 1.90168e-05, val loss: 2.10436e-05, min loss: 1.17151e-05\n",
      "Epoch: 63900, elapsed: 1.01e+01, train loss: 1.41594e-05, val loss: 1.53359e-05, min loss: 1.17151e-05\n",
      "Epoch: 64000, elapsed: 9.98e+00, train loss: 1.17450e-05, val loss: 1.33868e-05, min loss: 1.17151e-05\n",
      "Epoch: 64100, elapsed: 1.03e+01, train loss: 2.19077e-05, val loss: 2.41066e-05, min loss: 1.17151e-05\n",
      "Epoch: 64200, elapsed: 1.01e+01, train loss: 1.24808e-05, val loss: 1.42283e-05, min loss: 1.17151e-05\n",
      "Epoch: 64300, elapsed: 1.00e+01, train loss: 1.18893e-05, val loss: 1.36482e-05, min loss: 1.17151e-05\n",
      "Epoch: 64400, elapsed: 1.06e+01, train loss: 1.74680e-05, val loss: 1.94913e-05, min loss: 1.17151e-05\n",
      "Epoch: 64500, elapsed: 1.00e+01, train loss: 1.23229e-05, val loss: 1.35051e-05, min loss: 1.17151e-05\n",
      "Epoch: 64600, elapsed: 1.01e+01, train loss: 1.17174e-05, val loss: 1.35680e-05, min loss: 1.17151e-05\n",
      "Epoch: 64700, elapsed: 1.05e+01, train loss: 1.15827e-05, val loss: 1.33855e-05, min loss: 1.15827e-05\n",
      "Epoch: 64800, elapsed: 1.01e+01, train loss: 1.45503e-05, val loss: 1.65017e-05, min loss: 1.15827e-05\n",
      "Epoch: 64900, elapsed: 1.05e+01, train loss: 1.33494e-05, val loss: 1.51842e-05, min loss: 1.15827e-05\n",
      "Epoch: 65000, elapsed: 1.01e+01, train loss: 1.17019e-05, val loss: 1.35613e-05, min loss: 1.15827e-05\n",
      "Epoch: 65100, elapsed: 1.18e+01, train loss: 1.47816e-05, val loss: 1.63834e-05, min loss: 1.15827e-05\n",
      "Epoch: 65200, elapsed: 1.04e+01, train loss: 1.33751e-05, val loss: 1.55494e-05, min loss: 1.15827e-05\n",
      "Epoch: 65300, elapsed: 9.99e+00, train loss: 2.03497e-05, val loss: 2.18678e-05, min loss: 1.15827e-05\n",
      "Epoch: 65400, elapsed: 9.92e+00, train loss: 1.17637e-05, val loss: 1.41444e-05, min loss: 1.15827e-05\n",
      "Epoch: 65500, elapsed: 1.05e+01, train loss: 1.65651e-05, val loss: 1.79452e-05, min loss: 1.15827e-05\n",
      "Epoch: 65600, elapsed: 9.99e+00, train loss: 2.11183e-05, val loss: 2.20438e-05, min loss: 1.15827e-05\n",
      "Epoch: 65700, elapsed: 9.97e+00, train loss: 1.53216e-05, val loss: 1.64285e-05, min loss: 1.15827e-05\n",
      "Epoch: 65800, elapsed: 1.05e+01, train loss: 1.28650e-05, val loss: 1.32664e-05, min loss: 1.15827e-05\n",
      "Epoch: 65900, elapsed: 1.01e+01, train loss: 1.45205e-05, val loss: 1.76842e-05, min loss: 1.15827e-05\n",
      "Epoch: 66000, elapsed: 1.00e+01, train loss: 1.14697e-05, val loss: 1.33242e-05, min loss: 1.14697e-05\n",
      "Epoch: 66100, elapsed: 1.04e+01, train loss: 1.24215e-05, val loss: 1.43679e-05, min loss: 1.14697e-05\n",
      "Epoch: 66200, elapsed: 1.01e+01, train loss: 1.45325e-05, val loss: 1.69861e-05, min loss: 1.14697e-05\n",
      "Epoch: 66300, elapsed: 1.00e+01, train loss: 1.23869e-05, val loss: 1.37219e-05, min loss: 1.14697e-05\n",
      "Epoch: 66400, elapsed: 1.06e+01, train loss: 1.22084e-05, val loss: 1.43462e-05, min loss: 1.14697e-05\n",
      "Epoch: 66500, elapsed: 1.01e+01, train loss: 1.37014e-05, val loss: 1.53414e-05, min loss: 1.14697e-05\n",
      "Epoch: 66600, elapsed: 1.01e+01, train loss: 1.32278e-05, val loss: 1.50886e-05, min loss: 1.14697e-05\n",
      "Epoch: 66700, elapsed: 1.04e+01, train loss: 1.25935e-05, val loss: 1.44513e-05, min loss: 1.14697e-05\n",
      "Epoch: 66800, elapsed: 9.95e+00, train loss: 1.21035e-05, val loss: 1.41832e-05, min loss: 1.14697e-05\n",
      "Epoch: 66900, elapsed: 1.05e+01, train loss: 1.15089e-05, val loss: 1.33260e-05, min loss: 1.14697e-05\n",
      "Epoch: 67000, elapsed: 1.02e+01, train loss: 2.34714e-05, val loss: 2.76015e-05, min loss: 1.14697e-05\n",
      "Epoch: 67100, elapsed: 1.00e+01, train loss: 1.26324e-05, val loss: 1.52375e-05, min loss: 1.14697e-05\n",
      "Epoch: 67200, elapsed: 1.04e+01, train loss: 1.55784e-05, val loss: 1.63674e-05, min loss: 1.14697e-05\n",
      "Epoch: 67300, elapsed: 1.01e+01, train loss: 1.26768e-05, val loss: 1.45630e-05, min loss: 1.14697e-05\n",
      "Epoch: 67400, elapsed: 1.01e+01, train loss: 2.04032e-05, val loss: 2.19865e-05, min loss: 1.14697e-05\n",
      "Epoch: 67500, elapsed: 1.04e+01, train loss: 2.40983e-05, val loss: 2.88362e-05, min loss: 1.14697e-05\n",
      "Epoch: 67600, elapsed: 1.01e+01, train loss: 1.23292e-05, val loss: 1.36330e-05, min loss: 1.14697e-05\n",
      "Epoch: 67700, elapsed: 9.96e+00, train loss: 1.69603e-05, val loss: 2.03218e-05, min loss: 1.14697e-05\n",
      "Epoch: 67800, elapsed: 1.04e+01, train loss: 1.30075e-05, val loss: 1.44711e-05, min loss: 1.14697e-05\n",
      "Epoch: 67900, elapsed: 1.02e+01, train loss: 1.35596e-05, val loss: 1.50270e-05, min loss: 1.14697e-05\n",
      "Epoch: 68000, elapsed: 1.00e+01, train loss: 1.13381e-05, val loss: 1.31976e-05, min loss: 1.13381e-05\n",
      "Epoch: 68100, elapsed: 1.05e+01, train loss: 1.88599e-05, val loss: 1.76866e-05, min loss: 1.13381e-05\n",
      "Epoch: 68200, elapsed: 1.02e+01, train loss: 1.20438e-05, val loss: 1.38865e-05, min loss: 1.13381e-05\n",
      "Epoch: 68300, elapsed: 9.97e+00, train loss: 1.23218e-05, val loss: 1.45991e-05, min loss: 1.13381e-05\n",
      "Epoch: 68400, elapsed: 1.05e+01, train loss: 1.15261e-05, val loss: 1.36993e-05, min loss: 1.13381e-05\n",
      "Epoch: 68500, elapsed: 1.01e+01, train loss: 1.18543e-05, val loss: 1.38080e-05, min loss: 1.13381e-05\n",
      "Epoch: 68600, elapsed: 1.01e+01, train loss: 1.16496e-05, val loss: 1.36262e-05, min loss: 1.13381e-05\n",
      "Epoch: 68700, elapsed: 1.04e+01, train loss: 2.95694e-05, val loss: 2.62176e-05, min loss: 1.13381e-05\n",
      "Epoch: 68800, elapsed: 1.01e+01, train loss: 2.76016e-05, val loss: 2.93290e-05, min loss: 1.13381e-05\n",
      "Epoch: 68900, elapsed: 1.02e+01, train loss: 1.12119e-05, val loss: 1.31206e-05, min loss: 1.12119e-05\n",
      "Epoch: 69000, elapsed: 1.04e+01, train loss: 1.12285e-05, val loss: 1.31517e-05, min loss: 1.12119e-05\n",
      "Epoch: 69100, elapsed: 9.99e+00, train loss: 1.15157e-05, val loss: 1.36034e-05, min loss: 1.12119e-05\n",
      "Epoch: 69200, elapsed: 1.02e+01, train loss: 1.12348e-05, val loss: 1.31066e-05, min loss: 1.12119e-05\n",
      "Epoch: 69300, elapsed: 1.06e+01, train loss: 1.33552e-05, val loss: 1.55542e-05, min loss: 1.12119e-05\n",
      "Epoch: 69400, elapsed: 1.01e+01, train loss: 1.18259e-05, val loss: 1.38631e-05, min loss: 1.12119e-05\n",
      "Epoch: 69500, elapsed: 1.01e+01, train loss: 1.25472e-05, val loss: 1.43397e-05, min loss: 1.12119e-05\n",
      "Epoch: 69600, elapsed: 1.05e+01, train loss: 1.12750e-05, val loss: 1.30852e-05, min loss: 1.12119e-05\n",
      "Epoch: 69700, elapsed: 1.01e+01, train loss: 1.38210e-05, val loss: 1.55616e-05, min loss: 1.12119e-05\n",
      "Epoch: 69800, elapsed: 1.00e+01, train loss: 1.27037e-05, val loss: 1.50206e-05, min loss: 1.12119e-05\n",
      "Epoch: 69900, elapsed: 1.06e+01, train loss: 1.20922e-05, val loss: 1.39248e-05, min loss: 1.12119e-05\n",
      "Epoch: 70000, elapsed: 1.00e+01, train loss: 1.34684e-05, val loss: 1.43482e-05, min loss: 1.12119e-05\n",
      "Epoch: 70100, elapsed: 1.22e+01, train loss: 1.17955e-05, val loss: 1.32670e-05, min loss: 1.12119e-05\n",
      "Epoch: 70200, elapsed: 1.02e+01, train loss: 1.32589e-05, val loss: 1.52952e-05, min loss: 1.12119e-05\n",
      "Epoch: 70300, elapsed: 1.01e+01, train loss: 1.14843e-05, val loss: 1.33969e-05, min loss: 1.12119e-05\n",
      "Epoch: 70400, elapsed: 1.04e+01, train loss: 1.11255e-05, val loss: 1.32054e-05, min loss: 1.11255e-05\n",
      "Epoch: 70500, elapsed: 1.01e+01, train loss: 1.17271e-05, val loss: 1.34796e-05, min loss: 1.11255e-05\n",
      "Epoch: 70600, elapsed: 9.90e+00, train loss: 1.21725e-05, val loss: 1.41388e-05, min loss: 1.11255e-05\n",
      "Epoch: 70700, elapsed: 1.04e+01, train loss: 1.36698e-05, val loss: 1.55243e-05, min loss: 1.11255e-05\n",
      "Epoch: 70800, elapsed: 1.02e+01, train loss: 1.12447e-05, val loss: 1.31577e-05, min loss: 1.11255e-05\n",
      "Epoch: 70900, elapsed: 1.00e+01, train loss: 1.15424e-05, val loss: 1.39268e-05, min loss: 1.11255e-05\n",
      "Epoch: 71000, elapsed: 1.03e+01, train loss: 1.11515e-05, val loss: 1.31364e-05, min loss: 1.11255e-05\n",
      "Epoch: 71100, elapsed: 1.01e+01, train loss: 2.11953e-05, val loss: 2.25959e-05, min loss: 1.11255e-05\n",
      "Epoch: 71200, elapsed: 1.01e+01, train loss: 1.30247e-05, val loss: 1.42244e-05, min loss: 1.11255e-05\n",
      "Epoch: 71300, elapsed: 1.03e+01, train loss: 1.20115e-05, val loss: 1.40828e-05, min loss: 1.11255e-05\n",
      "Epoch: 71400, elapsed: 1.02e+01, train loss: 1.21579e-05, val loss: 1.43082e-05, min loss: 1.11255e-05\n",
      "Epoch: 71500, elapsed: 9.96e+00, train loss: 1.10604e-05, val loss: 1.30409e-05, min loss: 1.10604e-05\n",
      "Epoch: 71600, elapsed: 1.03e+01, train loss: 1.15241e-05, val loss: 1.32949e-05, min loss: 1.10604e-05\n",
      "Epoch: 71700, elapsed: 1.02e+01, train loss: 1.29897e-05, val loss: 1.46905e-05, min loss: 1.10604e-05\n",
      "Epoch: 71800, elapsed: 1.01e+01, train loss: 1.12194e-05, val loss: 1.31908e-05, min loss: 1.10604e-05\n",
      "Epoch: 71900, elapsed: 1.04e+01, train loss: 1.13722e-05, val loss: 1.34826e-05, min loss: 1.10604e-05\n",
      "Epoch: 72000, elapsed: 1.01e+01, train loss: 1.24236e-05, val loss: 1.34016e-05, min loss: 1.10604e-05\n",
      "Epoch: 72100, elapsed: 9.95e+00, train loss: 1.10207e-05, val loss: 1.28943e-05, min loss: 1.10207e-05\n",
      "Epoch: 72200, elapsed: 1.04e+01, train loss: 1.27575e-05, val loss: 1.53022e-05, min loss: 1.10207e-05\n",
      "Epoch: 72300, elapsed: 1.02e+01, train loss: 1.53407e-05, val loss: 1.82282e-05, min loss: 1.10207e-05\n",
      "Epoch: 72400, elapsed: 1.00e+01, train loss: 1.19717e-05, val loss: 1.39230e-05, min loss: 1.10207e-05\n",
      "Epoch: 72500, elapsed: 1.04e+01, train loss: 1.26037e-05, val loss: 1.45320e-05, min loss: 1.10207e-05\n",
      "Epoch: 72600, elapsed: 1.01e+01, train loss: 1.12479e-05, val loss: 1.32989e-05, min loss: 1.10207e-05\n",
      "Epoch: 72700, elapsed: 9.96e+00, train loss: 2.05671e-05, val loss: 2.14606e-05, min loss: 1.10207e-05\n",
      "Epoch: 72800, elapsed: 1.03e+01, train loss: 1.11540e-05, val loss: 1.32855e-05, min loss: 1.10207e-05\n",
      "Epoch: 72900, elapsed: 1.02e+01, train loss: 2.83091e-05, val loss: 3.16575e-05, min loss: 1.10207e-05\n",
      "Epoch: 73000, elapsed: 9.97e+00, train loss: 1.23581e-05, val loss: 1.40027e-05, min loss: 1.10207e-05\n",
      "Epoch: 73100, elapsed: 1.01e+01, train loss: 1.59391e-05, val loss: 1.75184e-05, min loss: 1.10207e-05\n",
      "Epoch: 73200, elapsed: 1.06e+01, train loss: 1.12911e-05, val loss: 1.31199e-05, min loss: 1.10207e-05\n",
      "Epoch: 73300, elapsed: 9.96e+00, train loss: 1.12015e-05, val loss: 1.36238e-05, min loss: 1.10207e-05\n",
      "Epoch: 73400, elapsed: 1.01e+01, train loss: 1.90882e-05, val loss: 2.09564e-05, min loss: 1.10207e-05\n",
      "Epoch: 73500, elapsed: 1.06e+01, train loss: 1.11190e-05, val loss: 1.30747e-05, min loss: 1.10207e-05\n",
      "Epoch: 73600, elapsed: 1.00e+01, train loss: 1.10114e-05, val loss: 1.31446e-05, min loss: 1.10114e-05\n",
      "Epoch: 73700, elapsed: 1.01e+01, train loss: 1.08606e-05, val loss: 1.28538e-05, min loss: 1.08606e-05\n",
      "Epoch: 73800, elapsed: 1.05e+01, train loss: 1.29063e-05, val loss: 1.53037e-05, min loss: 1.08606e-05\n",
      "Epoch: 73900, elapsed: 1.01e+01, train loss: 1.18629e-05, val loss: 1.36684e-05, min loss: 1.08606e-05\n",
      "Epoch: 74000, elapsed: 1.02e+01, train loss: 1.98156e-05, val loss: 2.09004e-05, min loss: 1.08606e-05\n",
      "Epoch: 74100, elapsed: 1.05e+01, train loss: 1.30809e-05, val loss: 1.53869e-05, min loss: 1.08606e-05\n",
      "Epoch: 74200, elapsed: 9.98e+00, train loss: 1.23675e-05, val loss: 1.36812e-05, min loss: 1.08606e-05\n",
      "Epoch: 74300, elapsed: 9.95e+00, train loss: 1.86323e-05, val loss: 1.94945e-05, min loss: 1.08606e-05\n",
      "Epoch: 74400, elapsed: 1.05e+01, train loss: 1.59546e-05, val loss: 1.70965e-05, min loss: 1.08606e-05\n",
      "Epoch: 74500, elapsed: 1.00e+01, train loss: 1.36702e-05, val loss: 1.53778e-05, min loss: 1.08606e-05\n",
      "Epoch: 74600, elapsed: 1.00e+01, train loss: 1.10729e-05, val loss: 1.30512e-05, min loss: 1.08606e-05\n",
      "Epoch: 74700, elapsed: 1.07e+01, train loss: 1.21762e-05, val loss: 1.50564e-05, min loss: 1.08606e-05\n",
      "Epoch: 74800, elapsed: 1.00e+01, train loss: 1.43026e-05, val loss: 1.54775e-05, min loss: 1.08606e-05\n",
      "Epoch: 74900, elapsed: 1.01e+01, train loss: 1.18337e-05, val loss: 1.38542e-05, min loss: 1.08606e-05\n",
      "Epoch: 75000, elapsed: 1.06e+01, train loss: 1.08433e-05, val loss: 1.29965e-05, min loss: 1.08433e-05\n",
      "Epoch: 75100, elapsed: 1.19e+01, train loss: 1.08573e-05, val loss: 1.30073e-05, min loss: 1.08433e-05\n",
      "Epoch: 75200, elapsed: 1.05e+01, train loss: 1.10700e-05, val loss: 1.31340e-05, min loss: 1.08433e-05\n",
      "Epoch: 75300, elapsed: 1.01e+01, train loss: 1.76119e-05, val loss: 2.05229e-05, min loss: 1.08433e-05\n",
      "Epoch: 75400, elapsed: 1.00e+01, train loss: 1.38946e-05, val loss: 1.69645e-05, min loss: 1.08433e-05\n",
      "Epoch: 75500, elapsed: 1.04e+01, train loss: 1.72795e-05, val loss: 1.88361e-05, min loss: 1.08433e-05\n",
      "Epoch: 75600, elapsed: 1.02e+01, train loss: 1.42254e-05, val loss: 1.49222e-05, min loss: 1.08433e-05\n",
      "Epoch: 75700, elapsed: 9.96e+00, train loss: 1.07173e-05, val loss: 1.27961e-05, min loss: 1.07173e-05\n",
      "Epoch: 75800, elapsed: 1.04e+01, train loss: 1.07998e-05, val loss: 1.28471e-05, min loss: 1.07173e-05\n",
      "Epoch: 75900, elapsed: 1.02e+01, train loss: 1.18484e-05, val loss: 1.39920e-05, min loss: 1.07173e-05\n",
      "Epoch: 76000, elapsed: 9.90e+00, train loss: 1.31014e-05, val loss: 1.53164e-05, min loss: 1.07173e-05\n",
      "Epoch: 76100, elapsed: 1.01e+01, train loss: 1.32509e-05, val loss: 1.51320e-05, min loss: 1.07173e-05\n",
      "Epoch: 76200, elapsed: 1.05e+01, train loss: 1.10571e-05, val loss: 1.35135e-05, min loss: 1.07173e-05\n",
      "Epoch: 76300, elapsed: 9.99e+00, train loss: 1.26042e-05, val loss: 1.56101e-05, min loss: 1.07173e-05\n",
      "Epoch: 76400, elapsed: 1.01e+01, train loss: 1.15951e-05, val loss: 1.33726e-05, min loss: 1.07173e-05\n",
      "Epoch: 76500, elapsed: 1.05e+01, train loss: 1.17737e-05, val loss: 1.38879e-05, min loss: 1.07173e-05\n",
      "Epoch: 76600, elapsed: 9.92e+00, train loss: 1.19608e-05, val loss: 1.38582e-05, min loss: 1.07173e-05\n",
      "Epoch: 76700, elapsed: 9.87e+00, train loss: 1.24409e-05, val loss: 1.49237e-05, min loss: 1.07173e-05\n",
      "Epoch: 76800, elapsed: 1.04e+01, train loss: 1.14654e-05, val loss: 1.32014e-05, min loss: 1.07173e-05\n",
      "Epoch: 76900, elapsed: 1.01e+01, train loss: 1.40783e-05, val loss: 1.73453e-05, min loss: 1.07173e-05\n",
      "Epoch: 77000, elapsed: 1.00e+01, train loss: 3.35588e-05, val loss: 4.00547e-05, min loss: 1.07173e-05\n",
      "Epoch: 77100, elapsed: 1.05e+01, train loss: 2.29498e-05, val loss: 2.67333e-05, min loss: 1.07173e-05\n",
      "Epoch: 77200, elapsed: 1.02e+01, train loss: 1.35370e-05, val loss: 1.43911e-05, min loss: 1.07173e-05\n",
      "Epoch: 77300, elapsed: 1.01e+01, train loss: 1.53320e-05, val loss: 1.56842e-05, min loss: 1.07173e-05\n",
      "Epoch: 77400, elapsed: 1.05e+01, train loss: 1.35392e-05, val loss: 1.47935e-05, min loss: 1.07173e-05\n",
      "Epoch: 77500, elapsed: 1.01e+01, train loss: 1.06154e-05, val loss: 1.28237e-05, min loss: 1.06154e-05\n",
      "Epoch: 77600, elapsed: 9.98e+00, train loss: 1.11140e-05, val loss: 1.27918e-05, min loss: 1.06154e-05\n",
      "Epoch: 77700, elapsed: 1.04e+01, train loss: 1.66207e-05, val loss: 1.92653e-05, min loss: 1.06154e-05\n",
      "Epoch: 77800, elapsed: 1.01e+01, train loss: 1.07551e-05, val loss: 1.28915e-05, min loss: 1.06154e-05\n",
      "Epoch: 77900, elapsed: 9.95e+00, train loss: 1.08277e-05, val loss: 1.27543e-05, min loss: 1.06154e-05\n",
      "Epoch: 78000, elapsed: 1.05e+01, train loss: 1.05606e-05, val loss: 1.26457e-05, min loss: 1.05606e-05\n",
      "Epoch: 78100, elapsed: 1.03e+01, train loss: 1.05637e-05, val loss: 1.27047e-05, min loss: 1.05606e-05\n",
      "Epoch: 78200, elapsed: 1.01e+01, train loss: 1.12422e-05, val loss: 1.33921e-05, min loss: 1.05606e-05\n",
      "Epoch: 78300, elapsed: 1.04e+01, train loss: 1.08374e-05, val loss: 1.26109e-05, min loss: 1.05606e-05\n",
      "Epoch: 78400, elapsed: 1.02e+01, train loss: 1.05814e-05, val loss: 1.27104e-05, min loss: 1.05606e-05\n",
      "Epoch: 78500, elapsed: 1.00e+01, train loss: 1.34237e-05, val loss: 1.42164e-05, min loss: 1.05606e-05\n",
      "Epoch: 78600, elapsed: 1.04e+01, train loss: 1.09897e-05, val loss: 1.32553e-05, min loss: 1.05606e-05\n",
      "Epoch: 78700, elapsed: 1.02e+01, train loss: 1.11379e-05, val loss: 1.34409e-05, min loss: 1.05606e-05\n",
      "Epoch: 78800, elapsed: 1.01e+01, train loss: 1.08793e-05, val loss: 1.30671e-05, min loss: 1.05606e-05\n",
      "Epoch: 78900, elapsed: 9.98e+00, train loss: 1.05484e-05, val loss: 1.27085e-05, min loss: 1.05484e-05\n",
      "Epoch: 79000, elapsed: 1.04e+01, train loss: 1.05882e-05, val loss: 1.27129e-05, min loss: 1.05484e-05\n",
      "Epoch: 79100, elapsed: 1.01e+01, train loss: 1.33447e-05, val loss: 1.61452e-05, min loss: 1.05484e-05\n",
      "Epoch: 79200, elapsed: 1.02e+01, train loss: 1.05000e-05, val loss: 1.26599e-05, min loss: 1.05000e-05\n",
      "Epoch: 79300, elapsed: 1.05e+01, train loss: 1.25389e-05, val loss: 1.56316e-05, min loss: 1.05000e-05\n",
      "Epoch: 79400, elapsed: 1.01e+01, train loss: 1.63453e-05, val loss: 1.86392e-05, min loss: 1.05000e-05\n",
      "Epoch: 79500, elapsed: 1.00e+01, train loss: 1.06308e-05, val loss: 1.26042e-05, min loss: 1.05000e-05\n",
      "Epoch: 79600, elapsed: 1.05e+01, train loss: 2.06646e-05, val loss: 2.33958e-05, min loss: 1.05000e-05\n",
      "Epoch: 79700, elapsed: 1.01e+01, train loss: 1.06679e-05, val loss: 1.31596e-05, min loss: 1.05000e-05\n",
      "Epoch: 79800, elapsed: 1.01e+01, train loss: 1.07604e-05, val loss: 1.29308e-05, min loss: 1.05000e-05\n",
      "Epoch: 79900, elapsed: 1.06e+01, train loss: 1.41465e-05, val loss: 1.48363e-05, min loss: 1.05000e-05\n",
      "Epoch: 80000, elapsed: 1.01e+01, train loss: 1.05166e-05, val loss: 1.26058e-05, min loss: 1.05000e-05\n",
      "Epoch: 80100, elapsed: 1.18e+01, train loss: 1.31737e-05, val loss: 1.55122e-05, min loss: 1.05000e-05\n",
      "Epoch: 80200, elapsed: 1.05e+01, train loss: 1.15120e-05, val loss: 1.37655e-05, min loss: 1.05000e-05\n",
      "Epoch: 80300, elapsed: 9.91e+00, train loss: 1.06747e-05, val loss: 1.30360e-05, min loss: 1.05000e-05\n",
      "Epoch: 80400, elapsed: 1.01e+01, train loss: 1.98711e-05, val loss: 2.06318e-05, min loss: 1.05000e-05\n",
      "Epoch: 80500, elapsed: 1.06e+01, train loss: 1.20333e-05, val loss: 1.35904e-05, min loss: 1.05000e-05\n",
      "Epoch: 80600, elapsed: 1.01e+01, train loss: 1.16214e-05, val loss: 1.34610e-05, min loss: 1.05000e-05\n",
      "Epoch: 80700, elapsed: 1.00e+01, train loss: 1.10298e-05, val loss: 1.37230e-05, min loss: 1.05000e-05\n",
      "Epoch: 80800, elapsed: 1.05e+01, train loss: 1.05159e-05, val loss: 1.27768e-05, min loss: 1.05000e-05\n",
      "Epoch: 80900, elapsed: 1.00e+01, train loss: 1.96172e-05, val loss: 2.34676e-05, min loss: 1.05000e-05\n",
      "Epoch: 81000, elapsed: 1.01e+01, train loss: 1.57040e-05, val loss: 2.30046e-05, min loss: 1.05000e-05\n",
      "Epoch: 81100, elapsed: 1.05e+01, train loss: 1.15089e-05, val loss: 1.38222e-05, min loss: 1.05000e-05\n",
      "Epoch: 81200, elapsed: 1.00e+01, train loss: 1.06271e-05, val loss: 1.28902e-05, min loss: 1.05000e-05\n",
      "Epoch: 81300, elapsed: 9.90e+00, train loss: 1.68202e-05, val loss: 1.70782e-05, min loss: 1.05000e-05\n",
      "Epoch: 81400, elapsed: 1.05e+01, train loss: 3.24489e-05, val loss: 2.84957e-05, min loss: 1.05000e-05\n",
      "Epoch: 81500, elapsed: 1.02e+01, train loss: 1.13626e-05, val loss: 1.41669e-05, min loss: 1.05000e-05\n",
      "Epoch: 81600, elapsed: 1.00e+01, train loss: 1.24759e-05, val loss: 1.47923e-05, min loss: 1.05000e-05\n",
      "Epoch: 81700, elapsed: 1.01e+01, train loss: 1.07727e-05, val loss: 1.27770e-05, min loss: 1.05000e-05\n",
      "Epoch: 81800, elapsed: 1.05e+01, train loss: 1.04471e-05, val loss: 1.27334e-05, min loss: 1.04471e-05\n",
      "Epoch: 81900, elapsed: 9.98e+00, train loss: 1.11817e-05, val loss: 1.31915e-05, min loss: 1.04471e-05\n",
      "Epoch: 82000, elapsed: 9.99e+00, train loss: 1.12063e-05, val loss: 1.37132e-05, min loss: 1.04471e-05\n",
      "Epoch: 82100, elapsed: 1.06e+01, train loss: 1.89669e-05, val loss: 2.37656e-05, min loss: 1.04471e-05\n",
      "Epoch: 82200, elapsed: 9.91e+00, train loss: 1.51438e-05, val loss: 1.90249e-05, min loss: 1.04471e-05\n",
      "Epoch: 82300, elapsed: 1.00e+01, train loss: 1.18321e-05, val loss: 1.39889e-05, min loss: 1.04471e-05\n",
      "Epoch: 82400, elapsed: 1.04e+01, train loss: 1.10804e-05, val loss: 1.34214e-05, min loss: 1.04471e-05\n",
      "Epoch: 82500, elapsed: 1.01e+01, train loss: 1.42732e-05, val loss: 1.59838e-05, min loss: 1.04471e-05\n",
      "Epoch: 82600, elapsed: 1.00e+01, train loss: 1.18769e-05, val loss: 1.35076e-05, min loss: 1.04471e-05\n",
      "Epoch: 82700, elapsed: 1.04e+01, train loss: 1.02916e-05, val loss: 1.24887e-05, min loss: 1.02916e-05\n",
      "Epoch: 82800, elapsed: 1.00e+01, train loss: 1.55372e-05, val loss: 1.89886e-05, min loss: 1.02916e-05\n",
      "Epoch: 82900, elapsed: 9.89e+00, train loss: 1.07265e-05, val loss: 1.25150e-05, min loss: 1.02916e-05\n",
      "Epoch: 83000, elapsed: 1.04e+01, train loss: 1.06592e-05, val loss: 1.30614e-05, min loss: 1.02916e-05\n",
      "Epoch: 83100, elapsed: 1.01e+01, train loss: 1.17949e-05, val loss: 1.34514e-05, min loss: 1.02916e-05\n",
      "Epoch: 83200, elapsed: 1.02e+01, train loss: 1.07289e-05, val loss: 1.24484e-05, min loss: 1.02916e-05\n",
      "Epoch: 83300, elapsed: 1.00e+01, train loss: 1.04348e-05, val loss: 1.32667e-05, min loss: 1.02916e-05\n",
      "Epoch: 83400, elapsed: 1.04e+01, train loss: 1.10392e-05, val loss: 1.30804e-05, min loss: 1.02916e-05\n",
      "Epoch: 83500, elapsed: 1.02e+01, train loss: 1.02973e-05, val loss: 1.25531e-05, min loss: 1.02916e-05\n",
      "Epoch: 83600, elapsed: 1.00e+01, train loss: 1.02561e-05, val loss: 1.24253e-05, min loss: 1.02561e-05\n",
      "Epoch: 83700, elapsed: 1.05e+01, train loss: 1.03334e-05, val loss: 1.24589e-05, min loss: 1.02561e-05\n",
      "Epoch: 83800, elapsed: 1.01e+01, train loss: 1.06655e-05, val loss: 1.26882e-05, min loss: 1.02561e-05\n",
      "Epoch: 83900, elapsed: 9.99e+00, train loss: 1.52860e-05, val loss: 1.76256e-05, min loss: 1.02561e-05\n",
      "Epoch: 84000, elapsed: 1.03e+01, train loss: 1.15156e-05, val loss: 1.32290e-05, min loss: 1.02561e-05\n",
      "Epoch: 84100, elapsed: 1.01e+01, train loss: 1.29309e-05, val loss: 1.54464e-05, min loss: 1.02561e-05\n",
      "Epoch: 84200, elapsed: 1.00e+01, train loss: 2.32109e-05, val loss: 2.34841e-05, min loss: 1.02561e-05\n",
      "Epoch: 84300, elapsed: 1.05e+01, train loss: 1.02913e-05, val loss: 1.25402e-05, min loss: 1.02561e-05\n",
      "Epoch: 84400, elapsed: 1.01e+01, train loss: 1.56377e-05, val loss: 1.95955e-05, min loss: 1.02561e-05\n",
      "Epoch: 84500, elapsed: 9.92e+00, train loss: 1.32651e-05, val loss: 1.39892e-05, min loss: 1.02561e-05\n",
      "Epoch: 84600, elapsed: 1.06e+01, train loss: 1.08461e-05, val loss: 1.31311e-05, min loss: 1.02561e-05\n",
      "Epoch: 84700, elapsed: 1.01e+01, train loss: 1.46793e-05, val loss: 1.47023e-05, min loss: 1.02561e-05\n",
      "Epoch: 84800, elapsed: 9.94e+00, train loss: 1.06399e-05, val loss: 1.29432e-05, min loss: 1.02561e-05\n",
      "Epoch: 84900, elapsed: 1.01e+01, train loss: 1.03616e-05, val loss: 1.26268e-05, min loss: 1.02561e-05\n",
      "Epoch: 85000, elapsed: 1.05e+01, train loss: 1.01414e-05, val loss: 1.24571e-05, min loss: 1.01414e-05\n",
      "Epoch: 85100, elapsed: 1.19e+01, train loss: 1.72273e-05, val loss: 1.98765e-05, min loss: 1.01414e-05\n",
      "Epoch: 85200, elapsed: 1.04e+01, train loss: 1.21077e-05, val loss: 1.39457e-05, min loss: 1.01414e-05\n",
      "Epoch: 85300, elapsed: 1.02e+01, train loss: 1.05672e-05, val loss: 1.27529e-05, min loss: 1.01414e-05\n",
      "Epoch: 85400, elapsed: 1.00e+01, train loss: 1.07541e-05, val loss: 1.32838e-05, min loss: 1.01414e-05\n",
      "Epoch: 85500, elapsed: 1.01e+01, train loss: 1.51633e-05, val loss: 1.74680e-05, min loss: 1.01414e-05\n",
      "Epoch: 85600, elapsed: 1.07e+01, train loss: 1.11408e-05, val loss: 1.32571e-05, min loss: 1.01414e-05\n",
      "Epoch: 85700, elapsed: 9.95e+00, train loss: 1.05190e-05, val loss: 1.28631e-05, min loss: 1.01414e-05\n",
      "Epoch: 85800, elapsed: 1.01e+01, train loss: 1.43914e-05, val loss: 1.64834e-05, min loss: 1.01414e-05\n",
      "Epoch: 85900, elapsed: 1.05e+01, train loss: 1.29087e-05, val loss: 1.54867e-05, min loss: 1.01414e-05\n",
      "Epoch: 86000, elapsed: 9.94e+00, train loss: 1.25737e-05, val loss: 1.40958e-05, min loss: 1.01414e-05\n",
      "Epoch: 86100, elapsed: 1.00e+01, train loss: 1.06598e-05, val loss: 1.26703e-05, min loss: 1.01414e-05\n",
      "Epoch: 86200, elapsed: 1.04e+01, train loss: 1.01347e-05, val loss: 1.22428e-05, min loss: 1.01347e-05\n",
      "Epoch: 86300, elapsed: 1.02e+01, train loss: 1.37951e-05, val loss: 1.61479e-05, min loss: 1.01347e-05\n",
      "Epoch: 86400, elapsed: 1.00e+01, train loss: 2.28060e-05, val loss: 2.83265e-05, min loss: 1.01347e-05\n",
      "Epoch: 86500, elapsed: 1.04e+01, train loss: 2.73329e-05, val loss: 2.56982e-05, min loss: 1.01347e-05\n",
      "Epoch: 86600, elapsed: 1.00e+01, train loss: 1.85808e-05, val loss: 2.13114e-05, min loss: 1.01347e-05\n",
      "Epoch: 86700, elapsed: 9.99e+00, train loss: 1.14268e-05, val loss: 1.38289e-05, min loss: 1.01347e-05\n",
      "Epoch: 86800, elapsed: 1.01e+01, train loss: 1.07864e-05, val loss: 1.28703e-05, min loss: 1.01347e-05\n",
      "Epoch: 86900, elapsed: 1.04e+01, train loss: 1.16352e-05, val loss: 1.42368e-05, min loss: 1.01347e-05\n",
      "Epoch: 87000, elapsed: 9.96e+00, train loss: 1.03807e-05, val loss: 1.32124e-05, min loss: 1.01347e-05\n",
      "Epoch: 87100, elapsed: 1.00e+01, train loss: 1.00261e-05, val loss: 1.21780e-05, min loss: 1.00261e-05\n",
      "Epoch: 87200, elapsed: 1.05e+01, train loss: 1.01528e-05, val loss: 1.23620e-05, min loss: 1.00261e-05\n",
      "Epoch: 87300, elapsed: 1.00e+01, train loss: 1.01285e-05, val loss: 1.24584e-05, min loss: 1.00261e-05\n",
      "Epoch: 87400, elapsed: 9.94e+00, train loss: 9.97382e-06, val loss: 1.22280e-05, min loss: 9.97382e-06\n",
      "Epoch: 87500, elapsed: 1.05e+01, train loss: 1.00476e-05, val loss: 1.22143e-05, min loss: 9.97382e-06\n",
      "Epoch: 87600, elapsed: 1.00e+01, train loss: 1.02332e-05, val loss: 1.23849e-05, min loss: 9.97382e-06\n",
      "Epoch: 87700, elapsed: 9.95e+00, train loss: 1.14957e-05, val loss: 1.32342e-05, min loss: 9.97382e-06\n",
      "Epoch: 87800, elapsed: 1.04e+01, train loss: 9.96641e-06, val loss: 1.22020e-05, min loss: 9.96641e-06\n",
      "Epoch: 87900, elapsed: 1.02e+01, train loss: 9.98072e-06, val loss: 1.21929e-05, min loss: 9.96641e-06\n",
      "Epoch: 88000, elapsed: 9.96e+00, train loss: 1.15600e-05, val loss: 1.39478e-05, min loss: 9.96641e-06\n",
      "Epoch: 88100, elapsed: 1.00e+01, train loss: 1.21218e-05, val loss: 1.41621e-05, min loss: 9.96641e-06\n",
      "Epoch: 88200, elapsed: 1.05e+01, train loss: 1.15683e-05, val loss: 1.30407e-05, min loss: 9.96641e-06\n",
      "Epoch: 88300, elapsed: 1.01e+01, train loss: 1.05700e-05, val loss: 1.30570e-05, min loss: 9.96641e-06\n",
      "Epoch: 88400, elapsed: 1.01e+01, train loss: 1.21398e-05, val loss: 1.36927e-05, min loss: 9.96641e-06\n",
      "Epoch: 88500, elapsed: 1.05e+01, train loss: 2.68261e-05, val loss: 3.01423e-05, min loss: 9.96641e-06\n",
      "Epoch: 88600, elapsed: 1.01e+01, train loss: 1.08108e-05, val loss: 1.25343e-05, min loss: 9.96641e-06\n",
      "Epoch: 88700, elapsed: 1.00e+01, train loss: 1.26187e-05, val loss: 1.49201e-05, min loss: 9.96641e-06\n",
      "Epoch: 88800, elapsed: 1.05e+01, train loss: 1.90315e-05, val loss: 2.33023e-05, min loss: 9.96641e-06\n",
      "Epoch: 88900, elapsed: 1.01e+01, train loss: 1.04168e-05, val loss: 1.34256e-05, min loss: 9.96641e-06\n",
      "Epoch: 89000, elapsed: 1.00e+01, train loss: 1.05920e-05, val loss: 1.27295e-05, min loss: 9.96641e-06\n",
      "Epoch: 89100, elapsed: 1.00e+01, train loss: 1.00629e-05, val loss: 1.22501e-05, min loss: 9.96641e-06\n",
      "Epoch: 89200, elapsed: 1.05e+01, train loss: 1.00039e-05, val loss: 1.22143e-05, min loss: 9.96641e-06\n",
      "Epoch: 89300, elapsed: 9.93e+00, train loss: 1.13248e-05, val loss: 1.38938e-05, min loss: 9.96641e-06\n",
      "Epoch: 89400, elapsed: 9.90e+00, train loss: 9.90615e-06, val loss: 1.22116e-05, min loss: 9.90615e-06\n",
      "Epoch: 89500, elapsed: 1.06e+01, train loss: 9.88519e-06, val loss: 1.21477e-05, min loss: 9.88519e-06\n",
      "Epoch: 89600, elapsed: 9.86e+00, train loss: 1.07281e-05, val loss: 1.31928e-05, min loss: 9.88519e-06\n",
      "Epoch: 89700, elapsed: 9.99e+00, train loss: 1.40097e-05, val loss: 2.31422e-05, min loss: 9.88519e-06\n",
      "Epoch: 89800, elapsed: 1.04e+01, train loss: 9.83252e-06, val loss: 1.20832e-05, min loss: 9.83252e-06\n",
      "Epoch: 89900, elapsed: 1.02e+01, train loss: 9.99942e-06, val loss: 1.22534e-05, min loss: 9.83252e-06\n",
      "Epoch: 90000, elapsed: 9.94e+00, train loss: 2.24910e-05, val loss: 2.24570e-05, min loss: 9.83252e-06\n",
      "Epoch: 90100, elapsed: 1.23e+01, train loss: 9.96214e-06, val loss: 1.23069e-05, min loss: 9.83252e-06\n",
      "Epoch: 90200, elapsed: 1.01e+01, train loss: 1.03603e-05, val loss: 1.26948e-05, min loss: 9.83252e-06\n",
      "Epoch: 90300, elapsed: 9.83e+00, train loss: 1.12816e-05, val loss: 1.36059e-05, min loss: 9.83252e-06\n",
      "Epoch: 90400, elapsed: 1.05e+01, train loss: 1.40778e-05, val loss: 1.71616e-05, min loss: 9.83252e-06\n",
      "Epoch: 90500, elapsed: 1.01e+01, train loss: 2.59057e-05, val loss: 2.32710e-05, min loss: 9.83252e-06\n",
      "Epoch: 90600, elapsed: 9.94e+00, train loss: 1.29380e-05, val loss: 1.46229e-05, min loss: 9.83252e-06\n",
      "Epoch: 90700, elapsed: 1.01e+01, train loss: 1.24376e-05, val loss: 1.48401e-05, min loss: 9.83252e-06\n",
      "Epoch: 90800, elapsed: 1.05e+01, train loss: 1.11771e-05, val loss: 1.42029e-05, min loss: 9.83252e-06\n",
      "Epoch: 90900, elapsed: 9.92e+00, train loss: 1.10455e-05, val loss: 1.29848e-05, min loss: 9.83252e-06\n",
      "Epoch: 91000, elapsed: 1.01e+01, train loss: 2.69356e-05, val loss: 2.86016e-05, min loss: 9.83252e-06\n",
      "Epoch: 91100, elapsed: 1.05e+01, train loss: 1.09101e-05, val loss: 1.29366e-05, min loss: 9.83252e-06\n",
      "Epoch: 91200, elapsed: 9.93e+00, train loss: 1.80053e-05, val loss: 1.77405e-05, min loss: 9.83252e-06\n",
      "Epoch: 91300, elapsed: 9.88e+00, train loss: 1.07174e-05, val loss: 1.31540e-05, min loss: 9.83252e-06\n",
      "Epoch: 91400, elapsed: 1.06e+01, train loss: 1.65276e-05, val loss: 1.67155e-05, min loss: 9.83252e-06\n",
      "Epoch: 91500, elapsed: 1.01e+01, train loss: 1.46481e-05, val loss: 1.52675e-05, min loss: 9.83252e-06\n",
      "Epoch: 91600, elapsed: 9.82e+00, train loss: 9.80153e-06, val loss: 1.22458e-05, min loss: 9.80153e-06\n",
      "Epoch: 91700, elapsed: 9.95e+00, train loss: 1.43126e-05, val loss: 1.61637e-05, min loss: 9.80153e-06\n",
      "Epoch: 91800, elapsed: 1.05e+01, train loss: 9.84797e-06, val loss: 1.21952e-05, min loss: 9.80153e-06\n",
      "Epoch: 91900, elapsed: 1.00e+01, train loss: 9.96058e-06, val loss: 1.25585e-05, min loss: 9.80153e-06\n",
      "Epoch: 92000, elapsed: 9.96e+00, train loss: 1.00542e-05, val loss: 1.24388e-05, min loss: 9.80153e-06\n",
      "Epoch: 92100, elapsed: 1.05e+01, train loss: 9.72417e-06, val loss: 1.20026e-05, min loss: 9.72417e-06\n",
      "Epoch: 92200, elapsed: 1.00e+01, train loss: 1.01195e-05, val loss: 1.24869e-05, min loss: 9.72417e-06\n",
      "Epoch: 92300, elapsed: 9.90e+00, train loss: 1.08877e-05, val loss: 1.27970e-05, min loss: 9.72417e-06\n",
      "Epoch: 92400, elapsed: 1.05e+01, train loss: 1.05677e-05, val loss: 1.27900e-05, min loss: 9.72417e-06\n",
      "Epoch: 92500, elapsed: 1.02e+01, train loss: 1.36099e-05, val loss: 1.59805e-05, min loss: 9.72417e-06\n",
      "Epoch: 92600, elapsed: 1.02e+01, train loss: 1.36633e-05, val loss: 1.67264e-05, min loss: 9.72417e-06\n",
      "Epoch: 92700, elapsed: 1.00e+01, train loss: 1.14341e-05, val loss: 1.37782e-05, min loss: 9.72417e-06\n",
      "Epoch: 92800, elapsed: 1.06e+01, train loss: 1.42733e-05, val loss: 1.65049e-05, min loss: 9.72417e-06\n",
      "Epoch: 92900, elapsed: 1.01e+01, train loss: 1.03595e-05, val loss: 1.23310e-05, min loss: 9.72417e-06\n",
      "Epoch: 93000, elapsed: 1.01e+01, train loss: 1.06321e-05, val loss: 1.29520e-05, min loss: 9.72417e-06\n",
      "Epoch: 93100, elapsed: 1.04e+01, train loss: 1.01306e-05, val loss: 1.28724e-05, min loss: 9.72417e-06\n",
      "Epoch: 93200, elapsed: 9.99e+00, train loss: 1.19137e-05, val loss: 1.38544e-05, min loss: 9.72417e-06\n",
      "Epoch: 93300, elapsed: 9.86e+00, train loss: 1.04191e-05, val loss: 1.31520e-05, min loss: 9.72417e-06\n",
      "Epoch: 93400, elapsed: 1.04e+01, train loss: 9.74309e-06, val loss: 1.19790e-05, min loss: 9.72417e-06\n",
      "Epoch: 93500, elapsed: 1.02e+01, train loss: 2.29045e-05, val loss: 2.50205e-05, min loss: 9.72417e-06\n",
      "Epoch: 93600, elapsed: 9.91e+00, train loss: 1.09745e-05, val loss: 1.30302e-05, min loss: 9.72417e-06\n",
      "Epoch: 93700, elapsed: 1.01e+01, train loss: 9.77388e-06, val loss: 1.21284e-05, min loss: 9.72417e-06\n",
      "Epoch: 93800, elapsed: 1.06e+01, train loss: 9.98423e-06, val loss: 1.21830e-05, min loss: 9.72417e-06\n",
      "Epoch: 93900, elapsed: 9.99e+00, train loss: 1.10727e-05, val loss: 1.34467e-05, min loss: 9.72417e-06\n",
      "Epoch: 94000, elapsed: 9.97e+00, train loss: 1.62202e-05, val loss: 1.98760e-05, min loss: 9.72417e-06\n",
      "Epoch: 94100, elapsed: 1.05e+01, train loss: 1.03626e-05, val loss: 1.24775e-05, min loss: 9.72417e-06\n",
      "Epoch: 94200, elapsed: 9.99e+00, train loss: 1.10590e-05, val loss: 1.36403e-05, min loss: 9.72417e-06\n",
      "Epoch: 94300, elapsed: 9.92e+00, train loss: 1.04030e-05, val loss: 1.25766e-05, min loss: 9.72417e-06\n",
      "Epoch: 94400, elapsed: 1.05e+01, train loss: 9.98737e-06, val loss: 1.22233e-05, min loss: 9.72417e-06\n",
      "Epoch: 94500, elapsed: 1.00e+01, train loss: 1.01415e-05, val loss: 1.20987e-05, min loss: 9.72417e-06\n",
      "Epoch: 94600, elapsed: 1.00e+01, train loss: 9.99625e-06, val loss: 1.21094e-05, min loss: 9.72417e-06\n",
      "Epoch: 94700, elapsed: 9.95e+00, train loss: 1.75333e-05, val loss: 1.87765e-05, min loss: 9.72417e-06\n",
      "Epoch: 94800, elapsed: 1.06e+01, train loss: 1.08678e-05, val loss: 1.31733e-05, min loss: 9.72417e-06\n",
      "Epoch: 94900, elapsed: 1.00e+01, train loss: 1.67518e-05, val loss: 2.07053e-05, min loss: 9.72417e-06\n",
      "Epoch: 95000, elapsed: 1.01e+01, train loss: 1.11120e-05, val loss: 1.27411e-05, min loss: 9.72417e-06\n",
      "Epoch: 95100, elapsed: 1.24e+01, train loss: 9.60333e-06, val loss: 1.16971e-05, min loss: 9.60333e-06\n",
      "Epoch: 95200, elapsed: 1.01e+01, train loss: 1.00747e-05, val loss: 1.23104e-05, min loss: 9.60333e-06\n",
      "Epoch: 95300, elapsed: 9.97e+00, train loss: 1.08639e-05, val loss: 1.36971e-05, min loss: 9.60333e-06\n",
      "Epoch: 95400, elapsed: 1.05e+01, train loss: 1.33733e-05, val loss: 1.43174e-05, min loss: 9.60333e-06\n",
      "Epoch: 95500, elapsed: 1.01e+01, train loss: 9.61704e-06, val loss: 1.18270e-05, min loss: 9.60333e-06\n",
      "Epoch: 95600, elapsed: 9.94e+00, train loss: 9.51375e-06, val loss: 1.17073e-05, min loss: 9.51375e-06\n",
      "Epoch: 95700, elapsed: 1.01e+01, train loss: 9.60589e-06, val loss: 1.18913e-05, min loss: 9.51375e-06\n",
      "Epoch: 95800, elapsed: 1.05e+01, train loss: 1.92134e-05, val loss: 2.04851e-05, min loss: 9.51375e-06\n",
      "Epoch: 95900, elapsed: 9.97e+00, train loss: 1.04340e-05, val loss: 1.29348e-05, min loss: 9.51375e-06\n",
      "Epoch: 96000, elapsed: 1.00e+01, train loss: 1.03413e-05, val loss: 1.30547e-05, min loss: 9.51375e-06\n",
      "Epoch: 96100, elapsed: 1.06e+01, train loss: 9.53047e-06, val loss: 1.17673e-05, min loss: 9.51375e-06\n",
      "Epoch: 96200, elapsed: 1.00e+01, train loss: 9.61894e-06, val loss: 1.17720e-05, min loss: 9.51375e-06\n",
      "Epoch: 96300, elapsed: 1.00e+01, train loss: 1.19476e-05, val loss: 1.51114e-05, min loss: 9.51375e-06\n",
      "Epoch: 96400, elapsed: 1.05e+01, train loss: 1.28671e-05, val loss: 1.37354e-05, min loss: 9.51375e-06\n",
      "Epoch: 96500, elapsed: 1.02e+01, train loss: 9.86565e-06, val loss: 1.19664e-05, min loss: 9.51375e-06\n",
      "Epoch: 96600, elapsed: 9.97e+00, train loss: 9.96424e-06, val loss: 1.22007e-05, min loss: 9.51375e-06\n",
      "Epoch: 96700, elapsed: 1.00e+01, train loss: 9.60311e-06, val loss: 1.17933e-05, min loss: 9.51375e-06\n",
      "Epoch: 96800, elapsed: 1.05e+01, train loss: 1.65455e-05, val loss: 1.80724e-05, min loss: 9.51375e-06\n",
      "Epoch: 96900, elapsed: 1.01e+01, train loss: 1.05062e-05, val loss: 1.19366e-05, min loss: 9.51375e-06\n",
      "Epoch: 97000, elapsed: 1.01e+01, train loss: 1.67829e-05, val loss: 1.78038e-05, min loss: 9.51375e-06\n",
      "Epoch: 97100, elapsed: 1.06e+01, train loss: 1.00226e-05, val loss: 1.20366e-05, min loss: 9.51375e-06\n",
      "Epoch: 97200, elapsed: 1.01e+01, train loss: 9.72121e-06, val loss: 1.18129e-05, min loss: 9.51375e-06\n",
      "Epoch: 97300, elapsed: 9.99e+00, train loss: 1.10404e-05, val loss: 1.37817e-05, min loss: 9.51375e-06\n",
      "Epoch: 97400, elapsed: 1.00e+01, train loss: 1.33581e-05, val loss: 1.52702e-05, min loss: 9.51375e-06\n",
      "Epoch: 97500, elapsed: 1.06e+01, train loss: 1.03317e-05, val loss: 1.25483e-05, min loss: 9.51375e-06\n",
      "Epoch: 97600, elapsed: 1.01e+01, train loss: 1.07660e-05, val loss: 1.30898e-05, min loss: 9.51375e-06\n",
      "Epoch: 97700, elapsed: 9.95e+00, train loss: 1.39382e-05, val loss: 1.56647e-05, min loss: 9.51375e-06\n",
      "Epoch: 97800, elapsed: 1.04e+01, train loss: 1.09208e-05, val loss: 1.37095e-05, min loss: 9.51375e-06\n",
      "Epoch: 97900, elapsed: 1.02e+01, train loss: 1.03341e-05, val loss: 1.28330e-05, min loss: 9.51375e-06\n",
      "Epoch: 98000, elapsed: 9.97e+00, train loss: 1.29764e-05, val loss: 1.43793e-05, min loss: 9.51375e-06\n",
      "Epoch: 98100, elapsed: 9.94e+00, train loss: 1.00212e-05, val loss: 1.22431e-05, min loss: 9.51375e-06\n",
      "Epoch: 98200, elapsed: 1.06e+01, train loss: 9.53996e-06, val loss: 1.17637e-05, min loss: 9.51375e-06\n",
      "Epoch: 98300, elapsed: 1.01e+01, train loss: 1.04752e-05, val loss: 1.27091e-05, min loss: 9.51375e-06\n",
      "Epoch: 98400, elapsed: 9.92e+00, train loss: 1.09365e-05, val loss: 1.27710e-05, min loss: 9.51375e-06\n",
      "Epoch: 98500, elapsed: 1.06e+01, train loss: 1.41827e-05, val loss: 1.56551e-05, min loss: 9.51375e-06\n",
      "Epoch: 98600, elapsed: 1.00e+01, train loss: 1.40989e-05, val loss: 1.66780e-05, min loss: 9.51375e-06\n",
      "Epoch: 98700, elapsed: 1.01e+01, train loss: 1.54514e-05, val loss: 1.71199e-05, min loss: 9.51375e-06\n",
      "Epoch: 98800, elapsed: 1.00e+01, train loss: 1.01839e-05, val loss: 1.23694e-05, min loss: 9.51375e-06\n",
      "Epoch: 98900, elapsed: 1.07e+01, train loss: 1.09462e-05, val loss: 1.35659e-05, min loss: 9.51375e-06\n",
      "Epoch: 99000, elapsed: 1.01e+01, train loss: 9.69198e-06, val loss: 1.16584e-05, min loss: 9.51375e-06\n",
      "Epoch: 99100, elapsed: 1.02e+01, train loss: 1.03339e-05, val loss: 1.19632e-05, min loss: 9.51375e-06\n",
      "Epoch: 99200, elapsed: 1.04e+01, train loss: 1.93702e-05, val loss: 1.88927e-05, min loss: 9.51375e-06\n",
      "Epoch: 99300, elapsed: 1.00e+01, train loss: 1.00682e-05, val loss: 1.22791e-05, min loss: 9.51375e-06\n",
      "Epoch: 99400, elapsed: 9.88e+00, train loss: 9.67711e-06, val loss: 1.19838e-05, min loss: 9.51375e-06\n",
      "Epoch: 99500, elapsed: 1.05e+01, train loss: 1.83744e-05, val loss: 2.08530e-05, min loss: 9.51375e-06\n",
      "Epoch: 99600, elapsed: 1.01e+01, train loss: 9.72886e-06, val loss: 1.20502e-05, min loss: 9.51375e-06\n",
      "Epoch: 99700, elapsed: 9.96e+00, train loss: 1.05057e-05, val loss: 1.26115e-05, min loss: 9.51375e-06\n",
      "Epoch: 99800, elapsed: 1.01e+01, train loss: 1.09151e-05, val loss: 1.30334e-05, min loss: 9.51375e-06\n",
      "Epoch: 99900, elapsed: 1.05e+01, train loss: 1.18502e-05, val loss: 1.43689e-05, min loss: 9.51375e-06\n",
      "Epoch: 100000, elapsed: 1.01e+01, train loss: 9.53046e-06, val loss: 1.19144e-05, min loss: 9.51375e-06\n",
      "Epoch: 100100, elapsed: 1.18e+01, train loss: 9.42263e-06, val loss: 1.16155e-05, min loss: 9.42263e-06\n",
      "Epoch: 100200, elapsed: 1.05e+01, train loss: 1.02799e-05, val loss: 1.28387e-05, min loss: 9.42263e-06\n",
      "Epoch: 100300, elapsed: 1.01e+01, train loss: 9.47518e-06, val loss: 1.16850e-05, min loss: 9.42263e-06\n",
      "Epoch: 100400, elapsed: 1.00e+01, train loss: 9.80240e-06, val loss: 1.20202e-05, min loss: 9.42263e-06\n",
      "Epoch: 100500, elapsed: 1.05e+01, train loss: 9.27767e-06, val loss: 1.14780e-05, min loss: 9.27767e-06\n",
      "Epoch: 100600, elapsed: 1.01e+01, train loss: 1.00085e-05, val loss: 1.22203e-05, min loss: 9.27767e-06\n",
      "Epoch: 100700, elapsed: 9.97e+00, train loss: 1.21932e-05, val loss: 1.47213e-05, min loss: 9.27767e-06\n",
      "Epoch: 100800, elapsed: 1.01e+01, train loss: 9.46248e-06, val loss: 1.17236e-05, min loss: 9.27767e-06\n",
      "Epoch: 100900, elapsed: 1.06e+01, train loss: 9.39672e-06, val loss: 1.15156e-05, min loss: 9.27767e-06\n",
      "Epoch: 101000, elapsed: 1.01e+01, train loss: 9.73476e-06, val loss: 1.17681e-05, min loss: 9.27767e-06\n",
      "Epoch: 101100, elapsed: 9.97e+00, train loss: 9.40068e-06, val loss: 1.14236e-05, min loss: 9.27767e-06\n",
      "Epoch: 101200, elapsed: 1.05e+01, train loss: 1.21737e-05, val loss: 1.31255e-05, min loss: 9.27767e-06\n",
      "Epoch: 101300, elapsed: 1.02e+01, train loss: 9.80707e-06, val loss: 1.20738e-05, min loss: 9.27767e-06\n",
      "Epoch: 101400, elapsed: 1.00e+01, train loss: 1.16327e-05, val loss: 1.49427e-05, min loss: 9.27767e-06\n",
      "Epoch: 101500, elapsed: 1.00e+01, train loss: 9.46939e-06, val loss: 1.17198e-05, min loss: 9.27767e-06\n",
      "Epoch: 101600, elapsed: 1.05e+01, train loss: 2.31413e-05, val loss: 2.68148e-05, min loss: 9.27767e-06\n",
      "Epoch: 101700, elapsed: 1.01e+01, train loss: 1.17661e-05, val loss: 1.34416e-05, min loss: 9.27767e-06\n",
      "Epoch: 101800, elapsed: 1.00e+01, train loss: 9.54913e-06, val loss: 1.13486e-05, min loss: 9.27767e-06\n",
      "Epoch: 101900, elapsed: 1.04e+01, train loss: 9.19775e-06, val loss: 1.11980e-05, min loss: 9.19775e-06\n",
      "Epoch: 102000, elapsed: 1.02e+01, train loss: 9.24077e-06, val loss: 1.14616e-05, min loss: 9.19775e-06\n",
      "Epoch: 102100, elapsed: 9.97e+00, train loss: 1.09759e-05, val loss: 1.28875e-05, min loss: 9.19775e-06\n",
      "Epoch: 102200, elapsed: 9.93e+00, train loss: 9.53317e-06, val loss: 1.15833e-05, min loss: 9.19775e-06\n",
      "Epoch: 102300, elapsed: 1.04e+01, train loss: 1.38283e-05, val loss: 1.42606e-05, min loss: 9.19775e-06\n",
      "Epoch: 102400, elapsed: 1.02e+01, train loss: 9.23884e-06, val loss: 1.11984e-05, min loss: 9.19775e-06\n",
      "Epoch: 102500, elapsed: 1.00e+01, train loss: 1.50101e-05, val loss: 1.57287e-05, min loss: 9.19775e-06\n",
      "Epoch: 102600, elapsed: 1.01e+01, train loss: 1.55727e-05, val loss: 1.61774e-05, min loss: 9.19775e-06\n",
      "Epoch: 102700, elapsed: 1.07e+01, train loss: 9.10906e-06, val loss: 1.12040e-05, min loss: 9.10906e-06\n",
      "Epoch: 102800, elapsed: 1.00e+01, train loss: 9.69516e-06, val loss: 1.21967e-05, min loss: 9.10906e-06\n",
      "Epoch: 102900, elapsed: 1.01e+01, train loss: 9.17810e-06, val loss: 1.13287e-05, min loss: 9.10906e-06\n",
      "Epoch: 103000, elapsed: 1.05e+01, train loss: 9.36619e-06, val loss: 1.14330e-05, min loss: 9.10906e-06\n",
      "Epoch: 103100, elapsed: 1.01e+01, train loss: 1.42382e-05, val loss: 1.66441e-05, min loss: 9.10906e-06\n",
      "Epoch: 103200, elapsed: 9.97e+00, train loss: 9.21612e-06, val loss: 1.11084e-05, min loss: 9.10906e-06\n",
      "Epoch: 103300, elapsed: 9.93e+00, train loss: 1.44123e-05, val loss: 1.75597e-05, min loss: 9.10906e-06\n",
      "Epoch: 103400, elapsed: 1.06e+01, train loss: 9.38569e-06, val loss: 1.11426e-05, min loss: 9.10906e-06\n",
      "Epoch: 103500, elapsed: 1.02e+01, train loss: 9.15528e-06, val loss: 1.13048e-05, min loss: 9.10906e-06\n",
      "Epoch: 103600, elapsed: 1.01e+01, train loss: 9.71752e-06, val loss: 1.16602e-05, min loss: 9.10906e-06\n",
      "Epoch: 103700, elapsed: 1.05e+01, train loss: 9.47654e-06, val loss: 1.12487e-05, min loss: 9.10906e-06\n",
      "Epoch: 103800, elapsed: 1.00e+01, train loss: 9.81567e-06, val loss: 1.18730e-05, min loss: 9.10906e-06\n",
      "Epoch: 103900, elapsed: 9.88e+00, train loss: 1.02121e-05, val loss: 1.24687e-05, min loss: 9.10906e-06\n",
      "Epoch: 104000, elapsed: 9.92e+00, train loss: 9.13219e-06, val loss: 1.12022e-05, min loss: 9.10906e-06\n",
      "Epoch: 104100, elapsed: 1.06e+01, train loss: 1.90334e-05, val loss: 2.14185e-05, min loss: 9.10906e-06\n",
      "Epoch: 104200, elapsed: 9.93e+00, train loss: 9.72374e-06, val loss: 1.19003e-05, min loss: 9.10906e-06\n",
      "Epoch: 104300, elapsed: 9.99e+00, train loss: 1.11897e-05, val loss: 1.38711e-05, min loss: 9.10906e-06\n",
      "Epoch: 104400, elapsed: 1.05e+01, train loss: 1.17128e-05, val loss: 1.32567e-05, min loss: 9.10906e-06\n",
      "Epoch: 104500, elapsed: 1.00e+01, train loss: 1.25581e-05, val loss: 1.50122e-05, min loss: 9.10906e-06\n",
      "Epoch: 104600, elapsed: 1.02e+01, train loss: 9.13837e-06, val loss: 1.15797e-05, min loss: 9.10906e-06\n",
      "Epoch: 104700, elapsed: 9.99e+00, train loss: 1.12890e-05, val loss: 1.35008e-05, min loss: 9.10906e-06\n",
      "Epoch: 104800, elapsed: 1.06e+01, train loss: 9.96590e-06, val loss: 1.23554e-05, min loss: 9.10906e-06\n",
      "Epoch: 104900, elapsed: 1.00e+01, train loss: 9.31734e-06, val loss: 1.13496e-05, min loss: 9.10906e-06\n",
      "Epoch: 105000, elapsed: 9.97e+00, train loss: 8.97625e-06, val loss: 1.09866e-05, min loss: 8.97625e-06\n",
      "Epoch: 105100, elapsed: 1.23e+01, train loss: 1.04792e-05, val loss: 1.27933e-05, min loss: 8.97625e-06\n",
      "Epoch: 105200, elapsed: 1.01e+01, train loss: 1.01048e-05, val loss: 1.21088e-05, min loss: 8.97625e-06\n",
      "Epoch: 105300, elapsed: 9.92e+00, train loss: 9.07788e-06, val loss: 1.10650e-05, min loss: 8.97625e-06\n",
      "Epoch: 105400, elapsed: 1.05e+01, train loss: 1.63336e-05, val loss: 1.81140e-05, min loss: 8.97625e-06\n",
      "Epoch: 105500, elapsed: 1.02e+01, train loss: 1.49640e-05, val loss: 1.69396e-05, min loss: 8.97625e-06\n",
      "Epoch: 105600, elapsed: 9.98e+00, train loss: 8.97076e-06, val loss: 1.10040e-05, min loss: 8.97076e-06\n",
      "Epoch: 105700, elapsed: 9.92e+00, train loss: 1.25045e-05, val loss: 1.49742e-05, min loss: 8.97076e-06\n",
      "Epoch: 105800, elapsed: 1.06e+01, train loss: 1.09962e-05, val loss: 1.28475e-05, min loss: 8.97076e-06\n",
      "Epoch: 105900, elapsed: 1.03e+01, train loss: 9.62301e-06, val loss: 1.17065e-05, min loss: 8.97076e-06\n",
      "Epoch: 106000, elapsed: 1.01e+01, train loss: 9.74524e-06, val loss: 1.18483e-05, min loss: 8.97076e-06\n",
      "Epoch: 106100, elapsed: 1.01e+01, train loss: 9.30103e-06, val loss: 1.14761e-05, min loss: 8.97076e-06\n",
      "Epoch: 106200, elapsed: 1.06e+01, train loss: 1.22946e-05, val loss: 1.46595e-05, min loss: 8.97076e-06\n",
      "Epoch: 106300, elapsed: 1.00e+01, train loss: 1.74878e-05, val loss: 1.89589e-05, min loss: 8.97076e-06\n",
      "Epoch: 106400, elapsed: 9.98e+00, train loss: 9.41216e-06, val loss: 1.15178e-05, min loss: 8.97076e-06\n",
      "Epoch: 106500, elapsed: 1.05e+01, train loss: 1.15031e-05, val loss: 1.34696e-05, min loss: 8.97076e-06\n",
      "Epoch: 106600, elapsed: 1.02e+01, train loss: 8.87731e-06, val loss: 1.08832e-05, min loss: 8.87731e-06\n",
      "Epoch: 106700, elapsed: 1.01e+01, train loss: 1.72248e-05, val loss: 1.64737e-05, min loss: 8.87731e-06\n",
      "Epoch: 106800, elapsed: 9.94e+00, train loss: 9.84610e-06, val loss: 1.18823e-05, min loss: 8.87731e-06\n",
      "Epoch: 106900, elapsed: 1.07e+01, train loss: 9.00242e-06, val loss: 1.09786e-05, min loss: 8.87731e-06\n",
      "Epoch: 107000, elapsed: 1.03e+01, train loss: 1.66975e-05, val loss: 1.67800e-05, min loss: 8.87731e-06\n",
      "Epoch: 107100, elapsed: 9.98e+00, train loss: 1.00591e-05, val loss: 1.20514e-05, min loss: 8.87731e-06\n",
      "Epoch: 107200, elapsed: 1.05e+01, train loss: 8.96690e-06, val loss: 1.09351e-05, min loss: 8.87731e-06\n",
      "Epoch: 107300, elapsed: 1.02e+01, train loss: 9.30636e-06, val loss: 1.15328e-05, min loss: 8.87731e-06\n",
      "Epoch: 107400, elapsed: 1.01e+01, train loss: 2.10363e-05, val loss: 1.89491e-05, min loss: 8.87731e-06\n",
      "Epoch: 107500, elapsed: 1.00e+01, train loss: 1.14733e-05, val loss: 1.29358e-05, min loss: 8.87731e-06\n",
      "Epoch: 107600, elapsed: 1.05e+01, train loss: 9.41863e-06, val loss: 1.13776e-05, min loss: 8.87731e-06\n",
      "Epoch: 107700, elapsed: 1.01e+01, train loss: 1.13357e-05, val loss: 1.32152e-05, min loss: 8.87731e-06\n",
      "Epoch: 107800, elapsed: 9.94e+00, train loss: 9.17960e-06, val loss: 1.09967e-05, min loss: 8.87731e-06\n",
      "Epoch: 107900, elapsed: 1.01e+01, train loss: 9.30017e-06, val loss: 1.14947e-05, min loss: 8.87731e-06\n",
      "Epoch: 108000, elapsed: 1.07e+01, train loss: 8.97258e-06, val loss: 1.10312e-05, min loss: 8.87731e-06\n",
      "Epoch: 108100, elapsed: 1.01e+01, train loss: 1.05244e-05, val loss: 1.18069e-05, min loss: 8.87731e-06\n",
      "Epoch: 108200, elapsed: 1.00e+01, train loss: 9.04468e-06, val loss: 1.09972e-05, min loss: 8.87731e-06\n",
      "Epoch: 108300, elapsed: 1.06e+01, train loss: 9.16398e-06, val loss: 1.09242e-05, min loss: 8.87731e-06\n",
      "Epoch: 108400, elapsed: 1.02e+01, train loss: 8.78407e-06, val loss: 1.06744e-05, min loss: 8.78407e-06\n",
      "Epoch: 108500, elapsed: 1.01e+01, train loss: 8.69078e-06, val loss: 1.06204e-05, min loss: 8.69078e-06\n",
      "Epoch: 108600, elapsed: 1.01e+01, train loss: 8.76358e-06, val loss: 1.07217e-05, min loss: 8.69078e-06\n",
      "Epoch: 108700, elapsed: 1.06e+01, train loss: 9.02409e-06, val loss: 1.10604e-05, min loss: 8.69078e-06\n",
      "Epoch: 108800, elapsed: 1.01e+01, train loss: 1.70493e-05, val loss: 1.70436e-05, min loss: 8.69078e-06\n",
      "Epoch: 108900, elapsed: 9.95e+00, train loss: 9.38366e-06, val loss: 1.14233e-05, min loss: 8.69078e-06\n",
      "Epoch: 109000, elapsed: 1.06e+01, train loss: 8.68133e-06, val loss: 1.06733e-05, min loss: 8.68133e-06\n",
      "Epoch: 109100, elapsed: 1.02e+01, train loss: 9.51635e-06, val loss: 1.12426e-05, min loss: 8.68133e-06\n",
      "Epoch: 109200, elapsed: 1.01e+01, train loss: 1.22075e-05, val loss: 1.24784e-05, min loss: 8.68133e-06\n",
      "Epoch: 109300, elapsed: 1.01e+01, train loss: 9.39838e-06, val loss: 1.13870e-05, min loss: 8.68133e-06\n",
      "Epoch: 109400, elapsed: 1.05e+01, train loss: 9.03009e-06, val loss: 1.10661e-05, min loss: 8.68133e-06\n",
      "Epoch: 109500, elapsed: 1.01e+01, train loss: 3.02234e-05, val loss: 3.23167e-05, min loss: 8.68133e-06\n",
      "Epoch: 109600, elapsed: 9.98e+00, train loss: 2.11882e-05, val loss: 2.04035e-05, min loss: 8.68133e-06\n",
      "Epoch: 109700, elapsed: 1.01e+01, train loss: 8.77599e-06, val loss: 1.05333e-05, min loss: 8.68133e-06\n",
      "Epoch: 109800, elapsed: 1.06e+01, train loss: 1.50023e-05, val loss: 1.78156e-05, min loss: 8.68133e-06\n",
      "Epoch: 109900, elapsed: 1.00e+01, train loss: 1.43601e-05, val loss: 1.52584e-05, min loss: 8.68133e-06\n",
      "Epoch: 110000, elapsed: 9.92e+00, train loss: 8.81310e-06, val loss: 1.06640e-05, min loss: 8.68133e-06\n",
      "Epoch: 110100, elapsed: 1.24e+01, train loss: 8.95533e-06, val loss: 1.10583e-05, min loss: 8.68133e-06\n",
      "Epoch: 110200, elapsed: 1.02e+01, train loss: 3.86136e-05, val loss: 3.53445e-05, min loss: 8.68133e-06\n",
      "Epoch: 110300, elapsed: 1.01e+01, train loss: 9.37889e-06, val loss: 1.15327e-05, min loss: 8.68133e-06\n",
      "Epoch: 110400, elapsed: 1.01e+01, train loss: 8.53928e-06, val loss: 1.04037e-05, min loss: 8.53928e-06\n",
      "Epoch: 110500, elapsed: 1.07e+01, train loss: 8.53860e-06, val loss: 1.04112e-05, min loss: 8.53860e-06\n",
      "Epoch: 110600, elapsed: 1.00e+01, train loss: 8.54977e-06, val loss: 1.03973e-05, min loss: 8.53860e-06\n",
      "Epoch: 110700, elapsed: 1.00e+01, train loss: 1.26794e-05, val loss: 1.49296e-05, min loss: 8.53860e-06\n",
      "Epoch: 110800, elapsed: 1.07e+01, train loss: 8.88976e-06, val loss: 1.06567e-05, min loss: 8.53860e-06\n",
      "Epoch: 110900, elapsed: 1.02e+01, train loss: 8.61147e-06, val loss: 1.04832e-05, min loss: 8.53860e-06\n",
      "Epoch: 111000, elapsed: 9.90e+00, train loss: 8.61322e-06, val loss: 1.04242e-05, min loss: 8.53860e-06\n",
      "Epoch: 111100, elapsed: 1.00e+01, train loss: 9.44698e-06, val loss: 1.13628e-05, min loss: 8.53860e-06\n",
      "Epoch: 111200, elapsed: 1.06e+01, train loss: 1.04509e-05, val loss: 1.19637e-05, min loss: 8.53860e-06\n",
      "Epoch: 111300, elapsed: 1.01e+01, train loss: 8.99183e-06, val loss: 1.09746e-05, min loss: 8.53860e-06\n",
      "Epoch: 111400, elapsed: 1.01e+01, train loss: 8.74049e-06, val loss: 1.03929e-05, min loss: 8.53860e-06\n",
      "Epoch: 111500, elapsed: 1.01e+01, train loss: 9.65585e-06, val loss: 1.16805e-05, min loss: 8.53860e-06\n",
      "Epoch: 111600, elapsed: 1.06e+01, train loss: 9.73735e-06, val loss: 1.07731e-05, min loss: 8.53860e-06\n",
      "Epoch: 111700, elapsed: 1.00e+01, train loss: 9.15292e-06, val loss: 1.10159e-05, min loss: 8.53860e-06\n",
      "Epoch: 111800, elapsed: 9.95e+00, train loss: 8.74629e-06, val loss: 1.06755e-05, min loss: 8.53860e-06\n",
      "Epoch: 111900, elapsed: 1.06e+01, train loss: 9.21928e-06, val loss: 1.07819e-05, min loss: 8.53860e-06\n",
      "Epoch: 112000, elapsed: 1.02e+01, train loss: 8.58356e-06, val loss: 1.04048e-05, min loss: 8.53860e-06\n",
      "Epoch: 112100, elapsed: 9.94e+00, train loss: 8.79364e-06, val loss: 1.13286e-05, min loss: 8.53860e-06\n",
      "Epoch: 112200, elapsed: 1.01e+01, train loss: 9.67039e-06, val loss: 1.14273e-05, min loss: 8.53860e-06\n",
      "Epoch: 112300, elapsed: 1.06e+01, train loss: 9.49527e-06, val loss: 1.15500e-05, min loss: 8.53860e-06\n",
      "Epoch: 112400, elapsed: 9.95e+00, train loss: 9.89081e-06, val loss: 1.14438e-05, min loss: 8.53860e-06\n",
      "Epoch: 112500, elapsed: 1.00e+01, train loss: 1.00691e-05, val loss: 1.08621e-05, min loss: 8.53860e-06\n",
      "Epoch: 112600, elapsed: 1.02e+01, train loss: 8.62645e-06, val loss: 1.02003e-05, min loss: 8.53860e-06\n",
      "Epoch: 112700, elapsed: 1.06e+01, train loss: 1.13355e-05, val loss: 1.37600e-05, min loss: 8.53860e-06\n",
      "Epoch: 112800, elapsed: 1.00e+01, train loss: 1.03880e-05, val loss: 1.21610e-05, min loss: 8.53860e-06\n",
      "Epoch: 112900, elapsed: 1.01e+01, train loss: 8.99543e-06, val loss: 1.05283e-05, min loss: 8.53860e-06\n",
      "Epoch: 113000, elapsed: 1.04e+01, train loss: 1.29427e-05, val loss: 1.48984e-05, min loss: 8.53860e-06\n",
      "Epoch: 113100, elapsed: 1.00e+01, train loss: 1.40736e-05, val loss: 1.56886e-05, min loss: 8.53860e-06\n",
      "Epoch: 113200, elapsed: 1.01e+01, train loss: 1.07771e-05, val loss: 1.23808e-05, min loss: 8.53860e-06\n",
      "Epoch: 113300, elapsed: 9.99e+00, train loss: 2.24950e-05, val loss: 2.22418e-05, min loss: 8.53860e-06\n",
      "Epoch: 113400, elapsed: 1.05e+01, train loss: 9.93937e-06, val loss: 1.19482e-05, min loss: 8.53860e-06\n",
      "Epoch: 113500, elapsed: 1.01e+01, train loss: 8.47588e-06, val loss: 1.01511e-05, min loss: 8.47588e-06\n",
      "Epoch: 113600, elapsed: 1.01e+01, train loss: 8.63723e-06, val loss: 1.02809e-05, min loss: 8.47588e-06\n",
      "Epoch: 113700, elapsed: 1.01e+01, train loss: 8.43337e-06, val loss: 1.04306e-05, min loss: 8.43337e-06\n",
      "Epoch: 113800, elapsed: 1.05e+01, train loss: 8.98255e-06, val loss: 1.01831e-05, min loss: 8.43337e-06\n",
      "Epoch: 113900, elapsed: 1.02e+01, train loss: 9.55153e-06, val loss: 1.13419e-05, min loss: 8.43337e-06\n",
      "Epoch: 114000, elapsed: 1.01e+01, train loss: 8.86940e-06, val loss: 1.05999e-05, min loss: 8.43337e-06\n",
      "Epoch: 114100, elapsed: 1.05e+01, train loss: 1.10028e-05, val loss: 1.22578e-05, min loss: 8.43337e-06\n",
      "Epoch: 114200, elapsed: 1.01e+01, train loss: 1.12930e-05, val loss: 1.33836e-05, min loss: 8.43337e-06\n",
      "Epoch: 114300, elapsed: 1.01e+01, train loss: 1.00132e-05, val loss: 1.19470e-05, min loss: 8.43337e-06\n",
      "Epoch: 114400, elapsed: 9.97e+00, train loss: 9.27923e-06, val loss: 1.11603e-05, min loss: 8.43337e-06\n",
      "Epoch: 114500, elapsed: 1.06e+01, train loss: 9.86369e-06, val loss: 1.15909e-05, min loss: 8.43337e-06\n",
      "Epoch: 114600, elapsed: 1.02e+01, train loss: 1.01998e-05, val loss: 1.13178e-05, min loss: 8.43337e-06\n",
      "Epoch: 114700, elapsed: 1.00e+01, train loss: 1.06576e-05, val loss: 1.30917e-05, min loss: 8.43337e-06\n",
      "Epoch: 114800, elapsed: 9.89e+00, train loss: 8.63195e-06, val loss: 1.03791e-05, min loss: 8.43337e-06\n",
      "Epoch: 114900, elapsed: 1.08e+01, train loss: 2.19322e-05, val loss: 2.44725e-05, min loss: 8.43337e-06\n",
      "Epoch: 115000, elapsed: 1.01e+01, train loss: 2.13684e-05, val loss: 2.79619e-05, min loss: 8.43337e-06\n",
      "Epoch: 115100, elapsed: 1.18e+01, train loss: 8.15958e-06, val loss: 9.85340e-06, min loss: 8.15958e-06\n",
      "Epoch: 115200, elapsed: 1.06e+01, train loss: 8.17242e-06, val loss: 9.87071e-06, min loss: 8.15958e-06\n",
      "Epoch: 115300, elapsed: 1.01e+01, train loss: 8.32553e-06, val loss: 1.00071e-05, min loss: 8.15958e-06\n",
      "Epoch: 115400, elapsed: 1.01e+01, train loss: 1.44597e-05, val loss: 1.44087e-05, min loss: 8.15958e-06\n",
      "Epoch: 115500, elapsed: 9.92e+00, train loss: 9.13568e-06, val loss: 1.07786e-05, min loss: 8.15958e-06\n",
      "Epoch: 115600, elapsed: 1.06e+01, train loss: 8.26595e-06, val loss: 9.73477e-06, min loss: 8.15958e-06\n",
      "Epoch: 115700, elapsed: 1.01e+01, train loss: 1.36056e-05, val loss: 1.66502e-05, min loss: 8.15958e-06\n",
      "Epoch: 115800, elapsed: 1.01e+01, train loss: 9.48523e-06, val loss: 1.07619e-05, min loss: 8.15958e-06\n",
      "Epoch: 115900, elapsed: 1.02e+01, train loss: 1.69260e-05, val loss: 1.63204e-05, min loss: 8.15958e-06\n",
      "Epoch: 116000, elapsed: 1.06e+01, train loss: 9.24833e-06, val loss: 1.10899e-05, min loss: 8.15958e-06\n",
      "Epoch: 116100, elapsed: 1.03e+01, train loss: 8.76631e-06, val loss: 1.09862e-05, min loss: 8.15958e-06\n",
      "Epoch: 116200, elapsed: 9.89e+00, train loss: 8.54667e-06, val loss: 1.01384e-05, min loss: 8.15958e-06\n",
      "Epoch: 116300, elapsed: 1.06e+01, train loss: 8.36460e-06, val loss: 1.02366e-05, min loss: 8.15958e-06\n",
      "Epoch: 116400, elapsed: 1.02e+01, train loss: 8.19949e-06, val loss: 9.88829e-06, min loss: 8.15958e-06\n",
      "Epoch: 116500, elapsed: 1.03e+01, train loss: 8.14164e-06, val loss: 9.68836e-06, min loss: 8.14164e-06\n",
      "Epoch: 116600, elapsed: 1.00e+01, train loss: 8.47772e-06, val loss: 9.98692e-06, min loss: 8.14164e-06\n",
      "Epoch: 116700, elapsed: 1.06e+01, train loss: 8.17515e-06, val loss: 9.99104e-06, min loss: 8.14164e-06\n",
      "Epoch: 116800, elapsed: 1.02e+01, train loss: 2.54352e-05, val loss: 2.26233e-05, min loss: 8.14164e-06\n",
      "Epoch: 116900, elapsed: 1.00e+01, train loss: 9.45597e-06, val loss: 1.20944e-05, min loss: 8.14164e-06\n",
      "Epoch: 117000, elapsed: 9.99e+00, train loss: 8.76037e-06, val loss: 1.03717e-05, min loss: 8.14164e-06\n",
      "Epoch: 117100, elapsed: 1.06e+01, train loss: 8.61235e-06, val loss: 9.96078e-06, min loss: 8.14164e-06\n",
      "Epoch: 117200, elapsed: 1.01e+01, train loss: 8.43782e-06, val loss: 1.02457e-05, min loss: 8.14164e-06\n",
      "Epoch: 117300, elapsed: 1.00e+01, train loss: 8.06853e-06, val loss: 9.67848e-06, min loss: 8.06853e-06\n",
      "Epoch: 117400, elapsed: 1.00e+01, train loss: 8.05314e-06, val loss: 9.61679e-06, min loss: 8.05314e-06\n",
      "Epoch: 117500, elapsed: 1.07e+01, train loss: 1.97174e-05, val loss: 1.90586e-05, min loss: 8.05314e-06\n",
      "Epoch: 117600, elapsed: 1.00e+01, train loss: 9.31137e-06, val loss: 1.10249e-05, min loss: 8.05314e-06\n",
      "Epoch: 117700, elapsed: 1.01e+01, train loss: 8.70925e-06, val loss: 1.04646e-05, min loss: 8.05314e-06\n",
      "Epoch: 117800, elapsed: 1.06e+01, train loss: 9.79273e-06, val loss: 1.21186e-05, min loss: 8.05314e-06\n",
      "Epoch: 117900, elapsed: 1.01e+01, train loss: 9.62295e-06, val loss: 1.08506e-05, min loss: 8.05314e-06\n",
      "Epoch: 118000, elapsed: 1.00e+01, train loss: 8.21388e-06, val loss: 9.65844e-06, min loss: 8.05314e-06\n",
      "Epoch: 118100, elapsed: 1.00e+01, train loss: 1.83603e-05, val loss: 2.00547e-05, min loss: 8.05314e-06\n",
      "Epoch: 118200, elapsed: 1.05e+01, train loss: 9.00654e-06, val loss: 1.05264e-05, min loss: 8.05314e-06\n",
      "Epoch: 118300, elapsed: 1.02e+01, train loss: 8.89773e-06, val loss: 1.08196e-05, min loss: 8.05314e-06\n",
      "Epoch: 118400, elapsed: 1.01e+01, train loss: 1.79284e-05, val loss: 1.88805e-05, min loss: 8.05314e-06\n",
      "Epoch: 118500, elapsed: 1.01e+01, train loss: 8.75221e-06, val loss: 1.03387e-05, min loss: 8.05314e-06\n",
      "Epoch: 118600, elapsed: 1.06e+01, train loss: 1.39299e-05, val loss: 1.59553e-05, min loss: 8.05314e-06\n",
      "Epoch: 118700, elapsed: 1.01e+01, train loss: 8.00959e-06, val loss: 9.59294e-06, min loss: 8.00959e-06\n",
      "Epoch: 118800, elapsed: 1.00e+01, train loss: 7.96471e-06, val loss: 9.50865e-06, min loss: 7.96471e-06\n",
      "Epoch: 118900, elapsed: 1.01e+01, train loss: 9.67751e-06, val loss: 1.16007e-05, min loss: 7.96471e-06\n",
      "Epoch: 119000, elapsed: 1.06e+01, train loss: 2.20014e-05, val loss: 2.67215e-05, min loss: 7.96471e-06\n",
      "Epoch: 119100, elapsed: 1.01e+01, train loss: 2.09724e-05, val loss: 2.12205e-05, min loss: 7.96471e-06\n",
      "Epoch: 119200, elapsed: 9.98e+00, train loss: 9.13521e-06, val loss: 1.07085e-05, min loss: 7.96471e-06\n",
      "Epoch: 119300, elapsed: 1.01e+01, train loss: 7.87821e-06, val loss: 9.47827e-06, min loss: 7.87821e-06\n",
      "Epoch: 119400, elapsed: 1.06e+01, train loss: 1.76350e-05, val loss: 1.76104e-05, min loss: 7.87821e-06\n",
      "Epoch: 119500, elapsed: 1.01e+01, train loss: 8.36840e-06, val loss: 9.74211e-06, min loss: 7.87821e-06\n",
      "Epoch: 119600, elapsed: 1.01e+01, train loss: 1.01160e-05, val loss: 1.01925e-05, min loss: 7.87821e-06\n",
      "Epoch: 119700, elapsed: 1.07e+01, train loss: 8.84029e-06, val loss: 1.06761e-05, min loss: 7.87821e-06\n",
      "Epoch: 119800, elapsed: 1.02e+01, train loss: 1.19568e-05, val loss: 1.37953e-05, min loss: 7.87821e-06\n",
      "Epoch: 119900, elapsed: 1.01e+01, train loss: 9.97778e-06, val loss: 1.05696e-05, min loss: 7.87821e-06\n",
      "Epoch: 120000, elapsed: 9.89e+00, train loss: 9.70788e-06, val loss: 1.06236e-05, min loss: 7.87821e-06\n",
      "Epoch: 120100, elapsed: 1.26e+01, train loss: 9.05940e-06, val loss: 9.98268e-06, min loss: 7.87821e-06\n",
      "Epoch: 120200, elapsed: 1.00e+01, train loss: 8.50380e-06, val loss: 1.00294e-05, min loss: 7.87821e-06\n",
      "Epoch: 120300, elapsed: 1.01e+01, train loss: 8.86877e-06, val loss: 1.00317e-05, min loss: 7.87821e-06\n",
      "Epoch: 120400, elapsed: 1.00e+01, train loss: 7.79870e-06, val loss: 9.43138e-06, min loss: 7.79870e-06\n",
      "Epoch: 120500, elapsed: 1.09e+01, train loss: 8.27060e-06, val loss: 9.88357e-06, min loss: 7.79870e-06\n",
      "Epoch: 120600, elapsed: 1.01e+01, train loss: 7.81543e-06, val loss: 9.31419e-06, min loss: 7.79870e-06\n",
      "Epoch: 120700, elapsed: 1.00e+01, train loss: 8.50804e-06, val loss: 9.98528e-06, min loss: 7.79870e-06\n",
      "Epoch: 120800, elapsed: 1.02e+01, train loss: 9.59417e-06, val loss: 1.13110e-05, min loss: 7.79870e-06\n",
      "Epoch: 120900, elapsed: 1.06e+01, train loss: 7.94882e-06, val loss: 9.47348e-06, min loss: 7.79870e-06\n",
      "Epoch: 121000, elapsed: 1.00e+01, train loss: 7.79819e-06, val loss: 9.38432e-06, min loss: 7.79819e-06\n",
      "Epoch: 121100, elapsed: 1.01e+01, train loss: 7.97984e-06, val loss: 9.48570e-06, min loss: 7.79819e-06\n",
      "Epoch: 121200, elapsed: 1.06e+01, train loss: 9.13628e-06, val loss: 9.65691e-06, min loss: 7.79819e-06\n",
      "Epoch: 121300, elapsed: 1.02e+01, train loss: 8.11745e-06, val loss: 9.21750e-06, min loss: 7.79819e-06\n",
      "Epoch: 121400, elapsed: 1.01e+01, train loss: 9.77447e-06, val loss: 1.06217e-05, min loss: 7.79819e-06\n",
      "Epoch: 121500, elapsed: 1.00e+01, train loss: 8.02232e-06, val loss: 9.43104e-06, min loss: 7.79819e-06\n",
      "Epoch: 121600, elapsed: 1.06e+01, train loss: 7.67519e-06, val loss: 9.21904e-06, min loss: 7.67519e-06\n",
      "Epoch: 121700, elapsed: 1.02e+01, train loss: 1.00141e-05, val loss: 1.27082e-05, min loss: 7.67519e-06\n",
      "Epoch: 121800, elapsed: 1.01e+01, train loss: 8.57136e-06, val loss: 1.00028e-05, min loss: 7.67519e-06\n",
      "Epoch: 121900, elapsed: 1.02e+01, train loss: 1.03527e-05, val loss: 1.21092e-05, min loss: 7.67519e-06\n",
      "Epoch: 122000, elapsed: 1.06e+01, train loss: 7.96064e-06, val loss: 9.32034e-06, min loss: 7.67519e-06\n",
      "Epoch: 122100, elapsed: 1.02e+01, train loss: 1.04404e-05, val loss: 1.21439e-05, min loss: 7.67519e-06\n",
      "Epoch: 122200, elapsed: 1.01e+01, train loss: 8.72808e-06, val loss: 1.01307e-05, min loss: 7.67519e-06\n",
      "Epoch: 122300, elapsed: 1.00e+01, train loss: 8.02251e-06, val loss: 9.21214e-06, min loss: 7.67519e-06\n",
      "Epoch: 122400, elapsed: 1.05e+01, train loss: 7.63556e-06, val loss: 9.13132e-06, min loss: 7.63556e-06\n",
      "Epoch: 122500, elapsed: 1.01e+01, train loss: 7.63360e-06, val loss: 9.17127e-06, min loss: 7.63360e-06\n",
      "Epoch: 122600, elapsed: 1.01e+01, train loss: 8.15837e-06, val loss: 9.60138e-06, min loss: 7.63360e-06\n",
      "Epoch: 122700, elapsed: 1.01e+01, train loss: 9.01619e-06, val loss: 1.01527e-05, min loss: 7.63360e-06\n",
      "Epoch: 122800, elapsed: 1.05e+01, train loss: 7.58847e-06, val loss: 9.07756e-06, min loss: 7.58847e-06\n",
      "Epoch: 122900, elapsed: 1.02e+01, train loss: 7.59078e-06, val loss: 9.10621e-06, min loss: 7.58847e-06\n",
      "Epoch: 123000, elapsed: 1.00e+01, train loss: 7.72709e-06, val loss: 9.18737e-06, min loss: 7.58847e-06\n",
      "Epoch: 123100, elapsed: 1.00e+01, train loss: 7.77223e-06, val loss: 9.34261e-06, min loss: 7.58847e-06\n",
      "Epoch: 123200, elapsed: 1.07e+01, train loss: 1.80004e-05, val loss: 1.79895e-05, min loss: 7.58847e-06\n",
      "Epoch: 123300, elapsed: 1.01e+01, train loss: 8.09897e-06, val loss: 9.44308e-06, min loss: 7.58847e-06\n",
      "Epoch: 123400, elapsed: 1.00e+01, train loss: 7.59788e-06, val loss: 9.02235e-06, min loss: 7.58847e-06\n",
      "Epoch: 123500, elapsed: 1.01e+01, train loss: 7.89534e-06, val loss: 9.27958e-06, min loss: 7.58847e-06\n",
      "Epoch: 123600, elapsed: 1.06e+01, train loss: 1.37295e-05, val loss: 1.30869e-05, min loss: 7.58847e-06\n",
      "Epoch: 123700, elapsed: 1.01e+01, train loss: 1.01913e-05, val loss: 1.23013e-05, min loss: 7.58847e-06\n",
      "Epoch: 123800, elapsed: 1.00e+01, train loss: 7.80399e-06, val loss: 9.28403e-06, min loss: 7.58847e-06\n",
      "Epoch: 123900, elapsed: 1.00e+01, train loss: 7.56480e-06, val loss: 9.02452e-06, min loss: 7.56480e-06\n",
      "Epoch: 124000, elapsed: 1.07e+01, train loss: 7.90130e-06, val loss: 9.18554e-06, min loss: 7.56480e-06\n",
      "Epoch: 124100, elapsed: 1.01e+01, train loss: 9.02188e-06, val loss: 1.04312e-05, min loss: 7.56480e-06\n",
      "Epoch: 124200, elapsed: 1.01e+01, train loss: 8.08616e-06, val loss: 9.56625e-06, min loss: 7.56480e-06\n",
      "Epoch: 124300, elapsed: 1.04e+01, train loss: 8.64707e-06, val loss: 1.04606e-05, min loss: 7.56480e-06\n",
      "Epoch: 124400, elapsed: 1.01e+01, train loss: 7.47659e-06, val loss: 8.91949e-06, min loss: 7.47659e-06\n",
      "Epoch: 124500, elapsed: 1.00e+01, train loss: 7.50753e-06, val loss: 8.98887e-06, min loss: 7.47659e-06\n",
      "Epoch: 124600, elapsed: 1.00e+01, train loss: 7.78908e-06, val loss: 9.06121e-06, min loss: 7.47659e-06\n",
      "Epoch: 124700, elapsed: 1.05e+01, train loss: 7.52047e-06, val loss: 9.01210e-06, min loss: 7.47659e-06\n",
      "Epoch: 124800, elapsed: 1.02e+01, train loss: 8.16357e-06, val loss: 9.46318e-06, min loss: 7.47659e-06\n",
      "Epoch: 124900, elapsed: 1.01e+01, train loss: 9.23535e-06, val loss: 1.01628e-05, min loss: 7.47659e-06\n",
      "Epoch: 125000, elapsed: 9.99e+00, train loss: 8.72494e-06, val loss: 1.00322e-05, min loss: 7.47659e-06\n",
      "Epoch: 125100, elapsed: 1.24e+01, train loss: 1.07186e-05, val loss: 1.21091e-05, min loss: 7.47659e-06\n",
      "Epoch: 125200, elapsed: 1.02e+01, train loss: 7.52112e-06, val loss: 8.80539e-06, min loss: 7.47659e-06\n",
      "Epoch: 125300, elapsed: 1.01e+01, train loss: 2.18674e-05, val loss: 2.65179e-05, min loss: 7.47659e-06\n",
      "Epoch: 125400, elapsed: 1.01e+01, train loss: 7.79130e-06, val loss: 9.27217e-06, min loss: 7.47659e-06\n",
      "Epoch: 125500, elapsed: 1.06e+01, train loss: 8.46555e-06, val loss: 1.03679e-05, min loss: 7.47659e-06\n",
      "Epoch: 125600, elapsed: 9.99e+00, train loss: 7.49053e-06, val loss: 8.91253e-06, min loss: 7.47659e-06\n",
      "Epoch: 125700, elapsed: 9.96e+00, train loss: 8.10538e-06, val loss: 9.67953e-06, min loss: 7.47659e-06\n",
      "Epoch: 125800, elapsed: 1.00e+01, train loss: 1.61316e-05, val loss: 1.55685e-05, min loss: 7.47659e-06\n",
      "Epoch: 125900, elapsed: 1.07e+01, train loss: 7.43108e-06, val loss: 8.87086e-06, min loss: 7.43108e-06\n",
      "Epoch: 126000, elapsed: 1.02e+01, train loss: 8.52999e-06, val loss: 1.03031e-05, min loss: 7.43108e-06\n",
      "Epoch: 126100, elapsed: 1.00e+01, train loss: 8.67898e-06, val loss: 1.02777e-05, min loss: 7.43108e-06\n",
      "Epoch: 126200, elapsed: 9.94e+00, train loss: 7.68531e-06, val loss: 9.09775e-06, min loss: 7.43108e-06\n",
      "Epoch: 126300, elapsed: 1.05e+01, train loss: 8.51864e-06, val loss: 9.74183e-06, min loss: 7.43108e-06\n",
      "Epoch: 126400, elapsed: 1.01e+01, train loss: 7.43833e-06, val loss: 8.81943e-06, min loss: 7.43108e-06\n",
      "Epoch: 126500, elapsed: 9.91e+00, train loss: 7.39583e-06, val loss: 8.90688e-06, min loss: 7.39583e-06\n",
      "Epoch: 126600, elapsed: 1.00e+01, train loss: 8.92641e-06, val loss: 1.05792e-05, min loss: 7.39583e-06\n",
      "Epoch: 126700, elapsed: 1.06e+01, train loss: 7.63237e-06, val loss: 9.10528e-06, min loss: 7.39583e-06\n",
      "Epoch: 126800, elapsed: 1.01e+01, train loss: 7.53205e-06, val loss: 8.94583e-06, min loss: 7.39583e-06\n",
      "Epoch: 126900, elapsed: 1.00e+01, train loss: 7.65423e-06, val loss: 8.94083e-06, min loss: 7.39583e-06\n",
      "Epoch: 127000, elapsed: 1.01e+01, train loss: 7.35872e-06, val loss: 8.68286e-06, min loss: 7.35872e-06\n",
      "Epoch: 127100, elapsed: 1.07e+01, train loss: 7.33180e-06, val loss: 8.78896e-06, min loss: 7.33180e-06\n",
      "Epoch: 127200, elapsed: 1.01e+01, train loss: 7.34397e-06, val loss: 8.80754e-06, min loss: 7.33180e-06\n",
      "Epoch: 127300, elapsed: 1.01e+01, train loss: 1.02349e-05, val loss: 1.26470e-05, min loss: 7.33180e-06\n",
      "Epoch: 127400, elapsed: 1.00e+01, train loss: 1.53346e-05, val loss: 1.70434e-05, min loss: 7.33180e-06\n",
      "Epoch: 127500, elapsed: 1.06e+01, train loss: 7.30444e-06, val loss: 8.77064e-06, min loss: 7.30444e-06\n",
      "Epoch: 127600, elapsed: 1.00e+01, train loss: 8.28194e-06, val loss: 1.03246e-05, min loss: 7.30444e-06\n",
      "Epoch: 127700, elapsed: 1.01e+01, train loss: 2.68654e-05, val loss: 2.24267e-05, min loss: 7.30444e-06\n",
      "Epoch: 127800, elapsed: 1.01e+01, train loss: 1.39989e-05, val loss: 1.23835e-05, min loss: 7.30444e-06\n",
      "Epoch: 127900, elapsed: 1.07e+01, train loss: 7.81727e-06, val loss: 8.83788e-06, min loss: 7.30444e-06\n",
      "Epoch: 128000, elapsed: 1.01e+01, train loss: 1.96959e-05, val loss: 1.89870e-05, min loss: 7.30444e-06\n",
      "Epoch: 128100, elapsed: 1.00e+01, train loss: 7.64503e-06, val loss: 9.29030e-06, min loss: 7.30444e-06\n",
      "Epoch: 128200, elapsed: 1.06e+01, train loss: 1.14140e-05, val loss: 1.36235e-05, min loss: 7.30444e-06\n",
      "Epoch: 128300, elapsed: 1.02e+01, train loss: 9.30038e-06, val loss: 9.97548e-06, min loss: 7.30444e-06\n",
      "Epoch: 128400, elapsed: 1.01e+01, train loss: 7.58736e-06, val loss: 8.89857e-06, min loss: 7.30444e-06\n",
      "Epoch: 128500, elapsed: 1.00e+01, train loss: 7.22435e-06, val loss: 8.61859e-06, min loss: 7.22435e-06\n",
      "Epoch: 128600, elapsed: 1.05e+01, train loss: 7.20974e-06, val loss: 8.59148e-06, min loss: 7.20974e-06\n",
      "Epoch: 128700, elapsed: 1.02e+01, train loss: 7.26024e-06, val loss: 8.64664e-06, min loss: 7.20974e-06\n",
      "Epoch: 128800, elapsed: 1.03e+01, train loss: 9.75689e-06, val loss: 1.11842e-05, min loss: 7.20974e-06\n",
      "Epoch: 128900, elapsed: 1.01e+01, train loss: 9.61763e-06, val loss: 9.48273e-06, min loss: 7.20974e-06\n",
      "Epoch: 129000, elapsed: 1.06e+01, train loss: 7.67054e-06, val loss: 9.14421e-06, min loss: 7.20974e-06\n",
      "Epoch: 129100, elapsed: 1.02e+01, train loss: 7.28379e-06, val loss: 8.64564e-06, min loss: 7.20974e-06\n",
      "Epoch: 129200, elapsed: 1.01e+01, train loss: 1.11551e-05, val loss: 1.22495e-05, min loss: 7.20974e-06\n",
      "Epoch: 129300, elapsed: 1.00e+01, train loss: 1.27108e-05, val loss: 1.53456e-05, min loss: 7.20974e-06\n",
      "Epoch: 129400, elapsed: 1.05e+01, train loss: 1.07547e-05, val loss: 1.15086e-05, min loss: 7.20974e-06\n",
      "Epoch: 129500, elapsed: 1.04e+01, train loss: 7.37747e-06, val loss: 8.79226e-06, min loss: 7.20974e-06\n",
      "Epoch: 129600, elapsed: 9.96e+00, train loss: 1.19591e-05, val loss: 1.27975e-05, min loss: 7.20974e-06\n",
      "Epoch: 129700, elapsed: 9.97e+00, train loss: 1.10167e-05, val loss: 1.26420e-05, min loss: 7.20974e-06\n",
      "Epoch: 129800, elapsed: 1.05e+01, train loss: 7.59175e-06, val loss: 8.87674e-06, min loss: 7.20974e-06\n",
      "Epoch: 129900, elapsed: 1.03e+01, train loss: 7.28904e-06, val loss: 8.51276e-06, min loss: 7.20974e-06\n",
      "Epoch: 130000, elapsed: 1.01e+01, train loss: 7.37045e-06, val loss: 8.78704e-06, min loss: 7.20974e-06\n",
      "Epoch: 130100, elapsed: 1.18e+01, train loss: 8.10520e-06, val loss: 9.76808e-06, min loss: 7.20974e-06\n",
      "Epoch: 130200, elapsed: 1.05e+01, train loss: 8.43553e-06, val loss: 9.69822e-06, min loss: 7.20974e-06\n",
      "Epoch: 130300, elapsed: 1.01e+01, train loss: 1.17539e-05, val loss: 1.35242e-05, min loss: 7.20974e-06\n",
      "Epoch: 130400, elapsed: 1.01e+01, train loss: 7.87717e-06, val loss: 9.12145e-06, min loss: 7.20974e-06\n",
      "Epoch: 130500, elapsed: 1.00e+01, train loss: 8.46914e-06, val loss: 9.62143e-06, min loss: 7.20974e-06\n",
      "Epoch: 130600, elapsed: 1.06e+01, train loss: 1.98584e-05, val loss: 1.89455e-05, min loss: 7.20974e-06\n",
      "Epoch: 130700, elapsed: 1.01e+01, train loss: 7.47532e-06, val loss: 8.85228e-06, min loss: 7.20974e-06\n",
      "Epoch: 130800, elapsed: 1.00e+01, train loss: 2.91385e-05, val loss: 2.83730e-05, min loss: 7.20974e-06\n",
      "Epoch: 130900, elapsed: 1.00e+01, train loss: 1.27467e-05, val loss: 1.57180e-05, min loss: 7.20974e-06\n",
      "Epoch: 131000, elapsed: 1.06e+01, train loss: 1.55280e-05, val loss: 1.81243e-05, min loss: 7.20974e-06\n",
      "Epoch: 131100, elapsed: 1.00e+01, train loss: 8.04055e-06, val loss: 9.28445e-06, min loss: 7.20974e-06\n",
      "Epoch: 131200, elapsed: 1.01e+01, train loss: 7.53212e-06, val loss: 8.52993e-06, min loss: 7.20974e-06\n",
      "Epoch: 131300, elapsed: 1.00e+01, train loss: 7.97757e-06, val loss: 9.36238e-06, min loss: 7.20974e-06\n",
      "Epoch: 131400, elapsed: 1.06e+01, train loss: 8.61810e-06, val loss: 1.00416e-05, min loss: 7.20974e-06\n",
      "Epoch: 131500, elapsed: 1.00e+01, train loss: 9.60651e-06, val loss: 9.77002e-06, min loss: 7.20974e-06\n",
      "Epoch: 131600, elapsed: 1.00e+01, train loss: 7.81341e-06, val loss: 9.35993e-06, min loss: 7.20974e-06\n",
      "Epoch: 131700, elapsed: 1.01e+01, train loss: 9.56608e-06, val loss: 1.03206e-05, min loss: 7.20974e-06\n",
      "Epoch: 131800, elapsed: 1.05e+01, train loss: 7.85701e-06, val loss: 9.08412e-06, min loss: 7.20974e-06\n",
      "Epoch: 131900, elapsed: 1.01e+01, train loss: 9.29553e-06, val loss: 1.13009e-05, min loss: 7.20974e-06\n",
      "Epoch: 132000, elapsed: 1.01e+01, train loss: 7.73147e-06, val loss: 8.91781e-06, min loss: 7.20974e-06\n",
      "Epoch: 132100, elapsed: 1.00e+01, train loss: 7.41857e-06, val loss: 8.61647e-06, min loss: 7.20974e-06\n",
      "Epoch: 132200, elapsed: 1.06e+01, train loss: 1.05241e-05, val loss: 1.12560e-05, min loss: 7.20974e-06\n",
      "Epoch: 132300, elapsed: 1.02e+01, train loss: 7.60010e-06, val loss: 8.80749e-06, min loss: 7.20974e-06\n",
      "Epoch: 132400, elapsed: 1.00e+01, train loss: 7.27773e-06, val loss: 8.53710e-06, min loss: 7.20974e-06\n",
      "Epoch: 132500, elapsed: 1.01e+01, train loss: 6.98291e-06, val loss: 8.38569e-06, min loss: 6.98291e-06\n",
      "Epoch: 132600, elapsed: 1.05e+01, train loss: 7.02877e-06, val loss: 8.46094e-06, min loss: 6.98291e-06\n",
      "Epoch: 132700, elapsed: 1.02e+01, train loss: 7.83484e-06, val loss: 9.02196e-06, min loss: 6.98291e-06\n",
      "Epoch: 132800, elapsed: 9.99e+00, train loss: 7.05957e-06, val loss: 8.52845e-06, min loss: 6.98291e-06\n",
      "Epoch: 132900, elapsed: 1.01e+01, train loss: 1.07074e-05, val loss: 1.32513e-05, min loss: 6.98291e-06\n",
      "Epoch: 133000, elapsed: 1.06e+01, train loss: 7.37623e-06, val loss: 8.46258e-06, min loss: 6.98291e-06\n",
      "Epoch: 133100, elapsed: 1.01e+01, train loss: 7.01627e-06, val loss: 8.59637e-06, min loss: 6.98291e-06\n",
      "Epoch: 133200, elapsed: 1.00e+01, train loss: 7.58562e-06, val loss: 9.36022e-06, min loss: 6.98291e-06\n",
      "Epoch: 133300, elapsed: 1.00e+01, train loss: 7.06216e-06, val loss: 8.80436e-06, min loss: 6.98291e-06\n",
      "Epoch: 133400, elapsed: 1.04e+01, train loss: 1.22406e-05, val loss: 1.25200e-05, min loss: 6.98291e-06\n",
      "Epoch: 133500, elapsed: 1.01e+01, train loss: 7.04813e-06, val loss: 8.43984e-06, min loss: 6.98291e-06\n",
      "Epoch: 133600, elapsed: 1.00e+01, train loss: 6.94021e-06, val loss: 8.30436e-06, min loss: 6.94021e-06\n",
      "Epoch: 133700, elapsed: 1.01e+01, train loss: 6.93053e-06, val loss: 8.30319e-06, min loss: 6.93053e-06\n",
      "Epoch: 133800, elapsed: 1.06e+01, train loss: 6.92353e-06, val loss: 8.33557e-06, min loss: 6.92353e-06\n",
      "Epoch: 133900, elapsed: 1.02e+01, train loss: 7.44277e-06, val loss: 8.93068e-06, min loss: 6.92353e-06\n",
      "Epoch: 134000, elapsed: 1.01e+01, train loss: 7.91157e-06, val loss: 9.49212e-06, min loss: 6.92353e-06\n",
      "Epoch: 134100, elapsed: 1.01e+01, train loss: 1.30736e-05, val loss: 1.42595e-05, min loss: 6.92353e-06\n",
      "Epoch: 134200, elapsed: 1.05e+01, train loss: 6.98850e-06, val loss: 8.31773e-06, min loss: 6.92353e-06\n",
      "Epoch: 134300, elapsed: 1.02e+01, train loss: 1.09441e-05, val loss: 1.26378e-05, min loss: 6.92353e-06\n",
      "Epoch: 134400, elapsed: 9.91e+00, train loss: 8.03183e-06, val loss: 9.23277e-06, min loss: 6.92353e-06\n",
      "Epoch: 134500, elapsed: 1.01e+01, train loss: 7.76157e-06, val loss: 9.20955e-06, min loss: 6.92353e-06\n",
      "Epoch: 134600, elapsed: 1.07e+01, train loss: 1.07286e-05, val loss: 1.17145e-05, min loss: 6.92353e-06\n",
      "Epoch: 134700, elapsed: 1.02e+01, train loss: 7.22597e-06, val loss: 8.57605e-06, min loss: 6.92353e-06\n",
      "Epoch: 134800, elapsed: 1.00e+01, train loss: 9.93323e-06, val loss: 1.33911e-05, min loss: 6.92353e-06\n",
      "Epoch: 134900, elapsed: 9.96e+00, train loss: 9.11592e-06, val loss: 9.62815e-06, min loss: 6.92353e-06\n",
      "Epoch: 135000, elapsed: 1.06e+01, train loss: 6.85727e-06, val loss: 8.24439e-06, min loss: 6.85727e-06\n",
      "Epoch: 135100, elapsed: 1.20e+01, train loss: 6.92406e-06, val loss: 8.30465e-06, min loss: 6.85727e-06\n",
      "Epoch: 135200, elapsed: 9.93e+00, train loss: 6.92900e-06, val loss: 8.32855e-06, min loss: 6.85727e-06\n",
      "Epoch: 135300, elapsed: 1.01e+01, train loss: 6.89232e-06, val loss: 8.29024e-06, min loss: 6.85727e-06\n",
      "Epoch: 135400, elapsed: 1.04e+01, train loss: 7.52308e-06, val loss: 8.83802e-06, min loss: 6.85727e-06\n",
      "Epoch: 135500, elapsed: 1.00e+01, train loss: 6.88235e-06, val loss: 8.23559e-06, min loss: 6.85727e-06\n",
      "Epoch: 135600, elapsed: 1.01e+01, train loss: 7.91388e-06, val loss: 9.46818e-06, min loss: 6.85727e-06\n",
      "Epoch: 135700, elapsed: 1.01e+01, train loss: 8.44982e-06, val loss: 9.34004e-06, min loss: 6.85727e-06\n",
      "Epoch: 135800, elapsed: 1.05e+01, train loss: 9.08386e-06, val loss: 1.05141e-05, min loss: 6.85727e-06\n",
      "Epoch: 135900, elapsed: 1.01e+01, train loss: 1.93881e-05, val loss: 1.94197e-05, min loss: 6.85727e-06\n",
      "Epoch: 136000, elapsed: 9.88e+00, train loss: 8.25629e-06, val loss: 9.57677e-06, min loss: 6.85727e-06\n",
      "Epoch: 136100, elapsed: 1.00e+01, train loss: 6.86146e-06, val loss: 8.28715e-06, min loss: 6.85727e-06\n",
      "Epoch: 136200, elapsed: 1.05e+01, train loss: 7.12163e-06, val loss: 8.42046e-06, min loss: 6.85727e-06\n",
      "Epoch: 136300, elapsed: 1.01e+01, train loss: 6.81311e-06, val loss: 8.20990e-06, min loss: 6.81311e-06\n",
      "Epoch: 136400, elapsed: 1.00e+01, train loss: 9.89789e-06, val loss: 1.14810e-05, min loss: 6.81311e-06\n",
      "Epoch: 136500, elapsed: 9.90e+00, train loss: 8.95205e-06, val loss: 1.00697e-05, min loss: 6.81311e-06\n",
      "Epoch: 136600, elapsed: 1.07e+01, train loss: 7.73999e-06, val loss: 8.97641e-06, min loss: 6.81311e-06\n",
      "Epoch: 136700, elapsed: 1.01e+01, train loss: 1.04130e-05, val loss: 1.21654e-05, min loss: 6.81311e-06\n",
      "Epoch: 136800, elapsed: 1.01e+01, train loss: 9.89058e-06, val loss: 1.20506e-05, min loss: 6.81311e-06\n",
      "Epoch: 136900, elapsed: 1.01e+01, train loss: 8.69643e-06, val loss: 9.48670e-06, min loss: 6.81311e-06\n",
      "Epoch: 137000, elapsed: 1.01e+01, train loss: 7.35809e-06, val loss: 8.50130e-06, min loss: 6.81311e-06\n",
      "Epoch: 137100, elapsed: 1.06e+01, train loss: 7.03314e-06, val loss: 8.31060e-06, min loss: 6.81311e-06\n",
      "Epoch: 137200, elapsed: 1.00e+01, train loss: 7.15754e-06, val loss: 8.55230e-06, min loss: 6.81311e-06\n",
      "Epoch: 137300, elapsed: 1.01e+01, train loss: 9.99048e-06, val loss: 1.16988e-05, min loss: 6.81311e-06\n",
      "Epoch: 137400, elapsed: 1.00e+01, train loss: 8.38887e-06, val loss: 9.79827e-06, min loss: 6.81311e-06\n",
      "Epoch: 137500, elapsed: 1.06e+01, train loss: 6.98873e-06, val loss: 8.42404e-06, min loss: 6.81311e-06\n",
      "Epoch: 137600, elapsed: 1.01e+01, train loss: 9.47158e-06, val loss: 1.02644e-05, min loss: 6.81311e-06\n",
      "Epoch: 137700, elapsed: 9.97e+00, train loss: 7.16261e-06, val loss: 8.63141e-06, min loss: 6.81311e-06\n",
      "Epoch: 137800, elapsed: 9.97e+00, train loss: 1.59131e-05, val loss: 1.69013e-05, min loss: 6.81311e-06\n",
      "Epoch: 137900, elapsed: 1.07e+01, train loss: 9.82314e-06, val loss: 1.11538e-05, min loss: 6.81311e-06\n",
      "Epoch: 138000, elapsed: 1.01e+01, train loss: 8.51214e-06, val loss: 9.50040e-06, min loss: 6.81311e-06\n",
      "Epoch: 138100, elapsed: 1.01e+01, train loss: 6.71941e-06, val loss: 8.13571e-06, min loss: 6.71941e-06\n",
      "Epoch: 138200, elapsed: 1.01e+01, train loss: 6.72574e-06, val loss: 8.10201e-06, min loss: 6.71941e-06\n",
      "Epoch: 138300, elapsed: 1.06e+01, train loss: 7.26115e-06, val loss: 8.54704e-06, min loss: 6.71941e-06\n",
      "Epoch: 138400, elapsed: 1.01e+01, train loss: 7.10194e-06, val loss: 8.50589e-06, min loss: 6.71941e-06\n",
      "Epoch: 138500, elapsed: 1.01e+01, train loss: 8.01036e-06, val loss: 8.94461e-06, min loss: 6.71941e-06\n",
      "Epoch: 138600, elapsed: 1.01e+01, train loss: 9.62961e-06, val loss: 1.14567e-05, min loss: 6.71941e-06\n",
      "Epoch: 138700, elapsed: 1.07e+01, train loss: 7.53576e-06, val loss: 8.45182e-06, min loss: 6.71941e-06\n",
      "Epoch: 138800, elapsed: 1.00e+01, train loss: 6.83512e-06, val loss: 8.31320e-06, min loss: 6.71941e-06\n",
      "Epoch: 138900, elapsed: 1.00e+01, train loss: 6.71523e-06, val loss: 8.07873e-06, min loss: 6.71523e-06\n",
      "Epoch: 139000, elapsed: 1.01e+01, train loss: 7.02984e-06, val loss: 8.55422e-06, min loss: 6.71523e-06\n",
      "Epoch: 139100, elapsed: 1.05e+01, train loss: 7.82413e-06, val loss: 9.37110e-06, min loss: 6.71523e-06\n",
      "Epoch: 139200, elapsed: 1.01e+01, train loss: 8.09403e-06, val loss: 9.50189e-06, min loss: 6.71523e-06\n",
      "Epoch: 139300, elapsed: 9.96e+00, train loss: 7.93201e-06, val loss: 9.55181e-06, min loss: 6.71523e-06\n",
      "Epoch: 139400, elapsed: 1.01e+01, train loss: 8.35599e-06, val loss: 8.78815e-06, min loss: 6.71523e-06\n",
      "Epoch: 139500, elapsed: 1.06e+01, train loss: 1.55729e-05, val loss: 1.87145e-05, min loss: 6.71523e-06\n",
      "Epoch: 139600, elapsed: 1.02e+01, train loss: 7.32005e-06, val loss: 8.70357e-06, min loss: 6.71523e-06\n",
      "Epoch: 139700, elapsed: 9.98e+00, train loss: 8.45113e-06, val loss: 1.00751e-05, min loss: 6.71523e-06\n",
      "Epoch: 139800, elapsed: 9.92e+00, train loss: 7.59828e-06, val loss: 8.74758e-06, min loss: 6.71523e-06\n",
      "Epoch: 139900, elapsed: 1.06e+01, train loss: 7.48164e-06, val loss: 8.95129e-06, min loss: 6.71523e-06\n",
      "Epoch: 140000, elapsed: 1.01e+01, train loss: 7.69367e-06, val loss: 8.72179e-06, min loss: 6.71523e-06\n",
      "Epoch: 140100, elapsed: 1.19e+01, train loss: 8.87307e-06, val loss: 1.06963e-05, min loss: 6.71523e-06\n",
      "Epoch: 140200, elapsed: 9.99e+00, train loss: 8.09910e-06, val loss: 9.43113e-06, min loss: 6.71523e-06\n",
      "Epoch: 140300, elapsed: 1.07e+01, train loss: 8.18817e-06, val loss: 9.36720e-06, min loss: 6.71523e-06\n",
      "Epoch: 140400, elapsed: 1.00e+01, train loss: 9.04555e-06, val loss: 9.70270e-06, min loss: 6.71523e-06\n",
      "Epoch: 140500, elapsed: 1.00e+01, train loss: 1.61452e-05, val loss: 1.89638e-05, min loss: 6.71523e-06\n",
      "Epoch: 140600, elapsed: 1.01e+01, train loss: 7.09036e-06, val loss: 8.14404e-06, min loss: 6.71523e-06\n",
      "Epoch: 140700, elapsed: 1.07e+01, train loss: 9.30387e-06, val loss: 1.17930e-05, min loss: 6.71523e-06\n",
      "Epoch: 140800, elapsed: 1.01e+01, train loss: 7.18851e-06, val loss: 8.08401e-06, min loss: 6.71523e-06\n",
      "Epoch: 140900, elapsed: 1.00e+01, train loss: 6.95807e-06, val loss: 8.29696e-06, min loss: 6.71523e-06\n",
      "Epoch: 141000, elapsed: 1.01e+01, train loss: 8.32643e-06, val loss: 8.28835e-06, min loss: 6.71523e-06\n",
      "Epoch: 141100, elapsed: 9.97e+00, train loss: 7.91409e-06, val loss: 9.08912e-06, min loss: 6.71523e-06\n",
      "Epoch: 141200, elapsed: 1.07e+01, train loss: 6.61840e-06, val loss: 7.97788e-06, min loss: 6.61840e-06\n",
      "Epoch: 141300, elapsed: 1.01e+01, train loss: 6.62580e-06, val loss: 7.95142e-06, min loss: 6.61840e-06\n",
      "Epoch: 141400, elapsed: 1.01e+01, train loss: 7.46326e-06, val loss: 9.01872e-06, min loss: 6.61840e-06\n",
      "Epoch: 141500, elapsed: 9.99e+00, train loss: 8.81600e-06, val loss: 1.00526e-05, min loss: 6.61840e-06\n",
      "Epoch: 141600, elapsed: 1.06e+01, train loss: 6.87505e-06, val loss: 8.11223e-06, min loss: 6.61840e-06\n",
      "Epoch: 141700, elapsed: 1.02e+01, train loss: 6.71693e-06, val loss: 8.16248e-06, min loss: 6.61840e-06\n",
      "Epoch: 141800, elapsed: 1.00e+01, train loss: 6.68033e-06, val loss: 7.95203e-06, min loss: 6.61840e-06\n",
      "Epoch: 141900, elapsed: 9.98e+00, train loss: 1.00077e-05, val loss: 1.08733e-05, min loss: 6.61840e-06\n",
      "Epoch: 142000, elapsed: 1.06e+01, train loss: 1.08063e-05, val loss: 1.07147e-05, min loss: 6.61840e-06\n",
      "Epoch: 142100, elapsed: 1.01e+01, train loss: 6.63049e-06, val loss: 8.02720e-06, min loss: 6.61840e-06\n",
      "Epoch: 142200, elapsed: 1.01e+01, train loss: 9.77357e-06, val loss: 1.13010e-05, min loss: 6.61840e-06\n",
      "Epoch: 142300, elapsed: 1.00e+01, train loss: 7.48229e-06, val loss: 8.63263e-06, min loss: 6.61840e-06\n",
      "Epoch: 142400, elapsed: 1.05e+01, train loss: 7.15249e-06, val loss: 8.80248e-06, min loss: 6.61840e-06\n",
      "Epoch: 142500, elapsed: 1.03e+01, train loss: 7.05821e-06, val loss: 8.50468e-06, min loss: 6.61840e-06\n",
      "Epoch: 142600, elapsed: 1.00e+01, train loss: 6.80472e-06, val loss: 8.28577e-06, min loss: 6.61840e-06\n",
      "Epoch: 142700, elapsed: 1.02e+01, train loss: 6.65356e-06, val loss: 8.01953e-06, min loss: 6.61840e-06\n",
      "Epoch: 142800, elapsed: 1.06e+01, train loss: 9.79924e-06, val loss: 9.33318e-06, min loss: 6.61840e-06\n",
      "Epoch: 142900, elapsed: 1.02e+01, train loss: 6.94965e-06, val loss: 8.23624e-06, min loss: 6.61840e-06\n",
      "Epoch: 143000, elapsed: 1.00e+01, train loss: 8.70897e-06, val loss: 1.05618e-05, min loss: 6.61840e-06\n",
      "Epoch: 143100, elapsed: 9.96e+00, train loss: 7.66126e-06, val loss: 8.56867e-06, min loss: 6.61840e-06\n",
      "Epoch: 143200, elapsed: 1.01e+01, train loss: 6.68815e-06, val loss: 8.11816e-06, min loss: 6.61840e-06\n",
      "Epoch: 143300, elapsed: 1.07e+01, train loss: 6.51031e-06, val loss: 7.90095e-06, min loss: 6.51031e-06\n",
      "Epoch: 143400, elapsed: 1.01e+01, train loss: 1.62162e-05, val loss: 1.96768e-05, min loss: 6.51031e-06\n",
      "Epoch: 143500, elapsed: 1.01e+01, train loss: 8.09502e-06, val loss: 9.96948e-06, min loss: 6.51031e-06\n",
      "Epoch: 143600, elapsed: 1.01e+01, train loss: 6.61809e-06, val loss: 7.89296e-06, min loss: 6.51031e-06\n",
      "Epoch: 143700, elapsed: 1.06e+01, train loss: 6.59004e-06, val loss: 7.93696e-06, min loss: 6.51031e-06\n",
      "Epoch: 143800, elapsed: 1.01e+01, train loss: 7.10636e-06, val loss: 8.81089e-06, min loss: 6.51031e-06\n",
      "Epoch: 143900, elapsed: 1.00e+01, train loss: 6.73534e-06, val loss: 8.12550e-06, min loss: 6.51031e-06\n",
      "Epoch: 144000, elapsed: 1.00e+01, train loss: 7.64632e-06, val loss: 8.48748e-06, min loss: 6.51031e-06\n",
      "Epoch: 144100, elapsed: 1.06e+01, train loss: 6.83941e-06, val loss: 8.18964e-06, min loss: 6.51031e-06\n",
      "Epoch: 144200, elapsed: 1.02e+01, train loss: 6.62479e-06, val loss: 7.86912e-06, min loss: 6.51031e-06\n",
      "Epoch: 144300, elapsed: 1.02e+01, train loss: 1.65890e-05, val loss: 1.26781e-05, min loss: 6.51031e-06\n",
      "Epoch: 144400, elapsed: 1.00e+01, train loss: 7.66252e-06, val loss: 9.26258e-06, min loss: 6.51031e-06\n",
      "Epoch: 144500, elapsed: 1.06e+01, train loss: 1.11389e-05, val loss: 1.00765e-05, min loss: 6.51031e-06\n",
      "Epoch: 144600, elapsed: 9.99e+00, train loss: 6.73875e-06, val loss: 8.23834e-06, min loss: 6.51031e-06\n",
      "Epoch: 144700, elapsed: 1.00e+01, train loss: 6.57087e-06, val loss: 7.94083e-06, min loss: 6.51031e-06\n",
      "Epoch: 144800, elapsed: 1.01e+01, train loss: 1.69563e-05, val loss: 1.64437e-05, min loss: 6.51031e-06\n",
      "Epoch: 144900, elapsed: 1.05e+01, train loss: 6.90894e-06, val loss: 8.37136e-06, min loss: 6.51031e-06\n",
      "Epoch: 145000, elapsed: 1.01e+01, train loss: 6.44733e-06, val loss: 7.81177e-06, min loss: 6.44733e-06\n",
      "Epoch: 145100, elapsed: 1.18e+01, train loss: 6.83075e-06, val loss: 8.23880e-06, min loss: 6.44733e-06\n",
      "Epoch: 145200, elapsed: 1.02e+01, train loss: 1.52877e-05, val loss: 1.66585e-05, min loss: 6.44733e-06\n",
      "Epoch: 145300, elapsed: 1.07e+01, train loss: 6.46286e-06, val loss: 7.79223e-06, min loss: 6.44733e-06\n",
      "Epoch: 145400, elapsed: 1.01e+01, train loss: 9.52516e-06, val loss: 1.16146e-05, min loss: 6.44733e-06\n",
      "Epoch: 145500, elapsed: 1.01e+01, train loss: 6.56847e-06, val loss: 7.88589e-06, min loss: 6.44733e-06\n",
      "Epoch: 145600, elapsed: 1.00e+01, train loss: 1.57943e-05, val loss: 1.34179e-05, min loss: 6.44733e-06\n",
      "Epoch: 145700, elapsed: 1.07e+01, train loss: 9.11986e-06, val loss: 9.49811e-06, min loss: 6.44733e-06\n",
      "Epoch: 145800, elapsed: 1.02e+01, train loss: 8.20589e-06, val loss: 1.00772e-05, min loss: 6.44733e-06\n",
      "Epoch: 145900, elapsed: 1.00e+01, train loss: 6.48240e-06, val loss: 8.11213e-06, min loss: 6.44733e-06\n",
      "Epoch: 146000, elapsed: 1.00e+01, train loss: 6.48472e-06, val loss: 7.80450e-06, min loss: 6.44733e-06\n",
      "Epoch: 146100, elapsed: 1.01e+01, train loss: 7.98839e-06, val loss: 9.74119e-06, min loss: 6.44733e-06\n",
      "Epoch: 146200, elapsed: 1.07e+01, train loss: 6.85470e-06, val loss: 8.13255e-06, min loss: 6.44733e-06\n",
      "Epoch: 146300, elapsed: 1.01e+01, train loss: 7.14449e-06, val loss: 8.50361e-06, min loss: 6.44733e-06\n",
      "Epoch: 146400, elapsed: 1.02e+01, train loss: 6.54162e-06, val loss: 7.83602e-06, min loss: 6.44733e-06\n",
      "Epoch: 146500, elapsed: 1.01e+01, train loss: 6.57802e-06, val loss: 7.81913e-06, min loss: 6.44733e-06\n",
      "Epoch: 146600, elapsed: 1.06e+01, train loss: 6.64627e-06, val loss: 7.88504e-06, min loss: 6.44733e-06\n",
      "Epoch: 146700, elapsed: 1.01e+01, train loss: 6.92279e-06, val loss: 8.26621e-06, min loss: 6.44733e-06\n",
      "Epoch: 146800, elapsed: 9.97e+00, train loss: 7.15514e-06, val loss: 8.04032e-06, min loss: 6.44733e-06\n",
      "Epoch: 146900, elapsed: 9.98e+00, train loss: 8.15970e-06, val loss: 8.98130e-06, min loss: 6.44733e-06\n",
      "Epoch: 147000, elapsed: 1.08e+01, train loss: 8.17360e-06, val loss: 8.93917e-06, min loss: 6.44733e-06\n",
      "Epoch: 147100, elapsed: 1.01e+01, train loss: 6.48062e-06, val loss: 7.81536e-06, min loss: 6.44733e-06\n",
      "Epoch: 147200, elapsed: 1.00e+01, train loss: 6.40258e-06, val loss: 7.71934e-06, min loss: 6.40258e-06\n",
      "Epoch: 147300, elapsed: 9.92e+00, train loss: 6.36663e-06, val loss: 7.67238e-06, min loss: 6.36663e-06\n",
      "Epoch: 147400, elapsed: 1.00e+01, train loss: 6.38183e-06, val loss: 7.69378e-06, min loss: 6.36663e-06\n",
      "Epoch: 147500, elapsed: 1.06e+01, train loss: 7.33323e-06, val loss: 8.83056e-06, min loss: 6.36663e-06\n",
      "Epoch: 147600, elapsed: 9.94e+00, train loss: 7.78952e-06, val loss: 9.16671e-06, min loss: 6.36663e-06\n",
      "Epoch: 147700, elapsed: 9.95e+00, train loss: 6.63590e-06, val loss: 8.08587e-06, min loss: 6.36663e-06\n",
      "Epoch: 147800, elapsed: 1.01e+01, train loss: 8.14384e-06, val loss: 8.49252e-06, min loss: 6.36663e-06\n",
      "Epoch: 147900, elapsed: 1.06e+01, train loss: 9.16212e-06, val loss: 9.14750e-06, min loss: 6.36663e-06\n",
      "Epoch: 148000, elapsed: 1.02e+01, train loss: 6.33288e-06, val loss: 7.66127e-06, min loss: 6.33288e-06\n",
      "Epoch: 148100, elapsed: 1.00e+01, train loss: 6.37075e-06, val loss: 7.67992e-06, min loss: 6.33288e-06\n",
      "Epoch: 148200, elapsed: 1.01e+01, train loss: 6.76990e-06, val loss: 8.34192e-06, min loss: 6.33288e-06\n",
      "Epoch: 148300, elapsed: 1.05e+01, train loss: 7.11166e-06, val loss: 8.56843e-06, min loss: 6.33288e-06\n",
      "Epoch: 148400, elapsed: 1.00e+01, train loss: 8.48317e-06, val loss: 8.48194e-06, min loss: 6.33288e-06\n",
      "Epoch: 148500, elapsed: 1.00e+01, train loss: 6.69697e-06, val loss: 8.11596e-06, min loss: 6.33288e-06\n",
      "Epoch: 148600, elapsed: 1.01e+01, train loss: 1.20673e-05, val loss: 1.43984e-05, min loss: 6.33288e-06\n",
      "Epoch: 148700, elapsed: 1.06e+01, train loss: 7.44196e-06, val loss: 9.28091e-06, min loss: 6.33288e-06\n",
      "Epoch: 148800, elapsed: 1.02e+01, train loss: 6.67634e-06, val loss: 7.96100e-06, min loss: 6.33288e-06\n",
      "Epoch: 148900, elapsed: 1.01e+01, train loss: 9.00542e-06, val loss: 1.03886e-05, min loss: 6.33288e-06\n",
      "Epoch: 149000, elapsed: 1.02e+01, train loss: 6.37864e-06, val loss: 7.70121e-06, min loss: 6.33288e-06\n",
      "Epoch: 149100, elapsed: 1.01e+01, train loss: 1.21800e-05, val loss: 1.41088e-05, min loss: 6.33288e-06\n",
      "Epoch: 149200, elapsed: 1.07e+01, train loss: 6.87362e-06, val loss: 8.18815e-06, min loss: 6.33288e-06\n",
      "Epoch: 149300, elapsed: 1.00e+01, train loss: 7.45383e-06, val loss: 9.10766e-06, min loss: 6.33288e-06\n",
      "Epoch: 149400, elapsed: 1.00e+01, train loss: 6.81826e-06, val loss: 8.56849e-06, min loss: 6.33288e-06\n",
      "Epoch: 149500, elapsed: 1.01e+01, train loss: 6.42450e-06, val loss: 7.83310e-06, min loss: 6.33288e-06\n",
      "Epoch: 149600, elapsed: 1.06e+01, train loss: 1.04960e-05, val loss: 1.18070e-05, min loss: 6.33288e-06\n",
      "Epoch: 149700, elapsed: 1.01e+01, train loss: 2.50972e-05, val loss: 2.16313e-05, min loss: 6.33288e-06\n",
      "Epoch: 149800, elapsed: 9.99e+00, train loss: 1.90891e-05, val loss: 1.87710e-05, min loss: 6.33288e-06\n",
      "Epoch: 149900, elapsed: 1.00e+01, train loss: 6.99600e-06, val loss: 7.91593e-06, min loss: 6.33288e-06\n",
      "Epoch: 150000, elapsed: 1.06e+01, train loss: 6.34678e-06, val loss: 7.67920e-06, min loss: 6.33288e-06\n",
      "Epoch: 150100, elapsed: 1.19e+01, train loss: 6.32333e-06, val loss: 7.65052e-06, min loss: 6.32333e-06\n",
      "Epoch: 150200, elapsed: 1.00e+01, train loss: 6.91335e-06, val loss: 7.67492e-06, min loss: 6.32333e-06\n",
      "Epoch: 150300, elapsed: 1.00e+01, train loss: 7.72889e-06, val loss: 8.97355e-06, min loss: 6.32333e-06\n",
      "Epoch: 150400, elapsed: 1.05e+01, train loss: 6.98156e-06, val loss: 8.28375e-06, min loss: 6.32333e-06\n",
      "Epoch: 150500, elapsed: 1.01e+01, train loss: 6.41638e-06, val loss: 7.72382e-06, min loss: 6.32333e-06\n",
      "Epoch: 150600, elapsed: 9.98e+00, train loss: 6.27628e-06, val loss: 7.61073e-06, min loss: 6.27628e-06\n",
      "Epoch: 150700, elapsed: 1.01e+01, train loss: 1.21635e-05, val loss: 1.43591e-05, min loss: 6.27628e-06\n",
      "Epoch: 150800, elapsed: 9.92e+00, train loss: 7.06937e-06, val loss: 7.95127e-06, min loss: 6.27628e-06\n",
      "Epoch: 150900, elapsed: 1.06e+01, train loss: 6.60069e-06, val loss: 7.96171e-06, min loss: 6.27628e-06\n",
      "Epoch: 151000, elapsed: 1.01e+01, train loss: 1.42843e-05, val loss: 1.53291e-05, min loss: 6.27628e-06\n",
      "Epoch: 151100, elapsed: 1.01e+01, train loss: 7.53968e-06, val loss: 8.78750e-06, min loss: 6.27628e-06\n",
      "Epoch: 151200, elapsed: 9.94e+00, train loss: 7.27555e-06, val loss: 8.27637e-06, min loss: 6.27628e-06\n",
      "Epoch: 151300, elapsed: 1.06e+01, train loss: 6.49577e-06, val loss: 7.71895e-06, min loss: 6.27628e-06\n",
      "Epoch: 151400, elapsed: 1.00e+01, train loss: 1.00986e-05, val loss: 1.17230e-05, min loss: 6.27628e-06\n",
      "Epoch: 151500, elapsed: 1.02e+01, train loss: 6.65132e-06, val loss: 8.10893e-06, min loss: 6.27628e-06\n",
      "Epoch: 151600, elapsed: 9.98e+00, train loss: 6.67849e-06, val loss: 7.92092e-06, min loss: 6.27628e-06\n",
      "Epoch: 151700, elapsed: 1.07e+01, train loss: 1.02251e-05, val loss: 1.22181e-05, min loss: 6.27628e-06\n",
      "Epoch: 151800, elapsed: 1.01e+01, train loss: 6.85548e-06, val loss: 7.80422e-06, min loss: 6.27628e-06\n",
      "Epoch: 151900, elapsed: 9.90e+00, train loss: 6.86868e-06, val loss: 8.05251e-06, min loss: 6.27628e-06\n",
      "Epoch: 152000, elapsed: 9.92e+00, train loss: 6.68067e-06, val loss: 7.94916e-06, min loss: 6.27628e-06\n",
      "Epoch: 152100, elapsed: 9.97e+00, train loss: 6.27096e-06, val loss: 7.59257e-06, min loss: 6.27096e-06\n",
      "Epoch: 152200, elapsed: 1.07e+01, train loss: 6.24525e-06, val loss: 7.55716e-06, min loss: 6.24525e-06\n",
      "Epoch: 152300, elapsed: 1.02e+01, train loss: 6.60261e-06, val loss: 7.99061e-06, min loss: 6.24525e-06\n",
      "Epoch: 152400, elapsed: 1.01e+01, train loss: 6.68499e-06, val loss: 7.67646e-06, min loss: 6.24525e-06\n",
      "Epoch: 152500, elapsed: 9.91e+00, train loss: 1.49638e-05, val loss: 1.69548e-05, min loss: 6.24525e-06\n",
      "Epoch: 152600, elapsed: 1.05e+01, train loss: 7.07512e-06, val loss: 8.26533e-06, min loss: 6.24525e-06\n",
      "Epoch: 152700, elapsed: 9.98e+00, train loss: 1.44899e-05, val loss: 1.81031e-05, min loss: 6.24525e-06\n",
      "Epoch: 152800, elapsed: 1.00e+01, train loss: 8.38765e-06, val loss: 9.55875e-06, min loss: 6.24525e-06\n",
      "Epoch: 152900, elapsed: 1.01e+01, train loss: 1.63437e-05, val loss: 1.89472e-05, min loss: 6.24525e-06\n",
      "Epoch: 153000, elapsed: 1.06e+01, train loss: 1.02391e-05, val loss: 1.21578e-05, min loss: 6.24525e-06\n",
      "Epoch: 153100, elapsed: 1.02e+01, train loss: 6.35204e-06, val loss: 7.81960e-06, min loss: 6.24525e-06\n",
      "Epoch: 153200, elapsed: 9.96e+00, train loss: 6.21747e-06, val loss: 7.50654e-06, min loss: 6.21747e-06\n",
      "Epoch: 153300, elapsed: 1.01e+01, train loss: 6.31710e-06, val loss: 7.69704e-06, min loss: 6.21747e-06\n",
      "Epoch: 153400, elapsed: 1.01e+01, train loss: 1.39120e-05, val loss: 1.27097e-05, min loss: 6.21747e-06\n",
      "Epoch: 153500, elapsed: 1.06e+01, train loss: 6.93032e-06, val loss: 8.21205e-06, min loss: 6.21747e-06\n",
      "Epoch: 153600, elapsed: 1.01e+01, train loss: 6.18005e-06, val loss: 7.50864e-06, min loss: 6.18005e-06\n",
      "Epoch: 153700, elapsed: 1.00e+01, train loss: 6.24736e-06, val loss: 7.56168e-06, min loss: 6.18005e-06\n",
      "Epoch: 153800, elapsed: 1.01e+01, train loss: 6.20824e-06, val loss: 7.62245e-06, min loss: 6.18005e-06\n",
      "Epoch: 153900, elapsed: 1.05e+01, train loss: 6.32393e-06, val loss: 7.68282e-06, min loss: 6.18005e-06\n",
      "Epoch: 154000, elapsed: 1.02e+01, train loss: 6.22909e-06, val loss: 7.56352e-06, min loss: 6.18005e-06\n",
      "Epoch: 154100, elapsed: 9.97e+00, train loss: 6.70949e-06, val loss: 7.98631e-06, min loss: 6.18005e-06\n",
      "Epoch: 154200, elapsed: 9.91e+00, train loss: 1.63457e-05, val loss: 1.79850e-05, min loss: 6.18005e-06\n",
      "Epoch: 154300, elapsed: 1.00e+01, train loss: 6.41631e-06, val loss: 7.77475e-06, min loss: 6.18005e-06\n",
      "Epoch: 154400, elapsed: 1.06e+01, train loss: 6.52637e-06, val loss: 7.78799e-06, min loss: 6.18005e-06\n",
      "Epoch: 154500, elapsed: 1.01e+01, train loss: 7.52833e-06, val loss: 7.94222e-06, min loss: 6.18005e-06\n",
      "Epoch: 154600, elapsed: 9.88e+00, train loss: 9.80694e-06, val loss: 1.03720e-05, min loss: 6.18005e-06\n",
      "Epoch: 154700, elapsed: 1.00e+01, train loss: 7.24680e-06, val loss: 8.16220e-06, min loss: 6.18005e-06\n",
      "Epoch: 154800, elapsed: 1.06e+01, train loss: 6.21748e-06, val loss: 7.53582e-06, min loss: 6.18005e-06\n",
      "Epoch: 154900, elapsed: 1.00e+01, train loss: 7.28251e-06, val loss: 8.82687e-06, min loss: 6.18005e-06\n",
      "Epoch: 155000, elapsed: 1.01e+01, train loss: 7.62891e-06, val loss: 8.78910e-06, min loss: 6.18005e-06\n",
      "Epoch: 155100, elapsed: 1.18e+01, train loss: 6.31181e-06, val loss: 7.63023e-06, min loss: 6.18005e-06\n",
      "Epoch: 155200, elapsed: 1.07e+01, train loss: 9.57475e-06, val loss: 1.10631e-05, min loss: 6.18005e-06\n",
      "Epoch: 155300, elapsed: 1.00e+01, train loss: 7.37335e-06, val loss: 7.95732e-06, min loss: 6.18005e-06\n",
      "Epoch: 155400, elapsed: 9.95e+00, train loss: 7.22958e-06, val loss: 7.79218e-06, min loss: 6.18005e-06\n",
      "Epoch: 155500, elapsed: 9.91e+00, train loss: 6.67585e-06, val loss: 8.00867e-06, min loss: 6.18005e-06\n",
      "Epoch: 155600, elapsed: 1.06e+01, train loss: 1.10792e-05, val loss: 1.25273e-05, min loss: 6.18005e-06\n",
      "Epoch: 155700, elapsed: 1.01e+01, train loss: 6.79922e-06, val loss: 8.09915e-06, min loss: 6.18005e-06\n",
      "Epoch: 155800, elapsed: 1.00e+01, train loss: 6.24141e-06, val loss: 7.52950e-06, min loss: 6.18005e-06\n",
      "Epoch: 155900, elapsed: 1.00e+01, train loss: 6.22807e-06, val loss: 7.52245e-06, min loss: 6.18005e-06\n",
      "Epoch: 156000, elapsed: 9.93e+00, train loss: 7.49313e-06, val loss: 8.98518e-06, min loss: 6.18005e-06\n",
      "Epoch: 156100, elapsed: 1.06e+01, train loss: 7.12537e-06, val loss: 8.56401e-06, min loss: 6.18005e-06\n",
      "Epoch: 156200, elapsed: 1.01e+01, train loss: 6.44618e-06, val loss: 7.79636e-06, min loss: 6.18005e-06\n",
      "Epoch: 156300, elapsed: 9.84e+00, train loss: 6.53870e-06, val loss: 8.06753e-06, min loss: 6.18005e-06\n",
      "Epoch: 156400, elapsed: 9.95e+00, train loss: 6.26725e-06, val loss: 7.69645e-06, min loss: 6.18005e-06\n",
      "Epoch: 156500, elapsed: 1.06e+01, train loss: 6.16852e-06, val loss: 7.44766e-06, min loss: 6.16852e-06\n",
      "Epoch: 156600, elapsed: 1.01e+01, train loss: 9.37180e-06, val loss: 1.19179e-05, min loss: 6.16852e-06\n",
      "Epoch: 156700, elapsed: 1.00e+01, train loss: 1.05847e-05, val loss: 1.19582e-05, min loss: 6.16852e-06\n",
      "Epoch: 156800, elapsed: 9.94e+00, train loss: 7.21746e-06, val loss: 8.99846e-06, min loss: 6.16852e-06\n",
      "Epoch: 156900, elapsed: 9.86e+00, train loss: 6.54532e-06, val loss: 7.94018e-06, min loss: 6.16852e-06\n",
      "Epoch: 157000, elapsed: 1.06e+01, train loss: 6.50086e-06, val loss: 7.71681e-06, min loss: 6.16852e-06\n",
      "Epoch: 157100, elapsed: 9.99e+00, train loss: 1.06454e-05, val loss: 9.94030e-06, min loss: 6.16852e-06\n",
      "Epoch: 157200, elapsed: 9.89e+00, train loss: 6.60141e-06, val loss: 7.76877e-06, min loss: 6.16852e-06\n",
      "Epoch: 157300, elapsed: 1.01e+01, train loss: 6.12089e-06, val loss: 7.40552e-06, min loss: 6.12089e-06\n",
      "Epoch: 157400, elapsed: 1.06e+01, train loss: 6.33527e-06, val loss: 7.66168e-06, min loss: 6.12089e-06\n",
      "Epoch: 157500, elapsed: 1.01e+01, train loss: 9.24034e-06, val loss: 9.97037e-06, min loss: 6.12089e-06\n",
      "Epoch: 157600, elapsed: 1.02e+01, train loss: 6.12035e-06, val loss: 7.37255e-06, min loss: 6.12035e-06\n",
      "Epoch: 157700, elapsed: 1.00e+01, train loss: 6.30764e-06, val loss: 7.68128e-06, min loss: 6.12035e-06\n",
      "Epoch: 157800, elapsed: 9.93e+00, train loss: 6.29867e-06, val loss: 7.92190e-06, min loss: 6.12035e-06\n",
      "Epoch: 157900, elapsed: 1.07e+01, train loss: 6.23903e-06, val loss: 7.42200e-06, min loss: 6.12035e-06\n",
      "Epoch: 158000, elapsed: 1.00e+01, train loss: 6.10856e-06, val loss: 7.43349e-06, min loss: 6.10856e-06\n",
      "Epoch: 158100, elapsed: 9.93e+00, train loss: 6.15853e-06, val loss: 7.36808e-06, min loss: 6.10856e-06\n",
      "Epoch: 158200, elapsed: 1.00e+01, train loss: 6.07900e-06, val loss: 7.43501e-06, min loss: 6.07900e-06\n",
      "Epoch: 158300, elapsed: 1.07e+01, train loss: 6.10493e-06, val loss: 7.40811e-06, min loss: 6.07900e-06\n",
      "Epoch: 158400, elapsed: 1.01e+01, train loss: 6.27948e-06, val loss: 7.64487e-06, min loss: 6.07900e-06\n",
      "Epoch: 158500, elapsed: 1.00e+01, train loss: 1.21094e-05, val loss: 1.50749e-05, min loss: 6.07900e-06\n",
      "Epoch: 158600, elapsed: 9.91e+00, train loss: 7.26705e-06, val loss: 9.60729e-06, min loss: 6.07900e-06\n",
      "Epoch: 158700, elapsed: 1.01e+01, train loss: 6.98378e-06, val loss: 8.57154e-06, min loss: 6.07900e-06\n",
      "Epoch: 158800, elapsed: 1.06e+01, train loss: 7.88680e-06, val loss: 8.99260e-06, min loss: 6.07900e-06\n",
      "Epoch: 158900, elapsed: 9.92e+00, train loss: 6.87595e-06, val loss: 8.49208e-06, min loss: 6.07900e-06\n",
      "Epoch: 159000, elapsed: 9.93e+00, train loss: 6.15692e-06, val loss: 7.48752e-06, min loss: 6.07900e-06\n",
      "Epoch: 159100, elapsed: 9.92e+00, train loss: 1.22013e-05, val loss: 1.42220e-05, min loss: 6.07900e-06\n",
      "Epoch: 159200, elapsed: 1.05e+01, train loss: 1.27145e-05, val loss: 1.19112e-05, min loss: 6.07900e-06\n",
      "Epoch: 159300, elapsed: 1.01e+01, train loss: 1.06543e-05, val loss: 1.31212e-05, min loss: 6.07900e-06\n",
      "Epoch: 159400, elapsed: 1.01e+01, train loss: 7.93023e-06, val loss: 8.10227e-06, min loss: 6.07900e-06\n",
      "Epoch: 159500, elapsed: 1.00e+01, train loss: 6.48401e-06, val loss: 7.74677e-06, min loss: 6.07900e-06\n",
      "Epoch: 159600, elapsed: 1.06e+01, train loss: 9.74337e-06, val loss: 1.15777e-05, min loss: 6.07900e-06\n",
      "Epoch: 159700, elapsed: 1.02e+01, train loss: 6.57019e-06, val loss: 7.89691e-06, min loss: 6.07900e-06\n",
      "Epoch: 159800, elapsed: 9.90e+00, train loss: 8.91431e-06, val loss: 1.07455e-05, min loss: 6.07900e-06\n",
      "Epoch: 159900, elapsed: 1.01e+01, train loss: 7.13978e-06, val loss: 8.47416e-06, min loss: 6.07900e-06\n",
      "Epoch: 160000, elapsed: 1.01e+01, train loss: 8.80115e-06, val loss: 1.04825e-05, min loss: 6.07900e-06\n",
      "Epoch: 160100, elapsed: 1.24e+01, train loss: 8.71652e-06, val loss: 1.05447e-05, min loss: 6.07900e-06\n",
      "Epoch: 160200, elapsed: 1.00e+01, train loss: 1.20015e-05, val loss: 8.69192e-06, min loss: 6.07900e-06\n",
      "Epoch: 160300, elapsed: 1.01e+01, train loss: 6.02219e-06, val loss: 7.33953e-06, min loss: 6.02219e-06\n",
      "Epoch: 160400, elapsed: 9.99e+00, train loss: 6.06574e-06, val loss: 7.40495e-06, min loss: 6.02219e-06\n",
      "Epoch: 160500, elapsed: 1.07e+01, train loss: 6.20653e-06, val loss: 7.55274e-06, min loss: 6.02219e-06\n",
      "Epoch: 160600, elapsed: 1.04e+01, train loss: 1.76978e-05, val loss: 1.64088e-05, min loss: 6.02219e-06\n",
      "Epoch: 160700, elapsed: 1.03e+01, train loss: 7.76666e-06, val loss: 8.12283e-06, min loss: 6.02219e-06\n",
      "Epoch: 160800, elapsed: 1.03e+01, train loss: 6.39822e-06, val loss: 7.79430e-06, min loss: 6.02219e-06\n",
      "Epoch: 160900, elapsed: 1.05e+01, train loss: 1.53782e-05, val loss: 1.51352e-05, min loss: 6.02219e-06\n",
      "Epoch: 161000, elapsed: 1.11e+01, train loss: 2.79567e-05, val loss: 3.39469e-05, min loss: 6.02219e-06\n",
      "Epoch: 161100, elapsed: 1.07e+01, train loss: 5.96343e-06, val loss: 7.27232e-06, min loss: 5.96343e-06\n",
      "Epoch: 161200, elapsed: 1.08e+01, train loss: 6.05891e-06, val loss: 7.49474e-06, min loss: 5.96343e-06\n",
      "Epoch: 161300, elapsed: 1.06e+01, train loss: 8.21705e-06, val loss: 8.91927e-06, min loss: 5.96343e-06\n",
      "Epoch: 161400, elapsed: 1.13e+01, train loss: 9.64570e-06, val loss: 1.27754e-05, min loss: 5.96343e-06\n",
      "Epoch: 161500, elapsed: 1.07e+01, train loss: 6.69400e-06, val loss: 8.08654e-06, min loss: 5.96343e-06\n",
      "Epoch: 161600, elapsed: 1.04e+01, train loss: 8.28861e-06, val loss: 9.98591e-06, min loss: 5.96343e-06\n",
      "Epoch: 161700, elapsed: 1.06e+01, train loss: 6.55845e-06, val loss: 7.70437e-06, min loss: 5.96343e-06\n",
      "Epoch: 161800, elapsed: 1.09e+01, train loss: 1.77141e-05, val loss: 2.13924e-05, min loss: 5.96343e-06\n",
      "Epoch: 161900, elapsed: 1.15e+01, train loss: 7.30098e-06, val loss: 8.94771e-06, min loss: 5.96343e-06\n",
      "Epoch: 162000, elapsed: 1.09e+01, train loss: 1.41879e-05, val loss: 1.32476e-05, min loss: 5.96343e-06\n",
      "Epoch: 162100, elapsed: 1.11e+01, train loss: 6.99013e-06, val loss: 8.07997e-06, min loss: 5.96343e-06\n",
      "Epoch: 162200, elapsed: 1.07e+01, train loss: 6.62566e-06, val loss: 8.02121e-06, min loss: 5.96343e-06\n",
      "Epoch: 162300, elapsed: 1.13e+01, train loss: 1.37509e-05, val loss: 1.32111e-05, min loss: 5.96343e-06\n",
      "Epoch: 162400, elapsed: 1.10e+01, train loss: 9.74488e-06, val loss: 1.21835e-05, min loss: 5.96343e-06\n",
      "Epoch: 162500, elapsed: 1.10e+01, train loss: 8.87180e-06, val loss: 1.08700e-05, min loss: 5.96343e-06\n",
      "Epoch: 162600, elapsed: 1.08e+01, train loss: 1.76866e-05, val loss: 1.40557e-05, min loss: 5.96343e-06\n",
      "Epoch: 162700, elapsed: 1.08e+01, train loss: 1.65747e-05, val loss: 1.68861e-05, min loss: 5.96343e-06\n",
      "Epoch: 162800, elapsed: 1.15e+01, train loss: 7.42284e-06, val loss: 8.39922e-06, min loss: 5.96343e-06\n",
      "Epoch: 162900, elapsed: 1.08e+01, train loss: 6.51562e-06, val loss: 7.73955e-06, min loss: 5.96343e-06\n",
      "Epoch: 163000, elapsed: 1.10e+01, train loss: 2.28204e-05, val loss: 2.05926e-05, min loss: 5.96343e-06\n",
      "Epoch: 163100, elapsed: 1.09e+01, train loss: 1.03211e-05, val loss: 1.01712e-05, min loss: 5.96343e-06\n",
      "Epoch: 163200, elapsed: 1.16e+01, train loss: 6.38956e-06, val loss: 7.72458e-06, min loss: 5.96343e-06\n",
      "Epoch: 163300, elapsed: 1.08e+01, train loss: 8.14003e-06, val loss: 1.04440e-05, min loss: 5.96343e-06\n",
      "Epoch: 163400, elapsed: 1.08e+01, train loss: 6.06198e-06, val loss: 7.30394e-06, min loss: 5.96343e-06\n",
      "Epoch: 163500, elapsed: 1.09e+01, train loss: 6.39990e-06, val loss: 7.88326e-06, min loss: 5.96343e-06\n",
      "Epoch: 163600, elapsed: 1.09e+01, train loss: 7.52794e-06, val loss: 9.07857e-06, min loss: 5.96343e-06\n",
      "Epoch: 163700, elapsed: 1.15e+01, train loss: 6.68092e-06, val loss: 7.58456e-06, min loss: 5.96343e-06\n",
      "Epoch: 163800, elapsed: 1.10e+01, train loss: 5.94526e-06, val loss: 7.21158e-06, min loss: 5.94526e-06\n",
      "Epoch: 163900, elapsed: 1.10e+01, train loss: 5.91139e-06, val loss: 7.19289e-06, min loss: 5.91139e-06\n",
      "Epoch: 164000, elapsed: 1.08e+01, train loss: 6.32267e-06, val loss: 7.69757e-06, min loss: 5.91139e-06\n",
      "Epoch: 164100, elapsed: 1.15e+01, train loss: 6.58322e-06, val loss: 7.80615e-06, min loss: 5.91139e-06\n",
      "Epoch: 164200, elapsed: 1.10e+01, train loss: 6.19908e-06, val loss: 7.44304e-06, min loss: 5.91139e-06\n",
      "Epoch: 164300, elapsed: 1.09e+01, train loss: 6.03331e-06, val loss: 7.35927e-06, min loss: 5.91139e-06\n",
      "Epoch: 164400, elapsed: 1.09e+01, train loss: 5.99538e-06, val loss: 7.25231e-06, min loss: 5.91139e-06\n",
      "Epoch: 164500, elapsed: 1.09e+01, train loss: 8.51521e-06, val loss: 1.04743e-05, min loss: 5.91139e-06\n",
      "Epoch: 164600, elapsed: 1.17e+01, train loss: 8.04107e-06, val loss: 9.72336e-06, min loss: 5.91139e-06\n",
      "Epoch: 164700, elapsed: 1.08e+01, train loss: 6.35033e-06, val loss: 7.58892e-06, min loss: 5.91139e-06\n",
      "Epoch: 164800, elapsed: 1.09e+01, train loss: 7.36020e-06, val loss: 9.94244e-06, min loss: 5.91139e-06\n",
      "Epoch: 164900, elapsed: 1.08e+01, train loss: 8.89808e-06, val loss: 9.95725e-06, min loss: 5.91139e-06\n",
      "Epoch: 165000, elapsed: 1.15e+01, train loss: 5.98708e-06, val loss: 7.31103e-06, min loss: 5.91139e-06\n",
      "Epoch: 165100, elapsed: 1.28e+01, train loss: 6.90085e-06, val loss: 9.02153e-06, min loss: 5.91139e-06\n",
      "Epoch: 165200, elapsed: 1.09e+01, train loss: 5.86032e-06, val loss: 7.17810e-06, min loss: 5.86032e-06\n",
      "Epoch: 165300, elapsed: 1.08e+01, train loss: 5.85579e-06, val loss: 7.16229e-06, min loss: 5.85579e-06\n",
      "Epoch: 165400, elapsed: 1.08e+01, train loss: 5.91419e-06, val loss: 7.18527e-06, min loss: 5.85579e-06\n",
      "Epoch: 165500, elapsed: 1.15e+01, train loss: 6.38218e-06, val loss: 7.77050e-06, min loss: 5.85579e-06\n",
      "Epoch: 165600, elapsed: 1.09e+01, train loss: 1.17971e-05, val loss: 1.13969e-05, min loss: 5.85579e-06\n",
      "Epoch: 165700, elapsed: 1.08e+01, train loss: 6.90070e-06, val loss: 7.80783e-06, min loss: 5.85579e-06\n",
      "Epoch: 165800, elapsed: 1.08e+01, train loss: 6.11797e-06, val loss: 7.31404e-06, min loss: 5.85579e-06\n",
      "Epoch: 165900, elapsed: 1.16e+01, train loss: 5.90097e-06, val loss: 7.20669e-06, min loss: 5.85579e-06\n",
      "Epoch: 166000, elapsed: 1.09e+01, train loss: 7.53296e-06, val loss: 9.23605e-06, min loss: 5.85579e-06\n",
      "Epoch: 166100, elapsed: 1.11e+01, train loss: 6.57931e-06, val loss: 7.85024e-06, min loss: 5.85579e-06\n",
      "Epoch: 166200, elapsed: 1.09e+01, train loss: 6.01413e-06, val loss: 7.33169e-06, min loss: 5.85579e-06\n",
      "Epoch: 166300, elapsed: 1.09e+01, train loss: 6.33368e-06, val loss: 8.00646e-06, min loss: 5.85579e-06\n",
      "Epoch: 166400, elapsed: 1.17e+01, train loss: 8.02073e-06, val loss: 9.35597e-06, min loss: 5.85579e-06\n",
      "Epoch: 166500, elapsed: 1.08e+01, train loss: 7.20364e-06, val loss: 8.33888e-06, min loss: 5.85579e-06\n",
      "Epoch: 166600, elapsed: 1.09e+01, train loss: 6.07058e-06, val loss: 7.34826e-06, min loss: 5.85579e-06\n",
      "Epoch: 166700, elapsed: 1.08e+01, train loss: 1.05736e-05, val loss: 1.28345e-05, min loss: 5.85579e-06\n",
      "Epoch: 166800, elapsed: 1.15e+01, train loss: 7.42780e-06, val loss: 8.66387e-06, min loss: 5.85579e-06\n",
      "Epoch: 166900, elapsed: 1.08e+01, train loss: 7.22286e-06, val loss: 8.19641e-06, min loss: 5.85579e-06\n",
      "Epoch: 167000, elapsed: 1.08e+01, train loss: 1.50392e-05, val loss: 1.54451e-05, min loss: 5.85579e-06\n",
      "Epoch: 167100, elapsed: 1.07e+01, train loss: 7.08301e-06, val loss: 8.21347e-06, min loss: 5.85579e-06\n",
      "Epoch: 167200, elapsed: 1.08e+01, train loss: 6.30654e-06, val loss: 7.62818e-06, min loss: 5.85579e-06\n",
      "Epoch: 167300, elapsed: 1.15e+01, train loss: 5.84161e-06, val loss: 7.13571e-06, min loss: 5.84161e-06\n",
      "Epoch: 167400, elapsed: 1.08e+01, train loss: 9.27449e-06, val loss: 9.03024e-06, min loss: 5.84161e-06\n",
      "Epoch: 167500, elapsed: 1.11e+01, train loss: 1.02807e-05, val loss: 1.17817e-05, min loss: 5.84161e-06\n",
      "Epoch: 167600, elapsed: 1.09e+01, train loss: 6.02911e-06, val loss: 7.15555e-06, min loss: 5.84161e-06\n",
      "Epoch: 167700, elapsed: 1.08e+01, train loss: 5.83930e-06, val loss: 7.14580e-06, min loss: 5.83930e-06\n",
      "Epoch: 167800, elapsed: 1.14e+01, train loss: 1.28286e-05, val loss: 1.50203e-05, min loss: 5.83930e-06\n",
      "Epoch: 167900, elapsed: 1.09e+01, train loss: 5.80943e-06, val loss: 7.25857e-06, min loss: 5.80943e-06\n",
      "Epoch: 168000, elapsed: 1.08e+01, train loss: 5.95539e-06, val loss: 7.22547e-06, min loss: 5.80943e-06\n",
      "Epoch: 168100, elapsed: 1.10e+01, train loss: 6.07785e-06, val loss: 7.45709e-06, min loss: 5.80943e-06\n",
      "Epoch: 168200, elapsed: 1.14e+01, train loss: 6.43110e-06, val loss: 7.93269e-06, min loss: 5.80943e-06\n",
      "Epoch: 168300, elapsed: 1.10e+01, train loss: 7.11983e-06, val loss: 8.74676e-06, min loss: 5.80943e-06\n",
      "Epoch: 168400, elapsed: 1.08e+01, train loss: 1.11089e-05, val loss: 1.00042e-05, min loss: 5.80943e-06\n",
      "Epoch: 168500, elapsed: 1.10e+01, train loss: 6.45451e-06, val loss: 7.94594e-06, min loss: 5.80943e-06\n",
      "Epoch: 168600, elapsed: 1.08e+01, train loss: 6.57180e-06, val loss: 7.95552e-06, min loss: 5.80943e-06\n",
      "Epoch: 168700, elapsed: 1.15e+01, train loss: 7.00283e-06, val loss: 8.42090e-06, min loss: 5.80943e-06\n",
      "Epoch: 168800, elapsed: 1.08e+01, train loss: 6.67848e-06, val loss: 7.36096e-06, min loss: 5.80943e-06\n",
      "Epoch: 168900, elapsed: 1.08e+01, train loss: 6.03062e-06, val loss: 7.27760e-06, min loss: 5.80943e-06\n",
      "Epoch: 169000, elapsed: 1.08e+01, train loss: 5.80905e-06, val loss: 7.15341e-06, min loss: 5.80905e-06\n",
      "Epoch: 169100, elapsed: 1.09e+01, train loss: 1.23235e-05, val loss: 1.20942e-05, min loss: 5.80905e-06\n",
      "Epoch: 169200, elapsed: 1.15e+01, train loss: 8.96663e-06, val loss: 1.01420e-05, min loss: 5.80905e-06\n",
      "Epoch: 169300, elapsed: 1.08e+01, train loss: 5.81813e-06, val loss: 7.14702e-06, min loss: 5.80905e-06\n",
      "Epoch: 169400, elapsed: 1.08e+01, train loss: 5.89335e-06, val loss: 7.22508e-06, min loss: 5.80905e-06\n",
      "Epoch: 169500, elapsed: 1.07e+01, train loss: 5.87294e-06, val loss: 7.19559e-06, min loss: 5.80905e-06\n",
      "Epoch: 169600, elapsed: 1.15e+01, train loss: 6.21380e-06, val loss: 7.75571e-06, min loss: 5.80905e-06\n",
      "Epoch: 169700, elapsed: 1.09e+01, train loss: 5.77326e-06, val loss: 7.34447e-06, min loss: 5.77326e-06\n",
      "Epoch: 169800, elapsed: 1.09e+01, train loss: 6.14647e-06, val loss: 7.48803e-06, min loss: 5.77326e-06\n",
      "Epoch: 169900, elapsed: 1.09e+01, train loss: 6.16964e-06, val loss: 7.84591e-06, min loss: 5.77326e-06\n",
      "Epoch: 170000, elapsed: 1.07e+01, train loss: 1.76025e-05, val loss: 2.01419e-05, min loss: 5.77326e-06\n",
      "Epoch: 170100, elapsed: 1.38e+01, train loss: 1.51139e-05, val loss: 1.48955e-05, min loss: 5.77326e-06\n",
      "Epoch: 170200, elapsed: 1.10e+01, train loss: 7.37220e-06, val loss: 8.32211e-06, min loss: 5.77326e-06\n",
      "Epoch: 170300, elapsed: 1.08e+01, train loss: 6.48908e-06, val loss: 7.39246e-06, min loss: 5.77326e-06\n",
      "Epoch: 170400, elapsed: 1.09e+01, train loss: 5.97765e-06, val loss: 7.22038e-06, min loss: 5.77326e-06\n",
      "Epoch: 170500, elapsed: 1.17e+01, train loss: 5.74465e-06, val loss: 7.03996e-06, min loss: 5.74465e-06\n",
      "Epoch: 170600, elapsed: 1.10e+01, train loss: 6.60308e-06, val loss: 8.14163e-06, min loss: 5.74465e-06\n",
      "Epoch: 170700, elapsed: 1.08e+01, train loss: 6.75593e-06, val loss: 8.29381e-06, min loss: 5.74465e-06\n",
      "Epoch: 170800, elapsed: 1.09e+01, train loss: 8.03015e-06, val loss: 9.62012e-06, min loss: 5.74465e-06\n",
      "Epoch: 170900, elapsed: 1.08e+01, train loss: 6.38667e-06, val loss: 8.73485e-06, min loss: 5.74465e-06\n",
      "Epoch: 171000, elapsed: 1.15e+01, train loss: 5.95091e-06, val loss: 7.28427e-06, min loss: 5.74465e-06\n",
      "Epoch: 171100, elapsed: 1.08e+01, train loss: 1.68015e-05, val loss: 1.82422e-05, min loss: 5.74465e-06\n",
      "Epoch: 171200, elapsed: 1.08e+01, train loss: 8.01994e-06, val loss: 8.69581e-06, min loss: 5.74465e-06\n",
      "Epoch: 171300, elapsed: 1.07e+01, train loss: 9.01890e-06, val loss: 1.05457e-05, min loss: 5.74465e-06\n",
      "Epoch: 171400, elapsed: 1.09e+01, train loss: 7.30446e-06, val loss: 8.87738e-06, min loss: 5.74465e-06\n",
      "Epoch: 171500, elapsed: 1.17e+01, train loss: 2.27025e-05, val loss: 2.80634e-05, min loss: 5.74465e-06\n",
      "Epoch: 171600, elapsed: 1.09e+01, train loss: 5.69110e-06, val loss: 6.99725e-06, min loss: 5.69110e-06\n",
      "Epoch: 171700, elapsed: 1.10e+01, train loss: 5.75112e-06, val loss: 7.02858e-06, min loss: 5.69110e-06\n",
      "Epoch: 171800, elapsed: 1.08e+01, train loss: 5.68642e-06, val loss: 7.00329e-06, min loss: 5.68642e-06\n",
      "Epoch: 171900, elapsed: 1.15e+01, train loss: 6.16629e-06, val loss: 7.47777e-06, min loss: 5.68642e-06\n",
      "Epoch: 172000, elapsed: 1.12e+01, train loss: 5.89728e-06, val loss: 7.26201e-06, min loss: 5.68642e-06\n",
      "Epoch: 172100, elapsed: 1.10e+01, train loss: 6.24502e-06, val loss: 7.52305e-06, min loss: 5.68642e-06\n",
      "Epoch: 172200, elapsed: 1.10e+01, train loss: 6.63261e-06, val loss: 8.08940e-06, min loss: 5.68642e-06\n",
      "Epoch: 172300, elapsed: 1.09e+01, train loss: 7.15714e-06, val loss: 9.48702e-06, min loss: 5.68642e-06\n",
      "Epoch: 172400, elapsed: 1.16e+01, train loss: 6.43651e-06, val loss: 7.79601e-06, min loss: 5.68642e-06\n",
      "Epoch: 172500, elapsed: 1.10e+01, train loss: 6.10344e-06, val loss: 7.50208e-06, min loss: 5.68642e-06\n",
      "Epoch: 172600, elapsed: 1.08e+01, train loss: 6.05992e-06, val loss: 7.15023e-06, min loss: 5.68642e-06\n",
      "Epoch: 172700, elapsed: 1.08e+01, train loss: 9.44106e-06, val loss: 8.31047e-06, min loss: 5.68642e-06\n",
      "Epoch: 172800, elapsed: 1.08e+01, train loss: 6.09225e-06, val loss: 7.51662e-06, min loss: 5.68642e-06\n",
      "Epoch: 172900, elapsed: 1.17e+01, train loss: 6.62390e-06, val loss: 7.62818e-06, min loss: 5.68642e-06\n",
      "Epoch: 173000, elapsed: 1.11e+01, train loss: 5.84912e-06, val loss: 7.08090e-06, min loss: 5.68642e-06\n",
      "Epoch: 173100, elapsed: 1.10e+01, train loss: 5.65249e-06, val loss: 6.96861e-06, min loss: 5.65249e-06\n",
      "Epoch: 173200, elapsed: 1.09e+01, train loss: 5.70894e-06, val loss: 6.99900e-06, min loss: 5.65249e-06\n",
      "Epoch: 173300, elapsed: 1.11e+01, train loss: 7.53388e-06, val loss: 9.16441e-06, min loss: 5.65249e-06\n",
      "Epoch: 173400, elapsed: 1.16e+01, train loss: 5.65812e-06, val loss: 6.95992e-06, min loss: 5.65249e-06\n",
      "Epoch: 173500, elapsed: 1.09e+01, train loss: 6.44415e-06, val loss: 7.11988e-06, min loss: 5.65249e-06\n",
      "Epoch: 173600, elapsed: 1.09e+01, train loss: 6.21049e-06, val loss: 7.47653e-06, min loss: 5.65249e-06\n",
      "Epoch: 173700, elapsed: 1.08e+01, train loss: 5.92995e-06, val loss: 7.24266e-06, min loss: 5.65249e-06\n",
      "Epoch: 173800, elapsed: 1.17e+01, train loss: 8.73676e-06, val loss: 1.00191e-05, min loss: 5.65249e-06\n",
      "Epoch: 173900, elapsed: 1.09e+01, train loss: 5.69878e-06, val loss: 7.00787e-06, min loss: 5.65249e-06\n",
      "Epoch: 174000, elapsed: 1.08e+01, train loss: 5.70831e-06, val loss: 7.08724e-06, min loss: 5.65249e-06\n",
      "Epoch: 174100, elapsed: 1.09e+01, train loss: 6.44424e-06, val loss: 7.89730e-06, min loss: 5.65249e-06\n",
      "Epoch: 174200, elapsed: 1.08e+01, train loss: 8.27054e-06, val loss: 9.17705e-06, min loss: 5.65249e-06\n",
      "Epoch: 174300, elapsed: 1.16e+01, train loss: 5.64695e-06, val loss: 6.91810e-06, min loss: 5.64695e-06\n",
      "Epoch: 174400, elapsed: 1.09e+01, train loss: 1.06505e-05, val loss: 1.15133e-05, min loss: 5.64695e-06\n",
      "Epoch: 174500, elapsed: 1.10e+01, train loss: 1.16529e-05, val loss: 1.33331e-05, min loss: 5.64695e-06\n",
      "Epoch: 174600, elapsed: 1.09e+01, train loss: 6.41366e-06, val loss: 8.33849e-06, min loss: 5.64695e-06\n",
      "Epoch: 174700, elapsed: 1.09e+01, train loss: 8.75695e-06, val loss: 9.16081e-06, min loss: 5.64695e-06\n",
      "Epoch: 174800, elapsed: 1.18e+01, train loss: 6.01745e-06, val loss: 7.01086e-06, min loss: 5.64695e-06\n",
      "Epoch: 174900, elapsed: 1.10e+01, train loss: 8.18737e-06, val loss: 9.33637e-06, min loss: 5.64695e-06\n",
      "Epoch: 175000, elapsed: 1.08e+01, train loss: 6.09763e-06, val loss: 7.47161e-06, min loss: 5.64695e-06\n",
      "Epoch: 175100, elapsed: 1.29e+01, train loss: 5.80620e-06, val loss: 7.06657e-06, min loss: 5.64695e-06\n",
      "Epoch: 175200, elapsed: 1.17e+01, train loss: 1.05701e-05, val loss: 1.25843e-05, min loss: 5.64695e-06\n",
      "Epoch: 175300, elapsed: 1.11e+01, train loss: 5.88046e-06, val loss: 7.54561e-06, min loss: 5.64695e-06\n",
      "Epoch: 175400, elapsed: 1.09e+01, train loss: 5.64687e-06, val loss: 6.93571e-06, min loss: 5.64687e-06\n",
      "Epoch: 175500, elapsed: 1.08e+01, train loss: 1.29979e-05, val loss: 1.12201e-05, min loss: 5.64687e-06\n",
      "Epoch: 175600, elapsed: 1.08e+01, train loss: 7.01744e-06, val loss: 8.11486e-06, min loss: 5.64687e-06\n",
      "Epoch: 175700, elapsed: 1.17e+01, train loss: 5.80375e-06, val loss: 7.17108e-06, min loss: 5.64687e-06\n",
      "Epoch: 175800, elapsed: 1.10e+01, train loss: 7.28646e-06, val loss: 8.83489e-06, min loss: 5.64687e-06\n",
      "Epoch: 175900, elapsed: 1.09e+01, train loss: 6.90946e-06, val loss: 8.21486e-06, min loss: 5.64687e-06\n",
      "Epoch: 176000, elapsed: 1.08e+01, train loss: 6.06580e-06, val loss: 7.48040e-06, min loss: 5.64687e-06\n",
      "Epoch: 176100, elapsed: 1.09e+01, train loss: 6.97582e-06, val loss: 8.72343e-06, min loss: 5.64687e-06\n",
      "Epoch: 176200, elapsed: 1.17e+01, train loss: 6.69506e-06, val loss: 7.83873e-06, min loss: 5.64687e-06\n",
      "Epoch: 176300, elapsed: 1.09e+01, train loss: 7.67136e-06, val loss: 9.34916e-06, min loss: 5.64687e-06\n",
      "Epoch: 176400, elapsed: 1.09e+01, train loss: 1.09524e-05, val loss: 1.07141e-05, min loss: 5.64687e-06\n",
      "Epoch: 176500, elapsed: 1.08e+01, train loss: 5.72874e-06, val loss: 7.30481e-06, min loss: 5.64687e-06\n",
      "Epoch: 176600, elapsed: 1.15e+01, train loss: 5.79674e-06, val loss: 6.96096e-06, min loss: 5.64687e-06\n",
      "Epoch: 176700, elapsed: 1.11e+01, train loss: 5.58614e-06, val loss: 6.90008e-06, min loss: 5.58614e-06\n",
      "Epoch: 176800, elapsed: 1.08e+01, train loss: 6.08744e-06, val loss: 7.81783e-06, min loss: 5.58614e-06\n",
      "Epoch: 176900, elapsed: 1.08e+01, train loss: 1.19779e-05, val loss: 1.32633e-05, min loss: 5.58614e-06\n",
      "Epoch: 177000, elapsed: 1.09e+01, train loss: 5.59438e-06, val loss: 6.93901e-06, min loss: 5.58614e-06\n",
      "Epoch: 177100, elapsed: 1.17e+01, train loss: 5.70254e-06, val loss: 6.98453e-06, min loss: 5.58614e-06\n",
      "Epoch: 177200, elapsed: 1.11e+01, train loss: 6.13490e-06, val loss: 7.58190e-06, min loss: 5.58614e-06\n",
      "Epoch: 177300, elapsed: 1.10e+01, train loss: 1.13322e-05, val loss: 1.03461e-05, min loss: 5.58614e-06\n",
      "Epoch: 177400, elapsed: 1.10e+01, train loss: 6.10619e-06, val loss: 7.25244e-06, min loss: 5.58614e-06\n",
      "Epoch: 177500, elapsed: 1.11e+01, train loss: 6.41169e-06, val loss: 7.95321e-06, min loss: 5.58614e-06\n",
      "Epoch: 177600, elapsed: 1.17e+01, train loss: 1.47086e-05, val loss: 1.69750e-05, min loss: 5.58614e-06\n",
      "Epoch: 177700, elapsed: 1.10e+01, train loss: 5.98260e-06, val loss: 7.41786e-06, min loss: 5.58614e-06\n",
      "Epoch: 177800, elapsed: 1.08e+01, train loss: 6.47953e-06, val loss: 7.93534e-06, min loss: 5.58614e-06\n",
      "Epoch: 177900, elapsed: 1.10e+01, train loss: 5.61335e-06, val loss: 6.88225e-06, min loss: 5.58614e-06\n",
      "Epoch: 178000, elapsed: 1.09e+01, train loss: 6.74566e-06, val loss: 8.27736e-06, min loss: 5.58614e-06\n",
      "Epoch: 178100, elapsed: 1.17e+01, train loss: 7.72607e-06, val loss: 8.50699e-06, min loss: 5.58614e-06\n",
      "Epoch: 178200, elapsed: 1.11e+01, train loss: 5.75831e-06, val loss: 6.88526e-06, min loss: 5.58614e-06\n",
      "Epoch: 178300, elapsed: 1.08e+01, train loss: 5.71170e-06, val loss: 7.10570e-06, min loss: 5.58614e-06\n",
      "Epoch: 178400, elapsed: 1.09e+01, train loss: 5.55504e-06, val loss: 6.82878e-06, min loss: 5.55504e-06\n",
      "Epoch: 178500, elapsed: 1.09e+01, train loss: 5.61261e-06, val loss: 6.90877e-06, min loss: 5.55504e-06\n",
      "Epoch: 178600, elapsed: 1.17e+01, train loss: 5.58604e-06, val loss: 6.86551e-06, min loss: 5.55504e-06\n",
      "Epoch: 178700, elapsed: 1.09e+01, train loss: 5.55158e-06, val loss: 6.82031e-06, min loss: 5.55158e-06\n",
      "Epoch: 178800, elapsed: 1.09e+01, train loss: 5.51466e-06, val loss: 6.80878e-06, min loss: 5.51466e-06\n",
      "Epoch: 178900, elapsed: 1.08e+01, train loss: 5.50021e-06, val loss: 6.78732e-06, min loss: 5.50021e-06\n",
      "Epoch: 179000, elapsed: 1.16e+01, train loss: 5.50450e-06, val loss: 6.77171e-06, min loss: 5.50021e-06\n",
      "Epoch: 179100, elapsed: 1.11e+01, train loss: 5.78552e-06, val loss: 7.05544e-06, min loss: 5.50021e-06\n",
      "Epoch: 179200, elapsed: 1.09e+01, train loss: 5.49155e-06, val loss: 6.78592e-06, min loss: 5.49155e-06\n",
      "Epoch: 179300, elapsed: 1.09e+01, train loss: 9.30951e-06, val loss: 8.96744e-06, min loss: 5.49155e-06\n",
      "Epoch: 179400, elapsed: 1.09e+01, train loss: 5.81353e-06, val loss: 6.91833e-06, min loss: 5.49155e-06\n",
      "Epoch: 179500, elapsed: 1.16e+01, train loss: 5.99772e-06, val loss: 7.47491e-06, min loss: 5.49155e-06\n",
      "Epoch: 179600, elapsed: 1.10e+01, train loss: 1.02669e-05, val loss: 1.22314e-05, min loss: 5.49155e-06\n",
      "Epoch: 179700, elapsed: 1.08e+01, train loss: 5.55344e-06, val loss: 7.07287e-06, min loss: 5.49155e-06\n",
      "Epoch: 179800, elapsed: 1.10e+01, train loss: 5.66964e-06, val loss: 7.04244e-06, min loss: 5.49155e-06\n",
      "Epoch: 179900, elapsed: 1.09e+01, train loss: 5.75342e-06, val loss: 7.09221e-06, min loss: 5.49155e-06\n",
      "Epoch: 180000, elapsed: 1.16e+01, train loss: 1.61524e-05, val loss: 1.58261e-05, min loss: 5.49155e-06\n",
      "Epoch: 180100, elapsed: 1.30e+01, train loss: 7.82772e-06, val loss: 8.76642e-06, min loss: 5.49155e-06\n",
      "Epoch: 180200, elapsed: 1.10e+01, train loss: 5.87912e-06, val loss: 7.20551e-06, min loss: 5.49155e-06\n",
      "Epoch: 180300, elapsed: 1.08e+01, train loss: 5.62973e-06, val loss: 7.02426e-06, min loss: 5.49155e-06\n",
      "Epoch: 180400, elapsed: 1.08e+01, train loss: 6.02841e-06, val loss: 7.21886e-06, min loss: 5.49155e-06\n",
      "Epoch: 180500, elapsed: 1.17e+01, train loss: 6.91734e-06, val loss: 8.68016e-06, min loss: 5.49155e-06\n",
      "Epoch: 180600, elapsed: 1.09e+01, train loss: 6.09861e-06, val loss: 7.33057e-06, min loss: 5.49155e-06\n",
      "Epoch: 180700, elapsed: 1.10e+01, train loss: 6.42104e-06, val loss: 7.64461e-06, min loss: 5.49155e-06\n",
      "Epoch: 180800, elapsed: 1.09e+01, train loss: 6.61842e-06, val loss: 8.11910e-06, min loss: 5.49155e-06\n",
      "Epoch: 180900, elapsed: 1.16e+01, train loss: 8.46736e-06, val loss: 9.99678e-06, min loss: 5.49155e-06\n",
      "Epoch: 181000, elapsed: 1.12e+01, train loss: 6.82525e-06, val loss: 7.62725e-06, min loss: 5.49155e-06\n",
      "Epoch: 181100, elapsed: 1.09e+01, train loss: 6.11196e-06, val loss: 7.53138e-06, min loss: 5.49155e-06\n",
      "Epoch: 181200, elapsed: 1.09e+01, train loss: 5.46984e-06, val loss: 6.91167e-06, min loss: 5.46984e-06\n",
      "Epoch: 181300, elapsed: 1.09e+01, train loss: 9.46495e-06, val loss: 1.00028e-05, min loss: 5.46984e-06\n",
      "Epoch: 181400, elapsed: 1.18e+01, train loss: 5.47254e-06, val loss: 6.85455e-06, min loss: 5.46984e-06\n",
      "Epoch: 181500, elapsed: 1.10e+01, train loss: 5.59870e-06, val loss: 6.84934e-06, min loss: 5.46984e-06\n",
      "Epoch: 181600, elapsed: 1.10e+01, train loss: 6.24456e-06, val loss: 7.28922e-06, min loss: 5.46984e-06\n",
      "Epoch: 181700, elapsed: 1.10e+01, train loss: 5.77843e-06, val loss: 7.11931e-06, min loss: 5.46984e-06\n",
      "Epoch: 181800, elapsed: 1.08e+01, train loss: 6.17062e-06, val loss: 7.37570e-06, min loss: 5.46984e-06\n",
      "Epoch: 181900, elapsed: 1.17e+01, train loss: 6.49111e-06, val loss: 7.58619e-06, min loss: 5.46984e-06\n",
      "Epoch: 182000, elapsed: 1.10e+01, train loss: 5.55309e-06, val loss: 6.80363e-06, min loss: 5.46984e-06\n",
      "Epoch: 182100, elapsed: 1.10e+01, train loss: 6.65136e-06, val loss: 8.29948e-06, min loss: 5.46984e-06\n",
      "Epoch: 182200, elapsed: 1.10e+01, train loss: 5.54414e-06, val loss: 7.46209e-06, min loss: 5.46984e-06\n",
      "Epoch: 182300, elapsed: 1.09e+01, train loss: 5.60962e-06, val loss: 6.90787e-06, min loss: 5.46984e-06\n",
      "Epoch: 182400, elapsed: 1.16e+01, train loss: 1.27703e-05, val loss: 1.42470e-05, min loss: 5.46984e-06\n",
      "Epoch: 182500, elapsed: 1.13e+01, train loss: 5.66794e-06, val loss: 6.86654e-06, min loss: 5.46984e-06\n",
      "Epoch: 182600, elapsed: 1.11e+01, train loss: 6.01869e-06, val loss: 7.37996e-06, min loss: 5.46984e-06\n",
      "Epoch: 182700, elapsed: 1.09e+01, train loss: 6.56808e-06, val loss: 7.84125e-06, min loss: 5.46984e-06\n",
      "Epoch: 182800, elapsed: 1.09e+01, train loss: 6.87619e-06, val loss: 7.70023e-06, min loss: 5.46984e-06\n",
      "Epoch: 182900, elapsed: 1.16e+01, train loss: 6.91934e-06, val loss: 6.93036e-06, min loss: 5.46984e-06\n",
      "Epoch: 183000, elapsed: 1.11e+01, train loss: 6.61058e-06, val loss: 8.17858e-06, min loss: 5.46984e-06\n",
      "Epoch: 183100, elapsed: 1.11e+01, train loss: 5.73221e-06, val loss: 6.84195e-06, min loss: 5.46984e-06\n",
      "Epoch: 183200, elapsed: 1.10e+01, train loss: 9.39889e-06, val loss: 1.02501e-05, min loss: 5.46984e-06\n",
      "Epoch: 183300, elapsed: 1.09e+01, train loss: 8.89247e-06, val loss: 1.05466e-05, min loss: 5.46984e-06\n",
      "Epoch: 183400, elapsed: 1.17e+01, train loss: 5.47268e-06, val loss: 6.81323e-06, min loss: 5.46984e-06\n",
      "Epoch: 183500, elapsed: 1.11e+01, train loss: 9.94592e-06, val loss: 1.00158e-05, min loss: 5.46984e-06\n",
      "Epoch: 183600, elapsed: 1.10e+01, train loss: 5.72823e-06, val loss: 7.01896e-06, min loss: 5.46984e-06\n",
      "Epoch: 183700, elapsed: 1.10e+01, train loss: 5.51788e-06, val loss: 6.78607e-06, min loss: 5.46984e-06\n",
      "Epoch: 183800, elapsed: 1.11e+01, train loss: 5.64072e-06, val loss: 6.94682e-06, min loss: 5.46984e-06\n",
      "Epoch: 183900, elapsed: 1.17e+01, train loss: 7.46640e-06, val loss: 8.56132e-06, min loss: 5.46984e-06\n",
      "Epoch: 184000, elapsed: 1.10e+01, train loss: 9.31153e-06, val loss: 9.84521e-06, min loss: 5.46984e-06\n",
      "Epoch: 184100, elapsed: 1.09e+01, train loss: 5.48879e-06, val loss: 6.84151e-06, min loss: 5.46984e-06\n",
      "Epoch: 184200, elapsed: 1.09e+01, train loss: 8.42363e-06, val loss: 1.00582e-05, min loss: 5.46984e-06\n",
      "Epoch: 184300, elapsed: 1.10e+01, train loss: 6.49753e-06, val loss: 7.05341e-06, min loss: 5.46984e-06\n",
      "Epoch: 184400, elapsed: 1.17e+01, train loss: 7.24432e-06, val loss: 8.41904e-06, min loss: 5.46984e-06\n",
      "Epoch: 184500, elapsed: 1.11e+01, train loss: 5.58998e-06, val loss: 6.78406e-06, min loss: 5.46984e-06\n",
      "Epoch: 184600, elapsed: 1.09e+01, train loss: 5.43621e-06, val loss: 6.70197e-06, min loss: 5.43621e-06\n",
      "Epoch: 184700, elapsed: 1.08e+01, train loss: 5.75190e-06, val loss: 7.01699e-06, min loss: 5.43621e-06\n",
      "Epoch: 184800, elapsed: 1.09e+01, train loss: 7.97405e-06, val loss: 7.31781e-06, min loss: 5.43621e-06\n",
      "Epoch: 184900, elapsed: 1.18e+01, train loss: 5.34190e-06, val loss: 6.61372e-06, min loss: 5.34190e-06\n",
      "Epoch: 185000, elapsed: 1.09e+01, train loss: 5.34832e-06, val loss: 6.62054e-06, min loss: 5.34190e-06\n",
      "Epoch: 185100, elapsed: 1.28e+01, train loss: 5.33331e-06, val loss: 6.60697e-06, min loss: 5.33331e-06\n",
      "Epoch: 185200, elapsed: 1.09e+01, train loss: 5.66988e-06, val loss: 6.98156e-06, min loss: 5.33331e-06\n",
      "Epoch: 185300, elapsed: 1.16e+01, train loss: 6.09279e-06, val loss: 7.95528e-06, min loss: 5.33331e-06\n",
      "Epoch: 185400, elapsed: 1.10e+01, train loss: 5.45552e-06, val loss: 6.83282e-06, min loss: 5.33331e-06\n",
      "Epoch: 185500, elapsed: 1.08e+01, train loss: 5.60897e-06, val loss: 6.76134e-06, min loss: 5.33331e-06\n",
      "Epoch: 185600, elapsed: 1.09e+01, train loss: 5.33265e-06, val loss: 6.56734e-06, min loss: 5.33265e-06\n",
      "Epoch: 185700, elapsed: 1.09e+01, train loss: 5.38200e-06, val loss: 6.75312e-06, min loss: 5.33265e-06\n",
      "Epoch: 185800, elapsed: 1.19e+01, train loss: 5.37816e-06, val loss: 6.63540e-06, min loss: 5.33265e-06\n",
      "Epoch: 185900, elapsed: 1.09e+01, train loss: 8.40178e-06, val loss: 9.83096e-06, min loss: 5.33265e-06\n",
      "Epoch: 186000, elapsed: 1.09e+01, train loss: 6.17259e-06, val loss: 7.84416e-06, min loss: 5.33265e-06\n",
      "Epoch: 186100, elapsed: 1.09e+01, train loss: 6.46289e-06, val loss: 7.52740e-06, min loss: 5.33265e-06\n",
      "Epoch: 186200, elapsed: 1.10e+01, train loss: 5.50134e-06, val loss: 6.88740e-06, min loss: 5.33265e-06\n",
      "Epoch: 186300, elapsed: 1.17e+01, train loss: 5.91987e-06, val loss: 7.14812e-06, min loss: 5.33265e-06\n",
      "Epoch: 186400, elapsed: 1.10e+01, train loss: 5.44173e-06, val loss: 6.75269e-06, min loss: 5.33265e-06\n",
      "Epoch: 186500, elapsed: 1.10e+01, train loss: 5.81063e-06, val loss: 7.47785e-06, min loss: 5.33265e-06\n",
      "Epoch: 186600, elapsed: 1.09e+01, train loss: 6.28453e-06, val loss: 7.45819e-06, min loss: 5.33265e-06\n",
      "Epoch: 186700, elapsed: 1.08e+01, train loss: 1.16099e-05, val loss: 1.29923e-05, min loss: 5.33265e-06\n",
      "Epoch: 186800, elapsed: 1.17e+01, train loss: 5.49068e-06, val loss: 6.95480e-06, min loss: 5.33265e-06\n",
      "Epoch: 186900, elapsed: 1.11e+01, train loss: 5.38686e-06, val loss: 6.68734e-06, min loss: 5.33265e-06\n",
      "Epoch: 187000, elapsed: 1.09e+01, train loss: 6.04452e-06, val loss: 7.00800e-06, min loss: 5.33265e-06\n",
      "Epoch: 187100, elapsed: 1.09e+01, train loss: 5.33823e-06, val loss: 6.64507e-06, min loss: 5.33265e-06\n",
      "Epoch: 187200, elapsed: 1.09e+01, train loss: 7.18529e-06, val loss: 9.23331e-06, min loss: 5.33265e-06\n",
      "Epoch: 187300, elapsed: 1.18e+01, train loss: 1.15317e-05, val loss: 1.08449e-05, min loss: 5.33265e-06\n",
      "Epoch: 187400, elapsed: 1.10e+01, train loss: 5.72213e-06, val loss: 6.94059e-06, min loss: 5.33265e-06\n",
      "Epoch: 187500, elapsed: 1.09e+01, train loss: 5.40046e-06, val loss: 6.66006e-06, min loss: 5.33265e-06\n",
      "Epoch: 187600, elapsed: 1.08e+01, train loss: 6.35860e-06, val loss: 7.68450e-06, min loss: 5.33265e-06\n",
      "Epoch: 187700, elapsed: 1.09e+01, train loss: 6.48678e-06, val loss: 7.36553e-06, min loss: 5.33265e-06\n",
      "Epoch: 187800, elapsed: 1.16e+01, train loss: 6.76229e-06, val loss: 7.45531e-06, min loss: 5.33265e-06\n",
      "Epoch: 187900, elapsed: 1.10e+01, train loss: 1.87387e-05, val loss: 1.68999e-05, min loss: 5.33265e-06\n",
      "Epoch: 188000, elapsed: 1.09e+01, train loss: 6.31736e-06, val loss: 8.33464e-06, min loss: 5.33265e-06\n",
      "Epoch: 188100, elapsed: 1.09e+01, train loss: 6.98605e-06, val loss: 7.20599e-06, min loss: 5.33265e-06\n",
      "Epoch: 188200, elapsed: 1.07e+01, train loss: 6.80161e-06, val loss: 7.59921e-06, min loss: 5.33265e-06\n",
      "Epoch: 188300, elapsed: 1.17e+01, train loss: 5.49481e-06, val loss: 6.85859e-06, min loss: 5.33265e-06\n",
      "Epoch: 188400, elapsed: 1.10e+01, train loss: 5.64913e-06, val loss: 6.90990e-06, min loss: 5.33265e-06\n",
      "Epoch: 188500, elapsed: 1.09e+01, train loss: 6.21355e-06, val loss: 7.12220e-06, min loss: 5.33265e-06\n",
      "Epoch: 188600, elapsed: 1.09e+01, train loss: 5.37682e-06, val loss: 6.62157e-06, min loss: 5.33265e-06\n",
      "Epoch: 188700, elapsed: 1.09e+01, train loss: 1.04782e-05, val loss: 1.27311e-05, min loss: 5.33265e-06\n",
      "Epoch: 188800, elapsed: 1.16e+01, train loss: 1.05400e-05, val loss: 9.26908e-06, min loss: 5.33265e-06\n",
      "Epoch: 188900, elapsed: 1.10e+01, train loss: 1.65976e-05, val loss: 1.93809e-05, min loss: 5.33265e-06\n",
      "Epoch: 189000, elapsed: 1.08e+01, train loss: 1.56019e-05, val loss: 1.54340e-05, min loss: 5.33265e-06\n",
      "Epoch: 189100, elapsed: 1.09e+01, train loss: 5.76909e-06, val loss: 6.98545e-06, min loss: 5.33265e-06\n",
      "Epoch: 189200, elapsed: 1.08e+01, train loss: 6.48019e-06, val loss: 8.04949e-06, min loss: 5.33265e-06\n",
      "Epoch: 189300, elapsed: 1.17e+01, train loss: 5.20584e-06, val loss: 6.44130e-06, min loss: 5.20584e-06\n",
      "Epoch: 189400, elapsed: 1.11e+01, train loss: 5.63214e-06, val loss: 6.91603e-06, min loss: 5.20584e-06\n",
      "Epoch: 189500, elapsed: 1.10e+01, train loss: 5.71618e-06, val loss: 7.64964e-06, min loss: 5.20584e-06\n",
      "Epoch: 189600, elapsed: 1.10e+01, train loss: 5.29843e-06, val loss: 6.81308e-06, min loss: 5.20584e-06\n",
      "Epoch: 189700, elapsed: 1.08e+01, train loss: 6.07509e-06, val loss: 7.30899e-06, min loss: 5.20584e-06\n",
      "Epoch: 189800, elapsed: 1.16e+01, train loss: 5.24091e-06, val loss: 6.44143e-06, min loss: 5.20584e-06\n",
      "Epoch: 189900, elapsed: 1.09e+01, train loss: 5.21206e-06, val loss: 6.46398e-06, min loss: 5.20584e-06\n",
      "Epoch: 190000, elapsed: 1.08e+01, train loss: 5.76134e-06, val loss: 6.98923e-06, min loss: 5.20584e-06\n",
      "Epoch: 190100, elapsed: 1.29e+01, train loss: 5.84451e-06, val loss: 6.82843e-06, min loss: 5.20584e-06\n",
      "Epoch: 190200, elapsed: 1.08e+01, train loss: 5.75669e-06, val loss: 7.09290e-06, min loss: 5.20584e-06\n",
      "Epoch: 190300, elapsed: 1.16e+01, train loss: 5.53772e-06, val loss: 6.41866e-06, min loss: 5.20584e-06\n",
      "Epoch: 190400, elapsed: 1.10e+01, train loss: 5.26467e-06, val loss: 6.45836e-06, min loss: 5.20584e-06\n",
      "Epoch: 190500, elapsed: 1.10e+01, train loss: 5.38816e-06, val loss: 6.54425e-06, min loss: 5.20584e-06\n",
      "Epoch: 190600, elapsed: 1.07e+01, train loss: 5.18409e-06, val loss: 6.40913e-06, min loss: 5.18409e-06\n",
      "Epoch: 190700, elapsed: 1.09e+01, train loss: 1.00554e-05, val loss: 1.23396e-05, min loss: 5.18409e-06\n",
      "Epoch: 190800, elapsed: 1.17e+01, train loss: 7.48838e-06, val loss: 9.66575e-06, min loss: 5.18409e-06\n",
      "Epoch: 190900, elapsed: 1.10e+01, train loss: 6.89229e-06, val loss: 8.51549e-06, min loss: 5.18409e-06\n",
      "Epoch: 191000, elapsed: 1.10e+01, train loss: 6.00825e-06, val loss: 7.00014e-06, min loss: 5.18409e-06\n",
      "Epoch: 191100, elapsed: 1.08e+01, train loss: 7.66827e-06, val loss: 9.40655e-06, min loss: 5.18409e-06\n",
      "Epoch: 191200, elapsed: 1.09e+01, train loss: 7.88655e-06, val loss: 9.00626e-06, min loss: 5.18409e-06\n",
      "Epoch: 191300, elapsed: 1.19e+01, train loss: 5.56137e-06, val loss: 6.83930e-06, min loss: 5.18409e-06\n",
      "Epoch: 191400, elapsed: 1.11e+01, train loss: 5.65557e-06, val loss: 6.95526e-06, min loss: 5.18409e-06\n",
      "Epoch: 191500, elapsed: 1.10e+01, train loss: 8.69307e-06, val loss: 9.33529e-06, min loss: 5.18409e-06\n",
      "Epoch: 191600, elapsed: 1.10e+01, train loss: 6.05178e-06, val loss: 7.50998e-06, min loss: 5.18409e-06\n",
      "Epoch: 191700, elapsed: 1.09e+01, train loss: 5.19479e-06, val loss: 6.43885e-06, min loss: 5.18409e-06\n",
      "Epoch: 191800, elapsed: 1.18e+01, train loss: 5.65770e-06, val loss: 7.18896e-06, min loss: 5.18409e-06\n",
      "Epoch: 191900, elapsed: 1.10e+01, train loss: 5.16259e-06, val loss: 6.49440e-06, min loss: 5.16259e-06\n",
      "Epoch: 192000, elapsed: 1.09e+01, train loss: 5.70986e-06, val loss: 7.07659e-06, min loss: 5.16259e-06\n",
      "Epoch: 192100, elapsed: 1.08e+01, train loss: 8.89476e-06, val loss: 1.14122e-05, min loss: 5.16259e-06\n",
      "Epoch: 192200, elapsed: 1.10e+01, train loss: 5.20529e-06, val loss: 6.44372e-06, min loss: 5.16259e-06\n",
      "Epoch: 192300, elapsed: 1.18e+01, train loss: 5.11209e-06, val loss: 6.32172e-06, min loss: 5.11209e-06\n",
      "Epoch: 192400, elapsed: 1.10e+01, train loss: 5.12441e-06, val loss: 6.34553e-06, min loss: 5.11209e-06\n",
      "Epoch: 192500, elapsed: 1.10e+01, train loss: 5.16185e-06, val loss: 6.38537e-06, min loss: 5.11209e-06\n",
      "Epoch: 192600, elapsed: 1.08e+01, train loss: 5.20143e-06, val loss: 6.35748e-06, min loss: 5.11209e-06\n",
      "Epoch: 192700, elapsed: 1.08e+01, train loss: 5.76631e-06, val loss: 6.84886e-06, min loss: 5.11209e-06\n",
      "Epoch: 192800, elapsed: 1.17e+01, train loss: 7.44398e-06, val loss: 8.86652e-06, min loss: 5.11209e-06\n",
      "Epoch: 192900, elapsed: 1.10e+01, train loss: 6.23857e-06, val loss: 7.47246e-06, min loss: 5.11209e-06\n",
      "Epoch: 193000, elapsed: 1.09e+01, train loss: 7.83591e-06, val loss: 8.64493e-06, min loss: 5.11209e-06\n",
      "Epoch: 193100, elapsed: 1.09e+01, train loss: 6.10080e-06, val loss: 7.34475e-06, min loss: 5.11209e-06\n",
      "Epoch: 193200, elapsed: 1.10e+01, train loss: 1.39978e-05, val loss: 1.73969e-05, min loss: 5.11209e-06\n",
      "Epoch: 193300, elapsed: 1.17e+01, train loss: 1.13684e-05, val loss: 1.36911e-05, min loss: 5.11209e-06\n",
      "Epoch: 193400, elapsed: 1.09e+01, train loss: 5.24634e-06, val loss: 6.33186e-06, min loss: 5.11209e-06\n",
      "Epoch: 193500, elapsed: 1.07e+01, train loss: 5.44988e-06, val loss: 6.83900e-06, min loss: 5.11209e-06\n",
      "Epoch: 193600, elapsed: 1.08e+01, train loss: 7.27688e-06, val loss: 8.90705e-06, min loss: 5.11209e-06\n",
      "Epoch: 193700, elapsed: 1.09e+01, train loss: 6.17091e-06, val loss: 6.86536e-06, min loss: 5.11209e-06\n",
      "Epoch: 193800, elapsed: 1.19e+01, train loss: 8.06981e-06, val loss: 9.46999e-06, min loss: 5.11209e-06\n",
      "Epoch: 193900, elapsed: 1.10e+01, train loss: 5.69835e-06, val loss: 6.78560e-06, min loss: 5.11209e-06\n",
      "Epoch: 194000, elapsed: 1.09e+01, train loss: 7.89720e-06, val loss: 9.64442e-06, min loss: 5.11209e-06\n",
      "Epoch: 194100, elapsed: 1.09e+01, train loss: 5.45473e-06, val loss: 6.54650e-06, min loss: 5.11209e-06\n",
      "Epoch: 194200, elapsed: 1.09e+01, train loss: 5.32405e-06, val loss: 6.72822e-06, min loss: 5.11209e-06\n",
      "Epoch: 194300, elapsed: 1.18e+01, train loss: 8.10301e-06, val loss: 9.67088e-06, min loss: 5.11209e-06\n",
      "Epoch: 194400, elapsed: 1.09e+01, train loss: 9.06181e-06, val loss: 9.09178e-06, min loss: 5.11209e-06\n",
      "Epoch: 194500, elapsed: 1.08e+01, train loss: 7.92776e-06, val loss: 7.82884e-06, min loss: 5.11209e-06\n",
      "Epoch: 194600, elapsed: 1.09e+01, train loss: 1.07657e-05, val loss: 1.05915e-05, min loss: 5.11209e-06\n",
      "Epoch: 194700, elapsed: 1.09e+01, train loss: 1.22935e-05, val loss: 1.07844e-05, min loss: 5.11209e-06\n",
      "Epoch: 194800, elapsed: 1.17e+01, train loss: 5.11281e-06, val loss: 6.32034e-06, min loss: 5.11209e-06\n",
      "Epoch: 194900, elapsed: 1.10e+01, train loss: 5.16481e-06, val loss: 6.43638e-06, min loss: 5.11209e-06\n",
      "Epoch: 195000, elapsed: 1.10e+01, train loss: 5.23825e-06, val loss: 6.47736e-06, min loss: 5.11209e-06\n",
      "Epoch: 195100, elapsed: 1.28e+01, train loss: 5.67915e-06, val loss: 7.39961e-06, min loss: 5.11209e-06\n",
      "Epoch: 195200, elapsed: 1.09e+01, train loss: 5.62383e-06, val loss: 6.69050e-06, min loss: 5.11209e-06\n",
      "Epoch: 195300, elapsed: 1.16e+01, train loss: 6.03660e-06, val loss: 7.41361e-06, min loss: 5.11209e-06\n",
      "Epoch: 195400, elapsed: 1.09e+01, train loss: 9.23760e-06, val loss: 9.59421e-06, min loss: 5.11209e-06\n",
      "Epoch: 195500, elapsed: 1.08e+01, train loss: 5.22355e-06, val loss: 7.19191e-06, min loss: 5.11209e-06\n",
      "Epoch: 195600, elapsed: 1.08e+01, train loss: 5.74712e-06, val loss: 6.56896e-06, min loss: 5.11209e-06\n",
      "Epoch: 195700, elapsed: 1.09e+01, train loss: 6.47276e-06, val loss: 7.23123e-06, min loss: 5.11209e-06\n",
      "Epoch: 195800, elapsed: 1.16e+01, train loss: 5.47660e-06, val loss: 6.72793e-06, min loss: 5.11209e-06\n",
      "Epoch: 195900, elapsed: 1.09e+01, train loss: 5.16380e-06, val loss: 6.42312e-06, min loss: 5.11209e-06\n",
      "Epoch: 196000, elapsed: 1.10e+01, train loss: 7.76336e-06, val loss: 1.03087e-05, min loss: 5.11209e-06\n",
      "Epoch: 196100, elapsed: 1.10e+01, train loss: 9.60046e-06, val loss: 8.84344e-06, min loss: 5.11209e-06\n",
      "Epoch: 196200, elapsed: 1.10e+01, train loss: 5.70343e-06, val loss: 6.80389e-06, min loss: 5.11209e-06\n",
      "Epoch: 196300, elapsed: 1.16e+01, train loss: 5.24299e-06, val loss: 6.43834e-06, min loss: 5.11209e-06\n",
      "Epoch: 196400, elapsed: 1.10e+01, train loss: 6.72308e-06, val loss: 7.65207e-06, min loss: 5.11209e-06\n",
      "Epoch: 196500, elapsed: 1.07e+01, train loss: 6.45812e-06, val loss: 7.49478e-06, min loss: 5.11209e-06\n",
      "Epoch: 196600, elapsed: 1.08e+01, train loss: 1.41173e-05, val loss: 1.47991e-05, min loss: 5.11209e-06\n",
      "Epoch: 196700, elapsed: 1.09e+01, train loss: 5.29366e-06, val loss: 6.56752e-06, min loss: 5.11209e-06\n",
      "Epoch: 196800, elapsed: 1.17e+01, train loss: 5.02465e-06, val loss: 6.21545e-06, min loss: 5.02465e-06\n",
      "Epoch: 196900, elapsed: 1.12e+01, train loss: 5.20621e-06, val loss: 6.33867e-06, min loss: 5.02465e-06\n",
      "Epoch: 197000, elapsed: 1.09e+01, train loss: 8.66055e-06, val loss: 9.05171e-06, min loss: 5.02465e-06\n",
      "Epoch: 197100, elapsed: 1.08e+01, train loss: 5.11448e-06, val loss: 6.31082e-06, min loss: 5.02465e-06\n",
      "Epoch: 197200, elapsed: 1.09e+01, train loss: 6.55912e-06, val loss: 8.16882e-06, min loss: 5.02465e-06\n",
      "Epoch: 197300, elapsed: 1.16e+01, train loss: 7.99698e-06, val loss: 9.98809e-06, min loss: 5.02465e-06\n",
      "Epoch: 197400, elapsed: 1.12e+01, train loss: 9.15972e-06, val loss: 8.79979e-06, min loss: 5.02465e-06\n",
      "Epoch: 197500, elapsed: 1.09e+01, train loss: 5.32731e-06, val loss: 6.45741e-06, min loss: 5.02465e-06\n",
      "Epoch: 197600, elapsed: 1.08e+01, train loss: 6.36005e-06, val loss: 7.48166e-06, min loss: 5.02465e-06\n",
      "Epoch: 197700, elapsed: 1.09e+01, train loss: 5.02999e-06, val loss: 6.29349e-06, min loss: 5.02465e-06\n",
      "Epoch: 197800, elapsed: 1.16e+01, train loss: 6.95314e-06, val loss: 7.76131e-06, min loss: 5.02465e-06\n",
      "Epoch: 197900, elapsed: 1.12e+01, train loss: 5.34452e-06, val loss: 6.50438e-06, min loss: 5.02465e-06\n",
      "Epoch: 198000, elapsed: 1.10e+01, train loss: 5.55590e-06, val loss: 6.31696e-06, min loss: 5.02465e-06\n",
      "Epoch: 198100, elapsed: 1.08e+01, train loss: 6.42700e-06, val loss: 7.42786e-06, min loss: 5.02465e-06\n",
      "Epoch: 198200, elapsed: 1.09e+01, train loss: 7.70568e-06, val loss: 8.84196e-06, min loss: 5.02465e-06\n",
      "Epoch: 198300, elapsed: 1.16e+01, train loss: 4.98269e-06, val loss: 6.09908e-06, min loss: 4.98269e-06\n",
      "Epoch: 198400, elapsed: 1.10e+01, train loss: 4.97640e-06, val loss: 6.21680e-06, min loss: 4.97640e-06\n",
      "Epoch: 198500, elapsed: 1.09e+01, train loss: 5.26272e-06, val loss: 6.46378e-06, min loss: 4.97640e-06\n",
      "Epoch: 198600, elapsed: 1.09e+01, train loss: 1.02687e-05, val loss: 9.80325e-06, min loss: 4.97640e-06\n",
      "Epoch: 198700, elapsed: 1.09e+01, train loss: 7.73215e-06, val loss: 7.58896e-06, min loss: 4.97640e-06\n",
      "Epoch: 198800, elapsed: 1.10e+01, train loss: 4.93459e-06, val loss: 6.08029e-06, min loss: 4.93459e-06\n",
      "Epoch: 198900, elapsed: 1.19e+01, train loss: 1.11060e-05, val loss: 1.18818e-05, min loss: 4.93459e-06\n",
      "Epoch: 199000, elapsed: 1.10e+01, train loss: 5.24064e-06, val loss: 6.28156e-06, min loss: 4.93459e-06\n",
      "Epoch: 199100, elapsed: 1.08e+01, train loss: 5.66311e-06, val loss: 7.13634e-06, min loss: 4.93459e-06\n",
      "Epoch: 199200, elapsed: 1.10e+01, train loss: 5.03961e-06, val loss: 6.14394e-06, min loss: 4.93459e-06\n",
      "Epoch: 199300, elapsed: 1.10e+01, train loss: 4.92127e-06, val loss: 6.05711e-06, min loss: 4.92127e-06\n",
      "Epoch: 199400, elapsed: 1.19e+01, train loss: 4.94672e-06, val loss: 6.07787e-06, min loss: 4.92127e-06\n",
      "Epoch: 199500, elapsed: 1.10e+01, train loss: 4.98009e-06, val loss: 6.10978e-06, min loss: 4.92127e-06\n",
      "Epoch: 199600, elapsed: 1.10e+01, train loss: 7.32861e-06, val loss: 8.93442e-06, min loss: 4.92127e-06\n",
      "Epoch: 199700, elapsed: 1.08e+01, train loss: 6.09886e-06, val loss: 7.20197e-06, min loss: 4.92127e-06\n",
      "Epoch: 199800, elapsed: 1.08e+01, train loss: 5.03115e-06, val loss: 6.14313e-06, min loss: 4.92127e-06\n",
      "Epoch: 199900, elapsed: 1.18e+01, train loss: 1.04819e-05, val loss: 1.01936e-05, min loss: 4.92127e-06\n",
      "Epoch: 200000, elapsed: 1.10e+01, train loss: 5.23148e-06, val loss: 6.18413e-06, min loss: 4.92127e-06\n",
      "Epoch: 200100, elapsed: 1.29e+01, train loss: 4.95609e-06, val loss: 6.06223e-06, min loss: 4.92127e-06\n",
      "Epoch: 200200, elapsed: 1.10e+01, train loss: 5.77179e-06, val loss: 7.12401e-06, min loss: 4.92127e-06\n",
      "Epoch: 200300, elapsed: 1.11e+01, train loss: 5.47072e-06, val loss: 7.08434e-06, min loss: 4.92127e-06\n",
      "Epoch: 200400, elapsed: 1.17e+01, train loss: 4.98525e-06, val loss: 6.11009e-06, min loss: 4.92127e-06\n",
      "Epoch: 200500, elapsed: 1.10e+01, train loss: 7.05969e-06, val loss: 7.28532e-06, min loss: 4.92127e-06\n",
      "Epoch: 200600, elapsed: 1.09e+01, train loss: 5.50243e-06, val loss: 6.47272e-06, min loss: 4.92127e-06\n",
      "Epoch: 200700, elapsed: 1.08e+01, train loss: 5.01433e-06, val loss: 6.46005e-06, min loss: 4.92127e-06\n",
      "Epoch: 200800, elapsed: 1.11e+01, train loss: 1.35422e-05, val loss: 1.65924e-05, min loss: 4.92127e-06\n",
      "Epoch: 200900, elapsed: 1.19e+01, train loss: 1.35844e-05, val loss: 1.54786e-05, min loss: 4.92127e-06\n",
      "Epoch: 201000, elapsed: 1.11e+01, train loss: 6.66722e-06, val loss: 8.12456e-06, min loss: 4.92127e-06\n",
      "Epoch: 201100, elapsed: 1.10e+01, train loss: 4.91790e-06, val loss: 6.06026e-06, min loss: 4.91790e-06\n",
      "Epoch: 201200, elapsed: 1.09e+01, train loss: 5.02655e-06, val loss: 6.06086e-06, min loss: 4.91790e-06\n",
      "Epoch: 201300, elapsed: 1.09e+01, train loss: 6.41385e-06, val loss: 7.71575e-06, min loss: 4.91790e-06\n",
      "Epoch: 201400, elapsed: 1.18e+01, train loss: 4.99817e-06, val loss: 6.21933e-06, min loss: 4.91790e-06\n",
      "Epoch: 201500, elapsed: 1.11e+01, train loss: 7.07016e-06, val loss: 7.52624e-06, min loss: 4.91790e-06\n",
      "Epoch: 201600, elapsed: 1.10e+01, train loss: 6.00934e-06, val loss: 7.15709e-06, min loss: 4.91790e-06\n",
      "Epoch: 201700, elapsed: 1.10e+01, train loss: 4.91517e-06, val loss: 6.00642e-06, min loss: 4.91517e-06\n",
      "Epoch: 201800, elapsed: 1.09e+01, train loss: 1.44236e-05, val loss: 1.34417e-05, min loss: 4.91517e-06\n",
      "Epoch: 201900, elapsed: 1.18e+01, train loss: 5.79742e-06, val loss: 6.30559e-06, min loss: 4.91517e-06\n",
      "Epoch: 202000, elapsed: 1.12e+01, train loss: 4.97788e-06, val loss: 6.02360e-06, min loss: 4.91517e-06\n",
      "Epoch: 202100, elapsed: 1.11e+01, train loss: 1.10560e-05, val loss: 1.20711e-05, min loss: 4.91517e-06\n",
      "Epoch: 202200, elapsed: 1.09e+01, train loss: 6.09629e-06, val loss: 7.30717e-06, min loss: 4.91517e-06\n",
      "Epoch: 202300, elapsed: 1.10e+01, train loss: 4.90682e-06, val loss: 6.05446e-06, min loss: 4.90682e-06\n",
      "Epoch: 202400, elapsed: 1.17e+01, train loss: 1.08622e-05, val loss: 1.17431e-05, min loss: 4.90682e-06\n",
      "Epoch: 202500, elapsed: 1.12e+01, train loss: 5.00958e-06, val loss: 6.16932e-06, min loss: 4.90682e-06\n",
      "Epoch: 202600, elapsed: 1.09e+01, train loss: 6.31580e-06, val loss: 7.17948e-06, min loss: 4.90682e-06\n",
      "Epoch: 202700, elapsed: 1.08e+01, train loss: 6.24818e-06, val loss: 7.71439e-06, min loss: 4.90682e-06\n",
      "Epoch: 202800, elapsed: 1.09e+01, train loss: 8.15368e-06, val loss: 9.76963e-06, min loss: 4.90682e-06\n",
      "Epoch: 202900, elapsed: 1.10e+01, train loss: 6.03295e-06, val loss: 7.10959e-06, min loss: 4.90682e-06\n",
      "Epoch: 203000, elapsed: 1.18e+01, train loss: 1.04838e-05, val loss: 1.03610e-05, min loss: 4.90682e-06\n",
      "Epoch: 203100, elapsed: 1.11e+01, train loss: 5.38251e-06, val loss: 6.37947e-06, min loss: 4.90682e-06\n",
      "Epoch: 203200, elapsed: 1.09e+01, train loss: 8.06462e-06, val loss: 9.10011e-06, min loss: 4.90682e-06\n",
      "Epoch: 203300, elapsed: 1.11e+01, train loss: 5.78157e-06, val loss: 6.41297e-06, min loss: 4.90682e-06\n",
      "Epoch: 203400, elapsed: 1.09e+01, train loss: 5.32457e-06, val loss: 6.25817e-06, min loss: 4.90682e-06\n",
      "Epoch: 203500, elapsed: 1.19e+01, train loss: 5.05994e-06, val loss: 6.18209e-06, min loss: 4.90682e-06\n",
      "Epoch: 203600, elapsed: 1.09e+01, train loss: 8.11442e-06, val loss: 9.78882e-06, min loss: 4.90682e-06\n",
      "Epoch: 203700, elapsed: 1.09e+01, train loss: 6.22965e-06, val loss: 7.03444e-06, min loss: 4.90682e-06\n",
      "Epoch: 203800, elapsed: 1.08e+01, train loss: 4.92728e-06, val loss: 6.04929e-06, min loss: 4.90682e-06\n",
      "Epoch: 203900, elapsed: 1.09e+01, train loss: 5.53961e-06, val loss: 6.74201e-06, min loss: 4.90682e-06\n",
      "Epoch: 204000, elapsed: 1.17e+01, train loss: 5.70055e-06, val loss: 6.84309e-06, min loss: 4.90682e-06\n",
      "Epoch: 204100, elapsed: 1.10e+01, train loss: 5.05218e-06, val loss: 6.18499e-06, min loss: 4.90682e-06\n",
      "Epoch: 204200, elapsed: 1.09e+01, train loss: 4.93894e-06, val loss: 6.00242e-06, min loss: 4.90682e-06\n",
      "Epoch: 204300, elapsed: 1.10e+01, train loss: 5.01302e-06, val loss: 6.44731e-06, min loss: 4.90682e-06\n",
      "Epoch: 204400, elapsed: 1.09e+01, train loss: 4.81219e-06, val loss: 5.86564e-06, min loss: 4.81219e-06\n",
      "Epoch: 204500, elapsed: 1.20e+01, train loss: 4.87505e-06, val loss: 5.94956e-06, min loss: 4.81219e-06\n",
      "Epoch: 204600, elapsed: 1.12e+01, train loss: 4.79235e-06, val loss: 5.83479e-06, min loss: 4.79235e-06\n",
      "Epoch: 204700, elapsed: 1.11e+01, train loss: 4.85268e-06, val loss: 5.89963e-06, min loss: 4.79235e-06\n",
      "Epoch: 204800, elapsed: 1.10e+01, train loss: 9.20002e-06, val loss: 8.47918e-06, min loss: 4.79235e-06\n",
      "Epoch: 204900, elapsed: 1.10e+01, train loss: 5.52515e-06, val loss: 6.43812e-06, min loss: 4.79235e-06\n",
      "Epoch: 205000, elapsed: 1.10e+01, train loss: 6.91761e-06, val loss: 8.39820e-06, min loss: 4.79235e-06\n",
      "Epoch: 205100, elapsed: 1.41e+01, train loss: 6.37793e-06, val loss: 7.26473e-06, min loss: 4.79235e-06\n",
      "Epoch: 205200, elapsed: 1.10e+01, train loss: 6.75766e-06, val loss: 7.89277e-06, min loss: 4.79235e-06\n",
      "Epoch: 205300, elapsed: 1.11e+01, train loss: 4.87210e-06, val loss: 5.97375e-06, min loss: 4.79235e-06\n",
      "Epoch: 205400, elapsed: 1.10e+01, train loss: 4.87975e-06, val loss: 5.92929e-06, min loss: 4.79235e-06\n",
      "Epoch: 205500, elapsed: 1.09e+01, train loss: 4.76818e-06, val loss: 5.81833e-06, min loss: 4.76818e-06\n",
      "Epoch: 205600, elapsed: 1.17e+01, train loss: 4.77700e-06, val loss: 5.82138e-06, min loss: 4.76818e-06\n",
      "Epoch: 205700, elapsed: 1.10e+01, train loss: 4.77925e-06, val loss: 5.82934e-06, min loss: 4.76818e-06\n",
      "Epoch: 205800, elapsed: 1.09e+01, train loss: 4.78136e-06, val loss: 5.83877e-06, min loss: 4.76818e-06\n",
      "Epoch: 205900, elapsed: 1.09e+01, train loss: 4.83389e-06, val loss: 5.86823e-06, min loss: 4.76818e-06\n",
      "Epoch: 206000, elapsed: 1.11e+01, train loss: 6.19465e-06, val loss: 6.73068e-06, min loss: 4.76818e-06\n",
      "Epoch: 206100, elapsed: 1.19e+01, train loss: 6.14297e-06, val loss: 6.48077e-06, min loss: 4.76818e-06\n",
      "Epoch: 206200, elapsed: 1.11e+01, train loss: 4.97318e-06, val loss: 5.95930e-06, min loss: 4.76818e-06\n",
      "Epoch: 206300, elapsed: 1.10e+01, train loss: 4.75855e-06, val loss: 5.81483e-06, min loss: 4.75855e-06\n",
      "Epoch: 206400, elapsed: 1.09e+01, train loss: 5.26406e-06, val loss: 6.12873e-06, min loss: 4.75855e-06\n",
      "Epoch: 206500, elapsed: 1.10e+01, train loss: 8.51816e-06, val loss: 1.01535e-05, min loss: 4.75855e-06\n",
      "Epoch: 206600, elapsed: 1.18e+01, train loss: 4.79925e-06, val loss: 5.81824e-06, min loss: 4.75855e-06\n",
      "Epoch: 206700, elapsed: 1.11e+01, train loss: 5.32932e-06, val loss: 6.21997e-06, min loss: 4.75855e-06\n",
      "Epoch: 206800, elapsed: 1.10e+01, train loss: 4.97793e-06, val loss: 5.89275e-06, min loss: 4.75855e-06\n",
      "Epoch: 206900, elapsed: 1.09e+01, train loss: 6.26058e-06, val loss: 6.37810e-06, min loss: 4.75855e-06\n",
      "Epoch: 207000, elapsed: 1.09e+01, train loss: 5.12477e-06, val loss: 6.03281e-06, min loss: 4.75855e-06\n",
      "Epoch: 207100, elapsed: 1.17e+01, train loss: 6.74402e-06, val loss: 8.05117e-06, min loss: 4.75855e-06\n",
      "Epoch: 207200, elapsed: 1.11e+01, train loss: 5.39443e-06, val loss: 6.12011e-06, min loss: 4.75855e-06\n",
      "Epoch: 207300, elapsed: 1.09e+01, train loss: 5.91944e-06, val loss: 6.47670e-06, min loss: 4.75855e-06\n",
      "Epoch: 207400, elapsed: 1.10e+01, train loss: 8.28624e-06, val loss: 8.92574e-06, min loss: 4.75855e-06\n",
      "Epoch: 207500, elapsed: 1.11e+01, train loss: 7.31656e-06, val loss: 7.96183e-06, min loss: 4.75855e-06\n",
      "Epoch: 207600, elapsed: 1.09e+01, train loss: 4.75998e-06, val loss: 5.85244e-06, min loss: 4.75855e-06\n",
      "Epoch: 207700, elapsed: 1.19e+01, train loss: 4.83833e-06, val loss: 5.85534e-06, min loss: 4.75855e-06\n",
      "Epoch: 207800, elapsed: 1.09e+01, train loss: 5.48216e-06, val loss: 6.32935e-06, min loss: 4.75855e-06\n",
      "Epoch: 207900, elapsed: 1.08e+01, train loss: 7.76035e-06, val loss: 7.21041e-06, min loss: 4.75855e-06\n",
      "Epoch: 208000, elapsed: 1.10e+01, train loss: 6.39747e-06, val loss: 6.15640e-06, min loss: 4.75855e-06\n",
      "Epoch: 208100, elapsed: 1.09e+01, train loss: 6.43768e-06, val loss: 7.46625e-06, min loss: 4.75855e-06\n",
      "Epoch: 208200, elapsed: 1.18e+01, train loss: 6.85298e-06, val loss: 6.51272e-06, min loss: 4.75855e-06\n",
      "Epoch: 208300, elapsed: 1.10e+01, train loss: 4.71004e-06, val loss: 5.73256e-06, min loss: 4.71004e-06\n",
      "Epoch: 208400, elapsed: 1.10e+01, train loss: 4.74531e-06, val loss: 5.79360e-06, min loss: 4.71004e-06\n",
      "Epoch: 208500, elapsed: 1.09e+01, train loss: 4.96576e-06, val loss: 5.93911e-06, min loss: 4.71004e-06\n",
      "Epoch: 208600, elapsed: 1.09e+01, train loss: 8.46727e-06, val loss: 7.38202e-06, min loss: 4.71004e-06\n",
      "Epoch: 208700, elapsed: 1.16e+01, train loss: 1.74038e-05, val loss: 1.73060e-05, min loss: 4.71004e-06\n",
      "Epoch: 208800, elapsed: 1.11e+01, train loss: 5.81031e-06, val loss: 7.46461e-06, min loss: 4.71004e-06\n",
      "Epoch: 208900, elapsed: 1.09e+01, train loss: 5.03971e-06, val loss: 5.91772e-06, min loss: 4.71004e-06\n",
      "Epoch: 209000, elapsed: 1.09e+01, train loss: 4.88267e-06, val loss: 5.93542e-06, min loss: 4.71004e-06\n",
      "Epoch: 209100, elapsed: 1.10e+01, train loss: 9.96052e-06, val loss: 8.64644e-06, min loss: 4.71004e-06\n",
      "Epoch: 209200, elapsed: 1.10e+01, train loss: 4.88709e-06, val loss: 5.96317e-06, min loss: 4.71004e-06\n",
      "Epoch: 209300, elapsed: 1.17e+01, train loss: 4.89412e-06, val loss: 5.95672e-06, min loss: 4.71004e-06\n",
      "Epoch: 209400, elapsed: 1.10e+01, train loss: 6.54756e-06, val loss: 7.48934e-06, min loss: 4.71004e-06\n",
      "Epoch: 209500, elapsed: 1.08e+01, train loss: 1.21076e-05, val loss: 1.26357e-05, min loss: 4.71004e-06\n",
      "Epoch: 209600, elapsed: 1.09e+01, train loss: 4.69954e-06, val loss: 5.73918e-06, min loss: 4.69954e-06\n",
      "Epoch: 209700, elapsed: 1.09e+01, train loss: 4.78715e-06, val loss: 5.77827e-06, min loss: 4.69954e-06\n",
      "Epoch: 209800, elapsed: 1.19e+01, train loss: 5.34490e-06, val loss: 6.39124e-06, min loss: 4.69954e-06\n",
      "Epoch: 209900, elapsed: 1.11e+01, train loss: 5.22959e-06, val loss: 6.27342e-06, min loss: 4.69954e-06\n",
      "Epoch: 210000, elapsed: 1.09e+01, train loss: 4.77726e-06, val loss: 5.90409e-06, min loss: 4.69954e-06\n",
      "Epoch: 210100, elapsed: 1.30e+01, train loss: 9.78746e-06, val loss: 1.06763e-05, min loss: 4.69954e-06\n",
      "Epoch: 210200, elapsed: 1.08e+01, train loss: 1.88210e-05, val loss: 1.75675e-05, min loss: 4.69954e-06\n",
      "Epoch: 210300, elapsed: 1.18e+01, train loss: 1.56693e-05, val loss: 1.29122e-05, min loss: 4.69954e-06\n",
      "Epoch: 210400, elapsed: 1.11e+01, train loss: 5.83748e-06, val loss: 6.63552e-06, min loss: 4.69954e-06\n",
      "Epoch: 210500, elapsed: 1.09e+01, train loss: 4.95737e-06, val loss: 5.89015e-06, min loss: 4.69954e-06\n",
      "Epoch: 210600, elapsed: 1.10e+01, train loss: 4.70170e-06, val loss: 5.69134e-06, min loss: 4.69954e-06\n",
      "Epoch: 210700, elapsed: 1.09e+01, train loss: 4.69446e-06, val loss: 5.72486e-06, min loss: 4.69446e-06\n",
      "Epoch: 210800, elapsed: 1.18e+01, train loss: 4.68683e-06, val loss: 5.72798e-06, min loss: 4.68683e-06\n",
      "Epoch: 210900, elapsed: 1.09e+01, train loss: 5.66981e-06, val loss: 6.97249e-06, min loss: 4.68683e-06\n",
      "Epoch: 211000, elapsed: 1.10e+01, train loss: 5.79957e-06, val loss: 6.68674e-06, min loss: 4.68683e-06\n",
      "Epoch: 211100, elapsed: 1.09e+01, train loss: 1.09868e-05, val loss: 1.18256e-05, min loss: 4.68683e-06\n",
      "Epoch: 211200, elapsed: 1.09e+01, train loss: 7.35345e-06, val loss: 8.17068e-06, min loss: 4.68683e-06\n",
      "Epoch: 211300, elapsed: 1.10e+01, train loss: 6.60360e-06, val loss: 7.80836e-06, min loss: 4.68683e-06\n",
      "Epoch: 211400, elapsed: 1.18e+01, train loss: 5.52998e-06, val loss: 6.28466e-06, min loss: 4.68683e-06\n",
      "Epoch: 211500, elapsed: 1.09e+01, train loss: 4.99060e-06, val loss: 5.76306e-06, min loss: 4.68683e-06\n",
      "Epoch: 211600, elapsed: 1.08e+01, train loss: 4.99564e-06, val loss: 5.96514e-06, min loss: 4.68683e-06\n",
      "Epoch: 211700, elapsed: 1.08e+01, train loss: 5.14311e-06, val loss: 6.14643e-06, min loss: 4.68683e-06\n",
      "Epoch: 211800, elapsed: 1.09e+01, train loss: 5.82650e-06, val loss: 7.33128e-06, min loss: 4.68683e-06\n",
      "Epoch: 211900, elapsed: 1.20e+01, train loss: 4.72489e-06, val loss: 5.75894e-06, min loss: 4.68683e-06\n",
      "Epoch: 212000, elapsed: 1.10e+01, train loss: 4.64228e-06, val loss: 5.61495e-06, min loss: 4.64228e-06\n",
      "Epoch: 212100, elapsed: 1.10e+01, train loss: 4.64152e-06, val loss: 5.59928e-06, min loss: 4.64152e-06\n",
      "Epoch: 212200, elapsed: 1.09e+01, train loss: 4.64837e-06, val loss: 5.62326e-06, min loss: 4.64152e-06\n",
      "Epoch: 212300, elapsed: 1.09e+01, train loss: 4.91584e-06, val loss: 6.01029e-06, min loss: 4.64152e-06\n",
      "Epoch: 212400, elapsed: 1.16e+01, train loss: 4.95636e-06, val loss: 5.88422e-06, min loss: 4.64152e-06\n",
      "Epoch: 212500, elapsed: 1.11e+01, train loss: 6.05515e-06, val loss: 7.27042e-06, min loss: 4.64152e-06\n",
      "Epoch: 212600, elapsed: 1.11e+01, train loss: 5.09659e-06, val loss: 5.91878e-06, min loss: 4.64152e-06\n",
      "Epoch: 212700, elapsed: 1.10e+01, train loss: 5.35507e-06, val loss: 6.55107e-06, min loss: 4.64152e-06\n",
      "Epoch: 212800, elapsed: 1.08e+01, train loss: 6.32196e-06, val loss: 6.78851e-06, min loss: 4.64152e-06\n",
      "Epoch: 212900, elapsed: 1.10e+01, train loss: 4.63235e-06, val loss: 5.60021e-06, min loss: 4.63235e-06\n",
      "Epoch: 213000, elapsed: 1.19e+01, train loss: 5.27144e-06, val loss: 6.34949e-06, min loss: 4.63235e-06\n",
      "Epoch: 213100, elapsed: 1.11e+01, train loss: 5.67633e-06, val loss: 6.62667e-06, min loss: 4.63235e-06\n",
      "Epoch: 213200, elapsed: 1.09e+01, train loss: 5.66583e-06, val loss: 6.74897e-06, min loss: 4.63235e-06\n",
      "Epoch: 213300, elapsed: 1.10e+01, train loss: 7.12557e-06, val loss: 7.89041e-06, min loss: 4.63235e-06\n",
      "Epoch: 213400, elapsed: 1.09e+01, train loss: 4.61923e-06, val loss: 5.59630e-06, min loss: 4.61923e-06\n",
      "Epoch: 213500, elapsed: 1.17e+01, train loss: 4.65844e-06, val loss: 5.59605e-06, min loss: 4.61923e-06\n",
      "Epoch: 213600, elapsed: 1.11e+01, train loss: 4.62779e-06, val loss: 5.60954e-06, min loss: 4.61923e-06\n",
      "Epoch: 213700, elapsed: 1.11e+01, train loss: 4.62719e-06, val loss: 5.61259e-06, min loss: 4.61923e-06\n",
      "Epoch: 213800, elapsed: 1.10e+01, train loss: 4.65257e-06, val loss: 5.61549e-06, min loss: 4.61923e-06\n",
      "Epoch: 213900, elapsed: 1.11e+01, train loss: 6.77551e-06, val loss: 8.23772e-06, min loss: 4.61923e-06\n",
      "Epoch: 214000, elapsed: 1.10e+01, train loss: 6.05983e-06, val loss: 6.99012e-06, min loss: 4.61923e-06\n",
      "Epoch: 214100, elapsed: 1.19e+01, train loss: 8.75433e-06, val loss: 1.02553e-05, min loss: 4.61923e-06\n",
      "Epoch: 214200, elapsed: 1.10e+01, train loss: 5.16647e-06, val loss: 6.09474e-06, min loss: 4.61923e-06\n",
      "Epoch: 214300, elapsed: 1.09e+01, train loss: 4.78200e-06, val loss: 5.79695e-06, min loss: 4.61923e-06\n",
      "Epoch: 214400, elapsed: 1.09e+01, train loss: 5.09206e-06, val loss: 6.62634e-06, min loss: 4.61923e-06\n",
      "Epoch: 214500, elapsed: 1.09e+01, train loss: 6.69602e-06, val loss: 9.21543e-06, min loss: 4.61923e-06\n",
      "Epoch: 214600, elapsed: 1.18e+01, train loss: 1.70637e-05, val loss: 1.69987e-05, min loss: 4.61923e-06\n",
      "Epoch: 214700, elapsed: 1.11e+01, train loss: 6.57043e-06, val loss: 7.75643e-06, min loss: 4.61923e-06\n",
      "Epoch: 214800, elapsed: 1.10e+01, train loss: 4.66441e-06, val loss: 5.63205e-06, min loss: 4.61923e-06\n",
      "Epoch: 214900, elapsed: 1.09e+01, train loss: 1.03512e-05, val loss: 1.02456e-05, min loss: 4.61923e-06\n",
      "Epoch: 215000, elapsed: 1.10e+01, train loss: 4.93592e-06, val loss: 5.96907e-06, min loss: 4.61923e-06\n",
      "Epoch: 215100, elapsed: 1.39e+01, train loss: 5.61395e-06, val loss: 6.24387e-06, min loss: 4.61923e-06\n",
      "Epoch: 215200, elapsed: 1.12e+01, train loss: 5.14137e-06, val loss: 5.99099e-06, min loss: 4.61923e-06\n",
      "Epoch: 215300, elapsed: 1.10e+01, train loss: 5.69444e-06, val loss: 6.45179e-06, min loss: 4.61923e-06\n",
      "Epoch: 215400, elapsed: 1.10e+01, train loss: 7.13595e-06, val loss: 7.43189e-06, min loss: 4.61923e-06\n",
      "Epoch: 215500, elapsed: 1.09e+01, train loss: 4.87908e-06, val loss: 5.70912e-06, min loss: 4.61923e-06\n",
      "Epoch: 215600, elapsed: 1.10e+01, train loss: 8.21838e-06, val loss: 9.25451e-06, min loss: 4.61923e-06\n",
      "Epoch: 215700, elapsed: 1.19e+01, train loss: 7.98233e-06, val loss: 9.58459e-06, min loss: 4.61923e-06\n",
      "Epoch: 215800, elapsed: 1.12e+01, train loss: 6.68342e-06, val loss: 7.82528e-06, min loss: 4.61923e-06\n",
      "Epoch: 215900, elapsed: 1.10e+01, train loss: 5.04325e-06, val loss: 6.07862e-06, min loss: 4.61923e-06\n",
      "Epoch: 216000, elapsed: 1.12e+01, train loss: 1.08141e-05, val loss: 1.07609e-05, min loss: 4.61923e-06\n",
      "Epoch: 216100, elapsed: 1.10e+01, train loss: 5.35856e-06, val loss: 6.13168e-06, min loss: 4.61923e-06\n",
      "Epoch: 216200, elapsed: 1.19e+01, train loss: 5.24864e-06, val loss: 6.62610e-06, min loss: 4.61923e-06\n",
      "Epoch: 216300, elapsed: 1.10e+01, train loss: 4.87133e-06, val loss: 5.69281e-06, min loss: 4.61923e-06\n",
      "Epoch: 216400, elapsed: 1.09e+01, train loss: 4.76580e-06, val loss: 5.79929e-06, min loss: 4.61923e-06\n",
      "Epoch: 216500, elapsed: 1.10e+01, train loss: 4.55290e-06, val loss: 5.50548e-06, min loss: 4.55290e-06\n",
      "Epoch: 216600, elapsed: 1.11e+01, train loss: 4.63499e-06, val loss: 5.57194e-06, min loss: 4.55290e-06\n",
      "Epoch: 216700, elapsed: 1.11e+01, train loss: 4.59127e-06, val loss: 5.57856e-06, min loss: 4.55290e-06\n",
      "Epoch: 216800, elapsed: 1.18e+01, train loss: 6.44133e-06, val loss: 7.58951e-06, min loss: 4.55290e-06\n",
      "Epoch: 216900, elapsed: 1.11e+01, train loss: 4.74499e-06, val loss: 5.71000e-06, min loss: 4.55290e-06\n",
      "Epoch: 217000, elapsed: 1.09e+01, train loss: 5.58147e-06, val loss: 7.00748e-06, min loss: 4.55290e-06\n",
      "Epoch: 217100, elapsed: 1.09e+01, train loss: 8.66357e-06, val loss: 8.03589e-06, min loss: 4.55290e-06\n",
      "Epoch: 217200, elapsed: 1.11e+01, train loss: 4.65686e-06, val loss: 5.67790e-06, min loss: 4.55290e-06\n",
      "Epoch: 217300, elapsed: 1.17e+01, train loss: 5.75940e-06, val loss: 6.38632e-06, min loss: 4.55290e-06\n",
      "Epoch: 217400, elapsed: 1.11e+01, train loss: 5.92563e-06, val loss: 7.55722e-06, min loss: 4.55290e-06\n",
      "Epoch: 217500, elapsed: 1.11e+01, train loss: 4.99713e-06, val loss: 5.93296e-06, min loss: 4.55290e-06\n",
      "Epoch: 217600, elapsed: 1.09e+01, train loss: 4.64301e-06, val loss: 5.64363e-06, min loss: 4.55290e-06\n",
      "Epoch: 217700, elapsed: 1.10e+01, train loss: 4.62455e-06, val loss: 5.54670e-06, min loss: 4.55290e-06\n",
      "Epoch: 217800, elapsed: 1.12e+01, train loss: 5.44798e-06, val loss: 6.47648e-06, min loss: 4.55290e-06\n",
      "Epoch: 217900, elapsed: 1.21e+01, train loss: 5.78163e-06, val loss: 6.71740e-06, min loss: 4.55290e-06\n",
      "Epoch: 218000, elapsed: 1.12e+01, train loss: 4.75702e-06, val loss: 5.65598e-06, min loss: 4.55290e-06\n",
      "Epoch: 218100, elapsed: 1.09e+01, train loss: 9.47315e-06, val loss: 1.00823e-05, min loss: 4.55290e-06\n",
      "Epoch: 218200, elapsed: 1.08e+01, train loss: 5.00854e-06, val loss: 5.93970e-06, min loss: 4.55290e-06\n",
      "Epoch: 218300, elapsed: 1.10e+01, train loss: 4.57031e-06, val loss: 5.51954e-06, min loss: 4.55290e-06\n",
      "Epoch: 218400, elapsed: 1.19e+01, train loss: 4.76357e-06, val loss: 5.60696e-06, min loss: 4.55290e-06\n",
      "Epoch: 218500, elapsed: 1.13e+01, train loss: 4.84462e-06, val loss: 5.81909e-06, min loss: 4.55290e-06\n",
      "Epoch: 218600, elapsed: 1.10e+01, train loss: 6.44304e-06, val loss: 7.30310e-06, min loss: 4.55290e-06\n",
      "Epoch: 218700, elapsed: 1.10e+01, train loss: 4.86831e-06, val loss: 5.86162e-06, min loss: 4.55290e-06\n",
      "Epoch: 218800, elapsed: 1.09e+01, train loss: 1.54737e-05, val loss: 1.78708e-05, min loss: 4.55290e-06\n",
      "Epoch: 218900, elapsed: 1.08e+01, train loss: 8.57225e-06, val loss: 1.02631e-05, min loss: 4.55290e-06\n",
      "Epoch: 219000, elapsed: 1.19e+01, train loss: 7.05237e-06, val loss: 8.47279e-06, min loss: 4.55290e-06\n",
      "Epoch: 219100, elapsed: 1.10e+01, train loss: 5.40779e-06, val loss: 6.37363e-06, min loss: 4.55290e-06\n",
      "Epoch: 219200, elapsed: 1.10e+01, train loss: 4.92652e-06, val loss: 5.74581e-06, min loss: 4.55290e-06\n",
      "Epoch: 219300, elapsed: 1.10e+01, train loss: 4.74148e-06, val loss: 5.79965e-06, min loss: 4.55290e-06\n",
      "Epoch: 219400, elapsed: 1.09e+01, train loss: 4.63378e-06, val loss: 5.57664e-06, min loss: 4.55290e-06\n",
      "Epoch: 219500, elapsed: 1.18e+01, train loss: 4.67473e-06, val loss: 5.73751e-06, min loss: 4.55290e-06\n",
      "Epoch: 219600, elapsed: 1.09e+01, train loss: 4.83675e-06, val loss: 5.97126e-06, min loss: 4.55290e-06\n",
      "Epoch: 219700, elapsed: 1.10e+01, train loss: 7.43208e-06, val loss: 7.82778e-06, min loss: 4.55290e-06\n",
      "Epoch: 219800, elapsed: 1.08e+01, train loss: 4.52812e-06, val loss: 5.49111e-06, min loss: 4.52812e-06\n",
      "Epoch: 219900, elapsed: 1.09e+01, train loss: 4.65570e-06, val loss: 5.62218e-06, min loss: 4.52812e-06\n",
      "Epoch: 220000, elapsed: 1.11e+01, train loss: 1.19492e-05, val loss: 1.24271e-05, min loss: 4.52812e-06\n",
      "Epoch: 220100, elapsed: 1.41e+01, train loss: 5.96903e-06, val loss: 6.89797e-06, min loss: 4.52812e-06\n",
      "Epoch: 220200, elapsed: 1.09e+01, train loss: 4.58557e-06, val loss: 5.63876e-06, min loss: 4.52812e-06\n",
      "Epoch: 220300, elapsed: 1.09e+01, train loss: 4.61113e-06, val loss: 5.63726e-06, min loss: 4.52812e-06\n",
      "Epoch: 220400, elapsed: 1.09e+01, train loss: 4.51313e-06, val loss: 5.48343e-06, min loss: 4.51313e-06\n",
      "Epoch: 220500, elapsed: 1.10e+01, train loss: 7.71911e-06, val loss: 9.03268e-06, min loss: 4.51313e-06\n",
      "Epoch: 220600, elapsed: 1.17e+01, train loss: 6.44270e-06, val loss: 7.30676e-06, min loss: 4.51313e-06\n",
      "Epoch: 220700, elapsed: 1.08e+01, train loss: 4.51571e-06, val loss: 5.44636e-06, min loss: 4.51313e-06\n",
      "Epoch: 220800, elapsed: 1.11e+01, train loss: 4.64038e-06, val loss: 5.58897e-06, min loss: 4.51313e-06\n",
      "Epoch: 220900, elapsed: 1.10e+01, train loss: 4.54757e-06, val loss: 5.55061e-06, min loss: 4.51313e-06\n",
      "Epoch: 221000, elapsed: 1.10e+01, train loss: 4.82203e-06, val loss: 5.64635e-06, min loss: 4.51313e-06\n",
      "Epoch: 221100, elapsed: 1.18e+01, train loss: 4.47214e-06, val loss: 5.39730e-06, min loss: 4.47214e-06\n",
      "Epoch: 221200, elapsed: 1.13e+01, train loss: 4.52218e-06, val loss: 5.41005e-06, min loss: 4.47214e-06\n",
      "Epoch: 221300, elapsed: 1.12e+01, train loss: 4.51472e-06, val loss: 5.41372e-06, min loss: 4.47214e-06\n",
      "Epoch: 221400, elapsed: 1.11e+01, train loss: 4.55595e-06, val loss: 5.46347e-06, min loss: 4.47214e-06\n",
      "Epoch: 221500, elapsed: 1.10e+01, train loss: 5.68822e-06, val loss: 6.77373e-06, min loss: 4.47214e-06\n",
      "Epoch: 221600, elapsed: 1.09e+01, train loss: 5.86140e-06, val loss: 6.47114e-06, min loss: 4.47214e-06\n",
      "Epoch: 221700, elapsed: 1.20e+01, train loss: 7.27188e-06, val loss: 7.75641e-06, min loss: 4.47214e-06\n",
      "Epoch: 221800, elapsed: 1.12e+01, train loss: 5.37438e-06, val loss: 5.87865e-06, min loss: 4.47214e-06\n",
      "Epoch: 221900, elapsed: 1.09e+01, train loss: 4.66311e-06, val loss: 5.70512e-06, min loss: 4.47214e-06\n",
      "Epoch: 222000, elapsed: 1.12e+01, train loss: 5.26514e-06, val loss: 6.57033e-06, min loss: 4.47214e-06\n",
      "Epoch: 222100, elapsed: 1.11e+01, train loss: 4.90893e-06, val loss: 5.83005e-06, min loss: 4.47214e-06\n",
      "Epoch: 222200, elapsed: 1.10e+01, train loss: 4.51151e-06, val loss: 5.45885e-06, min loss: 4.47214e-06\n",
      "Epoch: 222300, elapsed: 1.19e+01, train loss: 4.46032e-06, val loss: 5.35413e-06, min loss: 4.46032e-06\n",
      "Epoch: 222400, elapsed: 1.09e+01, train loss: 4.53184e-06, val loss: 5.41591e-06, min loss: 4.46032e-06\n",
      "Epoch: 222500, elapsed: 1.09e+01, train loss: 4.84313e-06, val loss: 5.86712e-06, min loss: 4.46032e-06\n",
      "Epoch: 222600, elapsed: 1.09e+01, train loss: 1.17915e-05, val loss: 1.16680e-05, min loss: 4.46032e-06\n",
      "Epoch: 222700, elapsed: 1.11e+01, train loss: 5.15130e-06, val loss: 5.94580e-06, min loss: 4.46032e-06\n",
      "Epoch: 222800, elapsed: 1.18e+01, train loss: 4.97792e-06, val loss: 5.94948e-06, min loss: 4.46032e-06\n",
      "Epoch: 222900, elapsed: 1.12e+01, train loss: 5.95772e-06, val loss: 7.15462e-06, min loss: 4.46032e-06\n",
      "Epoch: 223000, elapsed: 1.09e+01, train loss: 4.53563e-06, val loss: 5.44095e-06, min loss: 4.46032e-06\n",
      "Epoch: 223100, elapsed: 1.09e+01, train loss: 7.50961e-06, val loss: 8.36262e-06, min loss: 4.46032e-06\n",
      "Epoch: 223200, elapsed: 1.10e+01, train loss: 8.97994e-06, val loss: 1.01302e-05, min loss: 4.46032e-06\n",
      "Epoch: 223300, elapsed: 1.10e+01, train loss: 4.63572e-06, val loss: 5.58107e-06, min loss: 4.46032e-06\n",
      "Epoch: 223400, elapsed: 1.18e+01, train loss: 5.46981e-06, val loss: 6.46274e-06, min loss: 4.46032e-06\n",
      "Epoch: 223500, elapsed: 1.12e+01, train loss: 6.38420e-06, val loss: 7.46541e-06, min loss: 4.46032e-06\n",
      "Epoch: 223600, elapsed: 1.09e+01, train loss: 5.26365e-06, val loss: 6.00901e-06, min loss: 4.46032e-06\n",
      "Epoch: 223700, elapsed: 1.11e+01, train loss: 4.97530e-06, val loss: 5.96804e-06, min loss: 4.46032e-06\n",
      "Epoch: 223800, elapsed: 1.09e+01, train loss: 4.92703e-06, val loss: 5.90681e-06, min loss: 4.46032e-06\n",
      "Epoch: 223900, elapsed: 1.17e+01, train loss: 5.96069e-06, val loss: 7.64224e-06, min loss: 4.46032e-06\n",
      "Epoch: 224000, elapsed: 1.11e+01, train loss: 4.42655e-06, val loss: 5.33725e-06, min loss: 4.42655e-06\n",
      "Epoch: 224100, elapsed: 1.08e+01, train loss: 4.46624e-06, val loss: 5.36247e-06, min loss: 4.42655e-06\n",
      "Epoch: 224200, elapsed: 1.09e+01, train loss: 4.43188e-06, val loss: 5.34707e-06, min loss: 4.42655e-06\n",
      "Epoch: 224300, elapsed: 1.10e+01, train loss: 4.47330e-06, val loss: 5.37377e-06, min loss: 4.42655e-06\n",
      "Epoch: 224400, elapsed: 1.08e+01, train loss: 7.67269e-06, val loss: 8.93649e-06, min loss: 4.42655e-06\n",
      "Epoch: 224500, elapsed: 1.19e+01, train loss: 4.46009e-06, val loss: 5.34958e-06, min loss: 4.42655e-06\n",
      "Epoch: 224600, elapsed: 1.11e+01, train loss: 4.99632e-06, val loss: 6.02902e-06, min loss: 4.42655e-06\n",
      "Epoch: 224700, elapsed: 1.10e+01, train loss: 4.41608e-06, val loss: 5.33449e-06, min loss: 4.41608e-06\n",
      "Epoch: 224800, elapsed: 1.10e+01, train loss: 4.48286e-06, val loss: 5.35657e-06, min loss: 4.41608e-06\n",
      "Epoch: 224900, elapsed: 1.10e+01, train loss: 1.83971e-05, val loss: 2.06656e-05, min loss: 4.41608e-06\n",
      "Epoch: 225000, elapsed: 1.18e+01, train loss: 4.55007e-06, val loss: 5.48531e-06, min loss: 4.41608e-06\n",
      "Epoch: 225100, elapsed: 1.32e+01, train loss: 4.64395e-06, val loss: 5.63785e-06, min loss: 4.41608e-06\n",
      "Epoch: 225200, elapsed: 1.11e+01, train loss: 4.64787e-06, val loss: 5.50984e-06, min loss: 4.41608e-06\n",
      "Epoch: 225300, elapsed: 1.10e+01, train loss: 4.75211e-06, val loss: 5.73955e-06, min loss: 4.41608e-06\n",
      "Epoch: 225400, elapsed: 1.10e+01, train loss: 1.34761e-05, val loss: 1.42341e-05, min loss: 4.41608e-06\n",
      "Epoch: 225500, elapsed: 1.10e+01, train loss: 5.00645e-06, val loss: 5.67422e-06, min loss: 4.41608e-06\n",
      "Epoch: 225600, elapsed: 1.17e+01, train loss: 5.13407e-06, val loss: 6.03515e-06, min loss: 4.41608e-06\n",
      "Epoch: 225700, elapsed: 1.10e+01, train loss: 1.28824e-05, val loss: 1.47913e-05, min loss: 4.41608e-06\n",
      "Epoch: 225800, elapsed: 1.09e+01, train loss: 5.44881e-06, val loss: 6.29860e-06, min loss: 4.41608e-06\n",
      "Epoch: 225900, elapsed: 1.11e+01, train loss: 4.48487e-06, val loss: 5.49357e-06, min loss: 4.41608e-06\n",
      "Epoch: 226000, elapsed: 1.09e+01, train loss: 9.08141e-06, val loss: 1.00308e-05, min loss: 4.41608e-06\n",
      "Epoch: 226100, elapsed: 1.18e+01, train loss: 4.53096e-06, val loss: 5.46646e-06, min loss: 4.41608e-06\n",
      "Epoch: 226200, elapsed: 1.11e+01, train loss: 5.79032e-06, val loss: 6.99953e-06, min loss: 4.41608e-06\n",
      "Epoch: 226300, elapsed: 1.10e+01, train loss: 5.05724e-06, val loss: 5.86788e-06, min loss: 4.41608e-06\n",
      "Epoch: 226400, elapsed: 1.09e+01, train loss: 5.70414e-06, val loss: 6.50811e-06, min loss: 4.41608e-06\n",
      "Epoch: 226500, elapsed: 1.09e+01, train loss: 6.54931e-06, val loss: 7.27231e-06, min loss: 4.41608e-06\n",
      "Epoch: 226600, elapsed: 1.10e+01, train loss: 5.12274e-06, val loss: 5.64682e-06, min loss: 4.41608e-06\n",
      "Epoch: 226700, elapsed: 1.18e+01, train loss: 4.39343e-06, val loss: 5.25709e-06, min loss: 4.39343e-06\n",
      "Epoch: 226800, elapsed: 1.11e+01, train loss: 4.38087e-06, val loss: 5.27410e-06, min loss: 4.38087e-06\n",
      "Epoch: 226900, elapsed: 1.08e+01, train loss: 4.53513e-06, val loss: 5.58010e-06, min loss: 4.38087e-06\n",
      "Epoch: 227000, elapsed: 1.10e+01, train loss: 5.32923e-06, val loss: 5.97295e-06, min loss: 4.38087e-06\n",
      "Epoch: 227100, elapsed: 1.10e+01, train loss: 4.52231e-06, val loss: 5.46808e-06, min loss: 4.38087e-06\n",
      "Epoch: 227200, elapsed: 1.08e+01, train loss: 4.50801e-06, val loss: 5.52616e-06, min loss: 4.38087e-06\n",
      "Epoch: 227300, elapsed: 1.18e+01, train loss: 4.38301e-06, val loss: 5.26862e-06, min loss: 4.38087e-06\n",
      "Epoch: 227400, elapsed: 1.11e+01, train loss: 6.72203e-06, val loss: 7.86793e-06, min loss: 4.38087e-06\n",
      "Epoch: 227500, elapsed: 1.09e+01, train loss: 4.78405e-06, val loss: 5.47137e-06, min loss: 4.38087e-06\n",
      "Epoch: 227600, elapsed: 1.09e+01, train loss: 4.52238e-06, val loss: 5.48338e-06, min loss: 4.38087e-06\n",
      "Epoch: 227700, elapsed: 1.09e+01, train loss: 4.42896e-06, val loss: 5.38174e-06, min loss: 4.38087e-06\n",
      "Epoch: 227800, elapsed: 1.18e+01, train loss: 4.62241e-06, val loss: 5.68560e-06, min loss: 4.38087e-06\n",
      "Epoch: 227900, elapsed: 1.11e+01, train loss: 5.01429e-06, val loss: 5.56359e-06, min loss: 4.38087e-06\n",
      "Epoch: 228000, elapsed: 1.09e+01, train loss: 4.60049e-06, val loss: 5.53517e-06, min loss: 4.38087e-06\n",
      "Epoch: 228100, elapsed: 1.11e+01, train loss: 4.36680e-06, val loss: 5.28215e-06, min loss: 4.36680e-06\n",
      "Epoch: 228200, elapsed: 1.09e+01, train loss: 6.54015e-06, val loss: 7.79164e-06, min loss: 4.36680e-06\n",
      "Epoch: 228300, elapsed: 1.11e+01, train loss: 6.53546e-06, val loss: 7.39083e-06, min loss: 4.36680e-06\n",
      "Epoch: 228400, elapsed: 1.18e+01, train loss: 4.49576e-06, val loss: 5.38084e-06, min loss: 4.36680e-06\n",
      "Epoch: 228500, elapsed: 1.12e+01, train loss: 4.62008e-06, val loss: 5.50159e-06, min loss: 4.36680e-06\n",
      "Epoch: 228600, elapsed: 1.09e+01, train loss: 4.46962e-06, val loss: 5.41745e-06, min loss: 4.36680e-06\n",
      "Epoch: 228700, elapsed: 1.10e+01, train loss: 2.09882e-05, val loss: 2.18236e-05, min loss: 4.36680e-06\n",
      "Epoch: 228800, elapsed: 1.10e+01, train loss: 4.39846e-06, val loss: 5.29870e-06, min loss: 4.36680e-06\n",
      "Epoch: 228900, elapsed: 1.11e+01, train loss: 4.35255e-06, val loss: 5.25417e-06, min loss: 4.35255e-06\n",
      "Epoch: 229000, elapsed: 1.20e+01, train loss: 8.42044e-06, val loss: 1.01280e-05, min loss: 4.35255e-06\n",
      "Epoch: 229100, elapsed: 1.11e+01, train loss: 4.53634e-06, val loss: 6.15511e-06, min loss: 4.35255e-06\n",
      "Epoch: 229200, elapsed: 1.10e+01, train loss: 6.33425e-06, val loss: 6.83105e-06, min loss: 4.35255e-06\n",
      "Epoch: 229300, elapsed: 1.10e+01, train loss: 4.79160e-06, val loss: 5.70020e-06, min loss: 4.35255e-06\n",
      "Epoch: 229400, elapsed: 1.11e+01, train loss: 4.46079e-06, val loss: 5.41439e-06, min loss: 4.35255e-06\n",
      "Epoch: 229500, elapsed: 1.18e+01, train loss: 5.49327e-06, val loss: 5.88309e-06, min loss: 4.35255e-06\n",
      "Epoch: 229600, elapsed: 1.12e+01, train loss: 4.34349e-06, val loss: 5.23400e-06, min loss: 4.34349e-06\n",
      "Epoch: 229700, elapsed: 1.11e+01, train loss: 4.46417e-06, val loss: 5.44322e-06, min loss: 4.34349e-06\n",
      "Epoch: 229800, elapsed: 1.10e+01, train loss: 4.64211e-06, val loss: 5.51466e-06, min loss: 4.34349e-06\n",
      "Epoch: 229900, elapsed: 1.10e+01, train loss: 4.66107e-06, val loss: 5.83712e-06, min loss: 4.34349e-06\n",
      "Epoch: 230000, elapsed: 1.09e+01, train loss: 7.89282e-06, val loss: 7.71711e-06, min loss: 4.34349e-06\n",
      "Epoch: 230100, elapsed: 1.39e+01, train loss: 4.37419e-06, val loss: 5.23684e-06, min loss: 4.34349e-06\n",
      "Epoch: 230200, elapsed: 1.10e+01, train loss: 4.63935e-06, val loss: 5.54974e-06, min loss: 4.34349e-06\n",
      "Epoch: 230300, elapsed: 1.11e+01, train loss: 4.81304e-06, val loss: 5.47993e-06, min loss: 4.34349e-06\n",
      "Epoch: 230400, elapsed: 1.12e+01, train loss: 4.89532e-06, val loss: 5.79866e-06, min loss: 4.34349e-06\n",
      "Epoch: 230500, elapsed: 1.09e+01, train loss: 4.79410e-06, val loss: 5.66565e-06, min loss: 4.34349e-06\n",
      "Epoch: 230600, elapsed: 1.19e+01, train loss: 5.09852e-06, val loss: 5.84554e-06, min loss: 4.34349e-06\n",
      "Epoch: 230700, elapsed: 1.12e+01, train loss: 5.23935e-06, val loss: 6.19664e-06, min loss: 4.34349e-06\n",
      "Epoch: 230800, elapsed: 1.10e+01, train loss: 4.99947e-06, val loss: 5.73295e-06, min loss: 4.34349e-06\n",
      "Epoch: 230900, elapsed: 1.10e+01, train loss: 4.34469e-06, val loss: 5.21355e-06, min loss: 4.34349e-06\n",
      "Epoch: 231000, elapsed: 1.09e+01, train loss: 4.42861e-06, val loss: 5.28780e-06, min loss: 4.34349e-06\n",
      "Epoch: 231100, elapsed: 1.09e+01, train loss: 4.32904e-06, val loss: 5.20293e-06, min loss: 4.32904e-06\n",
      "Epoch: 231200, elapsed: 1.19e+01, train loss: 4.56190e-06, val loss: 5.53172e-06, min loss: 4.32904e-06\n",
      "Epoch: 231300, elapsed: 1.11e+01, train loss: 5.01761e-06, val loss: 5.48282e-06, min loss: 4.32904e-06\n",
      "Epoch: 231400, elapsed: 1.09e+01, train loss: 4.51915e-06, val loss: 5.45049e-06, min loss: 4.32904e-06\n",
      "Epoch: 231500, elapsed: 1.11e+01, train loss: 4.31258e-06, val loss: 5.17215e-06, min loss: 4.31258e-06\n",
      "Epoch: 231600, elapsed: 1.08e+01, train loss: 4.31367e-06, val loss: 5.19522e-06, min loss: 4.31258e-06\n",
      "Epoch: 231700, elapsed: 1.10e+01, train loss: 4.33588e-06, val loss: 5.21545e-06, min loss: 4.31258e-06\n",
      "Epoch: 231800, elapsed: 1.18e+01, train loss: 1.45926e-05, val loss: 1.34762e-05, min loss: 4.31258e-06\n",
      "Epoch: 231900, elapsed: 1.12e+01, train loss: 4.30105e-06, val loss: 5.18274e-06, min loss: 4.30105e-06\n",
      "Epoch: 232000, elapsed: 1.10e+01, train loss: 4.31101e-06, val loss: 5.19187e-06, min loss: 4.30105e-06\n",
      "Epoch: 232100, elapsed: 1.10e+01, train loss: 4.57615e-06, val loss: 5.39644e-06, min loss: 4.30105e-06\n",
      "Epoch: 232200, elapsed: 1.08e+01, train loss: 4.37953e-06, val loss: 5.25071e-06, min loss: 4.30105e-06\n",
      "Epoch: 232300, elapsed: 1.19e+01, train loss: 4.50946e-06, val loss: 5.42035e-06, min loss: 4.30105e-06\n",
      "Epoch: 232400, elapsed: 1.11e+01, train loss: 6.51731e-06, val loss: 6.55927e-06, min loss: 4.30105e-06\n",
      "Epoch: 232500, elapsed: 1.10e+01, train loss: 4.60386e-06, val loss: 5.21178e-06, min loss: 4.30105e-06\n",
      "Epoch: 232600, elapsed: 1.08e+01, train loss: 7.68571e-06, val loss: 9.15448e-06, min loss: 4.30105e-06\n",
      "Epoch: 232700, elapsed: 1.10e+01, train loss: 7.35763e-06, val loss: 6.78966e-06, min loss: 4.30105e-06\n",
      "Epoch: 232800, elapsed: 1.10e+01, train loss: 4.97302e-06, val loss: 5.60861e-06, min loss: 4.30105e-06\n",
      "Epoch: 232900, elapsed: 1.17e+01, train loss: 4.30848e-06, val loss: 5.23761e-06, min loss: 4.30105e-06\n",
      "Epoch: 233000, elapsed: 1.11e+01, train loss: 1.15011e-05, val loss: 1.47852e-05, min loss: 4.30105e-06\n",
      "Epoch: 233100, elapsed: 1.11e+01, train loss: 4.29118e-06, val loss: 5.16071e-06, min loss: 4.29118e-06\n",
      "Epoch: 233200, elapsed: 1.10e+01, train loss: 4.33441e-06, val loss: 5.21470e-06, min loss: 4.29118e-06\n",
      "Epoch: 233300, elapsed: 1.10e+01, train loss: 5.32360e-06, val loss: 6.34961e-06, min loss: 4.29118e-06\n",
      "Epoch: 233400, elapsed: 1.10e+01, train loss: 5.98228e-06, val loss: 6.17891e-06, min loss: 4.29118e-06\n",
      "Epoch: 233500, elapsed: 1.19e+01, train loss: 4.37373e-06, val loss: 5.21313e-06, min loss: 4.29118e-06\n",
      "Epoch: 233600, elapsed: 1.11e+01, train loss: 1.56832e-05, val loss: 1.41799e-05, min loss: 4.29118e-06\n",
      "Epoch: 233700, elapsed: 1.10e+01, train loss: 5.64077e-06, val loss: 6.49529e-06, min loss: 4.29118e-06\n",
      "Epoch: 233800, elapsed: 1.11e+01, train loss: 5.37610e-06, val loss: 6.11607e-06, min loss: 4.29118e-06\n",
      "Epoch: 233900, elapsed: 1.11e+01, train loss: 5.44689e-06, val loss: 5.88411e-06, min loss: 4.29118e-06\n",
      "Epoch: 234000, elapsed: 1.10e+01, train loss: 7.25070e-06, val loss: 6.93843e-06, min loss: 4.29118e-06\n",
      "Epoch: 234100, elapsed: 1.18e+01, train loss: 5.39998e-06, val loss: 5.78918e-06, min loss: 4.29118e-06\n",
      "Epoch: 234200, elapsed: 1.11e+01, train loss: 4.64211e-06, val loss: 5.51552e-06, min loss: 4.29118e-06\n",
      "Epoch: 234300, elapsed: 1.11e+01, train loss: 8.91489e-06, val loss: 8.17869e-06, min loss: 4.29118e-06\n",
      "Epoch: 234400, elapsed: 1.11e+01, train loss: 4.32724e-06, val loss: 5.28031e-06, min loss: 4.29118e-06\n",
      "Epoch: 234500, elapsed: 1.09e+01, train loss: 4.43765e-06, val loss: 5.34578e-06, min loss: 4.29118e-06\n",
      "Epoch: 234600, elapsed: 1.17e+01, train loss: 1.01196e-05, val loss: 9.55295e-06, min loss: 4.29118e-06\n",
      "Epoch: 234700, elapsed: 1.11e+01, train loss: 5.38338e-06, val loss: 5.84128e-06, min loss: 4.29118e-06\n",
      "Epoch: 234800, elapsed: 1.12e+01, train loss: 4.61624e-06, val loss: 5.29518e-06, min loss: 4.29118e-06\n",
      "Epoch: 234900, elapsed: 1.09e+01, train loss: 4.61455e-06, val loss: 5.51654e-06, min loss: 4.29118e-06\n",
      "Epoch: 235000, elapsed: 1.11e+01, train loss: 4.56939e-06, val loss: 5.53965e-06, min loss: 4.29118e-06\n",
      "Epoch: 235100, elapsed: 1.31e+01, train loss: 6.87097e-06, val loss: 9.29847e-06, min loss: 4.29118e-06\n",
      "Epoch: 235200, elapsed: 1.20e+01, train loss: 1.49493e-05, val loss: 1.42597e-05, min loss: 4.29118e-06\n",
      "Epoch: 235300, elapsed: 1.10e+01, train loss: 6.77256e-06, val loss: 7.38183e-06, min loss: 4.29118e-06\n",
      "Epoch: 235400, elapsed: 1.10e+01, train loss: 4.67183e-06, val loss: 5.53558e-06, min loss: 4.29118e-06\n",
      "Epoch: 235500, elapsed: 1.10e+01, train loss: 4.31946e-06, val loss: 5.19777e-06, min loss: 4.29118e-06\n",
      "Epoch: 235600, elapsed: 1.08e+01, train loss: 4.63797e-06, val loss: 5.23538e-06, min loss: 4.29118e-06\n",
      "Epoch: 235700, elapsed: 1.09e+01, train loss: 4.36620e-06, val loss: 5.21822e-06, min loss: 4.29118e-06\n",
      "Epoch: 235800, elapsed: 1.18e+01, train loss: 4.45856e-06, val loss: 5.22205e-06, min loss: 4.29118e-06\n",
      "Epoch: 235900, elapsed: 1.10e+01, train loss: 4.41737e-06, val loss: 5.22683e-06, min loss: 4.29118e-06\n",
      "Epoch: 236000, elapsed: 1.10e+01, train loss: 9.21011e-06, val loss: 1.02835e-05, min loss: 4.29118e-06\n",
      "Epoch: 236100, elapsed: 1.10e+01, train loss: 5.87389e-06, val loss: 6.69429e-06, min loss: 4.29118e-06\n",
      "Epoch: 236200, elapsed: 1.09e+01, train loss: 4.86829e-06, val loss: 5.70928e-06, min loss: 4.29118e-06\n",
      "Epoch: 236300, elapsed: 1.18e+01, train loss: 4.47420e-06, val loss: 5.72240e-06, min loss: 4.29118e-06\n",
      "Epoch: 236400, elapsed: 1.11e+01, train loss: 5.58580e-06, val loss: 6.41156e-06, min loss: 4.29118e-06\n",
      "Epoch: 236500, elapsed: 1.11e+01, train loss: 4.72804e-06, val loss: 5.85413e-06, min loss: 4.29118e-06\n",
      "Epoch: 236600, elapsed: 1.11e+01, train loss: 4.41921e-06, val loss: 5.49296e-06, min loss: 4.29118e-06\n",
      "Epoch: 236700, elapsed: 1.11e+01, train loss: 4.82371e-06, val loss: 5.56747e-06, min loss: 4.29118e-06\n",
      "Epoch: 236800, elapsed: 1.08e+01, train loss: 4.43098e-06, val loss: 5.23271e-06, min loss: 4.29118e-06\n",
      "Epoch: 236900, elapsed: 1.17e+01, train loss: 4.24097e-06, val loss: 5.09981e-06, min loss: 4.24097e-06\n",
      "Epoch: 237000, elapsed: 1.10e+01, train loss: 4.27379e-06, val loss: 5.12953e-06, min loss: 4.24097e-06\n",
      "Epoch: 237100, elapsed: 1.10e+01, train loss: 4.31308e-06, val loss: 5.30144e-06, min loss: 4.24097e-06\n",
      "Epoch: 237200, elapsed: 1.10e+01, train loss: 4.22777e-06, val loss: 5.09128e-06, min loss: 4.22777e-06\n",
      "Epoch: 237300, elapsed: 1.09e+01, train loss: 4.33706e-06, val loss: 5.19871e-06, min loss: 4.22777e-06\n",
      "Epoch: 237400, elapsed: 1.09e+01, train loss: 5.22187e-06, val loss: 6.51324e-06, min loss: 4.22777e-06\n",
      "Epoch: 237500, elapsed: 1.18e+01, train loss: 4.76019e-06, val loss: 5.80417e-06, min loss: 4.22777e-06\n",
      "Epoch: 237600, elapsed: 1.10e+01, train loss: 4.98974e-06, val loss: 6.06377e-06, min loss: 4.22777e-06\n",
      "Epoch: 237700, elapsed: 1.10e+01, train loss: 1.34643e-05, val loss: 1.31781e-05, min loss: 4.22777e-06\n",
      "Epoch: 237800, elapsed: 1.10e+01, train loss: 5.06849e-06, val loss: 5.60522e-06, min loss: 4.22777e-06\n",
      "Epoch: 237900, elapsed: 1.09e+01, train loss: 4.97532e-06, val loss: 5.82394e-06, min loss: 4.22777e-06\n",
      "Epoch: 238000, elapsed: 1.11e+01, train loss: 4.25503e-06, val loss: 5.12844e-06, min loss: 4.22777e-06\n",
      "Epoch: 238100, elapsed: 1.21e+01, train loss: 4.22537e-06, val loss: 5.06815e-06, min loss: 4.22537e-06\n",
      "Epoch: 238200, elapsed: 1.10e+01, train loss: 4.39733e-06, val loss: 5.33188e-06, min loss: 4.22537e-06\n",
      "Epoch: 238300, elapsed: 1.09e+01, train loss: 1.16980e-05, val loss: 8.73728e-06, min loss: 4.22537e-06\n",
      "Epoch: 238400, elapsed: 1.09e+01, train loss: 1.06119e-05, val loss: 1.26689e-05, min loss: 4.22537e-06\n",
      "Epoch: 238500, elapsed: 1.09e+01, train loss: 6.09013e-06, val loss: 5.35070e-06, min loss: 4.22537e-06\n",
      "Epoch: 238600, elapsed: 1.09e+01, train loss: 4.38916e-06, val loss: 5.46298e-06, min loss: 4.22537e-06\n",
      "Epoch: 238700, elapsed: 1.18e+01, train loss: 4.20072e-06, val loss: 5.06058e-06, min loss: 4.20072e-06\n",
      "Epoch: 238800, elapsed: 1.10e+01, train loss: 4.23221e-06, val loss: 5.06607e-06, min loss: 4.20072e-06\n",
      "Epoch: 238900, elapsed: 1.09e+01, train loss: 4.23332e-06, val loss: 5.09402e-06, min loss: 4.20072e-06\n",
      "Epoch: 239000, elapsed: 1.10e+01, train loss: 4.86348e-06, val loss: 5.82979e-06, min loss: 4.20072e-06\n",
      "Epoch: 239100, elapsed: 1.08e+01, train loss: 1.55723e-05, val loss: 1.22634e-05, min loss: 4.20072e-06\n",
      "Epoch: 239200, elapsed: 1.09e+01, train loss: 1.46643e-05, val loss: 1.45310e-05, min loss: 4.20072e-06\n",
      "Epoch: 239300, elapsed: 1.19e+01, train loss: 6.48044e-06, val loss: 7.16606e-06, min loss: 4.20072e-06\n",
      "Epoch: 239400, elapsed: 1.10e+01, train loss: 4.46854e-06, val loss: 5.18432e-06, min loss: 4.20072e-06\n",
      "Epoch: 239500, elapsed: 1.10e+01, train loss: 5.36885e-06, val loss: 6.15992e-06, min loss: 4.20072e-06\n",
      "Epoch: 239600, elapsed: 1.11e+01, train loss: 4.59933e-06, val loss: 5.38936e-06, min loss: 4.20072e-06\n",
      "Epoch: 239700, elapsed: 1.10e+01, train loss: 4.23809e-06, val loss: 5.10585e-06, min loss: 4.20072e-06\n",
      "Epoch: 239800, elapsed: 1.18e+01, train loss: 4.21794e-06, val loss: 5.03661e-06, min loss: 4.20072e-06\n",
      "Epoch: 239900, elapsed: 1.11e+01, train loss: 4.20981e-06, val loss: 5.09694e-06, min loss: 4.20072e-06\n",
      "Epoch: 240000, elapsed: 1.10e+01, train loss: 4.45068e-06, val loss: 5.17265e-06, min loss: 4.20072e-06\n",
      "Epoch: 240100, elapsed: 1.30e+01, train loss: 5.14089e-06, val loss: 5.87925e-06, min loss: 4.20072e-06\n",
      "Epoch: 240200, elapsed: 1.11e+01, train loss: 4.64211e-06, val loss: 5.34716e-06, min loss: 4.20072e-06\n",
      "Epoch: 240300, elapsed: 1.12e+01, train loss: 4.28945e-06, val loss: 5.23788e-06, min loss: 4.20072e-06\n",
      "Epoch: 240400, elapsed: 1.19e+01, train loss: 4.45180e-06, val loss: 5.36651e-06, min loss: 4.20072e-06\n",
      "Epoch: 240500, elapsed: 1.10e+01, train loss: 5.00042e-06, val loss: 5.31376e-06, min loss: 4.20072e-06\n",
      "Epoch: 240600, elapsed: 1.09e+01, train loss: 4.71627e-06, val loss: 5.30215e-06, min loss: 4.20072e-06\n",
      "Epoch: 240700, elapsed: 1.10e+01, train loss: 4.34678e-06, val loss: 5.42476e-06, min loss: 4.20072e-06\n",
      "Epoch: 240800, elapsed: 1.10e+01, train loss: 4.57012e-06, val loss: 5.55305e-06, min loss: 4.20072e-06\n",
      "Epoch: 240900, elapsed: 1.09e+01, train loss: 5.59659e-06, val loss: 6.54785e-06, min loss: 4.20072e-06\n",
      "Epoch: 241000, elapsed: 1.20e+01, train loss: 5.87728e-06, val loss: 5.34313e-06, min loss: 4.20072e-06\n",
      "Epoch: 241100, elapsed: 1.10e+01, train loss: 4.25444e-06, val loss: 5.35522e-06, min loss: 4.20072e-06\n",
      "Epoch: 241200, elapsed: 1.10e+01, train loss: 4.17755e-06, val loss: 5.09233e-06, min loss: 4.17755e-06\n",
      "Epoch: 241300, elapsed: 1.09e+01, train loss: 4.53597e-06, val loss: 5.36091e-06, min loss: 4.17755e-06\n",
      "Epoch: 241400, elapsed: 1.10e+01, train loss: 5.81978e-06, val loss: 6.86105e-06, min loss: 4.17755e-06\n",
      "Epoch: 241500, elapsed: 1.09e+01, train loss: 4.19057e-06, val loss: 5.09540e-06, min loss: 4.17755e-06\n",
      "Epoch: 241600, elapsed: 1.19e+01, train loss: 4.37359e-06, val loss: 5.24906e-06, min loss: 4.17755e-06\n",
      "Epoch: 241700, elapsed: 1.08e+01, train loss: 1.44172e-05, val loss: 1.22230e-05, min loss: 4.17755e-06\n",
      "Epoch: 241800, elapsed: 1.08e+01, train loss: 1.61073e-05, val loss: 1.90264e-05, min loss: 4.17755e-06\n",
      "Epoch: 241900, elapsed: 1.10e+01, train loss: 4.45078e-06, val loss: 5.26550e-06, min loss: 4.17755e-06\n",
      "Epoch: 242000, elapsed: 1.10e+01, train loss: 9.81872e-06, val loss: 1.03302e-05, min loss: 4.17755e-06\n",
      "Epoch: 242100, elapsed: 1.09e+01, train loss: 5.30494e-06, val loss: 6.09906e-06, min loss: 4.17755e-06\n",
      "Epoch: 242200, elapsed: 1.18e+01, train loss: 4.55343e-06, val loss: 5.43548e-06, min loss: 4.17755e-06\n",
      "Epoch: 242300, elapsed: 1.10e+01, train loss: 4.43845e-06, val loss: 5.26706e-06, min loss: 4.17755e-06\n",
      "Epoch: 242400, elapsed: 1.09e+01, train loss: 4.83270e-06, val loss: 5.70643e-06, min loss: 4.17755e-06\n",
      "Epoch: 242500, elapsed: 1.10e+01, train loss: 4.64548e-06, val loss: 5.62717e-06, min loss: 4.17755e-06\n",
      "Epoch: 242600, elapsed: 1.10e+01, train loss: 4.22166e-06, val loss: 5.12700e-06, min loss: 4.17755e-06\n",
      "Epoch: 242700, elapsed: 1.19e+01, train loss: 4.15802e-06, val loss: 4.98282e-06, min loss: 4.15802e-06\n",
      "Epoch: 242800, elapsed: 1.11e+01, train loss: 4.23144e-06, val loss: 5.10208e-06, min loss: 4.15802e-06\n",
      "Epoch: 242900, elapsed: 1.09e+01, train loss: 4.31721e-06, val loss: 5.14087e-06, min loss: 4.15802e-06\n",
      "Epoch: 243000, elapsed: 1.08e+01, train loss: 4.35146e-06, val loss: 5.03931e-06, min loss: 4.15802e-06\n",
      "Epoch: 243100, elapsed: 1.10e+01, train loss: 4.39861e-06, val loss: 5.26043e-06, min loss: 4.15802e-06\n",
      "Epoch: 243200, elapsed: 1.11e+01, train loss: 6.90952e-06, val loss: 8.26386e-06, min loss: 4.15802e-06\n",
      "Epoch: 243300, elapsed: 1.17e+01, train loss: 5.78546e-06, val loss: 6.62815e-06, min loss: 4.15802e-06\n",
      "Epoch: 243400, elapsed: 1.09e+01, train loss: 4.74039e-06, val loss: 5.64805e-06, min loss: 4.15802e-06\n",
      "Epoch: 243500, elapsed: 1.09e+01, train loss: 8.00074e-06, val loss: 9.25704e-06, min loss: 4.15802e-06\n",
      "Epoch: 243600, elapsed: 1.09e+01, train loss: 6.03138e-06, val loss: 7.16769e-06, min loss: 4.15802e-06\n",
      "Epoch: 243700, elapsed: 1.10e+01, train loss: 4.15578e-06, val loss: 5.04004e-06, min loss: 4.15578e-06\n",
      "Epoch: 243800, elapsed: 1.09e+01, train loss: 4.66311e-06, val loss: 5.89698e-06, min loss: 4.15578e-06\n",
      "Epoch: 243900, elapsed: 1.19e+01, train loss: 4.24044e-06, val loss: 5.19924e-06, min loss: 4.15578e-06\n",
      "Epoch: 244000, elapsed: 1.11e+01, train loss: 4.24448e-06, val loss: 5.14589e-06, min loss: 4.15578e-06\n",
      "Epoch: 244100, elapsed: 1.09e+01, train loss: 4.15269e-06, val loss: 5.04389e-06, min loss: 4.15269e-06\n",
      "Epoch: 244200, elapsed: 1.11e+01, train loss: 9.24967e-06, val loss: 1.05144e-05, min loss: 4.15269e-06\n",
      "Epoch: 244300, elapsed: 1.10e+01, train loss: 4.40884e-06, val loss: 5.09940e-06, min loss: 4.15269e-06\n",
      "Epoch: 244400, elapsed: 1.09e+01, train loss: 7.27588e-06, val loss: 7.75985e-06, min loss: 4.15269e-06\n",
      "Epoch: 244500, elapsed: 1.20e+01, train loss: 5.08879e-06, val loss: 5.91748e-06, min loss: 4.15269e-06\n",
      "Epoch: 244600, elapsed: 1.11e+01, train loss: 8.98611e-06, val loss: 9.60534e-06, min loss: 4.15269e-06\n",
      "Epoch: 244700, elapsed: 1.11e+01, train loss: 4.48283e-06, val loss: 5.27526e-06, min loss: 4.15269e-06\n",
      "Epoch: 244800, elapsed: 1.10e+01, train loss: 1.14794e-05, val loss: 1.28267e-05, min loss: 4.15269e-06\n",
      "Epoch: 244900, elapsed: 1.09e+01, train loss: 4.43574e-06, val loss: 5.64350e-06, min loss: 4.15269e-06\n",
      "Epoch: 245000, elapsed: 1.10e+01, train loss: 4.14597e-06, val loss: 5.04592e-06, min loss: 4.14597e-06\n",
      "Epoch: 245100, elapsed: 1.40e+01, train loss: 4.48706e-06, val loss: 5.33441e-06, min loss: 4.14597e-06\n",
      "Epoch: 245200, elapsed: 1.10e+01, train loss: 6.01986e-06, val loss: 6.23946e-06, min loss: 4.14597e-06\n",
      "Epoch: 245300, elapsed: 1.10e+01, train loss: 4.57558e-06, val loss: 5.29140e-06, min loss: 4.14597e-06\n",
      "Epoch: 245400, elapsed: 1.09e+01, train loss: 4.39880e-06, val loss: 5.20866e-06, min loss: 4.14597e-06\n",
      "Epoch: 245500, elapsed: 1.08e+01, train loss: 5.82020e-06, val loss: 6.51388e-06, min loss: 4.14597e-06\n",
      "Epoch: 245600, elapsed: 1.10e+01, train loss: 4.88487e-06, val loss: 5.45564e-06, min loss: 4.14597e-06\n",
      "Epoch: 245700, elapsed: 1.20e+01, train loss: 7.23988e-06, val loss: 7.73561e-06, min loss: 4.14597e-06\n",
      "Epoch: 245800, elapsed: 1.13e+01, train loss: 5.67551e-06, val loss: 6.15036e-06, min loss: 4.14597e-06\n",
      "Epoch: 245900, elapsed: 1.10e+01, train loss: 4.41788e-06, val loss: 5.16891e-06, min loss: 4.14597e-06\n",
      "Epoch: 246000, elapsed: 1.10e+01, train loss: 4.25984e-06, val loss: 4.97356e-06, min loss: 4.14597e-06\n",
      "Epoch: 246100, elapsed: 1.09e+01, train loss: 8.30976e-06, val loss: 1.07463e-05, min loss: 4.14597e-06\n",
      "Epoch: 246200, elapsed: 1.09e+01, train loss: 4.14282e-06, val loss: 4.99373e-06, min loss: 4.14282e-06\n",
      "Epoch: 246300, elapsed: 1.18e+01, train loss: 4.12325e-06, val loss: 4.92989e-06, min loss: 4.12325e-06\n",
      "Epoch: 246400, elapsed: 1.11e+01, train loss: 4.09954e-06, val loss: 4.94832e-06, min loss: 4.09954e-06\n",
      "Epoch: 246500, elapsed: 1.10e+01, train loss: 4.14618e-06, val loss: 5.06753e-06, min loss: 4.09954e-06\n",
      "Epoch: 246600, elapsed: 1.11e+01, train loss: 4.35201e-06, val loss: 5.23876e-06, min loss: 4.09954e-06\n",
      "Epoch: 246700, elapsed: 1.10e+01, train loss: 1.40487e-05, val loss: 1.32350e-05, min loss: 4.09954e-06\n",
      "Epoch: 246800, elapsed: 1.09e+01, train loss: 5.82709e-06, val loss: 6.51876e-06, min loss: 4.09954e-06\n",
      "Epoch: 246900, elapsed: 1.18e+01, train loss: 4.81246e-06, val loss: 5.69491e-06, min loss: 4.09954e-06\n",
      "Epoch: 247000, elapsed: 1.10e+01, train loss: 4.24502e-06, val loss: 4.98934e-06, min loss: 4.09954e-06\n",
      "Epoch: 247100, elapsed: 1.10e+01, train loss: 5.07797e-06, val loss: 6.19209e-06, min loss: 4.09954e-06\n",
      "Epoch: 247200, elapsed: 1.09e+01, train loss: 4.76439e-06, val loss: 5.78158e-06, min loss: 4.09954e-06\n",
      "Epoch: 247300, elapsed: 1.09e+01, train loss: 4.09424e-06, val loss: 4.92338e-06, min loss: 4.09424e-06\n",
      "Epoch: 247400, elapsed: 1.09e+01, train loss: 4.11669e-06, val loss: 4.98356e-06, min loss: 4.09424e-06\n",
      "Epoch: 247500, elapsed: 1.19e+01, train loss: 4.20300e-06, val loss: 4.97184e-06, min loss: 4.09424e-06\n",
      "Epoch: 247600, elapsed: 1.12e+01, train loss: 4.10280e-06, val loss: 4.91698e-06, min loss: 4.09424e-06\n",
      "Epoch: 247700, elapsed: 1.09e+01, train loss: 6.89445e-06, val loss: 8.26194e-06, min loss: 4.09424e-06\n",
      "Epoch: 247800, elapsed: 1.09e+01, train loss: 5.05271e-06, val loss: 5.93407e-06, min loss: 4.09424e-06\n",
      "Epoch: 247900, elapsed: 1.10e+01, train loss: 4.19379e-06, val loss: 5.07389e-06, min loss: 4.09424e-06\n",
      "Epoch: 248000, elapsed: 1.10e+01, train loss: 6.47141e-06, val loss: 8.13584e-06, min loss: 4.09424e-06\n",
      "Epoch: 248100, elapsed: 1.19e+01, train loss: 4.07504e-06, val loss: 4.92173e-06, min loss: 4.07504e-06\n",
      "Epoch: 248200, elapsed: 1.09e+01, train loss: 4.07229e-06, val loss: 4.90086e-06, min loss: 4.07229e-06\n",
      "Epoch: 248300, elapsed: 1.11e+01, train loss: 4.16602e-06, val loss: 5.00758e-06, min loss: 4.07229e-06\n",
      "Epoch: 248400, elapsed: 1.10e+01, train loss: 5.27881e-06, val loss: 6.32982e-06, min loss: 4.07229e-06\n",
      "Epoch: 248500, elapsed: 1.09e+01, train loss: 4.82983e-06, val loss: 5.88843e-06, min loss: 4.07229e-06\n",
      "Epoch: 248600, elapsed: 1.09e+01, train loss: 4.13637e-06, val loss: 5.03553e-06, min loss: 4.07229e-06\n",
      "Epoch: 248700, elapsed: 1.21e+01, train loss: 7.34483e-06, val loss: 8.52810e-06, min loss: 4.07229e-06\n",
      "Epoch: 248800, elapsed: 1.11e+01, train loss: 5.53736e-06, val loss: 6.38593e-06, min loss: 4.07229e-06\n",
      "Epoch: 248900, elapsed: 1.10e+01, train loss: 4.25056e-06, val loss: 5.13251e-06, min loss: 4.07229e-06\n",
      "Epoch: 249000, elapsed: 1.10e+01, train loss: 8.11719e-06, val loss: 9.19568e-06, min loss: 4.07229e-06\n",
      "Epoch: 249100, elapsed: 1.09e+01, train loss: 4.06551e-06, val loss: 4.95741e-06, min loss: 4.06551e-06\n",
      "Epoch: 249200, elapsed: 1.10e+01, train loss: 4.48742e-06, val loss: 5.35784e-06, min loss: 4.06551e-06\n",
      "Epoch: 249300, elapsed: 1.19e+01, train loss: 5.48896e-06, val loss: 6.18215e-06, min loss: 4.06551e-06\n",
      "Epoch: 249400, elapsed: 1.10e+01, train loss: 4.75079e-06, val loss: 5.47556e-06, min loss: 4.06551e-06\n",
      "Epoch: 249500, elapsed: 1.10e+01, train loss: 4.87898e-06, val loss: 5.95392e-06, min loss: 4.06551e-06\n",
      "Epoch: 249600, elapsed: 1.10e+01, train loss: 1.13804e-05, val loss: 1.39466e-05, min loss: 4.06551e-06\n",
      "Epoch: 249700, elapsed: 1.09e+01, train loss: 4.59960e-06, val loss: 5.59527e-06, min loss: 4.06551e-06\n",
      "Epoch: 249800, elapsed: 1.09e+01, train loss: 1.31098e-05, val loss: 1.09048e-05, min loss: 4.06551e-06\n",
      "Epoch: 249900, elapsed: 1.19e+01, train loss: 5.02643e-06, val loss: 6.44473e-06, min loss: 4.06551e-06\n",
      "Epoch: 250000, elapsed: 1.12e+01, train loss: 4.56190e-06, val loss: 5.29229e-06, min loss: 4.06551e-06\n",
      "Epoch: 250100, elapsed: 1.31e+01, train loss: 4.15680e-06, val loss: 5.03204e-06, min loss: 4.06551e-06\n",
      "Epoch: 250200, elapsed: 1.11e+01, train loss: 4.30058e-06, val loss: 5.17212e-06, min loss: 4.06551e-06\n",
      "Epoch: 250300, elapsed: 1.09e+01, train loss: 7.01330e-06, val loss: 8.38821e-06, min loss: 4.06551e-06\n",
      "Epoch: 250400, elapsed: 1.18e+01, train loss: 1.46447e-05, val loss: 1.73126e-05, min loss: 4.06551e-06\n",
      "Epoch: 250500, elapsed: 1.11e+01, train loss: 4.12340e-06, val loss: 5.00760e-06, min loss: 4.06551e-06\n",
      "Epoch: 250600, elapsed: 1.11e+01, train loss: 4.17375e-06, val loss: 5.06687e-06, min loss: 4.06551e-06\n",
      "Epoch: 250700, elapsed: 1.10e+01, train loss: 5.56274e-06, val loss: 6.88787e-06, min loss: 4.06551e-06\n",
      "Epoch: 250800, elapsed: 1.08e+01, train loss: 5.30227e-06, val loss: 6.19940e-06, min loss: 4.06551e-06\n",
      "Epoch: 250900, elapsed: 1.12e+01, train loss: 4.63620e-06, val loss: 5.42541e-06, min loss: 4.06551e-06\n",
      "Epoch: 251000, elapsed: 1.18e+01, train loss: 4.06187e-06, val loss: 4.89419e-06, min loss: 4.06187e-06\n",
      "Epoch: 251100, elapsed: 1.10e+01, train loss: 4.28473e-06, val loss: 5.17591e-06, min loss: 4.06187e-06\n",
      "Epoch: 251200, elapsed: 1.10e+01, train loss: 4.35312e-06, val loss: 5.06778e-06, min loss: 4.06187e-06\n",
      "Epoch: 251300, elapsed: 1.08e+01, train loss: 4.80133e-06, val loss: 5.66743e-06, min loss: 4.06187e-06\n",
      "Epoch: 251400, elapsed: 1.10e+01, train loss: 4.36326e-06, val loss: 5.57618e-06, min loss: 4.06187e-06\n",
      "Epoch: 251500, elapsed: 1.11e+01, train loss: 5.01220e-06, val loss: 6.79903e-06, min loss: 4.06187e-06\n",
      "Epoch: 251600, elapsed: 1.18e+01, train loss: 4.08487e-06, val loss: 5.09538e-06, min loss: 4.06187e-06\n",
      "Epoch: 251700, elapsed: 1.11e+01, train loss: 4.38767e-06, val loss: 5.30660e-06, min loss: 4.06187e-06\n",
      "Epoch: 251800, elapsed: 1.10e+01, train loss: 4.32738e-06, val loss: 5.15764e-06, min loss: 4.06187e-06\n",
      "Epoch: 251900, elapsed: 1.09e+01, train loss: 5.33077e-06, val loss: 5.73249e-06, min loss: 4.06187e-06\n",
      "Epoch: 252000, elapsed: 1.11e+01, train loss: 4.11088e-06, val loss: 4.99374e-06, min loss: 4.06187e-06\n",
      "Epoch: 252100, elapsed: 1.10e+01, train loss: 9.15880e-06, val loss: 1.01275e-05, min loss: 4.06187e-06\n",
      "Epoch: 252200, elapsed: 1.18e+01, train loss: 6.78573e-06, val loss: 7.52873e-06, min loss: 4.06187e-06\n",
      "Epoch: 252300, elapsed: 1.10e+01, train loss: 7.37019e-06, val loss: 6.66744e-06, min loss: 4.06187e-06\n",
      "Epoch: 252400, elapsed: 1.11e+01, train loss: 5.50772e-06, val loss: 5.92457e-06, min loss: 4.06187e-06\n",
      "Epoch: 252500, elapsed: 1.10e+01, train loss: 5.59108e-06, val loss: 5.47104e-06, min loss: 4.06187e-06\n",
      "Epoch: 252600, elapsed: 1.10e+01, train loss: 4.01218e-06, val loss: 4.85031e-06, min loss: 4.01218e-06\n",
      "Epoch: 252700, elapsed: 1.09e+01, train loss: 4.02930e-06, val loss: 4.84845e-06, min loss: 4.01218e-06\n",
      "Epoch: 252800, elapsed: 1.18e+01, train loss: 4.00368e-06, val loss: 4.83851e-06, min loss: 4.00368e-06\n",
      "Epoch: 252900, elapsed: 1.10e+01, train loss: 4.04730e-06, val loss: 4.89012e-06, min loss: 4.00368e-06\n",
      "Epoch: 253000, elapsed: 1.08e+01, train loss: 4.05955e-06, val loss: 4.87856e-06, min loss: 4.00368e-06\n",
      "Epoch: 253100, elapsed: 1.10e+01, train loss: 4.03378e-06, val loss: 4.85470e-06, min loss: 4.00368e-06\n",
      "Epoch: 253200, elapsed: 1.10e+01, train loss: 4.01489e-06, val loss: 4.83550e-06, min loss: 4.00368e-06\n",
      "Epoch: 253300, elapsed: 1.10e+01, train loss: 4.05874e-06, val loss: 4.90290e-06, min loss: 4.00368e-06\n",
      "Epoch: 253400, elapsed: 1.19e+01, train loss: 6.40241e-06, val loss: 7.69832e-06, min loss: 4.00368e-06\n",
      "Epoch: 253500, elapsed: 1.10e+01, train loss: 5.18900e-06, val loss: 6.41810e-06, min loss: 4.00368e-06\n",
      "Epoch: 253600, elapsed: 1.10e+01, train loss: 7.12367e-06, val loss: 7.99577e-06, min loss: 4.00368e-06\n",
      "Epoch: 253700, elapsed: 1.09e+01, train loss: 4.49807e-06, val loss: 5.13454e-06, min loss: 4.00368e-06\n",
      "Epoch: 253800, elapsed: 1.09e+01, train loss: 4.38000e-06, val loss: 5.13460e-06, min loss: 4.00368e-06\n",
      "Epoch: 253900, elapsed: 1.09e+01, train loss: 9.11379e-06, val loss: 9.77801e-06, min loss: 4.00368e-06\n",
      "Epoch: 254000, elapsed: 1.10e+01, train loss: 4.59272e-06, val loss: 5.44560e-06, min loss: 4.00368e-06\n",
      "Epoch: 254100, elapsed: 1.20e+01, train loss: 6.97947e-06, val loss: 7.54303e-06, min loss: 4.00368e-06\n",
      "Epoch: 254200, elapsed: 1.10e+01, train loss: 4.15098e-06, val loss: 4.99564e-06, min loss: 4.00368e-06\n",
      "Epoch: 254300, elapsed: 1.09e+01, train loss: 5.56889e-06, val loss: 5.83701e-06, min loss: 4.00368e-06\n",
      "Epoch: 254400, elapsed: 1.10e+01, train loss: 7.41392e-06, val loss: 7.23353e-06, min loss: 4.00368e-06\n",
      "Epoch: 254500, elapsed: 1.10e+01, train loss: 8.59674e-06, val loss: 8.41899e-06, min loss: 4.00368e-06\n",
      "Epoch: 254600, elapsed: 1.09e+01, train loss: 5.43020e-06, val loss: 6.75269e-06, min loss: 4.00368e-06\n",
      "Epoch: 254700, elapsed: 1.19e+01, train loss: 3.97834e-06, val loss: 4.80863e-06, min loss: 3.97834e-06\n",
      "Epoch: 254800, elapsed: 1.12e+01, train loss: 4.23053e-06, val loss: 5.12345e-06, min loss: 3.97834e-06\n",
      "Epoch: 254900, elapsed: 1.10e+01, train loss: 4.50216e-06, val loss: 4.89794e-06, min loss: 3.97834e-06\n",
      "Epoch: 255000, elapsed: 1.10e+01, train loss: 3.99204e-06, val loss: 4.82156e-06, min loss: 3.97834e-06\n",
      "Epoch: 255100, elapsed: 1.28e+01, train loss: 3.99882e-06, val loss: 4.79876e-06, min loss: 3.97834e-06\n",
      "Epoch: 255200, elapsed: 1.19e+01, train loss: 4.37102e-06, val loss: 5.21259e-06, min loss: 3.97834e-06\n",
      "Epoch: 255300, elapsed: 1.10e+01, train loss: 5.78378e-06, val loss: 6.53709e-06, min loss: 3.97834e-06\n",
      "Epoch: 255400, elapsed: 1.10e+01, train loss: 4.76735e-06, val loss: 5.42807e-06, min loss: 3.97834e-06\n",
      "Epoch: 255500, elapsed: 1.08e+01, train loss: 4.20198e-06, val loss: 5.15384e-06, min loss: 3.97834e-06\n",
      "Epoch: 255600, elapsed: 1.07e+01, train loss: 4.30223e-06, val loss: 5.11789e-06, min loss: 3.97834e-06\n",
      "Epoch: 255700, elapsed: 1.09e+01, train loss: 4.88663e-06, val loss: 5.79490e-06, min loss: 3.97834e-06\n",
      "Epoch: 255800, elapsed: 1.09e+01, train loss: 4.31378e-06, val loss: 5.23928e-06, min loss: 3.97834e-06\n",
      "Epoch: 255900, elapsed: 1.20e+01, train loss: 5.48273e-06, val loss: 5.53668e-06, min loss: 3.97834e-06\n",
      "Epoch: 256000, elapsed: 1.09e+01, train loss: 4.48685e-06, val loss: 5.14982e-06, min loss: 3.97834e-06\n",
      "Epoch: 256100, elapsed: 1.09e+01, train loss: 4.82486e-06, val loss: 5.78015e-06, min loss: 3.97834e-06\n",
      "Epoch: 256200, elapsed: 1.09e+01, train loss: 4.00864e-06, val loss: 4.87728e-06, min loss: 3.97834e-06\n",
      "Epoch: 256300, elapsed: 1.08e+01, train loss: 4.03798e-06, val loss: 4.90833e-06, min loss: 3.97834e-06\n",
      "Epoch: 256400, elapsed: 1.07e+01, train loss: 4.22679e-06, val loss: 4.92010e-06, min loss: 3.97834e-06\n",
      "Epoch: 256500, elapsed: 1.20e+01, train loss: 4.00846e-06, val loss: 4.95060e-06, min loss: 3.97834e-06\n",
      "Epoch: 256600, elapsed: 1.10e+01, train loss: 4.01204e-06, val loss: 4.81125e-06, min loss: 3.97834e-06\n",
      "Epoch: 256700, elapsed: 1.08e+01, train loss: 4.00218e-06, val loss: 4.83357e-06, min loss: 3.97834e-06\n",
      "Epoch: 256800, elapsed: 1.08e+01, train loss: 4.00654e-06, val loss: 4.84254e-06, min loss: 3.97834e-06\n",
      "Epoch: 256900, elapsed: 1.09e+01, train loss: 4.15224e-06, val loss: 4.80860e-06, min loss: 3.97834e-06\n",
      "Epoch: 257000, elapsed: 1.11e+01, train loss: 4.00081e-06, val loss: 4.91285e-06, min loss: 3.97834e-06\n",
      "Epoch: 257100, elapsed: 1.17e+01, train loss: 4.12229e-06, val loss: 5.05135e-06, min loss: 3.97834e-06\n",
      "Epoch: 257200, elapsed: 1.11e+01, train loss: 4.33079e-06, val loss: 4.95488e-06, min loss: 3.97834e-06\n",
      "Epoch: 257300, elapsed: 1.11e+01, train loss: 7.27459e-06, val loss: 8.82263e-06, min loss: 3.97834e-06\n",
      "Epoch: 257400, elapsed: 1.09e+01, train loss: 5.75550e-06, val loss: 6.49702e-06, min loss: 3.97834e-06\n",
      "Epoch: 257500, elapsed: 1.10e+01, train loss: 4.43643e-06, val loss: 5.24481e-06, min loss: 3.97834e-06\n",
      "Epoch: 257600, elapsed: 1.09e+01, train loss: 4.43412e-06, val loss: 4.91023e-06, min loss: 3.97834e-06\n",
      "Epoch: 257700, elapsed: 1.17e+01, train loss: 3.94616e-06, val loss: 4.77025e-06, min loss: 3.94616e-06\n",
      "Epoch: 257800, elapsed: 1.10e+01, train loss: 4.07028e-06, val loss: 4.85330e-06, min loss: 3.94616e-06\n",
      "Epoch: 257900, elapsed: 1.10e+01, train loss: 3.99819e-06, val loss: 4.80599e-06, min loss: 3.94616e-06\n",
      "Epoch: 258000, elapsed: 1.09e+01, train loss: 4.21359e-06, val loss: 4.98834e-06, min loss: 3.94616e-06\n",
      "Epoch: 258100, elapsed: 1.08e+01, train loss: 4.36369e-06, val loss: 5.07181e-06, min loss: 3.94616e-06\n",
      "Epoch: 258200, elapsed: 1.10e+01, train loss: 4.05352e-06, val loss: 4.79844e-06, min loss: 3.94616e-06\n",
      "Epoch: 258300, elapsed: 1.18e+01, train loss: 4.12377e-06, val loss: 4.90968e-06, min loss: 3.94616e-06\n",
      "Epoch: 258400, elapsed: 1.10e+01, train loss: 6.78784e-06, val loss: 8.29454e-06, min loss: 3.94616e-06\n",
      "Epoch: 258500, elapsed: 1.09e+01, train loss: 5.20413e-06, val loss: 5.01119e-06, min loss: 3.94616e-06\n",
      "Epoch: 258600, elapsed: 1.09e+01, train loss: 4.60068e-06, val loss: 5.18137e-06, min loss: 3.94616e-06\n",
      "Epoch: 258700, elapsed: 1.11e+01, train loss: 4.16013e-06, val loss: 4.98170e-06, min loss: 3.94616e-06\n",
      "Epoch: 258800, elapsed: 1.08e+01, train loss: 5.35197e-06, val loss: 6.32241e-06, min loss: 3.94616e-06\n",
      "Epoch: 258900, elapsed: 1.09e+01, train loss: 4.97244e-06, val loss: 5.72495e-06, min loss: 3.94616e-06\n",
      "Epoch: 259000, elapsed: 1.18e+01, train loss: 5.37413e-06, val loss: 5.74010e-06, min loss: 3.94616e-06\n",
      "Epoch: 259100, elapsed: 1.09e+01, train loss: 4.34662e-06, val loss: 5.08953e-06, min loss: 3.94616e-06\n",
      "Epoch: 259200, elapsed: 1.09e+01, train loss: 4.00150e-06, val loss: 4.81366e-06, min loss: 3.94616e-06\n",
      "Epoch: 259300, elapsed: 1.08e+01, train loss: 4.00143e-06, val loss: 4.78576e-06, min loss: 3.94616e-06\n",
      "Epoch: 259400, elapsed: 1.08e+01, train loss: 4.06427e-06, val loss: 4.85596e-06, min loss: 3.94616e-06\n",
      "Epoch: 259500, elapsed: 1.10e+01, train loss: 3.94738e-06, val loss: 4.78183e-06, min loss: 3.94616e-06\n",
      "Epoch: 259600, elapsed: 1.21e+01, train loss: 3.99023e-06, val loss: 4.87476e-06, min loss: 3.94616e-06\n",
      "Epoch: 259700, elapsed: 1.11e+01, train loss: 8.43924e-06, val loss: 8.16618e-06, min loss: 3.94616e-06\n",
      "Epoch: 259800, elapsed: 1.08e+01, train loss: 8.02625e-06, val loss: 9.12628e-06, min loss: 3.94616e-06\n",
      "Epoch: 259900, elapsed: 1.09e+01, train loss: 4.05847e-06, val loss: 4.90082e-06, min loss: 3.94616e-06\n",
      "Epoch: 260000, elapsed: 1.10e+01, train loss: 3.96552e-06, val loss: 4.80404e-06, min loss: 3.94616e-06\n",
      "Epoch: 260100, elapsed: 1.31e+01, train loss: 3.90696e-06, val loss: 4.72054e-06, min loss: 3.90696e-06\n",
      "Epoch: 260200, elapsed: 1.19e+01, train loss: 3.99665e-06, val loss: 4.77668e-06, min loss: 3.90696e-06\n",
      "Epoch: 260300, elapsed: 1.08e+01, train loss: 7.74523e-06, val loss: 8.19861e-06, min loss: 3.90696e-06\n",
      "Epoch: 260400, elapsed: 1.08e+01, train loss: 4.13938e-06, val loss: 4.96179e-06, min loss: 3.90696e-06\n",
      "Epoch: 260500, elapsed: 1.09e+01, train loss: 7.23871e-06, val loss: 7.41946e-06, min loss: 3.90696e-06\n",
      "Epoch: 260600, elapsed: 1.10e+01, train loss: 3.91050e-06, val loss: 4.75833e-06, min loss: 3.90696e-06\n",
      "Epoch: 260700, elapsed: 1.09e+01, train loss: 3.90703e-06, val loss: 4.73481e-06, min loss: 3.90696e-06\n",
      "Epoch: 260800, elapsed: 1.19e+01, train loss: 6.81909e-06, val loss: 7.23552e-06, min loss: 3.90696e-06\n",
      "Epoch: 260900, elapsed: 1.11e+01, train loss: 4.85225e-06, val loss: 5.30464e-06, min loss: 3.90696e-06\n",
      "Epoch: 261000, elapsed: 1.10e+01, train loss: 4.38739e-06, val loss: 5.16656e-06, min loss: 3.90696e-06\n",
      "Epoch: 261100, elapsed: 1.08e+01, train loss: 4.79440e-06, val loss: 5.27814e-06, min loss: 3.90696e-06\n",
      "Epoch: 261200, elapsed: 1.10e+01, train loss: 4.66255e-06, val loss: 5.38714e-06, min loss: 3.90696e-06\n",
      "Epoch: 261300, elapsed: 1.11e+01, train loss: 3.95849e-06, val loss: 4.74326e-06, min loss: 3.90696e-06\n",
      "Epoch: 261400, elapsed: 1.18e+01, train loss: 8.72529e-06, val loss: 9.08984e-06, min loss: 3.90696e-06\n",
      "Epoch: 261500, elapsed: 1.09e+01, train loss: 3.95280e-06, val loss: 4.74465e-06, min loss: 3.90696e-06\n",
      "Epoch: 261600, elapsed: 1.09e+01, train loss: 1.05052e-05, val loss: 1.21565e-05, min loss: 3.90696e-06\n",
      "Epoch: 261700, elapsed: 1.10e+01, train loss: 5.70897e-06, val loss: 7.44586e-06, min loss: 3.90696e-06\n",
      "Epoch: 261800, elapsed: 1.10e+01, train loss: 4.40738e-06, val loss: 5.63855e-06, min loss: 3.90696e-06\n",
      "Epoch: 261900, elapsed: 1.10e+01, train loss: 4.23865e-06, val loss: 4.88208e-06, min loss: 3.90696e-06\n",
      "Epoch: 262000, elapsed: 1.20e+01, train loss: 4.23081e-06, val loss: 5.00748e-06, min loss: 3.90696e-06\n",
      "Epoch: 262100, elapsed: 1.11e+01, train loss: 5.04366e-06, val loss: 6.10535e-06, min loss: 3.90696e-06\n",
      "Epoch: 262200, elapsed: 1.09e+01, train loss: 3.91415e-06, val loss: 4.75355e-06, min loss: 3.90696e-06\n",
      "Epoch: 262300, elapsed: 1.09e+01, train loss: 4.06240e-06, val loss: 4.85107e-06, min loss: 3.90696e-06\n",
      "Epoch: 262400, elapsed: 1.09e+01, train loss: 3.93762e-06, val loss: 4.79990e-06, min loss: 3.90696e-06\n",
      "Epoch: 262500, elapsed: 1.10e+01, train loss: 5.18944e-06, val loss: 6.24019e-06, min loss: 3.90696e-06\n",
      "Epoch: 262600, elapsed: 1.12e+01, train loss: 4.07254e-06, val loss: 4.74883e-06, min loss: 3.90696e-06\n",
      "Epoch: 262700, elapsed: 1.22e+01, train loss: 4.18298e-06, val loss: 5.00196e-06, min loss: 3.90696e-06\n",
      "Epoch: 262800, elapsed: 1.09e+01, train loss: 4.13897e-06, val loss: 4.95100e-06, min loss: 3.90696e-06\n",
      "Epoch: 262900, elapsed: 1.09e+01, train loss: 4.20305e-06, val loss: 4.88189e-06, min loss: 3.90696e-06\n",
      "Epoch: 263000, elapsed: 1.10e+01, train loss: 6.75237e-06, val loss: 7.33014e-06, min loss: 3.90696e-06\n",
      "Epoch: 263100, elapsed: 1.10e+01, train loss: 4.47582e-06, val loss: 6.66247e-06, min loss: 3.90696e-06\n",
      "Epoch: 263200, elapsed: 1.10e+01, train loss: 7.34350e-06, val loss: 8.50178e-06, min loss: 3.90696e-06\n",
      "Epoch: 263300, elapsed: 1.18e+01, train loss: 4.33039e-06, val loss: 5.06731e-06, min loss: 3.90696e-06\n",
      "Epoch: 263400, elapsed: 1.09e+01, train loss: 4.09973e-06, val loss: 4.97358e-06, min loss: 3.90696e-06\n",
      "Epoch: 263500, elapsed: 1.09e+01, train loss: 4.48425e-06, val loss: 4.97641e-06, min loss: 3.90696e-06\n",
      "Epoch: 263600, elapsed: 1.09e+01, train loss: 4.07652e-06, val loss: 4.71219e-06, min loss: 3.90696e-06\n",
      "Epoch: 263700, elapsed: 1.08e+01, train loss: 3.91486e-06, val loss: 4.70160e-06, min loss: 3.90696e-06\n",
      "Epoch: 263800, elapsed: 1.10e+01, train loss: 5.01620e-06, val loss: 6.14085e-06, min loss: 3.90696e-06\n",
      "Epoch: 263900, elapsed: 1.18e+01, train loss: 6.16139e-06, val loss: 7.70044e-06, min loss: 3.90696e-06\n",
      "Epoch: 264000, elapsed: 1.09e+01, train loss: 1.08304e-05, val loss: 1.10350e-05, min loss: 3.90696e-06\n",
      "Epoch: 264100, elapsed: 1.10e+01, train loss: 4.34720e-06, val loss: 5.56515e-06, min loss: 3.90696e-06\n",
      "Epoch: 264200, elapsed: 1.09e+01, train loss: 5.09787e-06, val loss: 5.01703e-06, min loss: 3.90696e-06\n",
      "Epoch: 264300, elapsed: 1.09e+01, train loss: 5.01153e-06, val loss: 5.60126e-06, min loss: 3.90696e-06\n",
      "Epoch: 264400, elapsed: 1.10e+01, train loss: 3.95775e-06, val loss: 4.69313e-06, min loss: 3.90696e-06\n",
      "Epoch: 264500, elapsed: 1.19e+01, train loss: 7.54645e-06, val loss: 7.06881e-06, min loss: 3.90696e-06\n",
      "Epoch: 264600, elapsed: 1.12e+01, train loss: 5.69313e-06, val loss: 7.36618e-06, min loss: 3.90696e-06\n",
      "Epoch: 264700, elapsed: 1.09e+01, train loss: 3.84208e-06, val loss: 4.66572e-06, min loss: 3.84208e-06\n",
      "Epoch: 264800, elapsed: 1.09e+01, train loss: 3.94231e-06, val loss: 4.71619e-06, min loss: 3.84208e-06\n",
      "Epoch: 264900, elapsed: 1.10e+01, train loss: 4.12559e-06, val loss: 5.03364e-06, min loss: 3.84208e-06\n",
      "Epoch: 265000, elapsed: 1.08e+01, train loss: 4.67431e-06, val loss: 5.45259e-06, min loss: 3.84208e-06\n",
      "Epoch: 265100, elapsed: 1.40e+01, train loss: 6.63782e-06, val loss: 6.61118e-06, min loss: 3.84208e-06\n",
      "Epoch: 265200, elapsed: 1.11e+01, train loss: 3.91560e-06, val loss: 4.80694e-06, min loss: 3.84208e-06\n",
      "Epoch: 265300, elapsed: 1.10e+01, train loss: 4.41298e-06, val loss: 5.29175e-06, min loss: 3.84208e-06\n",
      "Epoch: 265400, elapsed: 1.11e+01, train loss: 4.98352e-06, val loss: 5.76247e-06, min loss: 3.84208e-06\n",
      "Epoch: 265500, elapsed: 1.10e+01, train loss: 3.90020e-06, val loss: 4.74137e-06, min loss: 3.84208e-06\n",
      "Epoch: 265600, elapsed: 1.10e+01, train loss: 1.01830e-05, val loss: 1.15797e-05, min loss: 3.84208e-06\n",
      "Epoch: 265700, elapsed: 1.20e+01, train loss: 4.11954e-06, val loss: 5.09836e-06, min loss: 3.84208e-06\n",
      "Epoch: 265800, elapsed: 1.10e+01, train loss: 4.83163e-06, val loss: 5.69306e-06, min loss: 3.84208e-06\n",
      "Epoch: 265900, elapsed: 1.11e+01, train loss: 7.61710e-06, val loss: 6.40157e-06, min loss: 3.84208e-06\n",
      "Epoch: 266000, elapsed: 1.10e+01, train loss: 5.23124e-06, val loss: 6.11109e-06, min loss: 3.84208e-06\n",
      "Epoch: 266100, elapsed: 1.10e+01, train loss: 4.15714e-06, val loss: 4.91156e-06, min loss: 3.84208e-06\n",
      "Epoch: 266200, elapsed: 1.10e+01, train loss: 6.37540e-06, val loss: 8.04594e-06, min loss: 3.84208e-06\n",
      "Epoch: 266300, elapsed: 1.10e+01, train loss: 3.84505e-06, val loss: 4.65256e-06, min loss: 3.84208e-06\n",
      "Epoch: 266400, elapsed: 1.19e+01, train loss: 3.93533e-06, val loss: 4.78628e-06, min loss: 3.84208e-06\n",
      "Epoch: 266500, elapsed: 1.10e+01, train loss: 7.89349e-06, val loss: 1.00065e-05, min loss: 3.84208e-06\n",
      "Epoch: 266600, elapsed: 1.10e+01, train loss: 3.91393e-06, val loss: 4.75074e-06, min loss: 3.84208e-06\n",
      "Epoch: 266700, elapsed: 1.11e+01, train loss: 5.64024e-06, val loss: 6.57510e-06, min loss: 3.84208e-06\n",
      "Epoch: 266800, elapsed: 1.09e+01, train loss: 3.89639e-06, val loss: 4.63154e-06, min loss: 3.84208e-06\n",
      "Epoch: 266900, elapsed: 1.09e+01, train loss: 6.82509e-06, val loss: 8.14227e-06, min loss: 3.84208e-06\n",
      "Epoch: 267000, elapsed: 1.19e+01, train loss: 4.55824e-06, val loss: 5.05561e-06, min loss: 3.84208e-06\n",
      "Epoch: 267100, elapsed: 1.10e+01, train loss: 3.86303e-06, val loss: 4.61912e-06, min loss: 3.84208e-06\n",
      "Epoch: 267200, elapsed: 1.11e+01, train loss: 3.94121e-06, val loss: 4.77030e-06, min loss: 3.84208e-06\n",
      "Epoch: 267300, elapsed: 1.10e+01, train loss: 5.46300e-06, val loss: 6.08585e-06, min loss: 3.84208e-06\n",
      "Epoch: 267400, elapsed: 1.09e+01, train loss: 4.08995e-06, val loss: 4.93160e-06, min loss: 3.84208e-06\n",
      "Epoch: 267500, elapsed: 1.10e+01, train loss: 3.83674e-06, val loss: 4.67929e-06, min loss: 3.83674e-06\n",
      "Epoch: 267600, elapsed: 1.08e+01, train loss: 6.86839e-06, val loss: 7.50967e-06, min loss: 3.83674e-06\n",
      "Epoch: 267700, elapsed: 1.19e+01, train loss: 4.24732e-06, val loss: 5.18809e-06, min loss: 3.83674e-06\n",
      "Epoch: 267800, elapsed: 1.10e+01, train loss: 4.24153e-06, val loss: 5.10731e-06, min loss: 3.83674e-06\n",
      "Epoch: 267900, elapsed: 1.09e+01, train loss: 4.50730e-06, val loss: 5.51375e-06, min loss: 3.83674e-06\n",
      "Epoch: 268000, elapsed: 1.10e+01, train loss: 3.88424e-06, val loss: 4.75031e-06, min loss: 3.83674e-06\n",
      "Epoch: 268100, elapsed: 1.09e+01, train loss: 3.81980e-06, val loss: 4.62837e-06, min loss: 3.81980e-06\n",
      "Epoch: 268200, elapsed: 1.10e+01, train loss: 3.80753e-06, val loss: 4.59468e-06, min loss: 3.80753e-06\n",
      "Epoch: 268300, elapsed: 1.18e+01, train loss: 3.93070e-06, val loss: 4.75489e-06, min loss: 3.80753e-06\n",
      "Epoch: 268400, elapsed: 1.11e+01, train loss: 6.17367e-06, val loss: 6.06171e-06, min loss: 3.80753e-06\n",
      "Epoch: 268500, elapsed: 1.08e+01, train loss: 4.41140e-06, val loss: 5.25680e-06, min loss: 3.80753e-06\n",
      "Epoch: 268600, elapsed: 1.09e+01, train loss: 4.06674e-06, val loss: 4.84685e-06, min loss: 3.80753e-06\n",
      "Epoch: 268700, elapsed: 1.09e+01, train loss: 4.83114e-06, val loss: 5.25869e-06, min loss: 3.80753e-06\n",
      "Epoch: 268800, elapsed: 1.08e+01, train loss: 4.21596e-06, val loss: 4.93562e-06, min loss: 3.80753e-06\n",
      "Epoch: 268900, elapsed: 1.18e+01, train loss: 4.67355e-06, val loss: 5.19082e-06, min loss: 3.80753e-06\n",
      "Epoch: 269000, elapsed: 1.11e+01, train loss: 4.01826e-06, val loss: 4.65003e-06, min loss: 3.80753e-06\n",
      "Epoch: 269100, elapsed: 1.09e+01, train loss: 3.97675e-06, val loss: 4.72406e-06, min loss: 3.80753e-06\n",
      "Epoch: 269200, elapsed: 1.10e+01, train loss: 3.95624e-06, val loss: 4.76851e-06, min loss: 3.80753e-06\n",
      "Epoch: 269300, elapsed: 1.08e+01, train loss: 6.01175e-06, val loss: 5.31020e-06, min loss: 3.80753e-06\n",
      "Epoch: 269400, elapsed: 1.09e+01, train loss: 4.67137e-06, val loss: 5.30556e-06, min loss: 3.80753e-06\n",
      "Epoch: 269500, elapsed: 1.09e+01, train loss: 6.97858e-06, val loss: 7.89216e-06, min loss: 3.80753e-06\n",
      "Epoch: 269600, elapsed: 1.20e+01, train loss: 4.62884e-06, val loss: 5.43217e-06, min loss: 3.80753e-06\n",
      "Epoch: 269700, elapsed: 1.09e+01, train loss: 3.88835e-06, val loss: 4.66186e-06, min loss: 3.80753e-06\n",
      "Epoch: 269800, elapsed: 1.11e+01, train loss: 7.45743e-06, val loss: 8.63171e-06, min loss: 3.80753e-06\n",
      "Epoch: 269900, elapsed: 1.10e+01, train loss: 3.97364e-06, val loss: 4.84113e-06, min loss: 3.80753e-06\n",
      "Epoch: 270000, elapsed: 1.11e+01, train loss: 4.63742e-06, val loss: 5.19521e-06, min loss: 3.80753e-06\n",
      "Epoch: 270100, elapsed: 1.30e+01, train loss: 4.12190e-06, val loss: 4.79818e-06, min loss: 3.80753e-06\n",
      "Epoch: 270200, elapsed: 1.18e+01, train loss: 3.77243e-06, val loss: 4.56836e-06, min loss: 3.77243e-06\n",
      "Epoch: 270300, elapsed: 1.09e+01, train loss: 4.20331e-06, val loss: 5.05768e-06, min loss: 3.77243e-06\n",
      "Epoch: 270400, elapsed: 1.11e+01, train loss: 1.46071e-05, val loss: 1.67079e-05, min loss: 3.77243e-06\n",
      "Epoch: 270500, elapsed: 1.08e+01, train loss: 4.49578e-06, val loss: 5.48459e-06, min loss: 3.77243e-06\n",
      "Epoch: 270600, elapsed: 1.08e+01, train loss: 1.02942e-05, val loss: 9.49110e-06, min loss: 3.77243e-06\n",
      "Epoch: 270700, elapsed: 1.10e+01, train loss: 4.07091e-06, val loss: 4.91523e-06, min loss: 3.77243e-06\n",
      "Epoch: 270800, elapsed: 1.20e+01, train loss: 3.95251e-06, val loss: 4.72066e-06, min loss: 3.77243e-06\n",
      "Epoch: 270900, elapsed: 1.11e+01, train loss: 3.80600e-06, val loss: 4.61481e-06, min loss: 3.77243e-06\n",
      "Epoch: 271000, elapsed: 1.09e+01, train loss: 3.84694e-06, val loss: 5.04905e-06, min loss: 3.77243e-06\n",
      "Epoch: 271100, elapsed: 1.10e+01, train loss: 4.17903e-06, val loss: 5.14927e-06, min loss: 3.77243e-06\n",
      "Epoch: 271200, elapsed: 1.09e+01, train loss: 4.17818e-06, val loss: 4.92747e-06, min loss: 3.77243e-06\n",
      "Epoch: 271300, elapsed: 1.08e+01, train loss: 3.81362e-06, val loss: 4.60432e-06, min loss: 3.77243e-06\n",
      "Epoch: 271400, elapsed: 1.19e+01, train loss: 4.69558e-06, val loss: 5.36048e-06, min loss: 3.77243e-06\n",
      "Epoch: 271500, elapsed: 1.10e+01, train loss: 3.74585e-06, val loss: 4.53469e-06, min loss: 3.74585e-06\n",
      "Epoch: 271600, elapsed: 1.10e+01, train loss: 3.74683e-06, val loss: 4.55187e-06, min loss: 3.74585e-06\n",
      "Epoch: 271700, elapsed: 1.10e+01, train loss: 3.75904e-06, val loss: 4.54880e-06, min loss: 3.74585e-06\n",
      "Epoch: 271800, elapsed: 1.09e+01, train loss: 4.01403e-06, val loss: 4.87507e-06, min loss: 3.74585e-06\n",
      "Epoch: 271900, elapsed: 1.09e+01, train loss: 5.56499e-06, val loss: 6.56769e-06, min loss: 3.74585e-06\n",
      "Epoch: 272000, elapsed: 1.09e+01, train loss: 1.06353e-05, val loss: 1.04619e-05, min loss: 3.74585e-06\n",
      "Epoch: 272100, elapsed: 1.19e+01, train loss: 4.20632e-06, val loss: 5.08810e-06, min loss: 3.74585e-06\n",
      "Epoch: 272200, elapsed: 1.10e+01, train loss: 3.88980e-06, val loss: 4.61250e-06, min loss: 3.74585e-06\n",
      "Epoch: 272300, elapsed: 1.09e+01, train loss: 3.79018e-06, val loss: 4.53679e-06, min loss: 3.74585e-06\n",
      "Epoch: 272400, elapsed: 1.10e+01, train loss: 3.77074e-06, val loss: 4.56364e-06, min loss: 3.74585e-06\n",
      "Epoch: 272500, elapsed: 1.09e+01, train loss: 4.20506e-06, val loss: 5.15844e-06, min loss: 3.74585e-06\n",
      "Epoch: 272600, elapsed: 1.10e+01, train loss: 4.92017e-06, val loss: 5.74990e-06, min loss: 3.74585e-06\n",
      "Epoch: 272700, elapsed: 1.20e+01, train loss: 5.10142e-06, val loss: 6.10642e-06, min loss: 3.74585e-06\n",
      "Epoch: 272800, elapsed: 1.10e+01, train loss: 3.82270e-06, val loss: 4.66527e-06, min loss: 3.74585e-06\n",
      "Epoch: 272900, elapsed: 1.09e+01, train loss: 3.85943e-06, val loss: 4.75664e-06, min loss: 3.74585e-06\n",
      "Epoch: 273000, elapsed: 1.09e+01, train loss: 4.06170e-06, val loss: 4.96582e-06, min loss: 3.74585e-06\n",
      "Epoch: 273100, elapsed: 1.08e+01, train loss: 4.00869e-06, val loss: 5.09707e-06, min loss: 3.74585e-06\n",
      "Epoch: 273200, elapsed: 1.09e+01, train loss: 3.72710e-06, val loss: 4.51717e-06, min loss: 3.72710e-06\n",
      "Epoch: 273300, elapsed: 1.11e+01, train loss: 3.77910e-06, val loss: 4.54140e-06, min loss: 3.72710e-06\n",
      "Epoch: 273400, elapsed: 1.19e+01, train loss: 3.86703e-06, val loss: 4.68899e-06, min loss: 3.72710e-06\n",
      "Epoch: 273500, elapsed: 1.10e+01, train loss: 4.10933e-06, val loss: 5.19475e-06, min loss: 3.72710e-06\n",
      "Epoch: 273600, elapsed: 1.10e+01, train loss: 9.55770e-06, val loss: 1.06633e-05, min loss: 3.72710e-06\n",
      "Epoch: 273700, elapsed: 1.10e+01, train loss: 3.77051e-06, val loss: 4.57380e-06, min loss: 3.72710e-06\n",
      "Epoch: 273800, elapsed: 1.09e+01, train loss: 3.84653e-06, val loss: 4.67212e-06, min loss: 3.72710e-06\n",
      "Epoch: 273900, elapsed: 1.08e+01, train loss: 3.98382e-06, val loss: 4.77029e-06, min loss: 3.72710e-06\n",
      "Epoch: 274000, elapsed: 1.19e+01, train loss: 6.29204e-06, val loss: 7.48684e-06, min loss: 3.72710e-06\n",
      "Epoch: 274100, elapsed: 1.12e+01, train loss: 8.58187e-06, val loss: 8.22820e-06, min loss: 3.72710e-06\n",
      "Epoch: 274200, elapsed: 1.09e+01, train loss: 1.22592e-05, val loss: 1.19339e-05, min loss: 3.72710e-06\n",
      "Epoch: 274300, elapsed: 1.08e+01, train loss: 3.73619e-06, val loss: 4.94970e-06, min loss: 3.72710e-06\n",
      "Epoch: 274400, elapsed: 1.08e+01, train loss: 1.10880e-05, val loss: 1.29624e-05, min loss: 3.72710e-06\n",
      "Epoch: 274500, elapsed: 1.08e+01, train loss: 1.05597e-05, val loss: 1.26419e-05, min loss: 3.72710e-06\n",
      "Epoch: 274600, elapsed: 1.18e+01, train loss: 3.76531e-06, val loss: 4.54984e-06, min loss: 3.72710e-06\n",
      "Epoch: 274700, elapsed: 1.10e+01, train loss: 4.20503e-06, val loss: 4.88735e-06, min loss: 3.72710e-06\n",
      "Epoch: 274800, elapsed: 1.11e+01, train loss: 3.80113e-06, val loss: 4.58041e-06, min loss: 3.72710e-06\n",
      "Epoch: 274900, elapsed: 1.10e+01, train loss: 3.72174e-06, val loss: 4.48985e-06, min loss: 3.72174e-06\n",
      "Epoch: 275000, elapsed: 1.10e+01, train loss: 3.72161e-06, val loss: 4.51078e-06, min loss: 3.72161e-06\n",
      "Epoch: 275100, elapsed: 1.29e+01, train loss: 3.99980e-06, val loss: 4.86640e-06, min loss: 3.72161e-06\n",
      "Epoch: 275200, elapsed: 1.08e+01, train loss: 1.26738e-05, val loss: 1.39727e-05, min loss: 3.72161e-06\n",
      "Epoch: 275300, elapsed: 1.17e+01, train loss: 4.00730e-06, val loss: 5.32870e-06, min loss: 3.72161e-06\n",
      "Epoch: 275400, elapsed: 1.09e+01, train loss: 4.00289e-06, val loss: 4.86209e-06, min loss: 3.72161e-06\n",
      "Epoch: 275500, elapsed: 1.11e+01, train loss: 3.86654e-06, val loss: 4.65819e-06, min loss: 3.72161e-06\n",
      "Epoch: 275600, elapsed: 1.08e+01, train loss: 4.96714e-06, val loss: 6.43601e-06, min loss: 3.72161e-06\n",
      "Epoch: 275700, elapsed: 1.09e+01, train loss: 3.83070e-06, val loss: 4.59139e-06, min loss: 3.72161e-06\n",
      "Epoch: 275800, elapsed: 1.09e+01, train loss: 3.72518e-06, val loss: 4.48640e-06, min loss: 3.72161e-06\n",
      "Epoch: 275900, elapsed: 1.19e+01, train loss: 5.95561e-06, val loss: 7.01758e-06, min loss: 3.72161e-06\n",
      "Epoch: 276000, elapsed: 1.09e+01, train loss: 3.98191e-06, val loss: 4.82473e-06, min loss: 3.72161e-06\n",
      "Epoch: 276100, elapsed: 1.10e+01, train loss: 4.91528e-06, val loss: 5.22470e-06, min loss: 3.72161e-06\n",
      "Epoch: 276200, elapsed: 1.09e+01, train loss: 4.04475e-06, val loss: 4.67856e-06, min loss: 3.72161e-06\n",
      "Epoch: 276300, elapsed: 1.09e+01, train loss: 5.12028e-06, val loss: 5.77773e-06, min loss: 3.72161e-06\n",
      "Epoch: 276400, elapsed: 1.10e+01, train loss: 4.08501e-06, val loss: 4.98309e-06, min loss: 3.72161e-06\n",
      "Epoch: 276500, elapsed: 1.10e+01, train loss: 4.07838e-06, val loss: 4.69460e-06, min loss: 3.72161e-06\n",
      "Epoch: 276600, elapsed: 1.21e+01, train loss: 5.33214e-06, val loss: 5.27240e-06, min loss: 3.72161e-06\n",
      "Epoch: 276700, elapsed: 1.10e+01, train loss: 7.97306e-06, val loss: 9.07196e-06, min loss: 3.72161e-06\n",
      "Epoch: 276800, elapsed: 1.10e+01, train loss: 4.66867e-06, val loss: 5.34255e-06, min loss: 3.72161e-06\n",
      "Epoch: 276900, elapsed: 1.10e+01, train loss: 4.16435e-06, val loss: 4.71996e-06, min loss: 3.72161e-06\n",
      "Epoch: 277000, elapsed: 1.09e+01, train loss: 3.86241e-06, val loss: 4.64517e-06, min loss: 3.72161e-06\n",
      "Epoch: 277100, elapsed: 1.11e+01, train loss: 3.78767e-06, val loss: 4.54379e-06, min loss: 3.72161e-06\n",
      "Epoch: 277200, elapsed: 1.19e+01, train loss: 5.00993e-06, val loss: 5.88986e-06, min loss: 3.72161e-06\n",
      "Epoch: 277300, elapsed: 1.10e+01, train loss: 3.69328e-06, val loss: 4.45276e-06, min loss: 3.69328e-06\n",
      "Epoch: 277400, elapsed: 1.10e+01, train loss: 4.44511e-06, val loss: 5.19435e-06, min loss: 3.69328e-06\n",
      "Epoch: 277500, elapsed: 1.09e+01, train loss: 5.61954e-06, val loss: 5.31228e-06, min loss: 3.69328e-06\n",
      "Epoch: 277600, elapsed: 1.09e+01, train loss: 4.27663e-06, val loss: 4.66438e-06, min loss: 3.69328e-06\n",
      "Epoch: 277700, elapsed: 1.10e+01, train loss: 3.75391e-06, val loss: 4.65001e-06, min loss: 3.69328e-06\n",
      "Epoch: 277800, elapsed: 1.09e+01, train loss: 4.30367e-06, val loss: 5.18443e-06, min loss: 3.69328e-06\n",
      "Epoch: 277900, elapsed: 1.20e+01, train loss: 4.05684e-06, val loss: 4.78170e-06, min loss: 3.69328e-06\n",
      "Epoch: 278000, elapsed: 1.11e+01, train loss: 3.72450e-06, val loss: 4.53092e-06, min loss: 3.69328e-06\n",
      "Epoch: 278100, elapsed: 1.11e+01, train loss: 3.81597e-06, val loss: 4.70387e-06, min loss: 3.69328e-06\n",
      "Epoch: 278200, elapsed: 1.10e+01, train loss: 3.71283e-06, val loss: 4.54359e-06, min loss: 3.69328e-06\n",
      "Epoch: 278300, elapsed: 1.09e+01, train loss: 3.74154e-06, val loss: 4.54495e-06, min loss: 3.69328e-06\n",
      "Epoch: 278400, elapsed: 1.11e+01, train loss: 4.84465e-06, val loss: 5.70062e-06, min loss: 3.69328e-06\n",
      "Epoch: 278500, elapsed: 1.20e+01, train loss: 4.15475e-06, val loss: 4.88102e-06, min loss: 3.69328e-06\n",
      "Epoch: 278600, elapsed: 1.09e+01, train loss: 6.97269e-06, val loss: 8.14977e-06, min loss: 3.69328e-06\n",
      "Epoch: 278700, elapsed: 1.10e+01, train loss: 4.67405e-06, val loss: 5.40236e-06, min loss: 3.69328e-06\n",
      "Epoch: 278800, elapsed: 1.10e+01, train loss: 5.13809e-06, val loss: 5.79388e-06, min loss: 3.69328e-06\n",
      "Epoch: 278900, elapsed: 1.09e+01, train loss: 5.26941e-06, val loss: 6.29672e-06, min loss: 3.69328e-06\n",
      "Epoch: 279000, elapsed: 1.09e+01, train loss: 4.72194e-06, val loss: 5.62230e-06, min loss: 3.69328e-06\n",
      "Epoch: 279100, elapsed: 1.11e+01, train loss: 3.88554e-06, val loss: 4.80765e-06, min loss: 3.69328e-06\n",
      "Epoch: 279200, elapsed: 1.19e+01, train loss: 3.65937e-06, val loss: 4.47034e-06, min loss: 3.65937e-06\n",
      "Epoch: 279300, elapsed: 1.09e+01, train loss: 3.67472e-06, val loss: 4.46233e-06, min loss: 3.65937e-06\n",
      "Epoch: 279400, elapsed: 1.10e+01, train loss: 4.12307e-06, val loss: 4.59188e-06, min loss: 3.65937e-06\n",
      "Epoch: 279500, elapsed: 1.09e+01, train loss: 1.06925e-05, val loss: 1.07759e-05, min loss: 3.65937e-06\n",
      "Epoch: 279600, elapsed: 1.10e+01, train loss: 3.85970e-06, val loss: 4.52931e-06, min loss: 3.65937e-06\n",
      "Epoch: 279700, elapsed: 1.10e+01, train loss: 3.64405e-06, val loss: 4.43419e-06, min loss: 3.64405e-06\n",
      "Epoch: 279800, elapsed: 1.19e+01, train loss: 5.33591e-06, val loss: 6.70784e-06, min loss: 3.64405e-06\n",
      "Epoch: 279900, elapsed: 1.10e+01, train loss: 3.80191e-06, val loss: 4.61865e-06, min loss: 3.64405e-06\n",
      "Epoch: 280000, elapsed: 1.12e+01, train loss: 7.71897e-06, val loss: 9.30364e-06, min loss: 3.64405e-06\n",
      "Epoch: 280100, elapsed: 1.31e+01, train loss: 5.35841e-06, val loss: 6.77394e-06, min loss: 3.64405e-06\n",
      "Epoch: 280200, elapsed: 1.09e+01, train loss: 4.16942e-06, val loss: 5.05172e-06, min loss: 3.64405e-06\n",
      "Epoch: 280300, elapsed: 1.11e+01, train loss: 3.71024e-06, val loss: 4.41334e-06, min loss: 3.64405e-06\n",
      "Epoch: 280400, elapsed: 1.19e+01, train loss: 3.69067e-06, val loss: 4.40590e-06, min loss: 3.64405e-06\n",
      "Epoch: 280500, elapsed: 1.11e+01, train loss: 3.72330e-06, val loss: 4.51120e-06, min loss: 3.64405e-06\n",
      "Epoch: 280600, elapsed: 1.10e+01, train loss: 3.68408e-06, val loss: 4.46331e-06, min loss: 3.64405e-06\n",
      "Epoch: 280700, elapsed: 1.10e+01, train loss: 5.09653e-06, val loss: 6.12775e-06, min loss: 3.64405e-06\n",
      "Epoch: 280800, elapsed: 1.10e+01, train loss: 2.14577e-05, val loss: 1.82935e-05, min loss: 3.64405e-06\n",
      "Epoch: 280900, elapsed: 1.10e+01, train loss: 3.61298e-06, val loss: 4.38870e-06, min loss: 3.61298e-06\n",
      "Epoch: 281000, elapsed: 1.09e+01, train loss: 3.72944e-06, val loss: 4.50809e-06, min loss: 3.61298e-06\n",
      "Epoch: 281100, elapsed: 1.19e+01, train loss: 7.31730e-06, val loss: 6.99167e-06, min loss: 3.61298e-06\n",
      "Epoch: 281200, elapsed: 1.11e+01, train loss: 3.85451e-06, val loss: 4.59362e-06, min loss: 3.61298e-06\n",
      "Epoch: 281300, elapsed: 1.12e+01, train loss: 4.09462e-06, val loss: 5.06979e-06, min loss: 3.61298e-06\n",
      "Epoch: 281400, elapsed: 1.11e+01, train loss: 4.28820e-06, val loss: 4.91081e-06, min loss: 3.61298e-06\n",
      "Epoch: 281500, elapsed: 1.08e+01, train loss: 8.98487e-06, val loss: 1.02775e-05, min loss: 3.61298e-06\n",
      "Epoch: 281600, elapsed: 1.09e+01, train loss: 4.76992e-06, val loss: 5.31008e-06, min loss: 3.61298e-06\n",
      "Epoch: 281700, elapsed: 1.18e+01, train loss: 7.48197e-06, val loss: 8.65975e-06, min loss: 3.61298e-06\n",
      "Epoch: 281800, elapsed: 1.13e+01, train loss: 4.86230e-06, val loss: 4.52969e-06, min loss: 3.61298e-06\n",
      "Epoch: 281900, elapsed: 1.09e+01, train loss: 3.62635e-06, val loss: 4.35974e-06, min loss: 3.61298e-06\n",
      "Epoch: 282000, elapsed: 1.09e+01, train loss: 3.65818e-06, val loss: 4.43107e-06, min loss: 3.61298e-06\n",
      "Epoch: 282100, elapsed: 1.11e+01, train loss: 3.78589e-06, val loss: 4.63767e-06, min loss: 3.61298e-06\n",
      "Epoch: 282200, elapsed: 1.09e+01, train loss: 3.61311e-06, val loss: 4.36888e-06, min loss: 3.61298e-06\n",
      "Epoch: 282300, elapsed: 1.09e+01, train loss: 5.36532e-06, val loss: 6.44764e-06, min loss: 3.61298e-06\n",
      "Epoch: 282400, elapsed: 1.19e+01, train loss: 4.77246e-06, val loss: 5.51624e-06, min loss: 3.61298e-06\n",
      "Epoch: 282500, elapsed: 1.11e+01, train loss: 7.96734e-06, val loss: 7.20537e-06, min loss: 3.61298e-06\n",
      "Epoch: 282600, elapsed: 1.10e+01, train loss: 4.64259e-06, val loss: 5.20569e-06, min loss: 3.61298e-06\n",
      "Epoch: 282700, elapsed: 1.09e+01, train loss: 4.75090e-06, val loss: 5.46925e-06, min loss: 3.61298e-06\n",
      "Epoch: 282800, elapsed: 1.09e+01, train loss: 3.82380e-06, val loss: 4.59015e-06, min loss: 3.61298e-06\n",
      "Epoch: 282900, elapsed: 1.09e+01, train loss: 3.97170e-06, val loss: 4.47335e-06, min loss: 3.61298e-06\n",
      "Epoch: 283000, elapsed: 1.11e+01, train loss: 4.03448e-06, val loss: 4.90182e-06, min loss: 3.61298e-06\n",
      "Epoch: 283100, elapsed: 1.21e+01, train loss: 4.74099e-06, val loss: 5.30494e-06, min loss: 3.61298e-06\n",
      "Epoch: 283200, elapsed: 1.11e+01, train loss: 4.10730e-06, val loss: 4.95094e-06, min loss: 3.61298e-06\n",
      "Epoch: 283300, elapsed: 1.10e+01, train loss: 3.60409e-06, val loss: 4.36809e-06, min loss: 3.60409e-06\n",
      "Epoch: 283400, elapsed: 1.10e+01, train loss: 3.61893e-06, val loss: 4.37676e-06, min loss: 3.60409e-06\n",
      "Epoch: 283500, elapsed: 1.09e+01, train loss: 3.79065e-06, val loss: 4.69897e-06, min loss: 3.60409e-06\n",
      "Epoch: 283600, elapsed: 1.10e+01, train loss: 3.99468e-06, val loss: 4.83151e-06, min loss: 3.60409e-06\n",
      "Epoch: 283700, elapsed: 1.19e+01, train loss: 3.66691e-06, val loss: 4.53914e-06, min loss: 3.60409e-06\n",
      "Epoch: 283800, elapsed: 1.12e+01, train loss: 3.59789e-06, val loss: 4.34202e-06, min loss: 3.59789e-06\n",
      "Epoch: 283900, elapsed: 1.11e+01, train loss: 3.57905e-06, val loss: 4.34703e-06, min loss: 3.57905e-06\n",
      "Epoch: 284000, elapsed: 1.10e+01, train loss: 3.62776e-06, val loss: 4.38738e-06, min loss: 3.57905e-06\n",
      "Epoch: 284100, elapsed: 1.10e+01, train loss: 3.64065e-06, val loss: 4.40305e-06, min loss: 3.57905e-06\n",
      "Epoch: 284200, elapsed: 1.10e+01, train loss: 3.89812e-06, val loss: 4.73657e-06, min loss: 3.57905e-06\n",
      "Epoch: 284300, elapsed: 1.10e+01, train loss: 1.04732e-05, val loss: 1.24280e-05, min loss: 3.57905e-06\n",
      "Epoch: 284400, elapsed: 1.21e+01, train loss: 6.89890e-06, val loss: 8.35576e-06, min loss: 3.57905e-06\n",
      "Epoch: 284500, elapsed: 1.11e+01, train loss: 8.03635e-06, val loss: 6.09659e-06, min loss: 3.57905e-06\n",
      "Epoch: 284600, elapsed: 1.10e+01, train loss: 1.27609e-05, val loss: 1.47344e-05, min loss: 3.57905e-06\n",
      "Epoch: 284700, elapsed: 1.11e+01, train loss: 1.26192e-05, val loss: 1.36132e-05, min loss: 3.57905e-06\n",
      "Epoch: 284800, elapsed: 1.10e+01, train loss: 6.27117e-06, val loss: 5.51240e-06, min loss: 3.57905e-06\n",
      "Epoch: 284900, elapsed: 1.12e+01, train loss: 4.49937e-06, val loss: 5.61530e-06, min loss: 3.57905e-06\n",
      "Epoch: 285000, elapsed: 1.19e+01, train loss: 3.55190e-06, val loss: 4.30446e-06, min loss: 3.55190e-06\n",
      "Epoch: 285100, elapsed: 1.31e+01, train loss: 4.76941e-06, val loss: 5.12780e-06, min loss: 3.55190e-06\n",
      "Epoch: 285200, elapsed: 1.11e+01, train loss: 7.17809e-06, val loss: 6.62225e-06, min loss: 3.55190e-06\n",
      "Epoch: 285300, elapsed: 1.09e+01, train loss: 8.56557e-06, val loss: 1.12292e-05, min loss: 3.55190e-06\n",
      "Epoch: 285400, elapsed: 1.11e+01, train loss: 3.65194e-06, val loss: 4.36152e-06, min loss: 3.55190e-06\n",
      "Epoch: 285500, elapsed: 1.10e+01, train loss: 3.57482e-06, val loss: 4.32160e-06, min loss: 3.55190e-06\n",
      "Epoch: 285600, elapsed: 1.10e+01, train loss: 3.58214e-06, val loss: 4.36684e-06, min loss: 3.55190e-06\n",
      "Epoch: 285700, elapsed: 1.19e+01, train loss: 9.26630e-06, val loss: 8.36413e-06, min loss: 3.55190e-06\n",
      "Epoch: 285800, elapsed: 1.10e+01, train loss: 3.84969e-06, val loss: 4.44049e-06, min loss: 3.55190e-06\n",
      "Epoch: 285900, elapsed: 1.09e+01, train loss: 4.04410e-06, val loss: 4.77459e-06, min loss: 3.55190e-06\n",
      "Epoch: 286000, elapsed: 1.10e+01, train loss: 3.68178e-06, val loss: 4.44299e-06, min loss: 3.55190e-06\n",
      "Epoch: 286100, elapsed: 1.11e+01, train loss: 4.12811e-06, val loss: 5.04995e-06, min loss: 3.55190e-06\n",
      "Epoch: 286200, elapsed: 1.09e+01, train loss: 3.59048e-06, val loss: 4.39489e-06, min loss: 3.55190e-06\n",
      "Epoch: 286300, elapsed: 1.20e+01, train loss: 3.73298e-06, val loss: 4.56038e-06, min loss: 3.55190e-06\n",
      "Epoch: 286400, elapsed: 1.10e+01, train loss: 8.38100e-06, val loss: 8.83876e-06, min loss: 3.55190e-06\n",
      "Epoch: 286500, elapsed: 1.10e+01, train loss: 3.58836e-06, val loss: 4.33070e-06, min loss: 3.55190e-06\n",
      "Epoch: 286600, elapsed: 1.10e+01, train loss: 4.31538e-06, val loss: 5.20976e-06, min loss: 3.55190e-06\n",
      "Epoch: 286700, elapsed: 1.08e+01, train loss: 5.64350e-06, val loss: 7.26829e-06, min loss: 3.55190e-06\n",
      "Epoch: 286800, elapsed: 1.10e+01, train loss: 4.62181e-06, val loss: 5.38797e-06, min loss: 3.55190e-06\n",
      "Epoch: 286900, elapsed: 1.10e+01, train loss: 3.69519e-06, val loss: 4.51618e-06, min loss: 3.55190e-06\n",
      "Epoch: 287000, elapsed: 1.19e+01, train loss: 5.87926e-06, val loss: 7.25183e-06, min loss: 3.55190e-06\n",
      "Epoch: 287100, elapsed: 1.10e+01, train loss: 4.62078e-06, val loss: 5.40049e-06, min loss: 3.55190e-06\n",
      "Epoch: 287200, elapsed: 1.09e+01, train loss: 3.68821e-06, val loss: 4.37044e-06, min loss: 3.55190e-06\n",
      "Epoch: 287300, elapsed: 1.10e+01, train loss: 6.61313e-06, val loss: 6.12170e-06, min loss: 3.55190e-06\n",
      "Epoch: 287400, elapsed: 1.09e+01, train loss: 3.54955e-06, val loss: 4.28942e-06, min loss: 3.54955e-06\n",
      "Epoch: 287500, elapsed: 1.11e+01, train loss: 3.63538e-06, val loss: 4.39405e-06, min loss: 3.54955e-06\n",
      "Epoch: 287600, elapsed: 1.09e+01, train loss: 9.71226e-06, val loss: 1.06138e-05, min loss: 3.54955e-06\n",
      "Epoch: 287700, elapsed: 1.19e+01, train loss: 3.79448e-06, val loss: 4.33330e-06, min loss: 3.54955e-06\n",
      "Epoch: 287800, elapsed: 1.10e+01, train loss: 3.66933e-06, val loss: 4.43046e-06, min loss: 3.54955e-06\n",
      "Epoch: 287900, elapsed: 1.09e+01, train loss: 3.86267e-06, val loss: 4.67941e-06, min loss: 3.54955e-06\n",
      "Epoch: 288000, elapsed: 1.10e+01, train loss: 4.60341e-06, val loss: 5.65131e-06, min loss: 3.54955e-06\n",
      "Epoch: 288100, elapsed: 1.10e+01, train loss: 8.31076e-06, val loss: 7.79597e-06, min loss: 3.54955e-06\n",
      "Epoch: 288200, elapsed: 1.10e+01, train loss: 3.79742e-06, val loss: 4.52968e-06, min loss: 3.54955e-06\n",
      "Epoch: 288300, elapsed: 1.19e+01, train loss: 3.55563e-06, val loss: 4.33439e-06, min loss: 3.54955e-06\n",
      "Epoch: 288400, elapsed: 1.11e+01, train loss: 3.52258e-06, val loss: 4.29003e-06, min loss: 3.52258e-06\n",
      "Epoch: 288500, elapsed: 1.11e+01, train loss: 3.54901e-06, val loss: 4.23266e-06, min loss: 3.52258e-06\n",
      "Epoch: 288600, elapsed: 1.09e+01, train loss: 3.69996e-06, val loss: 4.33010e-06, min loss: 3.52258e-06\n",
      "Epoch: 288700, elapsed: 1.09e+01, train loss: 3.50776e-06, val loss: 4.25842e-06, min loss: 3.50776e-06\n",
      "Epoch: 288800, elapsed: 1.10e+01, train loss: 3.55404e-06, val loss: 4.31908e-06, min loss: 3.50776e-06\n",
      "Epoch: 288900, elapsed: 1.12e+01, train loss: 3.57799e-06, val loss: 4.24201e-06, min loss: 3.50776e-06\n",
      "Epoch: 289000, elapsed: 1.19e+01, train loss: 3.68495e-06, val loss: 4.44937e-06, min loss: 3.50776e-06\n",
      "Epoch: 289100, elapsed: 1.11e+01, train loss: 5.19403e-06, val loss: 5.19742e-06, min loss: 3.50776e-06\n",
      "Epoch: 289200, elapsed: 1.10e+01, train loss: 3.72915e-06, val loss: 4.46810e-06, min loss: 3.50776e-06\n",
      "Epoch: 289300, elapsed: 1.09e+01, train loss: 3.51081e-06, val loss: 4.24852e-06, min loss: 3.50776e-06\n",
      "Epoch: 289400, elapsed: 1.08e+01, train loss: 3.58577e-06, val loss: 4.32613e-06, min loss: 3.50776e-06\n",
      "Epoch: 289500, elapsed: 1.09e+01, train loss: 4.14462e-06, val loss: 4.77107e-06, min loss: 3.50776e-06\n",
      "Epoch: 289600, elapsed: 1.09e+01, train loss: 4.77744e-06, val loss: 6.59513e-06, min loss: 3.50776e-06\n",
      "Epoch: 289700, elapsed: 1.18e+01, train loss: 1.42753e-05, val loss: 1.60989e-05, min loss: 3.50776e-06\n",
      "Epoch: 289800, elapsed: 1.10e+01, train loss: 4.18799e-06, val loss: 5.05983e-06, min loss: 3.50776e-06\n",
      "Epoch: 289900, elapsed: 1.10e+01, train loss: 5.33072e-06, val loss: 6.22687e-06, min loss: 3.50776e-06\n",
      "Epoch: 290000, elapsed: 1.08e+01, train loss: 3.63273e-06, val loss: 4.38735e-06, min loss: 3.50776e-06\n",
      "Epoch: 290100, elapsed: 1.30e+01, train loss: 4.06146e-06, val loss: 4.72534e-06, min loss: 3.50776e-06\n",
      "Epoch: 290200, elapsed: 1.09e+01, train loss: 4.69591e-06, val loss: 5.54756e-06, min loss: 3.50776e-06\n",
      "Epoch: 290300, elapsed: 1.18e+01, train loss: 3.52341e-06, val loss: 4.28569e-06, min loss: 3.50776e-06\n",
      "Epoch: 290400, elapsed: 1.09e+01, train loss: 5.71120e-06, val loss: 5.64724e-06, min loss: 3.50776e-06\n",
      "Epoch: 290500, elapsed: 1.10e+01, train loss: 4.30357e-06, val loss: 4.75418e-06, min loss: 3.50776e-06\n",
      "Epoch: 290600, elapsed: 1.09e+01, train loss: 6.41507e-06, val loss: 4.88807e-06, min loss: 3.50776e-06\n",
      "Epoch: 290700, elapsed: 1.08e+01, train loss: 3.47317e-06, val loss: 4.21151e-06, min loss: 3.47317e-06\n",
      "Epoch: 290800, elapsed: 1.10e+01, train loss: 3.84792e-06, val loss: 4.67826e-06, min loss: 3.47317e-06\n",
      "Epoch: 290900, elapsed: 1.09e+01, train loss: 4.72207e-06, val loss: 4.71294e-06, min loss: 3.47317e-06\n",
      "Epoch: 291000, elapsed: 1.20e+01, train loss: 4.08873e-06, val loss: 4.73975e-06, min loss: 3.47317e-06\n",
      "Epoch: 291100, elapsed: 1.09e+01, train loss: 7.04383e-06, val loss: 8.99701e-06, min loss: 3.47317e-06\n",
      "Epoch: 291200, elapsed: 1.09e+01, train loss: 3.62072e-06, val loss: 4.29245e-06, min loss: 3.47317e-06\n",
      "Epoch: 291300, elapsed: 1.09e+01, train loss: 3.45892e-06, val loss: 4.20799e-06, min loss: 3.45892e-06\n",
      "Epoch: 291400, elapsed: 1.10e+01, train loss: 4.06948e-06, val loss: 4.72082e-06, min loss: 3.45892e-06\n",
      "Epoch: 291500, elapsed: 1.11e+01, train loss: 4.02810e-06, val loss: 4.54701e-06, min loss: 3.45892e-06\n",
      "Epoch: 291600, elapsed: 1.11e+01, train loss: 3.51589e-06, val loss: 4.21846e-06, min loss: 3.45892e-06\n",
      "Epoch: 291700, elapsed: 1.19e+01, train loss: 3.73780e-06, val loss: 4.41347e-06, min loss: 3.45892e-06\n",
      "Epoch: 291800, elapsed: 1.11e+01, train loss: 3.49161e-06, val loss: 4.23523e-06, min loss: 3.45892e-06\n",
      "Epoch: 291900, elapsed: 1.08e+01, train loss: 1.03089e-05, val loss: 1.04730e-05, min loss: 3.45892e-06\n",
      "Epoch: 292000, elapsed: 1.08e+01, train loss: 3.49770e-06, val loss: 4.28711e-06, min loss: 3.45892e-06\n",
      "Epoch: 292100, elapsed: 1.08e+01, train loss: 3.60882e-06, val loss: 4.37922e-06, min loss: 3.45892e-06\n",
      "Epoch: 292200, elapsed: 1.09e+01, train loss: 3.83810e-06, val loss: 4.54183e-06, min loss: 3.45892e-06\n",
      "Epoch: 292300, elapsed: 1.20e+01, train loss: 3.45002e-06, val loss: 4.17999e-06, min loss: 3.45002e-06\n",
      "Epoch: 292400, elapsed: 1.10e+01, train loss: 6.19355e-06, val loss: 6.62604e-06, min loss: 3.45002e-06\n",
      "Epoch: 292500, elapsed: 1.10e+01, train loss: 1.06170e-05, val loss: 7.70002e-06, min loss: 3.45002e-06\n",
      "Epoch: 292600, elapsed: 1.10e+01, train loss: 4.41424e-06, val loss: 5.46680e-06, min loss: 3.45002e-06\n",
      "Epoch: 292700, elapsed: 1.09e+01, train loss: 3.93805e-06, val loss: 4.84706e-06, min loss: 3.45002e-06\n",
      "Epoch: 292800, elapsed: 1.08e+01, train loss: 3.46247e-06, val loss: 4.21237e-06, min loss: 3.45002e-06\n",
      "Epoch: 292900, elapsed: 1.10e+01, train loss: 3.97463e-06, val loss: 4.84286e-06, min loss: 3.45002e-06\n",
      "Epoch: 293000, elapsed: 1.20e+01, train loss: 4.10202e-06, val loss: 5.08063e-06, min loss: 3.45002e-06\n",
      "Epoch: 293100, elapsed: 1.11e+01, train loss: 4.16328e-06, val loss: 5.17570e-06, min loss: 3.45002e-06\n",
      "Epoch: 293200, elapsed: 1.10e+01, train loss: 3.43092e-06, val loss: 4.14819e-06, min loss: 3.43092e-06\n",
      "Epoch: 293300, elapsed: 1.09e+01, train loss: 3.42981e-06, val loss: 4.15676e-06, min loss: 3.42981e-06\n",
      "Epoch: 293400, elapsed: 1.10e+01, train loss: 3.50727e-06, val loss: 4.19804e-06, min loss: 3.42981e-06\n",
      "Epoch: 293500, elapsed: 1.10e+01, train loss: 3.60608e-06, val loss: 4.35364e-06, min loss: 3.42981e-06\n",
      "Epoch: 293600, elapsed: 1.08e+01, train loss: 4.63120e-06, val loss: 4.73020e-06, min loss: 3.42981e-06\n",
      "Epoch: 293700, elapsed: 1.19e+01, train loss: 3.61723e-06, val loss: 4.29226e-06, min loss: 3.42981e-06\n",
      "Epoch: 293800, elapsed: 1.12e+01, train loss: 3.50291e-06, val loss: 4.22839e-06, min loss: 3.42981e-06\n",
      "Epoch: 293900, elapsed: 1.09e+01, train loss: 5.53296e-06, val loss: 6.63711e-06, min loss: 3.42981e-06\n",
      "Epoch: 294000, elapsed: 1.10e+01, train loss: 4.68037e-06, val loss: 5.30397e-06, min loss: 3.42981e-06\n",
      "Epoch: 294100, elapsed: 1.09e+01, train loss: 6.27900e-06, val loss: 6.41631e-06, min loss: 3.42981e-06\n",
      "Epoch: 294200, elapsed: 1.10e+01, train loss: 5.25414e-06, val loss: 5.96554e-06, min loss: 3.42981e-06\n",
      "Epoch: 294300, elapsed: 1.09e+01, train loss: 4.26426e-06, val loss: 4.96161e-06, min loss: 3.42981e-06\n",
      "Epoch: 294400, elapsed: 1.22e+01, train loss: 9.62609e-06, val loss: 9.80965e-06, min loss: 3.42981e-06\n",
      "Epoch: 294500, elapsed: 1.10e+01, train loss: 4.31982e-06, val loss: 5.11434e-06, min loss: 3.42981e-06\n",
      "Epoch: 294600, elapsed: 1.11e+01, train loss: 4.13662e-06, val loss: 4.83009e-06, min loss: 3.42981e-06\n",
      "Epoch: 294700, elapsed: 1.10e+01, train loss: 3.41827e-06, val loss: 4.13197e-06, min loss: 3.41827e-06\n",
      "Epoch: 294800, elapsed: 1.09e+01, train loss: 3.68202e-06, val loss: 4.40597e-06, min loss: 3.41827e-06\n",
      "Epoch: 294900, elapsed: 1.12e+01, train loss: 4.44586e-06, val loss: 4.88505e-06, min loss: 3.41827e-06\n",
      "Epoch: 295000, elapsed: 1.21e+01, train loss: 3.72000e-06, val loss: 4.48863e-06, min loss: 3.41827e-06\n",
      "Epoch: 295100, elapsed: 1.32e+01, train loss: 4.00353e-06, val loss: 4.72986e-06, min loss: 3.41827e-06\n",
      "Epoch: 295200, elapsed: 1.08e+01, train loss: 3.50568e-06, val loss: 4.24068e-06, min loss: 3.41827e-06\n",
      "Epoch: 295300, elapsed: 1.09e+01, train loss: 5.37851e-06, val loss: 6.33040e-06, min loss: 3.41827e-06\n",
      "Epoch: 295400, elapsed: 1.08e+01, train loss: 5.91196e-06, val loss: 6.04258e-06, min loss: 3.41827e-06\n",
      "Epoch: 295500, elapsed: 1.08e+01, train loss: 3.75005e-06, val loss: 4.56250e-06, min loss: 3.41827e-06\n",
      "Epoch: 295600, elapsed: 1.11e+01, train loss: 3.49194e-06, val loss: 4.24304e-06, min loss: 3.41827e-06\n",
      "Epoch: 295700, elapsed: 1.20e+01, train loss: 5.59956e-06, val loss: 6.71671e-06, min loss: 3.41827e-06\n",
      "Epoch: 295800, elapsed: 1.11e+01, train loss: 4.84348e-06, val loss: 4.89949e-06, min loss: 3.41827e-06\n",
      "Epoch: 295900, elapsed: 1.10e+01, train loss: 4.24007e-06, val loss: 4.58484e-06, min loss: 3.41827e-06\n",
      "Epoch: 296000, elapsed: 1.09e+01, train loss: 3.89792e-06, val loss: 4.70875e-06, min loss: 3.41827e-06\n",
      "Epoch: 296100, elapsed: 1.08e+01, train loss: 3.40891e-06, val loss: 4.10089e-06, min loss: 3.40891e-06\n",
      "Epoch: 296200, elapsed: 1.10e+01, train loss: 3.71516e-06, val loss: 4.36578e-06, min loss: 3.40891e-06\n",
      "Epoch: 296300, elapsed: 1.09e+01, train loss: 9.50259e-06, val loss: 1.13075e-05, min loss: 3.40891e-06\n",
      "Epoch: 296400, elapsed: 1.20e+01, train loss: 3.83578e-06, val loss: 4.71656e-06, min loss: 3.40891e-06\n",
      "Epoch: 296500, elapsed: 1.10e+01, train loss: 5.56751e-06, val loss: 6.81827e-06, min loss: 3.40891e-06\n",
      "Epoch: 296600, elapsed: 1.09e+01, train loss: 1.11106e-05, val loss: 1.24613e-05, min loss: 3.40891e-06\n",
      "Epoch: 296700, elapsed: 1.08e+01, train loss: 4.22766e-06, val loss: 5.30078e-06, min loss: 3.40891e-06\n",
      "Epoch: 296800, elapsed: 1.10e+01, train loss: 5.25900e-06, val loss: 5.39815e-06, min loss: 3.40891e-06\n",
      "Epoch: 296900, elapsed: 1.10e+01, train loss: 3.50687e-06, val loss: 4.30076e-06, min loss: 3.40891e-06\n",
      "Epoch: 297000, elapsed: 1.19e+01, train loss: 3.72853e-06, val loss: 4.28405e-06, min loss: 3.40891e-06\n",
      "Epoch: 297100, elapsed: 1.10e+01, train loss: 4.08101e-06, val loss: 4.24613e-06, min loss: 3.40891e-06\n",
      "Epoch: 297200, elapsed: 1.09e+01, train loss: 3.70068e-06, val loss: 4.62870e-06, min loss: 3.40891e-06\n",
      "Epoch: 297300, elapsed: 1.10e+01, train loss: 4.04399e-06, val loss: 4.68543e-06, min loss: 3.40891e-06\n",
      "Epoch: 297400, elapsed: 1.09e+01, train loss: 3.72992e-06, val loss: 4.36673e-06, min loss: 3.40891e-06\n",
      "Epoch: 297500, elapsed: 1.10e+01, train loss: 3.38884e-06, val loss: 4.15273e-06, min loss: 3.38884e-06\n",
      "Epoch: 297600, elapsed: 1.11e+01, train loss: 3.37170e-06, val loss: 4.06731e-06, min loss: 3.37170e-06\n",
      "Epoch: 297700, elapsed: 1.18e+01, train loss: 3.57081e-06, val loss: 4.23887e-06, min loss: 3.37170e-06\n",
      "Epoch: 297800, elapsed: 1.11e+01, train loss: 3.64612e-06, val loss: 4.40416e-06, min loss: 3.37170e-06\n",
      "Epoch: 297900, elapsed: 1.11e+01, train loss: 4.92245e-06, val loss: 5.73105e-06, min loss: 3.37170e-06\n",
      "Epoch: 298000, elapsed: 1.09e+01, train loss: 3.71778e-06, val loss: 4.41800e-06, min loss: 3.37170e-06\n",
      "Epoch: 298100, elapsed: 1.09e+01, train loss: 4.01789e-06, val loss: 5.01972e-06, min loss: 3.37170e-06\n",
      "Epoch: 298200, elapsed: 1.09e+01, train loss: 4.23326e-06, val loss: 4.92529e-06, min loss: 3.37170e-06\n",
      "Epoch: 298300, elapsed: 1.09e+01, train loss: 3.50499e-06, val loss: 4.25226e-06, min loss: 3.37170e-06\n",
      "Epoch: 298400, elapsed: 1.22e+01, train loss: 3.35948e-06, val loss: 4.04574e-06, min loss: 3.35948e-06\n",
      "Epoch: 298500, elapsed: 1.10e+01, train loss: 3.37877e-06, val loss: 4.10514e-06, min loss: 3.35948e-06\n",
      "Epoch: 298600, elapsed: 1.11e+01, train loss: 3.36611e-06, val loss: 4.12273e-06, min loss: 3.35948e-06\n",
      "Epoch: 298700, elapsed: 1.09e+01, train loss: 3.35116e-06, val loss: 4.05281e-06, min loss: 3.35116e-06\n",
      "Epoch: 298800, elapsed: 1.09e+01, train loss: 3.37674e-06, val loss: 4.11755e-06, min loss: 3.35116e-06\n",
      "Epoch: 298900, elapsed: 1.11e+01, train loss: 3.43340e-06, val loss: 4.19405e-06, min loss: 3.35116e-06\n",
      "Epoch: 299000, elapsed: 1.09e+01, train loss: 3.34153e-06, val loss: 4.04255e-06, min loss: 3.34153e-06\n",
      "Epoch: 299100, elapsed: 1.21e+01, train loss: 3.61223e-06, val loss: 4.38385e-06, min loss: 3.34153e-06\n",
      "Epoch: 299200, elapsed: 1.11e+01, train loss: 7.24211e-06, val loss: 5.91629e-06, min loss: 3.34153e-06\n",
      "Epoch: 299300, elapsed: 1.10e+01, train loss: 3.75311e-06, val loss: 4.49655e-06, min loss: 3.34153e-06\n",
      "Epoch: 299400, elapsed: 1.10e+01, train loss: 3.96832e-06, val loss: 5.00213e-06, min loss: 3.34153e-06\n",
      "Epoch: 299500, elapsed: 1.10e+01, train loss: 3.37542e-06, val loss: 4.07385e-06, min loss: 3.34153e-06\n",
      "Epoch: 299600, elapsed: 1.10e+01, train loss: 7.43463e-06, val loss: 8.55430e-06, min loss: 3.34153e-06\n",
      "Epoch: 299700, elapsed: 1.12e+01, train loss: 3.96462e-06, val loss: 5.90169e-06, min loss: 3.34153e-06\n",
      "Epoch: 299800, elapsed: 1.22e+01, train loss: 4.09989e-06, val loss: 4.72409e-06, min loss: 3.34153e-06\n",
      "Epoch: 299900, elapsed: 1.11e+01, train loss: 3.62525e-06, val loss: 4.29911e-06, min loss: 3.34153e-06\n",
      "Epoch: 300000, elapsed: 1.10e+01, train loss: 3.43859e-06, val loss: 4.29031e-06, min loss: 3.34153e-06\n",
      "Epoch: 300100, elapsed: 1.31e+01, train loss: 3.34412e-06, val loss: 4.04748e-06, min loss: 3.34153e-06\n",
      "Epoch: 300200, elapsed: 1.09e+01, train loss: 5.23907e-06, val loss: 6.67182e-06, min loss: 3.34153e-06\n",
      "Epoch: 300300, elapsed: 1.10e+01, train loss: 3.41760e-06, val loss: 4.25004e-06, min loss: 3.34153e-06\n",
      "Epoch: 300400, elapsed: 1.20e+01, train loss: 3.47015e-06, val loss: 4.24858e-06, min loss: 3.34153e-06\n",
      "Epoch: 300500, elapsed: 1.11e+01, train loss: 4.70096e-06, val loss: 5.42575e-06, min loss: 3.34153e-06\n",
      "Epoch: 300600, elapsed: 1.10e+01, train loss: 3.43775e-06, val loss: 4.12884e-06, min loss: 3.34153e-06\n",
      "Epoch: 300700, elapsed: 1.09e+01, train loss: 6.63330e-06, val loss: 8.10589e-06, min loss: 3.34153e-06\n",
      "Epoch: 300800, elapsed: 1.12e+01, train loss: 7.93607e-06, val loss: 6.93017e-06, min loss: 3.34153e-06\n",
      "Epoch: 300900, elapsed: 1.09e+01, train loss: 4.46869e-06, val loss: 6.65931e-06, min loss: 3.34153e-06\n",
      "Epoch: 301000, elapsed: 1.09e+01, train loss: 3.80609e-06, val loss: 4.94192e-06, min loss: 3.34153e-06\n",
      "Epoch: 301100, elapsed: 1.20e+01, train loss: 3.30874e-06, val loss: 4.01486e-06, min loss: 3.30874e-06\n",
      "Epoch: 301200, elapsed: 1.11e+01, train loss: 3.56780e-06, val loss: 4.20891e-06, min loss: 3.30874e-06\n",
      "Epoch: 301300, elapsed: 1.10e+01, train loss: 4.80221e-06, val loss: 4.84942e-06, min loss: 3.30874e-06\n",
      "Epoch: 301400, elapsed: 1.10e+01, train loss: 8.91221e-06, val loss: 1.14610e-05, min loss: 3.30874e-06\n",
      "Epoch: 301500, elapsed: 1.09e+01, train loss: 3.33196e-06, val loss: 4.09124e-06, min loss: 3.30874e-06\n",
      "Epoch: 301600, elapsed: 1.09e+01, train loss: 4.43421e-06, val loss: 5.53136e-06, min loss: 3.30874e-06\n",
      "Epoch: 301700, elapsed: 1.09e+01, train loss: 3.39506e-06, val loss: 4.05505e-06, min loss: 3.30874e-06\n",
      "Epoch: 301800, elapsed: 1.19e+01, train loss: 3.29953e-06, val loss: 4.00959e-06, min loss: 3.29953e-06\n",
      "Epoch: 301900, elapsed: 1.11e+01, train loss: 3.31075e-06, val loss: 4.01743e-06, min loss: 3.29953e-06\n",
      "Epoch: 302000, elapsed: 1.10e+01, train loss: 3.30600e-06, val loss: 4.02874e-06, min loss: 3.29953e-06\n",
      "Epoch: 302100, elapsed: 1.09e+01, train loss: 3.32124e-06, val loss: 4.02226e-06, min loss: 3.29953e-06\n",
      "Epoch: 302200, elapsed: 1.08e+01, train loss: 3.61341e-06, val loss: 4.19613e-06, min loss: 3.29953e-06\n",
      "Epoch: 302300, elapsed: 1.10e+01, train loss: 4.98214e-06, val loss: 7.18498e-06, min loss: 3.29953e-06\n",
      "Epoch: 302400, elapsed: 1.10e+01, train loss: 1.25148e-05, val loss: 1.35490e-05, min loss: 3.29953e-06\n",
      "Epoch: 302500, elapsed: 1.19e+01, train loss: 1.13112e-05, val loss: 1.23018e-05, min loss: 3.29953e-06\n",
      "Epoch: 302600, elapsed: 1.09e+01, train loss: 3.50929e-06, val loss: 4.89160e-06, min loss: 3.29953e-06\n",
      "Epoch: 302700, elapsed: 1.10e+01, train loss: 1.20710e-05, val loss: 1.43457e-05, min loss: 3.29953e-06\n",
      "Epoch: 302800, elapsed: 1.08e+01, train loss: 3.41160e-06, val loss: 4.17557e-06, min loss: 3.29953e-06\n",
      "Epoch: 302900, elapsed: 1.10e+01, train loss: 4.61009e-06, val loss: 5.63305e-06, min loss: 3.29953e-06\n",
      "Epoch: 303000, elapsed: 1.10e+01, train loss: 4.07458e-06, val loss: 4.82292e-06, min loss: 3.29953e-06\n",
      "Epoch: 303100, elapsed: 1.09e+01, train loss: 5.75504e-06, val loss: 7.32766e-06, min loss: 3.29953e-06\n",
      "Epoch: 303200, elapsed: 1.18e+01, train loss: 3.28033e-06, val loss: 3.98541e-06, min loss: 3.28033e-06\n",
      "Epoch: 303300, elapsed: 1.12e+01, train loss: 3.31426e-06, val loss: 4.03209e-06, min loss: 3.28033e-06\n",
      "Epoch: 303400, elapsed: 1.10e+01, train loss: 3.30353e-06, val loss: 4.00162e-06, min loss: 3.28033e-06\n",
      "Epoch: 303500, elapsed: 1.09e+01, train loss: 3.38389e-06, val loss: 4.16973e-06, min loss: 3.28033e-06\n",
      "Epoch: 303600, elapsed: 1.08e+01, train loss: 3.37535e-06, val loss: 4.09679e-06, min loss: 3.28033e-06\n",
      "Epoch: 303700, elapsed: 1.09e+01, train loss: 3.39279e-06, val loss: 4.08718e-06, min loss: 3.28033e-06\n",
      "Epoch: 303800, elapsed: 1.08e+01, train loss: 3.43390e-06, val loss: 4.13801e-06, min loss: 3.28033e-06\n",
      "Epoch: 303900, elapsed: 1.20e+01, train loss: 8.56659e-06, val loss: 9.88740e-06, min loss: 3.28033e-06\n",
      "Epoch: 304000, elapsed: 1.10e+01, train loss: 5.25988e-06, val loss: 5.85249e-06, min loss: 3.28033e-06\n",
      "Epoch: 304100, elapsed: 1.09e+01, train loss: 3.27529e-06, val loss: 3.98458e-06, min loss: 3.27529e-06\n",
      "Epoch: 304200, elapsed: 1.09e+01, train loss: 3.27977e-06, val loss: 3.98623e-06, min loss: 3.27529e-06\n",
      "Epoch: 304300, elapsed: 1.10e+01, train loss: 3.28144e-06, val loss: 4.01562e-06, min loss: 3.27529e-06\n",
      "Epoch: 304400, elapsed: 1.08e+01, train loss: 3.57254e-06, val loss: 4.25014e-06, min loss: 3.27529e-06\n",
      "Epoch: 304500, elapsed: 1.10e+01, train loss: 3.27818e-06, val loss: 3.98320e-06, min loss: 3.27529e-06\n",
      "Epoch: 304600, elapsed: 1.19e+01, train loss: 3.54180e-06, val loss: 4.28816e-06, min loss: 3.27529e-06\n",
      "Epoch: 304700, elapsed: 1.11e+01, train loss: 4.86862e-06, val loss: 5.81188e-06, min loss: 3.27529e-06\n",
      "Epoch: 304800, elapsed: 1.09e+01, train loss: 4.99779e-06, val loss: 5.31606e-06, min loss: 3.27529e-06\n",
      "Epoch: 304900, elapsed: 1.09e+01, train loss: 3.30190e-06, val loss: 3.98566e-06, min loss: 3.27529e-06\n",
      "Epoch: 305000, elapsed: 1.10e+01, train loss: 5.34848e-06, val loss: 6.59807e-06, min loss: 3.27529e-06\n",
      "Epoch: 305100, elapsed: 1.33e+01, train loss: 4.11702e-06, val loss: 4.75571e-06, min loss: 3.27529e-06\n",
      "Epoch: 305200, elapsed: 1.19e+01, train loss: 3.33343e-06, val loss: 4.04201e-06, min loss: 3.27529e-06\n",
      "Epoch: 305300, elapsed: 1.12e+01, train loss: 3.48999e-06, val loss: 4.15199e-06, min loss: 3.27529e-06\n",
      "Epoch: 305400, elapsed: 1.12e+01, train loss: 8.48446e-06, val loss: 8.48363e-06, min loss: 3.27529e-06\n",
      "Epoch: 305500, elapsed: 1.09e+01, train loss: 3.89562e-06, val loss: 4.88677e-06, min loss: 3.27529e-06\n",
      "Epoch: 305600, elapsed: 1.09e+01, train loss: 3.30900e-06, val loss: 4.02019e-06, min loss: 3.27529e-06\n",
      "Epoch: 305700, elapsed: 1.08e+01, train loss: 4.98269e-06, val loss: 6.01867e-06, min loss: 3.27529e-06\n",
      "Epoch: 305800, elapsed: 1.09e+01, train loss: 1.35798e-05, val loss: 1.50751e-05, min loss: 3.27529e-06\n",
      "Epoch: 305900, elapsed: 1.21e+01, train loss: 3.50010e-06, val loss: 4.32407e-06, min loss: 3.27529e-06\n",
      "Epoch: 306000, elapsed: 1.11e+01, train loss: 3.25314e-06, val loss: 3.93336e-06, min loss: 3.25314e-06\n",
      "Epoch: 306100, elapsed: 1.10e+01, train loss: 3.38073e-06, val loss: 4.09401e-06, min loss: 3.25314e-06\n",
      "Epoch: 306200, elapsed: 1.11e+01, train loss: 6.96956e-06, val loss: 6.01563e-06, min loss: 3.25314e-06\n",
      "Epoch: 306300, elapsed: 1.08e+01, train loss: 3.69286e-06, val loss: 4.22397e-06, min loss: 3.25314e-06\n",
      "Epoch: 306400, elapsed: 1.09e+01, train loss: 3.68285e-06, val loss: 4.24011e-06, min loss: 3.25314e-06\n",
      "Epoch: 306500, elapsed: 1.09e+01, train loss: 3.70148e-06, val loss: 4.32337e-06, min loss: 3.25314e-06\n",
      "Epoch: 306600, elapsed: 1.11e+01, train loss: 3.23705e-06, val loss: 3.93699e-06, min loss: 3.23705e-06\n",
      "Epoch: 306700, elapsed: 1.20e+01, train loss: 3.31262e-06, val loss: 3.99608e-06, min loss: 3.23705e-06\n",
      "Epoch: 306800, elapsed: 1.10e+01, train loss: 8.70527e-06, val loss: 9.19397e-06, min loss: 3.23705e-06\n",
      "Epoch: 306900, elapsed: 1.08e+01, train loss: 3.49212e-06, val loss: 4.28420e-06, min loss: 3.23705e-06\n",
      "Epoch: 307000, elapsed: 1.10e+01, train loss: 4.66501e-06, val loss: 5.33497e-06, min loss: 3.23705e-06\n",
      "Epoch: 307100, elapsed: 1.08e+01, train loss: 3.36813e-06, val loss: 4.07998e-06, min loss: 3.23705e-06\n",
      "Epoch: 307200, elapsed: 1.09e+01, train loss: 3.69019e-06, val loss: 4.26135e-06, min loss: 3.23705e-06\n",
      "Epoch: 307300, elapsed: 1.10e+01, train loss: 3.30220e-06, val loss: 3.98388e-06, min loss: 3.23705e-06\n",
      "Epoch: 307400, elapsed: 1.19e+01, train loss: 3.41422e-06, val loss: 4.24267e-06, min loss: 3.23705e-06\n",
      "Epoch: 307500, elapsed: 1.10e+01, train loss: 7.05684e-06, val loss: 8.29676e-06, min loss: 3.23705e-06\n",
      "Epoch: 307600, elapsed: 1.10e+01, train loss: 3.53801e-06, val loss: 4.04553e-06, min loss: 3.23705e-06\n",
      "Epoch: 307700, elapsed: 1.08e+01, train loss: 3.53564e-06, val loss: 4.20649e-06, min loss: 3.23705e-06\n",
      "Epoch: 307800, elapsed: 1.10e+01, train loss: 4.06382e-06, val loss: 4.66535e-06, min loss: 3.23705e-06\n",
      "Epoch: 307900, elapsed: 1.11e+01, train loss: 3.21617e-06, val loss: 3.91382e-06, min loss: 3.21617e-06\n",
      "Epoch: 308000, elapsed: 1.10e+01, train loss: 3.26200e-06, val loss: 4.04509e-06, min loss: 3.21617e-06\n",
      "Epoch: 308100, elapsed: 1.18e+01, train loss: 6.58492e-06, val loss: 7.10107e-06, min loss: 3.21617e-06\n",
      "Epoch: 308200, elapsed: 1.11e+01, train loss: 3.58425e-06, val loss: 4.34264e-06, min loss: 3.21617e-06\n",
      "Epoch: 308300, elapsed: 1.08e+01, train loss: 3.25996e-06, val loss: 4.06570e-06, min loss: 3.21617e-06\n",
      "Epoch: 308400, elapsed: 1.09e+01, train loss: 3.49971e-06, val loss: 4.16965e-06, min loss: 3.21617e-06\n",
      "Epoch: 308500, elapsed: 1.11e+01, train loss: 4.25896e-06, val loss: 4.79333e-06, min loss: 3.21617e-06\n",
      "Epoch: 308600, elapsed: 1.10e+01, train loss: 4.87790e-06, val loss: 5.78308e-06, min loss: 3.21617e-06\n",
      "Epoch: 308700, elapsed: 1.09e+01, train loss: 3.45766e-06, val loss: 4.17405e-06, min loss: 3.21617e-06\n",
      "Epoch: 308800, elapsed: 1.20e+01, train loss: 4.84240e-06, val loss: 5.52738e-06, min loss: 3.21617e-06\n",
      "Epoch: 308900, elapsed: 1.10e+01, train loss: 4.25355e-06, val loss: 5.05903e-06, min loss: 3.21617e-06\n",
      "Epoch: 309000, elapsed: 1.10e+01, train loss: 5.28869e-06, val loss: 6.24826e-06, min loss: 3.21617e-06\n",
      "Epoch: 309100, elapsed: 1.09e+01, train loss: 3.92064e-06, val loss: 4.38015e-06, min loss: 3.21617e-06\n",
      "Epoch: 309200, elapsed: 1.08e+01, train loss: 3.25601e-06, val loss: 3.94184e-06, min loss: 3.21617e-06\n",
      "Epoch: 309300, elapsed: 1.10e+01, train loss: 3.31232e-06, val loss: 4.08052e-06, min loss: 3.21617e-06\n",
      "Epoch: 309400, elapsed: 1.09e+01, train loss: 3.40590e-06, val loss: 4.07732e-06, min loss: 3.21617e-06\n",
      "Epoch: 309500, elapsed: 1.20e+01, train loss: 4.16078e-06, val loss: 4.94138e-06, min loss: 3.21617e-06\n",
      "Epoch: 309600, elapsed: 1.10e+01, train loss: 3.74294e-06, val loss: 4.58399e-06, min loss: 3.21617e-06\n",
      "Epoch: 309700, elapsed: 1.10e+01, train loss: 3.42421e-06, val loss: 4.13252e-06, min loss: 3.21617e-06\n",
      "Epoch: 309800, elapsed: 1.10e+01, train loss: 3.81906e-06, val loss: 4.09862e-06, min loss: 3.21617e-06\n",
      "Epoch: 309900, elapsed: 1.08e+01, train loss: 3.26038e-06, val loss: 3.97367e-06, min loss: 3.21617e-06\n",
      "Epoch: 310000, elapsed: 1.11e+01, train loss: 3.62499e-06, val loss: 4.30205e-06, min loss: 3.21617e-06\n",
      "Epoch: 310100, elapsed: 1.39e+01, train loss: 1.22703e-05, val loss: 1.42957e-05, min loss: 3.21617e-06\n",
      "Epoch: 310200, elapsed: 1.10e+01, train loss: 3.45244e-06, val loss: 4.23676e-06, min loss: 3.21617e-06\n",
      "Epoch: 310300, elapsed: 1.09e+01, train loss: 4.04180e-06, val loss: 5.09103e-06, min loss: 3.21617e-06\n",
      "Epoch: 310400, elapsed: 1.09e+01, train loss: 4.06336e-06, val loss: 4.65414e-06, min loss: 3.21617e-06\n",
      "Epoch: 310500, elapsed: 1.08e+01, train loss: 3.50496e-06, val loss: 4.21345e-06, min loss: 3.21617e-06\n",
      "Epoch: 310600, elapsed: 1.09e+01, train loss: 4.46105e-06, val loss: 5.25070e-06, min loss: 3.21617e-06\n",
      "Epoch: 310700, elapsed: 1.09e+01, train loss: 4.31311e-06, val loss: 5.10745e-06, min loss: 3.21617e-06\n",
      "Epoch: 310800, elapsed: 1.21e+01, train loss: 3.74136e-06, val loss: 4.69915e-06, min loss: 3.21617e-06\n",
      "Epoch: 310900, elapsed: 1.10e+01, train loss: 3.20505e-06, val loss: 3.89989e-06, min loss: 3.20505e-06\n",
      "Epoch: 311000, elapsed: 1.09e+01, train loss: 8.39666e-06, val loss: 8.16859e-06, min loss: 3.20505e-06\n",
      "Epoch: 311100, elapsed: 1.09e+01, train loss: 3.41329e-06, val loss: 4.15399e-06, min loss: 3.20505e-06\n",
      "Epoch: 311200, elapsed: 1.09e+01, train loss: 3.17216e-06, val loss: 3.85087e-06, min loss: 3.17216e-06\n",
      "Epoch: 311300, elapsed: 1.10e+01, train loss: 3.18948e-06, val loss: 3.87435e-06, min loss: 3.17216e-06\n",
      "Epoch: 311400, elapsed: 1.09e+01, train loss: 3.30774e-06, val loss: 4.23707e-06, min loss: 3.17216e-06\n",
      "Epoch: 311500, elapsed: 1.19e+01, train loss: 4.17921e-06, val loss: 4.49771e-06, min loss: 3.17216e-06\n",
      "Epoch: 311600, elapsed: 1.11e+01, train loss: 3.70809e-06, val loss: 4.19875e-06, min loss: 3.17216e-06\n",
      "Epoch: 311700, elapsed: 1.11e+01, train loss: 3.23959e-06, val loss: 3.89020e-06, min loss: 3.17216e-06\n",
      "Epoch: 311800, elapsed: 1.09e+01, train loss: 3.18056e-06, val loss: 3.88456e-06, min loss: 3.17216e-06\n",
      "Epoch: 311900, elapsed: 1.09e+01, train loss: 3.20181e-06, val loss: 3.93264e-06, min loss: 3.17216e-06\n",
      "Epoch: 312000, elapsed: 1.11e+01, train loss: 3.79516e-06, val loss: 4.38719e-06, min loss: 3.17216e-06\n",
      "Epoch: 312100, elapsed: 1.11e+01, train loss: 1.30261e-05, val loss: 1.40033e-05, min loss: 3.17216e-06\n",
      "Epoch: 312200, elapsed: 1.09e+01, train loss: 7.77277e-06, val loss: 9.16039e-06, min loss: 3.17216e-06\n",
      "Epoch: 312300, elapsed: 1.21e+01, train loss: 1.48895e-05, val loss: 1.37070e-05, min loss: 3.17216e-06\n",
      "Epoch: 312400, elapsed: 1.08e+01, train loss: 4.35519e-06, val loss: 5.07914e-06, min loss: 3.17216e-06\n",
      "Epoch: 312500, elapsed: 1.09e+01, train loss: 3.15795e-06, val loss: 3.84320e-06, min loss: 3.15795e-06\n",
      "Epoch: 312600, elapsed: 1.09e+01, train loss: 3.27102e-06, val loss: 3.88130e-06, min loss: 3.15795e-06\n",
      "Epoch: 312700, elapsed: 1.09e+01, train loss: 3.36807e-06, val loss: 3.99745e-06, min loss: 3.15795e-06\n",
      "Epoch: 312800, elapsed: 1.09e+01, train loss: 5.81805e-06, val loss: 6.75712e-06, min loss: 3.15795e-06\n",
      "Epoch: 312900, elapsed: 1.09e+01, train loss: 3.65108e-06, val loss: 4.30997e-06, min loss: 3.15795e-06\n",
      "Epoch: 313000, elapsed: 1.18e+01, train loss: 3.75115e-06, val loss: 4.61066e-06, min loss: 3.15795e-06\n",
      "Epoch: 313100, elapsed: 1.11e+01, train loss: 4.21898e-06, val loss: 4.29010e-06, min loss: 3.15795e-06\n",
      "Epoch: 313200, elapsed: 1.10e+01, train loss: 5.01326e-06, val loss: 5.22909e-06, min loss: 3.15795e-06\n",
      "Epoch: 313300, elapsed: 1.10e+01, train loss: 3.39972e-06, val loss: 4.06864e-06, min loss: 3.15795e-06\n",
      "Epoch: 313400, elapsed: 1.10e+01, train loss: 4.09490e-06, val loss: 4.85972e-06, min loss: 3.15795e-06\n",
      "Epoch: 313500, elapsed: 1.10e+01, train loss: 4.88612e-06, val loss: 5.45792e-06, min loss: 3.15795e-06\n",
      "Epoch: 313600, elapsed: 1.11e+01, train loss: 3.43132e-06, val loss: 3.98931e-06, min loss: 3.15795e-06\n",
      "Epoch: 313700, elapsed: 1.22e+01, train loss: 3.31080e-06, val loss: 4.46187e-06, min loss: 3.15795e-06\n",
      "Epoch: 313800, elapsed: 1.10e+01, train loss: 4.45656e-06, val loss: 5.73569e-06, min loss: 3.15795e-06\n",
      "Epoch: 313900, elapsed: 1.10e+01, train loss: 4.00778e-06, val loss: 4.68843e-06, min loss: 3.15795e-06\n",
      "Epoch: 314000, elapsed: 1.09e+01, train loss: 3.49750e-06, val loss: 4.06780e-06, min loss: 3.15795e-06\n",
      "Epoch: 314100, elapsed: 1.10e+01, train loss: 3.75413e-06, val loss: 4.43384e-06, min loss: 3.15795e-06\n",
      "Epoch: 314200, elapsed: 1.10e+01, train loss: 3.24237e-06, val loss: 3.95571e-06, min loss: 3.15795e-06\n",
      "Epoch: 314300, elapsed: 1.09e+01, train loss: 3.93118e-06, val loss: 4.12034e-06, min loss: 3.15795e-06\n",
      "Epoch: 314400, elapsed: 1.21e+01, train loss: 4.54877e-06, val loss: 4.70434e-06, min loss: 3.15795e-06\n",
      "Epoch: 314500, elapsed: 1.10e+01, train loss: 4.05477e-06, val loss: 4.70624e-06, min loss: 3.15795e-06\n",
      "Epoch: 314600, elapsed: 1.10e+01, train loss: 3.92792e-06, val loss: 5.25167e-06, min loss: 3.15795e-06\n",
      "Epoch: 314700, elapsed: 1.08e+01, train loss: 7.08532e-06, val loss: 9.03153e-06, min loss: 3.15795e-06\n",
      "Epoch: 314800, elapsed: 1.10e+01, train loss: 4.72567e-06, val loss: 5.80299e-06, min loss: 3.15795e-06\n",
      "Epoch: 314900, elapsed: 1.09e+01, train loss: 4.33441e-06, val loss: 5.19570e-06, min loss: 3.15795e-06\n",
      "Epoch: 315000, elapsed: 1.11e+01, train loss: 3.20112e-06, val loss: 3.94306e-06, min loss: 3.15795e-06\n",
      "Epoch: 315100, elapsed: 1.40e+01, train loss: 3.15717e-06, val loss: 3.84321e-06, min loss: 3.15717e-06\n",
      "Epoch: 315200, elapsed: 1.10e+01, train loss: 3.17947e-06, val loss: 3.92914e-06, min loss: 3.15717e-06\n",
      "Epoch: 315300, elapsed: 1.08e+01, train loss: 3.48216e-06, val loss: 4.29774e-06, min loss: 3.15717e-06\n",
      "Epoch: 315400, elapsed: 1.10e+01, train loss: 3.29376e-06, val loss: 3.99371e-06, min loss: 3.15717e-06\n",
      "Epoch: 315500, elapsed: 1.10e+01, train loss: 3.38972e-06, val loss: 4.07806e-06, min loss: 3.15717e-06\n",
      "Epoch: 315600, elapsed: 1.09e+01, train loss: 6.57894e-06, val loss: 7.09430e-06, min loss: 3.15717e-06\n",
      "Epoch: 315700, elapsed: 1.09e+01, train loss: 3.22553e-06, val loss: 3.81712e-06, min loss: 3.15717e-06\n",
      "Epoch: 315800, elapsed: 1.20e+01, train loss: 3.30578e-06, val loss: 3.98440e-06, min loss: 3.15717e-06\n",
      "Epoch: 315900, elapsed: 1.10e+01, train loss: 3.14104e-06, val loss: 3.86223e-06, min loss: 3.14104e-06\n",
      "Epoch: 316000, elapsed: 1.10e+01, train loss: 3.26128e-06, val loss: 3.96954e-06, min loss: 3.14104e-06\n",
      "Epoch: 316100, elapsed: 1.09e+01, train loss: 6.57108e-06, val loss: 6.00417e-06, min loss: 3.14104e-06\n",
      "Epoch: 316200, elapsed: 1.08e+01, train loss: 3.18433e-06, val loss: 3.90222e-06, min loss: 3.14104e-06\n",
      "Epoch: 316300, elapsed: 1.09e+01, train loss: 3.12642e-06, val loss: 3.82680e-06, min loss: 3.12642e-06\n",
      "Epoch: 316400, elapsed: 1.09e+01, train loss: 3.21204e-06, val loss: 3.85886e-06, min loss: 3.12642e-06\n",
      "Epoch: 316500, elapsed: 1.20e+01, train loss: 3.21871e-06, val loss: 3.86454e-06, min loss: 3.12642e-06\n",
      "Epoch: 316600, elapsed: 1.12e+01, train loss: 5.25907e-06, val loss: 5.66900e-06, min loss: 3.12642e-06\n",
      "Epoch: 316700, elapsed: 1.09e+01, train loss: 3.57573e-06, val loss: 4.75730e-06, min loss: 3.12642e-06\n",
      "Epoch: 316800, elapsed: 1.11e+01, train loss: 4.02588e-06, val loss: 4.75485e-06, min loss: 3.12642e-06\n",
      "Epoch: 316900, elapsed: 1.09e+01, train loss: 3.34537e-06, val loss: 4.05085e-06, min loss: 3.12642e-06\n",
      "Epoch: 317000, elapsed: 1.10e+01, train loss: 3.23240e-06, val loss: 3.86786e-06, min loss: 3.12642e-06\n",
      "Epoch: 317100, elapsed: 1.09e+01, train loss: 3.18202e-06, val loss: 3.82311e-06, min loss: 3.12642e-06\n",
      "Epoch: 317200, elapsed: 1.18e+01, train loss: 3.52216e-06, val loss: 4.26652e-06, min loss: 3.12642e-06\n",
      "Epoch: 317300, elapsed: 1.11e+01, train loss: 5.95263e-06, val loss: 4.62371e-06, min loss: 3.12642e-06\n",
      "Epoch: 317400, elapsed: 1.09e+01, train loss: 1.00795e-05, val loss: 1.18048e-05, min loss: 3.12642e-06\n",
      "Epoch: 317500, elapsed: 1.09e+01, train loss: 1.12061e-05, val loss: 1.29241e-05, min loss: 3.12642e-06\n",
      "Epoch: 317600, elapsed: 1.08e+01, train loss: 1.05097e-05, val loss: 1.20013e-05, min loss: 3.12642e-06\n",
      "Epoch: 317700, elapsed: 1.09e+01, train loss: 5.09445e-06, val loss: 4.60571e-06, min loss: 3.12642e-06\n",
      "Epoch: 317800, elapsed: 1.08e+01, train loss: 3.55065e-06, val loss: 3.88586e-06, min loss: 3.12642e-06\n",
      "Epoch: 317900, elapsed: 1.20e+01, train loss: 3.55911e-06, val loss: 3.87485e-06, min loss: 3.12642e-06\n",
      "Epoch: 318000, elapsed: 1.11e+01, train loss: 3.96137e-06, val loss: 4.62898e-06, min loss: 3.12642e-06\n",
      "Epoch: 318100, elapsed: 1.11e+01, train loss: 3.16686e-06, val loss: 3.89473e-06, min loss: 3.12642e-06\n",
      "Epoch: 318200, elapsed: 1.11e+01, train loss: 3.20587e-06, val loss: 3.88784e-06, min loss: 3.12642e-06\n",
      "Epoch: 318300, elapsed: 1.10e+01, train loss: 7.27851e-06, val loss: 8.86031e-06, min loss: 3.12642e-06\n",
      "Epoch: 318400, elapsed: 1.10e+01, train loss: 9.01749e-06, val loss: 8.29676e-06, min loss: 3.12642e-06\n",
      "Epoch: 318500, elapsed: 1.09e+01, train loss: 3.08310e-06, val loss: 3.82416e-06, min loss: 3.08310e-06\n",
      "Epoch: 318600, elapsed: 1.09e+01, train loss: 3.41040e-06, val loss: 4.16372e-06, min loss: 3.08310e-06\n",
      "Epoch: 318700, elapsed: 1.20e+01, train loss: 3.53947e-06, val loss: 4.31163e-06, min loss: 3.08310e-06\n",
      "Epoch: 318800, elapsed: 1.11e+01, train loss: 3.82800e-06, val loss: 4.58734e-06, min loss: 3.08310e-06\n",
      "Epoch: 318900, elapsed: 1.10e+01, train loss: 9.28370e-06, val loss: 1.05946e-05, min loss: 3.08310e-06\n",
      "Epoch: 319000, elapsed: 1.09e+01, train loss: 3.12487e-06, val loss: 4.06282e-06, min loss: 3.08310e-06\n",
      "Epoch: 319100, elapsed: 1.09e+01, train loss: 3.26109e-06, val loss: 4.02576e-06, min loss: 3.08310e-06\n",
      "Epoch: 319200, elapsed: 1.10e+01, train loss: 5.49121e-06, val loss: 6.70340e-06, min loss: 3.08310e-06\n",
      "Epoch: 319300, elapsed: 1.11e+01, train loss: 4.36070e-06, val loss: 5.00476e-06, min loss: 3.08310e-06\n",
      "Epoch: 319400, elapsed: 1.21e+01, train loss: 4.95017e-06, val loss: 5.98694e-06, min loss: 3.08310e-06\n",
      "Epoch: 319500, elapsed: 1.12e+01, train loss: 3.79282e-06, val loss: 4.41597e-06, min loss: 3.08310e-06\n",
      "Epoch: 319600, elapsed: 1.11e+01, train loss: 4.47479e-06, val loss: 5.41542e-06, min loss: 3.08310e-06\n",
      "Epoch: 319700, elapsed: 1.09e+01, train loss: 3.89044e-06, val loss: 4.42978e-06, min loss: 3.08310e-06\n",
      "Epoch: 319800, elapsed: 1.10e+01, train loss: 1.10842e-05, val loss: 1.31627e-05, min loss: 3.08310e-06\n",
      "Epoch: 319900, elapsed: 1.12e+01, train loss: 3.16540e-06, val loss: 3.90054e-06, min loss: 3.08310e-06\n",
      "Epoch: 320000, elapsed: 1.11e+01, train loss: 3.14543e-06, val loss: 3.84551e-06, min loss: 3.08310e-06\n",
      "Epoch: 320100, elapsed: 1.41e+01, train loss: 3.31998e-06, val loss: 4.02151e-06, min loss: 3.08310e-06\n",
      "Epoch: 320200, elapsed: 1.10e+01, train loss: 7.68169e-06, val loss: 8.91325e-06, min loss: 3.08310e-06\n",
      "Epoch: 320300, elapsed: 1.11e+01, train loss: 3.91237e-06, val loss: 4.38197e-06, min loss: 3.08310e-06\n",
      "Epoch: 320400, elapsed: 1.09e+01, train loss: 3.12409e-06, val loss: 3.76323e-06, min loss: 3.08310e-06\n",
      "Epoch: 320500, elapsed: 1.10e+01, train loss: 6.99739e-06, val loss: 8.78553e-06, min loss: 3.08310e-06\n",
      "Epoch: 320600, elapsed: 1.09e+01, train loss: 7.90587e-06, val loss: 9.34463e-06, min loss: 3.08310e-06\n",
      "Epoch: 320700, elapsed: 1.10e+01, train loss: 4.78860e-06, val loss: 5.06427e-06, min loss: 3.08310e-06\n",
      "Epoch: 320800, elapsed: 1.21e+01, train loss: 3.35781e-06, val loss: 3.96406e-06, min loss: 3.08310e-06\n",
      "Epoch: 320900, elapsed: 1.11e+01, train loss: 4.54957e-06, val loss: 5.37621e-06, min loss: 3.08310e-06\n",
      "Epoch: 321000, elapsed: 1.11e+01, train loss: 4.07478e-06, val loss: 4.74140e-06, min loss: 3.08310e-06\n",
      "Epoch: 321100, elapsed: 1.11e+01, train loss: 3.07999e-06, val loss: 3.82042e-06, min loss: 3.07999e-06\n",
      "Epoch: 321200, elapsed: 1.09e+01, train loss: 3.17572e-06, val loss: 3.90428e-06, min loss: 3.07999e-06\n",
      "Epoch: 321300, elapsed: 1.09e+01, train loss: 6.69252e-06, val loss: 7.75479e-06, min loss: 3.07999e-06\n",
      "Epoch: 321400, elapsed: 1.09e+01, train loss: 3.10397e-06, val loss: 3.82069e-06, min loss: 3.07999e-06\n",
      "Epoch: 321500, elapsed: 1.21e+01, train loss: 8.13810e-06, val loss: 9.12704e-06, min loss: 3.07999e-06\n",
      "Epoch: 321600, elapsed: 1.13e+01, train loss: 3.10440e-06, val loss: 3.74794e-06, min loss: 3.07999e-06\n",
      "Epoch: 321700, elapsed: 1.11e+01, train loss: 5.73561e-06, val loss: 6.07929e-06, min loss: 3.07999e-06\n",
      "Epoch: 321800, elapsed: 1.10e+01, train loss: 3.28130e-06, val loss: 4.31291e-06, min loss: 3.07999e-06\n",
      "Epoch: 321900, elapsed: 1.09e+01, train loss: 3.29600e-06, val loss: 3.93616e-06, min loss: 3.07999e-06\n",
      "Epoch: 322000, elapsed: 1.09e+01, train loss: 3.06357e-06, val loss: 3.74043e-06, min loss: 3.06357e-06\n",
      "Epoch: 322100, elapsed: 1.10e+01, train loss: 3.06004e-06, val loss: 3.75961e-06, min loss: 3.06004e-06\n",
      "Epoch: 322200, elapsed: 1.22e+01, train loss: 3.38588e-06, val loss: 4.08551e-06, min loss: 3.06004e-06\n",
      "Epoch: 322300, elapsed: 1.10e+01, train loss: 3.45900e-06, val loss: 3.93080e-06, min loss: 3.06004e-06\n",
      "Epoch: 322400, elapsed: 1.09e+01, train loss: 3.25399e-06, val loss: 3.82587e-06, min loss: 3.06004e-06\n",
      "Epoch: 322500, elapsed: 1.10e+01, train loss: 3.05388e-06, val loss: 3.72971e-06, min loss: 3.05388e-06\n",
      "Epoch: 322600, elapsed: 1.10e+01, train loss: 3.04840e-06, val loss: 3.73912e-06, min loss: 3.04840e-06\n",
      "Epoch: 322700, elapsed: 1.09e+01, train loss: 3.21976e-06, val loss: 3.96679e-06, min loss: 3.04840e-06\n",
      "Epoch: 322800, elapsed: 1.09e+01, train loss: 6.08512e-06, val loss: 7.17866e-06, min loss: 3.04840e-06\n",
      "Epoch: 322900, elapsed: 1.10e+01, train loss: 4.25077e-06, val loss: 4.99765e-06, min loss: 3.04840e-06\n",
      "Epoch: 323000, elapsed: 1.23e+01, train loss: 4.86969e-06, val loss: 5.70043e-06, min loss: 3.04840e-06\n",
      "Epoch: 323100, elapsed: 1.12e+01, train loss: 3.41863e-06, val loss: 4.44284e-06, min loss: 3.04840e-06\n",
      "Epoch: 323200, elapsed: 1.10e+01, train loss: 6.07063e-06, val loss: 7.39530e-06, min loss: 3.04840e-06\n",
      "Epoch: 323300, elapsed: 1.10e+01, train loss: 4.21929e-06, val loss: 5.03700e-06, min loss: 3.04840e-06\n",
      "Epoch: 323400, elapsed: 1.09e+01, train loss: 3.20618e-06, val loss: 3.83500e-06, min loss: 3.04840e-06\n",
      "Epoch: 323500, elapsed: 1.09e+01, train loss: 3.04787e-06, val loss: 3.72941e-06, min loss: 3.04787e-06\n",
      "Epoch: 323600, elapsed: 1.10e+01, train loss: 3.02016e-06, val loss: 3.71954e-06, min loss: 3.02016e-06\n",
      "Epoch: 323700, elapsed: 1.22e+01, train loss: 3.03388e-06, val loss: 3.73054e-06, min loss: 3.02016e-06\n",
      "Epoch: 323800, elapsed: 1.10e+01, train loss: 3.20122e-06, val loss: 3.96353e-06, min loss: 3.02016e-06\n",
      "Epoch: 323900, elapsed: 1.10e+01, train loss: 6.40936e-06, val loss: 6.66364e-06, min loss: 3.02016e-06\n",
      "Epoch: 324000, elapsed: 1.10e+01, train loss: 5.86788e-06, val loss: 7.05241e-06, min loss: 3.02016e-06\n",
      "Epoch: 324100, elapsed: 1.09e+01, train loss: 3.15225e-06, val loss: 4.17660e-06, min loss: 3.02016e-06\n",
      "Epoch: 324200, elapsed: 1.10e+01, train loss: 3.12318e-06, val loss: 3.72215e-06, min loss: 3.02016e-06\n",
      "Epoch: 324300, elapsed: 1.10e+01, train loss: 3.29880e-06, val loss: 3.94915e-06, min loss: 3.02016e-06\n",
      "Epoch: 324400, elapsed: 1.22e+01, train loss: 3.13342e-06, val loss: 3.90712e-06, min loss: 3.02016e-06\n",
      "Epoch: 324500, elapsed: 1.10e+01, train loss: 3.03325e-06, val loss: 3.70645e-06, min loss: 3.02016e-06\n",
      "Epoch: 324600, elapsed: 1.10e+01, train loss: 3.01144e-06, val loss: 3.70480e-06, min loss: 3.01144e-06\n",
      "Epoch: 324700, elapsed: 1.11e+01, train loss: 3.02465e-06, val loss: 3.69688e-06, min loss: 3.01144e-06\n",
      "Epoch: 324800, elapsed: 1.10e+01, train loss: 3.04091e-06, val loss: 3.72752e-06, min loss: 3.01144e-06\n",
      "Epoch: 324900, elapsed: 1.10e+01, train loss: 5.67648e-06, val loss: 5.55876e-06, min loss: 3.01144e-06\n",
      "Epoch: 325000, elapsed: 1.08e+01, train loss: 3.32294e-06, val loss: 4.10581e-06, min loss: 3.01144e-06\n",
      "Epoch: 325100, elapsed: 1.43e+01, train loss: 3.00480e-06, val loss: 3.70649e-06, min loss: 3.00480e-06\n",
      "Epoch: 325200, elapsed: 1.11e+01, train loss: 3.00395e-06, val loss: 3.70933e-06, min loss: 3.00395e-06\n",
      "Epoch: 325300, elapsed: 1.11e+01, train loss: 3.00717e-06, val loss: 3.71347e-06, min loss: 3.00395e-06\n",
      "Epoch: 325400, elapsed: 1.12e+01, train loss: 3.80954e-06, val loss: 4.63435e-06, min loss: 3.00395e-06\n",
      "Epoch: 325500, elapsed: 1.10e+01, train loss: 3.45204e-06, val loss: 4.27029e-06, min loss: 3.00395e-06\n",
      "Epoch: 325600, elapsed: 1.09e+01, train loss: 9.75327e-06, val loss: 1.04378e-05, min loss: 3.00395e-06\n",
      "Epoch: 325700, elapsed: 1.11e+01, train loss: 3.69729e-06, val loss: 4.90992e-06, min loss: 3.00395e-06\n",
      "Epoch: 325800, elapsed: 1.23e+01, train loss: 3.47232e-06, val loss: 4.62896e-06, min loss: 3.00395e-06\n",
      "Epoch: 325900, elapsed: 1.11e+01, train loss: 5.26473e-06, val loss: 5.78733e-06, min loss: 3.00395e-06\n",
      "Epoch: 326000, elapsed: 1.11e+01, train loss: 3.06701e-06, val loss: 4.19489e-06, min loss: 3.00395e-06\n",
      "Epoch: 326100, elapsed: 1.11e+01, train loss: 3.48009e-06, val loss: 4.32536e-06, min loss: 3.00395e-06\n",
      "Epoch: 326200, elapsed: 1.10e+01, train loss: 3.06633e-06, val loss: 3.78423e-06, min loss: 3.00395e-06\n",
      "Epoch: 326300, elapsed: 1.10e+01, train loss: 3.01832e-06, val loss: 3.71255e-06, min loss: 3.00395e-06\n",
      "Epoch: 326400, elapsed: 1.09e+01, train loss: 3.05064e-06, val loss: 3.74261e-06, min loss: 3.00395e-06\n",
      "Epoch: 326500, elapsed: 1.12e+01, train loss: 3.91732e-06, val loss: 4.77966e-06, min loss: 3.00395e-06\n",
      "Epoch: 326600, elapsed: 1.21e+01, train loss: 5.98719e-06, val loss: 6.95540e-06, min loss: 3.00395e-06\n",
      "Epoch: 326700, elapsed: 1.11e+01, train loss: 4.47053e-06, val loss: 4.81464e-06, min loss: 3.00395e-06\n",
      "Epoch: 326800, elapsed: 1.09e+01, train loss: 3.10122e-06, val loss: 3.68849e-06, min loss: 3.00395e-06\n",
      "Epoch: 326900, elapsed: 1.11e+01, train loss: 4.04377e-06, val loss: 4.23301e-06, min loss: 3.00395e-06\n",
      "Epoch: 327000, elapsed: 1.10e+01, train loss: 7.73047e-06, val loss: 7.27512e-06, min loss: 3.00395e-06\n",
      "Epoch: 327100, elapsed: 1.09e+01, train loss: 4.36690e-06, val loss: 5.25037e-06, min loss: 3.00395e-06\n",
      "Epoch: 327200, elapsed: 1.10e+01, train loss: 5.35380e-06, val loss: 5.85384e-06, min loss: 3.00395e-06\n",
      "Epoch: 327300, elapsed: 1.20e+01, train loss: 3.65280e-06, val loss: 4.30177e-06, min loss: 3.00395e-06\n",
      "Epoch: 327400, elapsed: 1.10e+01, train loss: 2.99722e-06, val loss: 3.69630e-06, min loss: 2.99722e-06\n",
      "Epoch: 327500, elapsed: 1.11e+01, train loss: 3.02619e-06, val loss: 3.70762e-06, min loss: 2.99722e-06\n",
      "Epoch: 327600, elapsed: 1.10e+01, train loss: 3.00078e-06, val loss: 3.67768e-06, min loss: 2.99722e-06\n",
      "Epoch: 327700, elapsed: 1.10e+01, train loss: 6.90055e-06, val loss: 7.01258e-06, min loss: 2.99722e-06\n",
      "Epoch: 327800, elapsed: 1.10e+01, train loss: 3.03657e-06, val loss: 4.14603e-06, min loss: 2.99722e-06\n",
      "Epoch: 327900, elapsed: 1.11e+01, train loss: 4.29749e-06, val loss: 4.63722e-06, min loss: 2.99722e-06\n",
      "Epoch: 328000, elapsed: 1.22e+01, train loss: 3.06128e-06, val loss: 3.72816e-06, min loss: 2.99722e-06\n",
      "Epoch: 328100, elapsed: 1.10e+01, train loss: 3.30660e-06, val loss: 4.09342e-06, min loss: 2.99722e-06\n",
      "Epoch: 328200, elapsed: 1.09e+01, train loss: 2.99398e-06, val loss: 3.69794e-06, min loss: 2.99398e-06\n",
      "Epoch: 328300, elapsed: 1.11e+01, train loss: 3.23504e-06, val loss: 4.13877e-06, min loss: 2.99398e-06\n",
      "Epoch: 328400, elapsed: 1.10e+01, train loss: 3.00227e-06, val loss: 3.71067e-06, min loss: 2.99398e-06\n",
      "Epoch: 328500, elapsed: 1.11e+01, train loss: 3.16543e-06, val loss: 3.90315e-06, min loss: 2.99398e-06\n",
      "Epoch: 328600, elapsed: 1.11e+01, train loss: 4.92586e-06, val loss: 5.22607e-06, min loss: 2.99398e-06\n",
      "Epoch: 328700, elapsed: 1.10e+01, train loss: 3.40319e-06, val loss: 4.04116e-06, min loss: 2.99398e-06\n",
      "Epoch: 328800, elapsed: 1.20e+01, train loss: 3.03491e-06, val loss: 3.76211e-06, min loss: 2.99398e-06\n",
      "Epoch: 328900, elapsed: 1.11e+01, train loss: 3.09857e-06, val loss: 3.72928e-06, min loss: 2.99398e-06\n",
      "Epoch: 329000, elapsed: 1.10e+01, train loss: 3.40956e-06, val loss: 4.10014e-06, min loss: 2.99398e-06\n",
      "Epoch: 329100, elapsed: 1.09e+01, train loss: 3.47759e-06, val loss: 4.26354e-06, min loss: 2.99398e-06\n",
      "Epoch: 329200, elapsed: 1.10e+01, train loss: 2.99761e-06, val loss: 3.70210e-06, min loss: 2.99398e-06\n",
      "Epoch: 329300, elapsed: 1.10e+01, train loss: 3.01608e-06, val loss: 3.69828e-06, min loss: 2.99398e-06\n",
      "Epoch: 329400, elapsed: 1.11e+01, train loss: 4.33240e-06, val loss: 4.92772e-06, min loss: 2.99398e-06\n",
      "Epoch: 329500, elapsed: 1.21e+01, train loss: 3.29882e-06, val loss: 3.97635e-06, min loss: 2.99398e-06\n",
      "Epoch: 329600, elapsed: 1.10e+01, train loss: 3.60455e-06, val loss: 4.14326e-06, min loss: 2.99398e-06\n",
      "Epoch: 329700, elapsed: 1.09e+01, train loss: 5.01369e-06, val loss: 5.92067e-06, min loss: 2.99398e-06\n",
      "Epoch: 329800, elapsed: 1.11e+01, train loss: 3.45571e-06, val loss: 3.89679e-06, min loss: 2.99398e-06\n",
      "Epoch: 329900, elapsed: 1.09e+01, train loss: 3.00721e-06, val loss: 3.68635e-06, min loss: 2.99398e-06\n",
      "Epoch: 330000, elapsed: 1.09e+01, train loss: 3.07140e-06, val loss: 3.74082e-06, min loss: 2.99398e-06\n",
      "Epoch: 330100, elapsed: 1.31e+01, train loss: 3.04137e-06, val loss: 3.80617e-06, min loss: 2.99398e-06\n",
      "Epoch: 330200, elapsed: 1.20e+01, train loss: 3.10407e-06, val loss: 3.80326e-06, min loss: 2.99398e-06\n",
      "Epoch: 330300, elapsed: 1.10e+01, train loss: 6.92870e-06, val loss: 7.16732e-06, min loss: 2.99398e-06\n",
      "Epoch: 330400, elapsed: 1.10e+01, train loss: 2.97456e-06, val loss: 3.69897e-06, min loss: 2.97456e-06\n",
      "Epoch: 330500, elapsed: 1.09e+01, train loss: 3.37053e-06, val loss: 4.15186e-06, min loss: 2.97456e-06\n",
      "Epoch: 330600, elapsed: 1.08e+01, train loss: 2.97942e-06, val loss: 3.88460e-06, min loss: 2.97456e-06\n",
      "Epoch: 330700, elapsed: 1.10e+01, train loss: 3.61090e-06, val loss: 4.03595e-06, min loss: 2.97456e-06\n",
      "Epoch: 330800, elapsed: 1.10e+01, train loss: 3.01553e-06, val loss: 3.66880e-06, min loss: 2.97456e-06\n",
      "Epoch: 330900, elapsed: 1.11e+01, train loss: 3.05158e-06, val loss: 3.74712e-06, min loss: 2.97456e-06\n",
      "Epoch: 331000, elapsed: 1.22e+01, train loss: 5.45198e-06, val loss: 4.44178e-06, min loss: 2.97456e-06\n",
      "Epoch: 331100, elapsed: 1.10e+01, train loss: 4.47836e-06, val loss: 4.27517e-06, min loss: 2.97456e-06\n",
      "Epoch: 331200, elapsed: 1.10e+01, train loss: 4.30878e-06, val loss: 4.67107e-06, min loss: 2.97456e-06\n",
      "Epoch: 331300, elapsed: 1.09e+01, train loss: 4.65746e-06, val loss: 4.55865e-06, min loss: 2.97456e-06\n",
      "Epoch: 331400, elapsed: 1.10e+01, train loss: 5.04590e-06, val loss: 4.31662e-06, min loss: 2.97456e-06\n",
      "Epoch: 331500, elapsed: 1.11e+01, train loss: 5.34362e-06, val loss: 6.84970e-06, min loss: 2.97456e-06\n",
      "Epoch: 331600, elapsed: 1.10e+01, train loss: 3.75729e-06, val loss: 4.69573e-06, min loss: 2.97456e-06\n",
      "Epoch: 331700, elapsed: 1.20e+01, train loss: 3.30508e-06, val loss: 4.27743e-06, min loss: 2.97456e-06\n",
      "Epoch: 331800, elapsed: 1.10e+01, train loss: 8.20183e-06, val loss: 7.32513e-06, min loss: 2.97456e-06\n",
      "Epoch: 331900, elapsed: 1.10e+01, train loss: 3.36821e-06, val loss: 3.91297e-06, min loss: 2.97456e-06\n",
      "Epoch: 332000, elapsed: 1.10e+01, train loss: 4.83008e-06, val loss: 4.78571e-06, min loss: 2.97456e-06\n",
      "Epoch: 332100, elapsed: 1.07e+01, train loss: 3.11062e-06, val loss: 3.72496e-06, min loss: 2.97456e-06\n",
      "Epoch: 332200, elapsed: 1.11e+01, train loss: 3.00593e-06, val loss: 3.70758e-06, min loss: 2.97456e-06\n",
      "Epoch: 332300, elapsed: 1.10e+01, train loss: 5.65139e-06, val loss: 5.75543e-06, min loss: 2.97456e-06\n",
      "Epoch: 332400, elapsed: 1.21e+01, train loss: 5.12588e-06, val loss: 5.27077e-06, min loss: 2.97456e-06\n",
      "Epoch: 332500, elapsed: 1.11e+01, train loss: 3.01289e-06, val loss: 3.66763e-06, min loss: 2.97456e-06\n",
      "Epoch: 332600, elapsed: 1.11e+01, train loss: 3.16391e-06, val loss: 3.79386e-06, min loss: 2.97456e-06\n",
      "Epoch: 332700, elapsed: 1.09e+01, train loss: 3.11239e-06, val loss: 3.71016e-06, min loss: 2.97456e-06\n",
      "Epoch: 332800, elapsed: 1.09e+01, train loss: 2.99280e-06, val loss: 3.72687e-06, min loss: 2.97456e-06\n",
      "Epoch: 332900, elapsed: 1.11e+01, train loss: 3.13024e-06, val loss: 3.88208e-06, min loss: 2.97456e-06\n",
      "Epoch: 333000, elapsed: 1.11e+01, train loss: 5.01046e-06, val loss: 4.64981e-06, min loss: 2.97456e-06\n",
      "Epoch: 333100, elapsed: 1.10e+01, train loss: 3.54995e-06, val loss: 4.07155e-06, min loss: 2.97456e-06\n",
      "Epoch: 333200, elapsed: 1.21e+01, train loss: 3.24251e-06, val loss: 3.91591e-06, min loss: 2.97456e-06\n",
      "Epoch: 333300, elapsed: 1.11e+01, train loss: 3.03034e-06, val loss: 3.67346e-06, min loss: 2.97456e-06\n",
      "Epoch: 333400, elapsed: 1.11e+01, train loss: 3.08761e-06, val loss: 3.86531e-06, min loss: 2.97456e-06\n",
      "Epoch: 333500, elapsed: 1.10e+01, train loss: 3.36880e-06, val loss: 4.37717e-06, min loss: 2.97456e-06\n",
      "Epoch: 333600, elapsed: 1.11e+01, train loss: 3.35370e-06, val loss: 3.91336e-06, min loss: 2.97456e-06\n",
      "Epoch: 333700, elapsed: 1.12e+01, train loss: 3.03125e-06, val loss: 3.68174e-06, min loss: 2.97456e-06\n",
      "Epoch: 333800, elapsed: 1.11e+01, train loss: 2.99672e-06, val loss: 3.71137e-06, min loss: 2.97456e-06\n",
      "Epoch: 333900, elapsed: 1.21e+01, train loss: 3.31408e-06, val loss: 3.83019e-06, min loss: 2.97456e-06\n",
      "Epoch: 334000, elapsed: 1.10e+01, train loss: 4.22143e-06, val loss: 4.35083e-06, min loss: 2.97456e-06\n",
      "Epoch: 334100, elapsed: 1.10e+01, train loss: 3.02263e-06, val loss: 3.74136e-06, min loss: 2.97456e-06\n",
      "Epoch: 334200, elapsed: 1.10e+01, train loss: 3.06406e-06, val loss: 3.71812e-06, min loss: 2.97456e-06\n",
      "Epoch: 334300, elapsed: 1.08e+01, train loss: 5.74217e-06, val loss: 7.20263e-06, min loss: 2.97456e-06\n",
      "Epoch: 334400, elapsed: 1.10e+01, train loss: 3.69693e-06, val loss: 4.66973e-06, min loss: 2.97456e-06\n",
      "Epoch: 334500, elapsed: 1.11e+01, train loss: 6.36961e-06, val loss: 5.61494e-06, min loss: 2.97456e-06\n",
      "Epoch: 334600, elapsed: 1.11e+01, train loss: 9.99370e-06, val loss: 7.95896e-06, min loss: 2.97456e-06\n",
      "Epoch: 334700, elapsed: 1.22e+01, train loss: 8.08073e-06, val loss: 7.11642e-06, min loss: 2.97456e-06\n",
      "Epoch: 334800, elapsed: 1.12e+01, train loss: 4.20032e-06, val loss: 4.81066e-06, min loss: 2.97456e-06\n",
      "Epoch: 334900, elapsed: 1.10e+01, train loss: 2.94839e-06, val loss: 3.64819e-06, min loss: 2.94839e-06\n",
      "Epoch: 335000, elapsed: 1.11e+01, train loss: 2.91083e-06, val loss: 3.62191e-06, min loss: 2.91083e-06\n",
      "Epoch: 335100, elapsed: 1.31e+01, train loss: 3.09371e-06, val loss: 3.77328e-06, min loss: 2.91083e-06\n",
      "Epoch: 335200, elapsed: 1.10e+01, train loss: 2.98019e-06, val loss: 3.66423e-06, min loss: 2.91083e-06\n",
      "Epoch: 335300, elapsed: 1.10e+01, train loss: 3.58237e-06, val loss: 4.52615e-06, min loss: 2.91083e-06\n",
      "Epoch: 335400, elapsed: 1.22e+01, train loss: 2.92357e-06, val loss: 3.62670e-06, min loss: 2.91083e-06\n",
      "Epoch: 335500, elapsed: 1.12e+01, train loss: 3.67948e-06, val loss: 4.50330e-06, min loss: 2.91083e-06\n",
      "Epoch: 335600, elapsed: 1.10e+01, train loss: 2.95166e-06, val loss: 3.60699e-06, min loss: 2.91083e-06\n",
      "Epoch: 335700, elapsed: 1.10e+01, train loss: 3.01573e-06, val loss: 3.76081e-06, min loss: 2.91083e-06\n",
      "Epoch: 335800, elapsed: 1.10e+01, train loss: 2.93583e-06, val loss: 3.64287e-06, min loss: 2.91083e-06\n",
      "Epoch: 335900, elapsed: 1.10e+01, train loss: 3.04622e-06, val loss: 3.72833e-06, min loss: 2.91083e-06\n",
      "Epoch: 336000, elapsed: 1.09e+01, train loss: 5.48736e-06, val loss: 6.24402e-06, min loss: 2.91083e-06\n",
      "Epoch: 336100, elapsed: 1.20e+01, train loss: 6.24564e-06, val loss: 7.61394e-06, min loss: 2.91083e-06\n",
      "Epoch: 336200, elapsed: 1.11e+01, train loss: 3.49732e-06, val loss: 4.17031e-06, min loss: 2.91083e-06\n",
      "Epoch: 336300, elapsed: 1.10e+01, train loss: 3.18871e-06, val loss: 3.84943e-06, min loss: 2.91083e-06\n",
      "Epoch: 336400, elapsed: 1.10e+01, train loss: 5.69519e-06, val loss: 5.59626e-06, min loss: 2.91083e-06\n",
      "Epoch: 336500, elapsed: 1.10e+01, train loss: 2.97869e-06, val loss: 3.66748e-06, min loss: 2.91083e-06\n",
      "Epoch: 336600, elapsed: 1.10e+01, train loss: 3.00783e-06, val loss: 3.67588e-06, min loss: 2.91083e-06\n",
      "Epoch: 336700, elapsed: 1.10e+01, train loss: 2.89409e-06, val loss: 3.61583e-06, min loss: 2.89409e-06\n",
      "Epoch: 336800, elapsed: 1.10e+01, train loss: 3.19556e-06, val loss: 3.82584e-06, min loss: 2.89409e-06\n",
      "Epoch: 336900, elapsed: 1.22e+01, train loss: 3.40081e-06, val loss: 3.87184e-06, min loss: 2.89409e-06\n",
      "Epoch: 337000, elapsed: 1.10e+01, train loss: 3.72625e-06, val loss: 4.46357e-06, min loss: 2.89409e-06\n",
      "Epoch: 337100, elapsed: 1.09e+01, train loss: 2.98394e-06, val loss: 3.67248e-06, min loss: 2.89409e-06\n",
      "Epoch: 337200, elapsed: 1.10e+01, train loss: 3.26722e-06, val loss: 4.04824e-06, min loss: 2.89409e-06\n",
      "Epoch: 337300, elapsed: 1.10e+01, train loss: 3.33778e-06, val loss: 4.04178e-06, min loss: 2.89409e-06\n",
      "Epoch: 337400, elapsed: 1.09e+01, train loss: 4.32925e-06, val loss: 5.23397e-06, min loss: 2.89409e-06\n",
      "Epoch: 337500, elapsed: 1.10e+01, train loss: 4.08322e-06, val loss: 4.66076e-06, min loss: 2.89409e-06\n",
      "Epoch: 337600, elapsed: 1.22e+01, train loss: 6.09572e-06, val loss: 7.19260e-06, min loss: 2.89409e-06\n",
      "Epoch: 337700, elapsed: 1.10e+01, train loss: 3.24434e-06, val loss: 4.21514e-06, min loss: 2.89409e-06\n",
      "Epoch: 337800, elapsed: 1.10e+01, train loss: 2.97212e-06, val loss: 3.67215e-06, min loss: 2.89409e-06\n",
      "Epoch: 337900, elapsed: 1.09e+01, train loss: 2.95551e-06, val loss: 3.63226e-06, min loss: 2.89409e-06\n",
      "Epoch: 338000, elapsed: 1.08e+01, train loss: 2.98816e-06, val loss: 3.65718e-06, min loss: 2.89409e-06\n",
      "Epoch: 338100, elapsed: 1.10e+01, train loss: 6.01218e-06, val loss: 6.06703e-06, min loss: 2.89409e-06\n",
      "Epoch: 338200, elapsed: 1.10e+01, train loss: 5.05645e-06, val loss: 6.10977e-06, min loss: 2.89409e-06\n",
      "Epoch: 338300, elapsed: 1.09e+01, train loss: 3.68942e-06, val loss: 4.35594e-06, min loss: 2.89409e-06\n",
      "Epoch: 338400, elapsed: 1.21e+01, train loss: 3.27537e-06, val loss: 3.92082e-06, min loss: 2.89409e-06\n",
      "Epoch: 338500, elapsed: 1.09e+01, train loss: 2.95201e-06, val loss: 3.69834e-06, min loss: 2.89409e-06\n",
      "Epoch: 338600, elapsed: 1.09e+01, train loss: 3.74837e-06, val loss: 4.10174e-06, min loss: 2.89409e-06\n",
      "Epoch: 338700, elapsed: 1.11e+01, train loss: 6.01382e-06, val loss: 5.56595e-06, min loss: 2.89409e-06\n",
      "Epoch: 338800, elapsed: 1.11e+01, train loss: 2.87433e-06, val loss: 3.59412e-06, min loss: 2.87433e-06\n",
      "Epoch: 338900, elapsed: 1.09e+01, train loss: 4.04203e-06, val loss: 5.29958e-06, min loss: 2.87433e-06\n",
      "Epoch: 339000, elapsed: 1.10e+01, train loss: 3.08485e-06, val loss: 3.87547e-06, min loss: 2.87433e-06\n",
      "Epoch: 339100, elapsed: 1.21e+01, train loss: 4.53814e-06, val loss: 5.31443e-06, min loss: 2.87433e-06\n",
      "Epoch: 339200, elapsed: 1.10e+01, train loss: 2.88378e-06, val loss: 3.59129e-06, min loss: 2.87433e-06\n",
      "Epoch: 339300, elapsed: 1.09e+01, train loss: 2.90993e-06, val loss: 3.58315e-06, min loss: 2.87433e-06\n",
      "Epoch: 339400, elapsed: 1.11e+01, train loss: 2.86458e-06, val loss: 3.57526e-06, min loss: 2.86458e-06\n",
      "Epoch: 339500, elapsed: 1.10e+01, train loss: 3.48800e-06, val loss: 4.54052e-06, min loss: 2.86458e-06\n",
      "Epoch: 339600, elapsed: 1.10e+01, train loss: 6.94288e-06, val loss: 6.67761e-06, min loss: 2.86458e-06\n",
      "Epoch: 339700, elapsed: 1.10e+01, train loss: 3.14194e-06, val loss: 3.59523e-06, min loss: 2.86458e-06\n",
      "Epoch: 339800, elapsed: 1.10e+01, train loss: 3.10875e-06, val loss: 3.63847e-06, min loss: 2.86458e-06\n",
      "Epoch: 339900, elapsed: 1.21e+01, train loss: 2.98962e-06, val loss: 3.61783e-06, min loss: 2.86458e-06\n",
      "Epoch: 340000, elapsed: 1.11e+01, train loss: 3.22288e-06, val loss: 3.98098e-06, min loss: 2.86458e-06\n",
      "Epoch: 340100, elapsed: 1.30e+01, train loss: 3.01177e-06, val loss: 3.80186e-06, min loss: 2.86458e-06\n",
      "Epoch: 340200, elapsed: 1.11e+01, train loss: 4.21995e-06, val loss: 5.25147e-06, min loss: 2.86458e-06\n",
      "Epoch: 340300, elapsed: 1.10e+01, train loss: 3.67105e-06, val loss: 4.66278e-06, min loss: 2.86458e-06\n",
      "Epoch: 340400, elapsed: 1.09e+01, train loss: 4.03997e-06, val loss: 5.00312e-06, min loss: 2.86458e-06\n",
      "Epoch: 340500, elapsed: 1.10e+01, train loss: 3.04440e-06, val loss: 3.85692e-06, min loss: 2.86458e-06\n",
      "Epoch: 340600, elapsed: 1.22e+01, train loss: 3.05357e-06, val loss: 3.86235e-06, min loss: 2.86458e-06\n",
      "Epoch: 340700, elapsed: 1.09e+01, train loss: 1.23572e-05, val loss: 1.26365e-05, min loss: 2.86458e-06\n",
      "Epoch: 340800, elapsed: 1.11e+01, train loss: 3.82446e-06, val loss: 4.79961e-06, min loss: 2.86458e-06\n",
      "Epoch: 340900, elapsed: 1.10e+01, train loss: 2.85917e-06, val loss: 3.56794e-06, min loss: 2.85917e-06\n",
      "Epoch: 341000, elapsed: 1.09e+01, train loss: 2.89278e-06, val loss: 3.63414e-06, min loss: 2.85917e-06\n",
      "Epoch: 341100, elapsed: 1.10e+01, train loss: 2.89567e-06, val loss: 3.62857e-06, min loss: 2.85917e-06\n",
      "Epoch: 341200, elapsed: 1.12e+01, train loss: 3.12150e-06, val loss: 3.84782e-06, min loss: 2.85917e-06\n",
      "Epoch: 341300, elapsed: 1.10e+01, train loss: 8.03492e-06, val loss: 7.13649e-06, min loss: 2.85917e-06\n",
      "Epoch: 341400, elapsed: 1.22e+01, train loss: 4.77001e-06, val loss: 4.91256e-06, min loss: 2.85917e-06\n",
      "Epoch: 341500, elapsed: 1.10e+01, train loss: 3.01312e-06, val loss: 3.80971e-06, min loss: 2.85917e-06\n",
      "Epoch: 341600, elapsed: 1.10e+01, train loss: 3.85054e-06, val loss: 4.44642e-06, min loss: 2.85917e-06\n",
      "Epoch: 341700, elapsed: 1.09e+01, train loss: 3.05245e-06, val loss: 3.78426e-06, min loss: 2.85917e-06\n",
      "Epoch: 341800, elapsed: 1.09e+01, train loss: 4.75188e-06, val loss: 4.62925e-06, min loss: 2.85917e-06\n",
      "Epoch: 341900, elapsed: 1.10e+01, train loss: 2.86294e-06, val loss: 3.59941e-06, min loss: 2.85917e-06\n",
      "Epoch: 342000, elapsed: 1.10e+01, train loss: 7.32878e-06, val loss: 7.62849e-06, min loss: 2.85917e-06\n",
      "Epoch: 342100, elapsed: 1.23e+01, train loss: 3.50641e-06, val loss: 4.18182e-06, min loss: 2.85917e-06\n",
      "Epoch: 342200, elapsed: 1.11e+01, train loss: 3.03619e-06, val loss: 3.67294e-06, min loss: 2.85917e-06\n",
      "Epoch: 342300, elapsed: 1.10e+01, train loss: 2.91089e-06, val loss: 3.61524e-06, min loss: 2.85917e-06\n",
      "Epoch: 342400, elapsed: 1.11e+01, train loss: 2.89088e-06, val loss: 3.58908e-06, min loss: 2.85917e-06\n",
      "Epoch: 342500, elapsed: 1.09e+01, train loss: 3.59426e-06, val loss: 3.99944e-06, min loss: 2.85917e-06\n",
      "Epoch: 342600, elapsed: 1.11e+01, train loss: 3.63193e-06, val loss: 4.36186e-06, min loss: 2.85917e-06\n",
      "Epoch: 342700, elapsed: 1.11e+01, train loss: 3.33488e-06, val loss: 3.99634e-06, min loss: 2.85917e-06\n",
      "Epoch: 342800, elapsed: 1.11e+01, train loss: 3.04059e-06, val loss: 3.73114e-06, min loss: 2.85917e-06\n",
      "Epoch: 342900, elapsed: 1.19e+01, train loss: 3.80785e-06, val loss: 4.36074e-06, min loss: 2.85917e-06\n",
      "Epoch: 343000, elapsed: 1.11e+01, train loss: 5.97467e-06, val loss: 6.24323e-06, min loss: 2.85917e-06\n",
      "Epoch: 343100, elapsed: 1.10e+01, train loss: 4.92232e-06, val loss: 6.05448e-06, min loss: 2.85917e-06\n",
      "Epoch: 343200, elapsed: 1.10e+01, train loss: 4.02516e-06, val loss: 4.62976e-06, min loss: 2.85917e-06\n",
      "Epoch: 343300, elapsed: 1.10e+01, train loss: 3.03927e-06, val loss: 3.73697e-06, min loss: 2.85917e-06\n",
      "Epoch: 343400, elapsed: 1.10e+01, train loss: 2.95418e-06, val loss: 3.60747e-06, min loss: 2.85917e-06\n",
      "Epoch: 343500, elapsed: 1.11e+01, train loss: 2.97128e-06, val loss: 3.66999e-06, min loss: 2.85917e-06\n",
      "Epoch: 343600, elapsed: 1.10e+01, train loss: 8.37149e-06, val loss: 1.03864e-05, min loss: 2.85917e-06\n",
      "Epoch: 343700, elapsed: 1.21e+01, train loss: 2.83510e-06, val loss: 3.55983e-06, min loss: 2.83510e-06\n",
      "Epoch: 343800, elapsed: 1.11e+01, train loss: 2.83269e-06, val loss: 3.56200e-06, min loss: 2.83269e-06\n",
      "Epoch: 343900, elapsed: 1.11e+01, train loss: 2.82944e-06, val loss: 3.54461e-06, min loss: 2.82944e-06\n",
      "Epoch: 344000, elapsed: 1.10e+01, train loss: 2.85450e-06, val loss: 3.61495e-06, min loss: 2.82944e-06\n",
      "Epoch: 344100, elapsed: 1.09e+01, train loss: 2.87083e-06, val loss: 3.61581e-06, min loss: 2.82944e-06\n",
      "Epoch: 344200, elapsed: 1.10e+01, train loss: 4.87294e-06, val loss: 6.36101e-06, min loss: 2.82944e-06\n",
      "Epoch: 344300, elapsed: 1.11e+01, train loss: 7.46461e-06, val loss: 9.01531e-06, min loss: 2.82944e-06\n",
      "Epoch: 344400, elapsed: 1.20e+01, train loss: 1.08048e-05, val loss: 1.19118e-05, min loss: 2.82944e-06\n",
      "Epoch: 344500, elapsed: 1.11e+01, train loss: 8.23145e-06, val loss: 8.96820e-06, min loss: 2.82944e-06\n",
      "Epoch: 344600, elapsed: 1.12e+01, train loss: 2.98882e-06, val loss: 3.81212e-06, min loss: 2.82944e-06\n",
      "Epoch: 344700, elapsed: 1.10e+01, train loss: 2.96537e-06, val loss: 3.68936e-06, min loss: 2.82944e-06\n",
      "Epoch: 344800, elapsed: 1.09e+01, train loss: 3.57869e-06, val loss: 3.78410e-06, min loss: 2.82944e-06\n",
      "Epoch: 344900, elapsed: 1.09e+01, train loss: 3.11531e-06, val loss: 3.86545e-06, min loss: 2.82944e-06\n",
      "Epoch: 345000, elapsed: 1.10e+01, train loss: 3.06225e-06, val loss: 3.77013e-06, min loss: 2.82944e-06\n",
      "Epoch: 345100, elapsed: 1.41e+01, train loss: 6.36199e-06, val loss: 7.99993e-06, min loss: 2.82944e-06\n",
      "Epoch: 345200, elapsed: 1.09e+01, train loss: 4.84035e-06, val loss: 5.61632e-06, min loss: 2.82944e-06\n",
      "Epoch: 345300, elapsed: 1.10e+01, train loss: 2.95943e-06, val loss: 3.87062e-06, min loss: 2.82944e-06\n",
      "Epoch: 345400, elapsed: 1.11e+01, train loss: 3.14455e-06, val loss: 3.86271e-06, min loss: 2.82944e-06\n",
      "Epoch: 345500, elapsed: 1.10e+01, train loss: 2.88363e-06, val loss: 3.60730e-06, min loss: 2.82944e-06\n",
      "Epoch: 345600, elapsed: 1.12e+01, train loss: 2.84433e-06, val loss: 3.55457e-06, min loss: 2.82944e-06\n",
      "Epoch: 345700, elapsed: 1.11e+01, train loss: 2.81151e-06, val loss: 3.53131e-06, min loss: 2.81151e-06\n",
      "Epoch: 345800, elapsed: 1.09e+01, train loss: 2.81429e-06, val loss: 3.52392e-06, min loss: 2.81151e-06\n",
      "Epoch: 345900, elapsed: 1.22e+01, train loss: 2.82951e-06, val loss: 3.52103e-06, min loss: 2.81151e-06\n",
      "Epoch: 346000, elapsed: 1.09e+01, train loss: 2.85935e-06, val loss: 3.54420e-06, min loss: 2.81151e-06\n",
      "Epoch: 346100, elapsed: 1.11e+01, train loss: 4.24733e-06, val loss: 4.49349e-06, min loss: 2.81151e-06\n",
      "Epoch: 346200, elapsed: 1.10e+01, train loss: 1.01888e-05, val loss: 8.83203e-06, min loss: 2.81151e-06\n",
      "Epoch: 346300, elapsed: 1.08e+01, train loss: 1.38851e-05, val loss: 1.33951e-05, min loss: 2.81151e-06\n",
      "Epoch: 346400, elapsed: 1.12e+01, train loss: 4.74626e-06, val loss: 6.08256e-06, min loss: 2.81151e-06\n",
      "Epoch: 346500, elapsed: 1.08e+01, train loss: 2.93120e-06, val loss: 3.65818e-06, min loss: 2.81151e-06\n",
      "Epoch: 346600, elapsed: 1.10e+01, train loss: 3.45138e-06, val loss: 4.31921e-06, min loss: 2.81151e-06\n",
      "Epoch: 346700, elapsed: 1.22e+01, train loss: 2.81515e-06, val loss: 3.68320e-06, min loss: 2.81151e-06\n",
      "Epoch: 346800, elapsed: 1.11e+01, train loss: 3.12346e-06, val loss: 4.01369e-06, min loss: 2.81151e-06\n",
      "Epoch: 346900, elapsed: 1.10e+01, train loss: 2.83019e-06, val loss: 3.53364e-06, min loss: 2.81151e-06\n",
      "Epoch: 347000, elapsed: 1.12e+01, train loss: 2.86229e-06, val loss: 3.67465e-06, min loss: 2.81151e-06\n",
      "Epoch: 347100, elapsed: 1.10e+01, train loss: 3.22637e-06, val loss: 3.85527e-06, min loss: 2.81151e-06\n",
      "Epoch: 347200, elapsed: 1.09e+01, train loss: 3.52607e-06, val loss: 4.02692e-06, min loss: 2.81151e-06\n",
      "Epoch: 347300, elapsed: 1.10e+01, train loss: 3.36936e-06, val loss: 3.84787e-06, min loss: 2.81151e-06\n",
      "Epoch: 347400, elapsed: 1.22e+01, train loss: 2.88826e-06, val loss: 3.53744e-06, min loss: 2.81151e-06\n",
      "Epoch: 347500, elapsed: 1.11e+01, train loss: 2.99689e-06, val loss: 3.70065e-06, min loss: 2.81151e-06\n",
      "Epoch: 347600, elapsed: 1.12e+01, train loss: 2.88633e-06, val loss: 3.60054e-06, min loss: 2.81151e-06\n",
      "Epoch: 347700, elapsed: 1.12e+01, train loss: 5.23059e-06, val loss: 6.23375e-06, min loss: 2.81151e-06\n",
      "Epoch: 347800, elapsed: 1.11e+01, train loss: 2.93640e-06, val loss: 3.66100e-06, min loss: 2.81151e-06\n",
      "Epoch: 347900, elapsed: 1.10e+01, train loss: 9.02105e-06, val loss: 1.08565e-05, min loss: 2.81151e-06\n",
      "Epoch: 348000, elapsed: 1.09e+01, train loss: 4.07499e-06, val loss: 3.98121e-06, min loss: 2.81151e-06\n",
      "Epoch: 348100, elapsed: 1.09e+01, train loss: 3.06486e-06, val loss: 3.68050e-06, min loss: 2.81151e-06\n",
      "Epoch: 348200, elapsed: 1.21e+01, train loss: 2.90793e-06, val loss: 3.65238e-06, min loss: 2.81151e-06\n",
      "Epoch: 348300, elapsed: 1.12e+01, train loss: 2.79455e-06, val loss: 3.50028e-06, min loss: 2.79455e-06\n",
      "Epoch: 348400, elapsed: 1.11e+01, train loss: 2.88724e-06, val loss: 3.59511e-06, min loss: 2.79455e-06\n",
      "Epoch: 348500, elapsed: 1.11e+01, train loss: 8.09522e-06, val loss: 8.44003e-06, min loss: 2.79455e-06\n",
      "Epoch: 348600, elapsed: 1.10e+01, train loss: 2.84912e-06, val loss: 3.52207e-06, min loss: 2.79455e-06\n",
      "Epoch: 348700, elapsed: 1.10e+01, train loss: 2.89262e-06, val loss: 3.67179e-06, min loss: 2.79455e-06\n",
      "Epoch: 348800, elapsed: 1.11e+01, train loss: 2.84423e-06, val loss: 3.53696e-06, min loss: 2.79455e-06\n",
      "Epoch: 348900, elapsed: 1.10e+01, train loss: 2.98598e-06, val loss: 3.86013e-06, min loss: 2.79455e-06\n",
      "Epoch: 349000, elapsed: 1.21e+01, train loss: 2.79308e-06, val loss: 3.57513e-06, min loss: 2.79308e-06\n",
      "Epoch: 349100, elapsed: 1.11e+01, train loss: 3.35374e-06, val loss: 4.09017e-06, min loss: 2.79308e-06\n",
      "Epoch: 349200, elapsed: 1.08e+01, train loss: 3.10642e-06, val loss: 3.85968e-06, min loss: 2.79308e-06\n",
      "Epoch: 349300, elapsed: 1.09e+01, train loss: 3.57121e-06, val loss: 3.98433e-06, min loss: 2.79308e-06\n",
      "Epoch: 349400, elapsed: 1.10e+01, train loss: 2.87287e-06, val loss: 3.50875e-06, min loss: 2.79308e-06\n",
      "Epoch: 349500, elapsed: 1.11e+01, train loss: 2.96948e-06, val loss: 3.62970e-06, min loss: 2.79308e-06\n",
      "Epoch: 349600, elapsed: 1.09e+01, train loss: 3.96784e-06, val loss: 4.80267e-06, min loss: 2.79308e-06\n",
      "Epoch: 349700, elapsed: 1.11e+01, train loss: 2.98969e-06, val loss: 3.66673e-06, min loss: 2.79308e-06\n",
      "Epoch: 349800, elapsed: 1.23e+01, train loss: 7.07308e-06, val loss: 8.59077e-06, min loss: 2.79308e-06\n",
      "Epoch: 349900, elapsed: 1.09e+01, train loss: 3.88925e-06, val loss: 4.17778e-06, min loss: 2.79308e-06\n",
      "Epoch: 350000, elapsed: 1.10e+01, train loss: 2.92609e-06, val loss: 3.57108e-06, min loss: 2.79308e-06\n",
      "Epoch: 350100, elapsed: 1.30e+01, train loss: 8.34815e-06, val loss: 9.66884e-06, min loss: 2.79308e-06\n",
      "Epoch: 350200, elapsed: 1.10e+01, train loss: 2.97219e-06, val loss: 3.68483e-06, min loss: 2.79308e-06\n",
      "Epoch: 350300, elapsed: 1.11e+01, train loss: 4.51986e-06, val loss: 4.18283e-06, min loss: 2.79308e-06\n",
      "Epoch: 350400, elapsed: 1.10e+01, train loss: 2.77225e-06, val loss: 3.48777e-06, min loss: 2.77225e-06\n",
      "Epoch: 350500, elapsed: 1.23e+01, train loss: 2.77698e-06, val loss: 3.49186e-06, min loss: 2.77225e-06\n",
      "Epoch: 350600, elapsed: 1.10e+01, train loss: 2.83289e-06, val loss: 3.52658e-06, min loss: 2.77225e-06\n",
      "Epoch: 350700, elapsed: 1.09e+01, train loss: 2.89061e-06, val loss: 3.54112e-06, min loss: 2.77225e-06\n",
      "Epoch: 350800, elapsed: 1.09e+01, train loss: 1.05952e-05, val loss: 9.50189e-06, min loss: 2.77225e-06\n",
      "Epoch: 350900, elapsed: 1.10e+01, train loss: 2.76705e-06, val loss: 3.48443e-06, min loss: 2.76705e-06\n",
      "Epoch: 351000, elapsed: 1.11e+01, train loss: 2.87549e-06, val loss: 3.53531e-06, min loss: 2.76705e-06\n",
      "Epoch: 351100, elapsed: 1.11e+01, train loss: 3.06834e-06, val loss: 3.73629e-06, min loss: 2.76705e-06\n",
      "Epoch: 351200, elapsed: 1.08e+01, train loss: 3.85608e-06, val loss: 4.89414e-06, min loss: 2.76705e-06\n",
      "Epoch: 351300, elapsed: 1.22e+01, train loss: 2.77320e-06, val loss: 3.50523e-06, min loss: 2.76705e-06\n",
      "Epoch: 351400, elapsed: 1.10e+01, train loss: 2.85268e-06, val loss: 3.57347e-06, min loss: 2.76705e-06\n",
      "Epoch: 351500, elapsed: 1.12e+01, train loss: 4.15030e-06, val loss: 5.40427e-06, min loss: 2.76705e-06\n",
      "Epoch: 351600, elapsed: 1.09e+01, train loss: 2.81582e-06, val loss: 3.52126e-06, min loss: 2.76705e-06\n",
      "Epoch: 351700, elapsed: 1.10e+01, train loss: 2.76246e-06, val loss: 3.48034e-06, min loss: 2.76246e-06\n",
      "Epoch: 351800, elapsed: 1.10e+01, train loss: 2.78746e-06, val loss: 3.50281e-06, min loss: 2.76246e-06\n",
      "Epoch: 351900, elapsed: 1.10e+01, train loss: 2.85016e-06, val loss: 3.51213e-06, min loss: 2.76246e-06\n",
      "Epoch: 352000, elapsed: 1.21e+01, train loss: 2.97213e-06, val loss: 3.60983e-06, min loss: 2.76246e-06\n",
      "Epoch: 352100, elapsed: 1.10e+01, train loss: 2.86272e-06, val loss: 3.51144e-06, min loss: 2.76246e-06\n",
      "Epoch: 352200, elapsed: 1.13e+01, train loss: 2.78114e-06, val loss: 3.52791e-06, min loss: 2.76246e-06\n",
      "Epoch: 352300, elapsed: 1.10e+01, train loss: 2.77175e-06, val loss: 3.49517e-06, min loss: 2.76246e-06\n",
      "Epoch: 352400, elapsed: 1.09e+01, train loss: 2.82498e-06, val loss: 3.51596e-06, min loss: 2.76246e-06\n",
      "Epoch: 352500, elapsed: 1.10e+01, train loss: 3.07305e-06, val loss: 3.72757e-06, min loss: 2.76246e-06\n",
      "Epoch: 352600, elapsed: 1.09e+01, train loss: 2.81648e-06, val loss: 3.55019e-06, min loss: 2.76246e-06\n",
      "Epoch: 352700, elapsed: 1.10e+01, train loss: 2.75800e-06, val loss: 3.49717e-06, min loss: 2.75800e-06\n",
      "Epoch: 352800, elapsed: 1.20e+01, train loss: 2.75352e-06, val loss: 3.47097e-06, min loss: 2.75352e-06\n",
      "Epoch: 352900, elapsed: 1.11e+01, train loss: 2.81724e-06, val loss: 3.54640e-06, min loss: 2.75352e-06\n",
      "Epoch: 353000, elapsed: 1.11e+01, train loss: 3.95735e-06, val loss: 4.87881e-06, min loss: 2.75352e-06\n",
      "Epoch: 353100, elapsed: 1.09e+01, train loss: 3.37265e-06, val loss: 4.20585e-06, min loss: 2.75352e-06\n",
      "Epoch: 353200, elapsed: 1.09e+01, train loss: 2.99891e-06, val loss: 3.73567e-06, min loss: 2.75352e-06\n",
      "Epoch: 353300, elapsed: 1.08e+01, train loss: 3.07964e-06, val loss: 3.94858e-06, min loss: 2.75352e-06\n",
      "Epoch: 353400, elapsed: 1.10e+01, train loss: 2.90266e-06, val loss: 3.63918e-06, min loss: 2.75352e-06\n",
      "Epoch: 353500, elapsed: 1.11e+01, train loss: 5.17110e-06, val loss: 6.20051e-06, min loss: 2.75352e-06\n",
      "Epoch: 353600, elapsed: 1.21e+01, train loss: 3.78962e-06, val loss: 5.15588e-06, min loss: 2.75352e-06\n",
      "Epoch: 353700, elapsed: 1.11e+01, train loss: 4.46451e-06, val loss: 5.30390e-06, min loss: 2.75352e-06\n",
      "Epoch: 353800, elapsed: 1.10e+01, train loss: 3.11441e-06, val loss: 3.78030e-06, min loss: 2.75352e-06\n",
      "Epoch: 353900, elapsed: 1.11e+01, train loss: 2.83206e-06, val loss: 3.53911e-06, min loss: 2.75352e-06\n",
      "Epoch: 354000, elapsed: 1.09e+01, train loss: 2.78287e-06, val loss: 3.51291e-06, min loss: 2.75352e-06\n",
      "Epoch: 354100, elapsed: 1.09e+01, train loss: 4.10776e-06, val loss: 4.80219e-06, min loss: 2.75352e-06\n",
      "Epoch: 354200, elapsed: 1.10e+01, train loss: 2.79773e-06, val loss: 3.47356e-06, min loss: 2.75352e-06\n",
      "Epoch: 354300, elapsed: 1.10e+01, train loss: 2.74851e-06, val loss: 3.50762e-06, min loss: 2.74851e-06\n",
      "Epoch: 354400, elapsed: 1.23e+01, train loss: 3.52010e-06, val loss: 4.61953e-06, min loss: 2.74851e-06\n",
      "Epoch: 354500, elapsed: 1.12e+01, train loss: 2.74279e-06, val loss: 3.45945e-06, min loss: 2.74279e-06\n",
      "Epoch: 354600, elapsed: 1.11e+01, train loss: 2.79994e-06, val loss: 3.48720e-06, min loss: 2.74279e-06\n",
      "Epoch: 354700, elapsed: 1.10e+01, train loss: 3.41404e-06, val loss: 4.00994e-06, min loss: 2.74279e-06\n",
      "Epoch: 354800, elapsed: 1.10e+01, train loss: 2.90567e-06, val loss: 3.47370e-06, min loss: 2.74279e-06\n",
      "Epoch: 354900, elapsed: 1.11e+01, train loss: 3.68352e-06, val loss: 4.04088e-06, min loss: 2.74279e-06\n",
      "Epoch: 355000, elapsed: 1.10e+01, train loss: 5.00505e-06, val loss: 4.92463e-06, min loss: 2.74279e-06\n",
      "Epoch: 355100, elapsed: 1.43e+01, train loss: 4.34438e-06, val loss: 5.59212e-06, min loss: 2.74279e-06\n",
      "Epoch: 355200, elapsed: 1.11e+01, train loss: 1.58731e-05, val loss: 1.75922e-05, min loss: 2.74279e-06\n",
      "Epoch: 355300, elapsed: 1.10e+01, train loss: 2.73681e-06, val loss: 3.46019e-06, min loss: 2.73681e-06\n",
      "Epoch: 355400, elapsed: 1.10e+01, train loss: 2.75008e-06, val loss: 3.46008e-06, min loss: 2.73681e-06\n",
      "Epoch: 355500, elapsed: 1.09e+01, train loss: 2.76948e-06, val loss: 3.48337e-06, min loss: 2.73681e-06\n",
      "Epoch: 355600, elapsed: 1.09e+01, train loss: 2.92258e-06, val loss: 3.73207e-06, min loss: 2.73681e-06\n",
      "Epoch: 355700, elapsed: 1.10e+01, train loss: 3.45934e-06, val loss: 3.93080e-06, min loss: 2.73681e-06\n",
      "Epoch: 355800, elapsed: 1.11e+01, train loss: 3.46800e-06, val loss: 4.04566e-06, min loss: 2.73681e-06\n",
      "Epoch: 355900, elapsed: 1.21e+01, train loss: 2.75884e-06, val loss: 3.44495e-06, min loss: 2.73681e-06\n",
      "Epoch: 356000, elapsed: 1.12e+01, train loss: 3.69541e-06, val loss: 4.64932e-06, min loss: 2.73681e-06\n",
      "Epoch: 356100, elapsed: 1.12e+01, train loss: 3.80679e-06, val loss: 4.83537e-06, min loss: 2.73681e-06\n",
      "Epoch: 356200, elapsed: 1.11e+01, train loss: 7.84670e-06, val loss: 9.34312e-06, min loss: 2.73681e-06\n",
      "Epoch: 356300, elapsed: 1.10e+01, train loss: 5.51408e-06, val loss: 5.19765e-06, min loss: 2.73681e-06\n",
      "Epoch: 356400, elapsed: 1.10e+01, train loss: 2.75658e-06, val loss: 3.53008e-06, min loss: 2.73681e-06\n",
      "Epoch: 356500, elapsed: 1.10e+01, train loss: 5.74104e-06, val loss: 6.71721e-06, min loss: 2.73681e-06\n",
      "Epoch: 356600, elapsed: 1.09e+01, train loss: 3.20706e-06, val loss: 3.89250e-06, min loss: 2.73681e-06\n",
      "Epoch: 356700, elapsed: 1.22e+01, train loss: 3.70488e-06, val loss: 4.79984e-06, min loss: 2.73681e-06\n",
      "Epoch: 356800, elapsed: 1.10e+01, train loss: 3.03674e-06, val loss: 3.60372e-06, min loss: 2.73681e-06\n",
      "Epoch: 356900, elapsed: 1.09e+01, train loss: 2.73494e-06, val loss: 3.46421e-06, min loss: 2.73494e-06\n",
      "Epoch: 357000, elapsed: 1.10e+01, train loss: 3.01383e-06, val loss: 3.70933e-06, min loss: 2.73494e-06\n",
      "Epoch: 357100, elapsed: 1.10e+01, train loss: 2.80603e-06, val loss: 3.58774e-06, min loss: 2.73494e-06\n",
      "Epoch: 357200, elapsed: 1.10e+01, train loss: 2.93153e-06, val loss: 3.76224e-06, min loss: 2.73494e-06\n",
      "Epoch: 357300, elapsed: 1.10e+01, train loss: 2.72968e-06, val loss: 3.47436e-06, min loss: 2.72968e-06\n",
      "Epoch: 357400, elapsed: 1.09e+01, train loss: 3.36252e-06, val loss: 3.71496e-06, min loss: 2.72968e-06\n",
      "Epoch: 357500, elapsed: 1.22e+01, train loss: 5.24249e-06, val loss: 6.13921e-06, min loss: 2.72968e-06\n",
      "Epoch: 357600, elapsed: 1.12e+01, train loss: 3.00808e-06, val loss: 3.59154e-06, min loss: 2.72968e-06\n",
      "Epoch: 357700, elapsed: 1.09e+01, train loss: 2.74552e-06, val loss: 3.44313e-06, min loss: 2.72968e-06\n",
      "Epoch: 357800, elapsed: 1.10e+01, train loss: 3.83866e-06, val loss: 4.54817e-06, min loss: 2.72968e-06\n",
      "Epoch: 357900, elapsed: 1.09e+01, train loss: 3.56731e-06, val loss: 4.30573e-06, min loss: 2.72968e-06\n",
      "Epoch: 358000, elapsed: 1.10e+01, train loss: 6.06843e-06, val loss: 5.79198e-06, min loss: 2.72968e-06\n",
      "Epoch: 358100, elapsed: 1.10e+01, train loss: 3.31863e-06, val loss: 4.10364e-06, min loss: 2.72968e-06\n",
      "Epoch: 358200, elapsed: 1.11e+01, train loss: 4.43295e-06, val loss: 5.32195e-06, min loss: 2.72968e-06\n",
      "Epoch: 358300, elapsed: 1.23e+01, train loss: 2.76141e-06, val loss: 3.50493e-06, min loss: 2.72968e-06\n",
      "Epoch: 358400, elapsed: 1.11e+01, train loss: 3.83152e-06, val loss: 3.82234e-06, min loss: 2.72968e-06\n",
      "Epoch: 358500, elapsed: 1.10e+01, train loss: 7.99229e-06, val loss: 8.76591e-06, min loss: 2.72968e-06\n",
      "Epoch: 358600, elapsed: 1.11e+01, train loss: 2.72768e-06, val loss: 3.49356e-06, min loss: 2.72768e-06\n",
      "Epoch: 358700, elapsed: 1.10e+01, train loss: 2.71806e-06, val loss: 3.42245e-06, min loss: 2.71806e-06\n",
      "Epoch: 358800, elapsed: 1.10e+01, train loss: 2.71776e-06, val loss: 3.44564e-06, min loss: 2.71776e-06\n",
      "Epoch: 358900, elapsed: 1.11e+01, train loss: 2.92579e-06, val loss: 3.75342e-06, min loss: 2.71776e-06\n",
      "Epoch: 359000, elapsed: 1.19e+01, train loss: 2.73946e-06, val loss: 3.65990e-06, min loss: 2.71776e-06\n",
      "Epoch: 359100, elapsed: 1.12e+01, train loss: 3.20878e-06, val loss: 4.26838e-06, min loss: 2.71776e-06\n",
      "Epoch: 359200, elapsed: 1.09e+01, train loss: 4.25962e-06, val loss: 4.50807e-06, min loss: 2.71776e-06\n",
      "Epoch: 359300, elapsed: 1.11e+01, train loss: 2.94855e-06, val loss: 3.60622e-06, min loss: 2.71776e-06\n",
      "Epoch: 359400, elapsed: 1.09e+01, train loss: 2.77365e-06, val loss: 3.49607e-06, min loss: 2.71776e-06\n",
      "Epoch: 359500, elapsed: 1.10e+01, train loss: 2.92055e-06, val loss: 3.66935e-06, min loss: 2.71776e-06\n",
      "Epoch: 359600, elapsed: 1.10e+01, train loss: 3.09327e-06, val loss: 3.75120e-06, min loss: 2.71776e-06\n",
      "Epoch: 359700, elapsed: 1.10e+01, train loss: 2.70298e-06, val loss: 3.41698e-06, min loss: 2.70298e-06\n",
      "Epoch: 359800, elapsed: 1.23e+01, train loss: 2.71156e-06, val loss: 3.42262e-06, min loss: 2.70298e-06\n",
      "Epoch: 359900, elapsed: 1.11e+01, train loss: 2.78539e-06, val loss: 3.48420e-06, min loss: 2.70298e-06\n",
      "Epoch: 360000, elapsed: 1.10e+01, train loss: 2.99924e-06, val loss: 3.71694e-06, min loss: 2.70298e-06\n",
      "Epoch: 360100, elapsed: 1.33e+01, train loss: 3.01638e-06, val loss: 3.70195e-06, min loss: 2.70298e-06\n",
      "Epoch: 360200, elapsed: 1.09e+01, train loss: 2.72206e-06, val loss: 3.43556e-06, min loss: 2.70298e-06\n",
      "Epoch: 360300, elapsed: 1.11e+01, train loss: 2.73401e-06, val loss: 3.43115e-06, min loss: 2.70298e-06\n",
      "Epoch: 360400, elapsed: 1.11e+01, train loss: 3.37647e-06, val loss: 4.27772e-06, min loss: 2.70298e-06\n",
      "Epoch: 360500, elapsed: 1.11e+01, train loss: 3.65551e-06, val loss: 3.72315e-06, min loss: 2.70298e-06\n",
      "Epoch: 360600, elapsed: 1.23e+01, train loss: 2.75135e-06, val loss: 3.55709e-06, min loss: 2.70298e-06\n",
      "Epoch: 360700, elapsed: 1.11e+01, train loss: 3.89542e-06, val loss: 4.69489e-06, min loss: 2.70298e-06\n",
      "Epoch: 360800, elapsed: 1.10e+01, train loss: 2.71009e-06, val loss: 3.45122e-06, min loss: 2.70298e-06\n",
      "Epoch: 360900, elapsed: 1.10e+01, train loss: 2.85293e-06, val loss: 3.74678e-06, min loss: 2.70298e-06\n",
      "Epoch: 361000, elapsed: 1.09e+01, train loss: 3.69312e-06, val loss: 4.87969e-06, min loss: 2.70298e-06\n",
      "Epoch: 361100, elapsed: 1.12e+01, train loss: 3.36290e-06, val loss: 4.00990e-06, min loss: 2.70298e-06\n",
      "Epoch: 361200, elapsed: 1.09e+01, train loss: 2.84779e-06, val loss: 3.59769e-06, min loss: 2.70298e-06\n",
      "Epoch: 361300, elapsed: 1.12e+01, train loss: 2.72096e-06, val loss: 3.42697e-06, min loss: 2.70298e-06\n",
      "Epoch: 361400, elapsed: 1.22e+01, train loss: 2.69745e-06, val loss: 3.41560e-06, min loss: 2.69745e-06\n",
      "Epoch: 361500, elapsed: 1.10e+01, train loss: 3.59839e-06, val loss: 4.16332e-06, min loss: 2.69745e-06\n",
      "Epoch: 361600, elapsed: 1.10e+01, train loss: 3.23973e-06, val loss: 3.97383e-06, min loss: 2.69745e-06\n",
      "Epoch: 361700, elapsed: 1.09e+01, train loss: 3.11746e-06, val loss: 3.63950e-06, min loss: 2.69745e-06\n",
      "Epoch: 361800, elapsed: 1.08e+01, train loss: 3.05761e-06, val loss: 4.22857e-06, min loss: 2.69745e-06\n",
      "Epoch: 361900, elapsed: 1.11e+01, train loss: 2.94438e-06, val loss: 3.78468e-06, min loss: 2.69745e-06\n",
      "Epoch: 362000, elapsed: 1.08e+01, train loss: 2.75669e-06, val loss: 3.44762e-06, min loss: 2.69745e-06\n",
      "Epoch: 362100, elapsed: 1.10e+01, train loss: 2.75488e-06, val loss: 3.49804e-06, min loss: 2.69745e-06\n",
      "Epoch: 362200, elapsed: 1.21e+01, train loss: 2.80038e-06, val loss: 3.58170e-06, min loss: 2.69745e-06\n",
      "Epoch: 362300, elapsed: 1.09e+01, train loss: 4.31306e-06, val loss: 5.70047e-06, min loss: 2.69745e-06\n",
      "Epoch: 362400, elapsed: 1.12e+01, train loss: 5.82372e-06, val loss: 6.20803e-06, min loss: 2.69745e-06\n",
      "Epoch: 362500, elapsed: 1.10e+01, train loss: 2.90542e-06, val loss: 3.71644e-06, min loss: 2.69745e-06\n",
      "Epoch: 362600, elapsed: 1.11e+01, train loss: 4.04795e-06, val loss: 3.71225e-06, min loss: 2.69745e-06\n",
      "Epoch: 362700, elapsed: 1.11e+01, train loss: 2.73036e-06, val loss: 3.43584e-06, min loss: 2.69745e-06\n",
      "Epoch: 362800, elapsed: 1.11e+01, train loss: 2.69168e-06, val loss: 3.39605e-06, min loss: 2.69168e-06\n",
      "Epoch: 362900, elapsed: 1.12e+01, train loss: 2.70207e-06, val loss: 3.42169e-06, min loss: 2.69168e-06\n",
      "Epoch: 363000, elapsed: 1.24e+01, train loss: 2.68759e-06, val loss: 3.40407e-06, min loss: 2.68759e-06\n",
      "Epoch: 363100, elapsed: 1.12e+01, train loss: 2.77767e-06, val loss: 3.51685e-06, min loss: 2.68759e-06\n",
      "Epoch: 363200, elapsed: 1.11e+01, train loss: 4.28761e-06, val loss: 3.93031e-06, min loss: 2.68759e-06\n",
      "Epoch: 363300, elapsed: 1.10e+01, train loss: 3.23084e-06, val loss: 3.89580e-06, min loss: 2.68759e-06\n",
      "Epoch: 363400, elapsed: 1.10e+01, train loss: 2.79352e-06, val loss: 3.49192e-06, min loss: 2.68759e-06\n",
      "Epoch: 363500, elapsed: 1.10e+01, train loss: 2.71017e-06, val loss: 3.40095e-06, min loss: 2.68759e-06\n",
      "Epoch: 363600, elapsed: 1.09e+01, train loss: 2.67936e-06, val loss: 3.40656e-06, min loss: 2.67936e-06\n",
      "Epoch: 363700, elapsed: 1.09e+01, train loss: 2.86588e-06, val loss: 3.52932e-06, min loss: 2.67936e-06\n",
      "Epoch: 363800, elapsed: 1.24e+01, train loss: 3.42289e-06, val loss: 4.21494e-06, min loss: 2.67936e-06\n",
      "Epoch: 363900, elapsed: 1.11e+01, train loss: 2.68048e-06, val loss: 3.42235e-06, min loss: 2.67936e-06\n",
      "Epoch: 364000, elapsed: 1.12e+01, train loss: 2.67425e-06, val loss: 3.40557e-06, min loss: 2.67425e-06\n",
      "Epoch: 364100, elapsed: 1.10e+01, train loss: 3.15020e-06, val loss: 3.71058e-06, min loss: 2.67425e-06\n",
      "Epoch: 364200, elapsed: 1.11e+01, train loss: 3.00512e-06, val loss: 3.96760e-06, min loss: 2.67425e-06\n",
      "Epoch: 364300, elapsed: 1.08e+01, train loss: 3.51461e-06, val loss: 3.86209e-06, min loss: 2.67425e-06\n",
      "Epoch: 364400, elapsed: 1.09e+01, train loss: 2.78460e-06, val loss: 3.63003e-06, min loss: 2.67425e-06\n",
      "Epoch: 364500, elapsed: 1.09e+01, train loss: 4.69602e-06, val loss: 5.75227e-06, min loss: 2.67425e-06\n",
      "Epoch: 364600, elapsed: 1.22e+01, train loss: 8.67502e-06, val loss: 6.99843e-06, min loss: 2.67425e-06\n",
      "Epoch: 364700, elapsed: 1.12e+01, train loss: 9.81058e-06, val loss: 9.71240e-06, min loss: 2.67425e-06\n",
      "Epoch: 364800, elapsed: 1.10e+01, train loss: 7.02549e-06, val loss: 5.55260e-06, min loss: 2.67425e-06\n",
      "Epoch: 364900, elapsed: 1.09e+01, train loss: 1.17601e-05, val loss: 1.37065e-05, min loss: 2.67425e-06\n",
      "Epoch: 365000, elapsed: 1.10e+01, train loss: 2.69313e-06, val loss: 3.42745e-06, min loss: 2.67425e-06\n",
      "Epoch: 365100, elapsed: 1.29e+01, train loss: 2.68153e-06, val loss: 3.39740e-06, min loss: 2.67425e-06\n",
      "Epoch: 365200, elapsed: 1.09e+01, train loss: 3.50800e-06, val loss: 4.33520e-06, min loss: 2.67425e-06\n",
      "Epoch: 365300, elapsed: 1.21e+01, train loss: 5.63225e-06, val loss: 7.69424e-06, min loss: 2.67425e-06\n",
      "Epoch: 365400, elapsed: 1.10e+01, train loss: 2.66942e-06, val loss: 3.39379e-06, min loss: 2.66942e-06\n",
      "Epoch: 365500, elapsed: 1.10e+01, train loss: 2.95980e-06, val loss: 3.66627e-06, min loss: 2.66942e-06\n",
      "Epoch: 365600, elapsed: 1.10e+01, train loss: 1.12183e-05, val loss: 1.19788e-05, min loss: 2.66942e-06\n",
      "Epoch: 365700, elapsed: 1.10e+01, train loss: 2.70621e-06, val loss: 3.44381e-06, min loss: 2.66942e-06\n",
      "Epoch: 365800, elapsed: 1.10e+01, train loss: 2.66245e-06, val loss: 3.39564e-06, min loss: 2.66245e-06\n",
      "Epoch: 365900, elapsed: 1.10e+01, train loss: 2.95417e-06, val loss: 3.74390e-06, min loss: 2.66245e-06\n",
      "Epoch: 366000, elapsed: 1.11e+01, train loss: 2.78852e-06, val loss: 3.40833e-06, min loss: 2.66245e-06\n",
      "Epoch: 366100, elapsed: 1.21e+01, train loss: 3.49989e-06, val loss: 3.87180e-06, min loss: 2.66245e-06\n",
      "Epoch: 366200, elapsed: 1.11e+01, train loss: 2.83128e-06, val loss: 3.57362e-06, min loss: 2.66245e-06\n",
      "Epoch: 366300, elapsed: 1.11e+01, train loss: 2.70960e-06, val loss: 3.38669e-06, min loss: 2.66245e-06\n",
      "Epoch: 366400, elapsed: 1.11e+01, train loss: 2.66326e-06, val loss: 3.40071e-06, min loss: 2.66245e-06\n",
      "Epoch: 366500, elapsed: 1.12e+01, train loss: 2.75602e-06, val loss: 3.48376e-06, min loss: 2.66245e-06\n",
      "Epoch: 366600, elapsed: 1.11e+01, train loss: 3.64261e-06, val loss: 4.20688e-06, min loss: 2.66245e-06\n",
      "Epoch: 366700, elapsed: 1.10e+01, train loss: 5.23591e-06, val loss: 5.56871e-06, min loss: 2.66245e-06\n",
      "Epoch: 366800, elapsed: 1.10e+01, train loss: 3.97894e-06, val loss: 4.92500e-06, min loss: 2.66245e-06\n",
      "Epoch: 366900, elapsed: 1.21e+01, train loss: 4.65023e-06, val loss: 5.98023e-06, min loss: 2.66245e-06\n",
      "Epoch: 367000, elapsed: 1.11e+01, train loss: 9.01922e-06, val loss: 8.77008e-06, min loss: 2.66245e-06\n",
      "Epoch: 367100, elapsed: 1.10e+01, train loss: 3.72116e-06, val loss: 4.26203e-06, min loss: 2.66245e-06\n",
      "Epoch: 367200, elapsed: 1.10e+01, train loss: 2.76041e-06, val loss: 3.43142e-06, min loss: 2.66245e-06\n",
      "Epoch: 367300, elapsed: 1.10e+01, train loss: 3.85305e-06, val loss: 4.51373e-06, min loss: 2.66245e-06\n",
      "Epoch: 367400, elapsed: 1.09e+01, train loss: 3.14839e-06, val loss: 3.87189e-06, min loss: 2.66245e-06\n",
      "Epoch: 367500, elapsed: 1.10e+01, train loss: 3.32701e-06, val loss: 4.17298e-06, min loss: 2.66245e-06\n",
      "Epoch: 367600, elapsed: 1.12e+01, train loss: 3.06776e-06, val loss: 3.91082e-06, min loss: 2.66245e-06\n",
      "Epoch: 367700, elapsed: 1.22e+01, train loss: 3.54519e-06, val loss: 4.64415e-06, min loss: 2.66245e-06\n",
      "Epoch: 367800, elapsed: 1.12e+01, train loss: 3.36006e-06, val loss: 4.35266e-06, min loss: 2.66245e-06\n",
      "Epoch: 367900, elapsed: 1.10e+01, train loss: 4.58210e-06, val loss: 5.42437e-06, min loss: 2.66245e-06\n",
      "Epoch: 368000, elapsed: 1.11e+01, train loss: 2.97843e-06, val loss: 3.77143e-06, min loss: 2.66245e-06\n",
      "Epoch: 368100, elapsed: 1.09e+01, train loss: 2.76481e-06, val loss: 3.40647e-06, min loss: 2.66245e-06\n",
      "Epoch: 368200, elapsed: 1.11e+01, train loss: 2.95427e-06, val loss: 3.77342e-06, min loss: 2.66245e-06\n",
      "Epoch: 368300, elapsed: 1.10e+01, train loss: 3.83252e-06, val loss: 4.30220e-06, min loss: 2.66245e-06\n",
      "Epoch: 368400, elapsed: 1.12e+01, train loss: 8.74725e-06, val loss: 9.00682e-06, min loss: 2.66245e-06\n",
      "Epoch: 368500, elapsed: 1.22e+01, train loss: 4.24697e-06, val loss: 4.71554e-06, min loss: 2.66245e-06\n",
      "Epoch: 368600, elapsed: 1.12e+01, train loss: 2.79622e-06, val loss: 3.48384e-06, min loss: 2.66245e-06\n",
      "Epoch: 368700, elapsed: 1.12e+01, train loss: 2.96339e-06, val loss: 3.90017e-06, min loss: 2.66245e-06\n",
      "Epoch: 368800, elapsed: 1.11e+01, train loss: 4.00412e-06, val loss: 5.02460e-06, min loss: 2.66245e-06\n",
      "Epoch: 368900, elapsed: 1.10e+01, train loss: 3.80079e-06, val loss: 4.84505e-06, min loss: 2.66245e-06\n",
      "Epoch: 369000, elapsed: 1.11e+01, train loss: 2.76509e-06, val loss: 3.56166e-06, min loss: 2.66245e-06\n",
      "Epoch: 369100, elapsed: 1.09e+01, train loss: 2.64304e-06, val loss: 3.36684e-06, min loss: 2.64304e-06\n",
      "Epoch: 369200, elapsed: 1.10e+01, train loss: 3.50897e-06, val loss: 4.15018e-06, min loss: 2.64304e-06\n",
      "Epoch: 369300, elapsed: 1.22e+01, train loss: 3.60321e-06, val loss: 4.30207e-06, min loss: 2.64304e-06\n",
      "Epoch: 369400, elapsed: 1.13e+01, train loss: 3.09392e-06, val loss: 3.83906e-06, min loss: 2.64304e-06\n",
      "Epoch: 369500, elapsed: 1.11e+01, train loss: 2.77915e-06, val loss: 3.50406e-06, min loss: 2.64304e-06\n",
      "Epoch: 369600, elapsed: 1.10e+01, train loss: 2.79314e-06, val loss: 3.37597e-06, min loss: 2.64304e-06\n",
      "Epoch: 369700, elapsed: 1.09e+01, train loss: 3.60034e-06, val loss: 4.77086e-06, min loss: 2.64304e-06\n",
      "Epoch: 369800, elapsed: 1.09e+01, train loss: 4.66342e-06, val loss: 4.00732e-06, min loss: 2.64304e-06\n",
      "Epoch: 369900, elapsed: 1.10e+01, train loss: 2.67738e-06, val loss: 3.39448e-06, min loss: 2.64304e-06\n",
      "Epoch: 370000, elapsed: 1.10e+01, train loss: 2.64178e-06, val loss: 3.38935e-06, min loss: 2.64178e-06\n",
      "Epoch: 370100, elapsed: 1.42e+01, train loss: 2.64543e-06, val loss: 3.37015e-06, min loss: 2.64178e-06\n",
      "Epoch: 370200, elapsed: 1.11e+01, train loss: 2.69277e-06, val loss: 3.39760e-06, min loss: 2.64178e-06\n",
      "Epoch: 370300, elapsed: 1.11e+01, train loss: 2.63506e-06, val loss: 3.36283e-06, min loss: 2.63506e-06\n",
      "Epoch: 370400, elapsed: 1.11e+01, train loss: 2.66583e-06, val loss: 3.37235e-06, min loss: 2.63506e-06\n",
      "Epoch: 370500, elapsed: 1.10e+01, train loss: 1.15983e-05, val loss: 1.32652e-05, min loss: 2.63506e-06\n",
      "Epoch: 370600, elapsed: 1.09e+01, train loss: 2.63830e-06, val loss: 3.39103e-06, min loss: 2.63506e-06\n",
      "Epoch: 370700, elapsed: 1.11e+01, train loss: 2.64219e-06, val loss: 3.36231e-06, min loss: 2.63506e-06\n",
      "Epoch: 370800, elapsed: 1.09e+01, train loss: 2.63624e-06, val loss: 3.39067e-06, min loss: 2.63506e-06\n",
      "Epoch: 370900, elapsed: 1.23e+01, train loss: 2.83017e-06, val loss: 4.26000e-06, min loss: 2.63506e-06\n",
      "Epoch: 371000, elapsed: 1.10e+01, train loss: 3.96944e-06, val loss: 4.85960e-06, min loss: 2.63506e-06\n",
      "Epoch: 371100, elapsed: 1.10e+01, train loss: 1.13869e-05, val loss: 1.30038e-05, min loss: 2.63506e-06\n",
      "Epoch: 371200, elapsed: 1.10e+01, train loss: 3.25300e-06, val loss: 4.14114e-06, min loss: 2.63506e-06\n",
      "Epoch: 371300, elapsed: 1.09e+01, train loss: 5.42343e-06, val loss: 6.65425e-06, min loss: 2.63506e-06\n",
      "Epoch: 371400, elapsed: 1.10e+01, train loss: 3.35619e-06, val loss: 4.71294e-06, min loss: 2.63506e-06\n",
      "Epoch: 371500, elapsed: 1.09e+01, train loss: 4.72484e-06, val loss: 5.70714e-06, min loss: 2.63506e-06\n",
      "Epoch: 371600, elapsed: 1.10e+01, train loss: 3.83896e-06, val loss: 3.99215e-06, min loss: 2.63506e-06\n",
      "Epoch: 371700, elapsed: 1.22e+01, train loss: 2.85205e-06, val loss: 3.39522e-06, min loss: 2.63506e-06\n",
      "Epoch: 371800, elapsed: 1.11e+01, train loss: 6.71556e-06, val loss: 7.34329e-06, min loss: 2.63506e-06\n",
      "Epoch: 371900, elapsed: 1.11e+01, train loss: 2.83522e-06, val loss: 3.71319e-06, min loss: 2.63506e-06\n",
      "Epoch: 372000, elapsed: 1.10e+01, train loss: 2.64365e-06, val loss: 3.36029e-06, min loss: 2.63506e-06\n",
      "Epoch: 372100, elapsed: 1.08e+01, train loss: 3.80617e-06, val loss: 4.37828e-06, min loss: 2.63506e-06\n",
      "Epoch: 372200, elapsed: 1.11e+01, train loss: 2.61750e-06, val loss: 3.34244e-06, min loss: 2.61750e-06\n",
      "Epoch: 372300, elapsed: 1.09e+01, train loss: 2.63795e-06, val loss: 3.37119e-06, min loss: 2.61750e-06\n",
      "Epoch: 372400, elapsed: 1.09e+01, train loss: 3.04513e-06, val loss: 3.80061e-06, min loss: 2.61750e-06\n",
      "Epoch: 372500, elapsed: 1.21e+01, train loss: 3.22209e-06, val loss: 3.72485e-06, min loss: 2.61750e-06\n",
      "Epoch: 372600, elapsed: 1.12e+01, train loss: 4.53386e-06, val loss: 5.18593e-06, min loss: 2.61750e-06\n",
      "Epoch: 372700, elapsed: 1.10e+01, train loss: 4.56111e-06, val loss: 5.11177e-06, min loss: 2.61750e-06\n",
      "Epoch: 372800, elapsed: 1.11e+01, train loss: 4.51173e-06, val loss: 5.62070e-06, min loss: 2.61750e-06\n",
      "Epoch: 372900, elapsed: 1.10e+01, train loss: 2.90263e-06, val loss: 3.61461e-06, min loss: 2.61750e-06\n",
      "Epoch: 373000, elapsed: 1.10e+01, train loss: 3.10535e-06, val loss: 3.92548e-06, min loss: 2.61750e-06\n",
      "Epoch: 373100, elapsed: 1.11e+01, train loss: 3.11353e-06, val loss: 3.90384e-06, min loss: 2.61750e-06\n",
      "Epoch: 373200, elapsed: 1.09e+01, train loss: 5.06175e-06, val loss: 5.40319e-06, min loss: 2.61750e-06\n",
      "Epoch: 373300, elapsed: 1.10e+01, train loss: 2.85257e-06, val loss: 4.52375e-06, min loss: 2.61750e-06\n",
      "Epoch: 373400, elapsed: 1.24e+01, train loss: 2.65868e-06, val loss: 3.38786e-06, min loss: 2.61750e-06\n",
      "Epoch: 373500, elapsed: 1.10e+01, train loss: 2.75724e-06, val loss: 3.46764e-06, min loss: 2.61750e-06\n",
      "Epoch: 373600, elapsed: 1.10e+01, train loss: 2.60839e-06, val loss: 3.33496e-06, min loss: 2.60839e-06\n",
      "Epoch: 373700, elapsed: 1.09e+01, train loss: 2.64549e-06, val loss: 3.34175e-06, min loss: 2.60839e-06\n",
      "Epoch: 373800, elapsed: 1.10e+01, train loss: 2.80414e-06, val loss: 3.58088e-06, min loss: 2.60839e-06\n",
      "Epoch: 373900, elapsed: 1.10e+01, train loss: 6.84554e-06, val loss: 8.22364e-06, min loss: 2.60839e-06\n",
      "Epoch: 374000, elapsed: 1.10e+01, train loss: 3.24336e-06, val loss: 3.59743e-06, min loss: 2.60839e-06\n",
      "Epoch: 374100, elapsed: 1.09e+01, train loss: 2.67949e-06, val loss: 3.39222e-06, min loss: 2.60839e-06\n",
      "Epoch: 374200, elapsed: 1.22e+01, train loss: 2.66188e-06, val loss: 3.38981e-06, min loss: 2.60839e-06\n",
      "Epoch: 374300, elapsed: 1.10e+01, train loss: 2.69673e-06, val loss: 3.37725e-06, min loss: 2.60839e-06\n",
      "Epoch: 374400, elapsed: 1.12e+01, train loss: 3.34300e-06, val loss: 3.75125e-06, min loss: 2.60839e-06\n",
      "Epoch: 374500, elapsed: 1.10e+01, train loss: 3.78734e-06, val loss: 4.79862e-06, min loss: 2.60839e-06\n",
      "Epoch: 374600, elapsed: 1.11e+01, train loss: 1.17183e-05, val loss: 1.17287e-05, min loss: 2.60839e-06\n",
      "Epoch: 374700, elapsed: 1.10e+01, train loss: 2.68000e-06, val loss: 3.34792e-06, min loss: 2.60839e-06\n",
      "Epoch: 374800, elapsed: 1.10e+01, train loss: 2.60587e-06, val loss: 3.33729e-06, min loss: 2.60587e-06\n",
      "Epoch: 374900, elapsed: 1.09e+01, train loss: 3.43775e-06, val loss: 4.39227e-06, min loss: 2.60587e-06\n",
      "Epoch: 375000, elapsed: 1.23e+01, train loss: 1.06501e-05, val loss: 1.04195e-05, min loss: 2.60587e-06\n",
      "Epoch: 375100, elapsed: 1.31e+01, train loss: 1.06478e-05, val loss: 1.05617e-05, min loss: 2.60587e-06\n",
      "Epoch: 375200, elapsed: 1.10e+01, train loss: 9.01632e-06, val loss: 6.28947e-06, min loss: 2.60587e-06\n",
      "Epoch: 375300, elapsed: 1.09e+01, train loss: 2.59691e-06, val loss: 3.31919e-06, min loss: 2.59691e-06\n",
      "Epoch: 375400, elapsed: 1.11e+01, train loss: 2.60891e-06, val loss: 3.31469e-06, min loss: 2.59691e-06\n",
      "Epoch: 375500, elapsed: 1.11e+01, train loss: 2.72442e-06, val loss: 3.95589e-06, min loss: 2.59691e-06\n",
      "Epoch: 375600, elapsed: 1.09e+01, train loss: 3.95816e-06, val loss: 5.01898e-06, min loss: 2.59691e-06\n",
      "Epoch: 375700, elapsed: 1.10e+01, train loss: 7.16322e-06, val loss: 8.14521e-06, min loss: 2.59691e-06\n",
      "Epoch: 375800, elapsed: 1.24e+01, train loss: 2.88982e-06, val loss: 3.79955e-06, min loss: 2.59691e-06\n",
      "Epoch: 375900, elapsed: 1.09e+01, train loss: 2.70123e-06, val loss: 3.41557e-06, min loss: 2.59691e-06\n",
      "Epoch: 376000, elapsed: 1.10e+01, train loss: 2.65340e-06, val loss: 3.36657e-06, min loss: 2.59691e-06\n",
      "Epoch: 376100, elapsed: 1.08e+01, train loss: 2.84551e-06, val loss: 3.52927e-06, min loss: 2.59691e-06\n",
      "Epoch: 376200, elapsed: 1.12e+01, train loss: 7.08043e-06, val loss: 8.87741e-06, min loss: 2.59691e-06\n",
      "Epoch: 376300, elapsed: 1.08e+01, train loss: 4.65043e-06, val loss: 6.07244e-06, min loss: 2.59691e-06\n",
      "Epoch: 376400, elapsed: 1.09e+01, train loss: 4.46759e-06, val loss: 4.01357e-06, min loss: 2.59691e-06\n",
      "Epoch: 376500, elapsed: 1.10e+01, train loss: 4.96057e-06, val loss: 6.52045e-06, min loss: 2.59691e-06\n",
      "Epoch: 376600, elapsed: 1.23e+01, train loss: 2.62114e-06, val loss: 3.35680e-06, min loss: 2.59691e-06\n",
      "Epoch: 376700, elapsed: 1.10e+01, train loss: 2.62088e-06, val loss: 3.33492e-06, min loss: 2.59691e-06\n",
      "Epoch: 376800, elapsed: 1.10e+01, train loss: 2.63924e-06, val loss: 3.39245e-06, min loss: 2.59691e-06\n",
      "Epoch: 376900, elapsed: 1.09e+01, train loss: 2.60123e-06, val loss: 3.32001e-06, min loss: 2.59691e-06\n",
      "Epoch: 377000, elapsed: 1.10e+01, train loss: 2.60677e-06, val loss: 3.32059e-06, min loss: 2.59691e-06\n",
      "Epoch: 377100, elapsed: 1.10e+01, train loss: 2.59274e-06, val loss: 3.31313e-06, min loss: 2.59274e-06\n",
      "Epoch: 377200, elapsed: 1.12e+01, train loss: 2.61004e-06, val loss: 3.32358e-06, min loss: 2.59274e-06\n",
      "Epoch: 377300, elapsed: 1.08e+01, train loss: 2.65744e-06, val loss: 3.36356e-06, min loss: 2.59274e-06\n",
      "Epoch: 377400, elapsed: 1.22e+01, train loss: 2.64604e-06, val loss: 3.35973e-06, min loss: 2.59274e-06\n",
      "Epoch: 377500, elapsed: 1.10e+01, train loss: 2.61255e-06, val loss: 3.32875e-06, min loss: 2.59274e-06\n",
      "Epoch: 377600, elapsed: 1.11e+01, train loss: 3.37793e-06, val loss: 4.12302e-06, min loss: 2.59274e-06\n",
      "Epoch: 377700, elapsed: 1.09e+01, train loss: 3.02731e-06, val loss: 3.38572e-06, min loss: 2.59274e-06\n",
      "Epoch: 377800, elapsed: 1.10e+01, train loss: 5.14254e-06, val loss: 6.04633e-06, min loss: 2.59274e-06\n",
      "Epoch: 377900, elapsed: 1.10e+01, train loss: 4.40752e-06, val loss: 4.39590e-06, min loss: 2.59274e-06\n",
      "Epoch: 378000, elapsed: 1.10e+01, train loss: 3.48724e-06, val loss: 5.20885e-06, min loss: 2.59274e-06\n",
      "Epoch: 378100, elapsed: 1.10e+01, train loss: 3.90872e-06, val loss: 4.61614e-06, min loss: 2.59274e-06\n",
      "Epoch: 378200, elapsed: 1.22e+01, train loss: 3.16703e-06, val loss: 4.00683e-06, min loss: 2.59274e-06\n",
      "Epoch: 378300, elapsed: 1.11e+01, train loss: 6.10801e-06, val loss: 6.12001e-06, min loss: 2.59274e-06\n",
      "Epoch: 378400, elapsed: 1.10e+01, train loss: 2.61958e-06, val loss: 3.34410e-06, min loss: 2.59274e-06\n",
      "Epoch: 378500, elapsed: 1.10e+01, train loss: 2.60389e-06, val loss: 3.35563e-06, min loss: 2.59274e-06\n",
      "Epoch: 378600, elapsed: 1.10e+01, train loss: 3.11810e-06, val loss: 3.85217e-06, min loss: 2.59274e-06\n",
      "Epoch: 378700, elapsed: 1.10e+01, train loss: 2.86375e-06, val loss: 3.42192e-06, min loss: 2.59274e-06\n",
      "Epoch: 378800, elapsed: 1.11e+01, train loss: 9.88138e-06, val loss: 8.59049e-06, min loss: 2.59274e-06\n",
      "Epoch: 378900, elapsed: 1.09e+01, train loss: 8.40374e-06, val loss: 7.38142e-06, min loss: 2.59274e-06\n",
      "Epoch: 379000, elapsed: 1.22e+01, train loss: 4.13045e-06, val loss: 5.42018e-06, min loss: 2.59274e-06\n",
      "Epoch: 379100, elapsed: 1.12e+01, train loss: 2.57511e-06, val loss: 3.29373e-06, min loss: 2.57511e-06\n",
      "Epoch: 379200, elapsed: 1.10e+01, train loss: 2.58835e-06, val loss: 3.30621e-06, min loss: 2.57511e-06\n",
      "Epoch: 379300, elapsed: 1.09e+01, train loss: 2.61586e-06, val loss: 3.32365e-06, min loss: 2.57511e-06\n",
      "Epoch: 379400, elapsed: 1.11e+01, train loss: 3.01489e-06, val loss: 3.50885e-06, min loss: 2.57511e-06\n",
      "Epoch: 379500, elapsed: 1.09e+01, train loss: 2.60523e-06, val loss: 3.38758e-06, min loss: 2.57511e-06\n",
      "Epoch: 379600, elapsed: 1.10e+01, train loss: 2.89178e-06, val loss: 3.66699e-06, min loss: 2.57511e-06\n",
      "Epoch: 379700, elapsed: 1.10e+01, train loss: 2.85390e-06, val loss: 3.85466e-06, min loss: 2.57511e-06\n",
      "Epoch: 379800, elapsed: 1.09e+01, train loss: 2.92129e-06, val loss: 3.69514e-06, min loss: 2.57511e-06\n",
      "Epoch: 379900, elapsed: 1.25e+01, train loss: 7.14968e-06, val loss: 7.78795e-06, min loss: 2.57511e-06\n",
      "Epoch: 380000, elapsed: 1.12e+01, train loss: 2.94674e-06, val loss: 3.75038e-06, min loss: 2.57511e-06\n",
      "Epoch: 380100, elapsed: 1.31e+01, train loss: 2.79344e-06, val loss: 3.49403e-06, min loss: 2.57511e-06\n",
      "Epoch: 380200, elapsed: 1.11e+01, train loss: 2.88811e-06, val loss: 3.63525e-06, min loss: 2.57511e-06\n",
      "Epoch: 380300, elapsed: 1.10e+01, train loss: 3.78468e-06, val loss: 4.09849e-06, min loss: 2.57511e-06\n",
      "Epoch: 380400, elapsed: 1.09e+01, train loss: 3.95990e-06, val loss: 4.41529e-06, min loss: 2.57511e-06\n",
      "Epoch: 380500, elapsed: 1.11e+01, train loss: 2.67810e-06, val loss: 3.37999e-06, min loss: 2.57511e-06\n",
      "Epoch: 380600, elapsed: 1.22e+01, train loss: 2.64302e-06, val loss: 3.35868e-06, min loss: 2.57511e-06\n",
      "Epoch: 380700, elapsed: 1.13e+01, train loss: 2.92601e-06, val loss: 3.60932e-06, min loss: 2.57511e-06\n",
      "Epoch: 380800, elapsed: 1.11e+01, train loss: 2.56894e-06, val loss: 3.29559e-06, min loss: 2.56894e-06\n",
      "Epoch: 380900, elapsed: 1.09e+01, train loss: 2.70623e-06, val loss: 3.41202e-06, min loss: 2.56894e-06\n",
      "Epoch: 381000, elapsed: 1.10e+01, train loss: 5.88552e-06, val loss: 6.71981e-06, min loss: 2.56894e-06\n",
      "Epoch: 381100, elapsed: 1.10e+01, train loss: 2.82991e-06, val loss: 3.56230e-06, min loss: 2.56894e-06\n",
      "Epoch: 381200, elapsed: 1.10e+01, train loss: 2.90854e-06, val loss: 3.65189e-06, min loss: 2.56894e-06\n",
      "Epoch: 381300, elapsed: 1.10e+01, train loss: 4.15828e-06, val loss: 5.01253e-06, min loss: 2.56894e-06\n",
      "Epoch: 381400, elapsed: 1.10e+01, train loss: 2.72624e-06, val loss: 3.48926e-06, min loss: 2.56894e-06\n",
      "Epoch: 381500, elapsed: 1.24e+01, train loss: 6.55499e-06, val loss: 7.23899e-06, min loss: 2.56894e-06\n",
      "Epoch: 381600, elapsed: 1.12e+01, train loss: 2.66433e-06, val loss: 3.33442e-06, min loss: 2.56894e-06\n",
      "Epoch: 381700, elapsed: 1.12e+01, train loss: 5.68419e-06, val loss: 6.43223e-06, min loss: 2.56894e-06\n",
      "Epoch: 381800, elapsed: 1.11e+01, train loss: 2.86568e-06, val loss: 3.58400e-06, min loss: 2.56894e-06\n",
      "Epoch: 381900, elapsed: 1.09e+01, train loss: 2.71104e-06, val loss: 3.43405e-06, min loss: 2.56894e-06\n",
      "Epoch: 382000, elapsed: 1.10e+01, train loss: 2.57744e-06, val loss: 3.30395e-06, min loss: 2.56894e-06\n",
      "Epoch: 382100, elapsed: 1.09e+01, train loss: 2.67795e-06, val loss: 3.41428e-06, min loss: 2.56894e-06\n",
      "Epoch: 382200, elapsed: 1.08e+01, train loss: 2.79441e-06, val loss: 3.67733e-06, min loss: 2.56894e-06\n",
      "Epoch: 382300, elapsed: 1.23e+01, train loss: 2.90650e-06, val loss: 3.83538e-06, min loss: 2.56894e-06\n",
      "Epoch: 382400, elapsed: 1.10e+01, train loss: 3.42661e-06, val loss: 4.50929e-06, min loss: 2.56894e-06\n",
      "Epoch: 382500, elapsed: 1.10e+01, train loss: 2.62259e-06, val loss: 3.30935e-06, min loss: 2.56894e-06\n",
      "Epoch: 382600, elapsed: 1.11e+01, train loss: 2.83079e-06, val loss: 3.52328e-06, min loss: 2.56894e-06\n",
      "Epoch: 382700, elapsed: 1.09e+01, train loss: 4.15591e-06, val loss: 6.09066e-06, min loss: 2.56894e-06\n",
      "Epoch: 382800, elapsed: 1.10e+01, train loss: 1.15354e-05, val loss: 1.23755e-05, min loss: 2.56894e-06\n",
      "Epoch: 382900, elapsed: 1.10e+01, train loss: 3.11224e-06, val loss: 4.01076e-06, min loss: 2.56894e-06\n",
      "Epoch: 383000, elapsed: 1.09e+01, train loss: 4.69015e-06, val loss: 5.73944e-06, min loss: 2.56894e-06\n",
      "Epoch: 383100, elapsed: 1.22e+01, train loss: 2.96840e-06, val loss: 3.57486e-06, min loss: 2.56894e-06\n",
      "Epoch: 383200, elapsed: 1.13e+01, train loss: 2.66330e-06, val loss: 3.37560e-06, min loss: 2.56894e-06\n",
      "Epoch: 383300, elapsed: 1.10e+01, train loss: 2.72705e-06, val loss: 3.36970e-06, min loss: 2.56894e-06\n",
      "Epoch: 383400, elapsed: 1.09e+01, train loss: 5.91469e-06, val loss: 7.15351e-06, min loss: 2.56894e-06\n",
      "Epoch: 383500, elapsed: 1.08e+01, train loss: 4.20836e-06, val loss: 4.73707e-06, min loss: 2.56894e-06\n",
      "Epoch: 383600, elapsed: 1.10e+01, train loss: 3.61533e-06, val loss: 4.11743e-06, min loss: 2.56894e-06\n",
      "Epoch: 383700, elapsed: 1.10e+01, train loss: 3.13729e-06, val loss: 4.11370e-06, min loss: 2.56894e-06\n",
      "Epoch: 383800, elapsed: 1.08e+01, train loss: 2.62375e-06, val loss: 3.37828e-06, min loss: 2.56894e-06\n",
      "Epoch: 383900, elapsed: 1.11e+01, train loss: 3.89748e-06, val loss: 4.34607e-06, min loss: 2.56894e-06\n",
      "Epoch: 384000, elapsed: 1.25e+01, train loss: 2.54569e-06, val loss: 3.26317e-06, min loss: 2.54569e-06\n",
      "Epoch: 384100, elapsed: 1.10e+01, train loss: 2.73962e-06, val loss: 3.58054e-06, min loss: 2.54569e-06\n",
      "Epoch: 384200, elapsed: 1.09e+01, train loss: 2.55627e-06, val loss: 3.27454e-06, min loss: 2.54569e-06\n",
      "Epoch: 384300, elapsed: 1.10e+01, train loss: 2.73637e-06, val loss: 3.44669e-06, min loss: 2.54569e-06\n",
      "Epoch: 384400, elapsed: 1.11e+01, train loss: 5.00543e-06, val loss: 5.85170e-06, min loss: 2.54569e-06\n",
      "Epoch: 384500, elapsed: 1.09e+01, train loss: 5.08195e-06, val loss: 6.16308e-06, min loss: 2.54569e-06\n",
      "Epoch: 384600, elapsed: 1.10e+01, train loss: 1.11130e-05, val loss: 1.15972e-05, min loss: 2.54569e-06\n",
      "Epoch: 384700, elapsed: 1.10e+01, train loss: 9.39394e-06, val loss: 9.57197e-06, min loss: 2.54569e-06\n",
      "Epoch: 384800, elapsed: 1.22e+01, train loss: 3.91723e-06, val loss: 3.97218e-06, min loss: 2.54569e-06\n",
      "Epoch: 384900, elapsed: 1.09e+01, train loss: 2.86497e-06, val loss: 3.60586e-06, min loss: 2.54569e-06\n",
      "Epoch: 385000, elapsed: 1.11e+01, train loss: 2.82710e-06, val loss: 3.65292e-06, min loss: 2.54569e-06\n",
      "Epoch: 385100, elapsed: 1.30e+01, train loss: 2.87052e-06, val loss: 3.65036e-06, min loss: 2.54569e-06\n",
      "Epoch: 385200, elapsed: 1.10e+01, train loss: 2.54036e-06, val loss: 3.27167e-06, min loss: 2.54036e-06\n",
      "Epoch: 385300, elapsed: 1.11e+01, train loss: 2.56696e-06, val loss: 3.27916e-06, min loss: 2.54036e-06\n",
      "Epoch: 385400, elapsed: 1.09e+01, train loss: 3.93938e-06, val loss: 4.72275e-06, min loss: 2.54036e-06\n",
      "Epoch: 385500, elapsed: 1.09e+01, train loss: 2.54606e-06, val loss: 3.25889e-06, min loss: 2.54036e-06\n",
      "Epoch: 385600, elapsed: 1.23e+01, train loss: 2.69817e-06, val loss: 3.41957e-06, min loss: 2.54036e-06\n",
      "Epoch: 385700, elapsed: 1.11e+01, train loss: 2.64883e-06, val loss: 3.31070e-06, min loss: 2.54036e-06\n",
      "Epoch: 385800, elapsed: 1.10e+01, train loss: 2.79175e-06, val loss: 3.43365e-06, min loss: 2.54036e-06\n",
      "Epoch: 385900, elapsed: 1.10e+01, train loss: 2.54009e-06, val loss: 3.26754e-06, min loss: 2.54009e-06\n",
      "Epoch: 386000, elapsed: 1.10e+01, train loss: 3.17709e-06, val loss: 4.04372e-06, min loss: 2.54009e-06\n",
      "Epoch: 386100, elapsed: 1.10e+01, train loss: 3.24130e-06, val loss: 4.00440e-06, min loss: 2.54009e-06\n",
      "Epoch: 386200, elapsed: 1.09e+01, train loss: 2.61546e-06, val loss: 3.36525e-06, min loss: 2.54009e-06\n",
      "Epoch: 386300, elapsed: 1.09e+01, train loss: 2.79417e-06, val loss: 3.56397e-06, min loss: 2.54009e-06\n",
      "Epoch: 386400, elapsed: 1.23e+01, train loss: 5.61612e-06, val loss: 5.03872e-06, min loss: 2.54009e-06\n",
      "Epoch: 386500, elapsed: 1.10e+01, train loss: 4.98587e-06, val loss: 5.80770e-06, min loss: 2.54009e-06\n",
      "Epoch: 386600, elapsed: 1.11e+01, train loss: 3.35777e-06, val loss: 4.75385e-06, min loss: 2.54009e-06\n",
      "Epoch: 386700, elapsed: 1.09e+01, train loss: 5.05046e-06, val loss: 5.51926e-06, min loss: 2.54009e-06\n",
      "Epoch: 386800, elapsed: 1.09e+01, train loss: 9.82106e-06, val loss: 6.66064e-06, min loss: 2.54009e-06\n",
      "Epoch: 386900, elapsed: 1.11e+01, train loss: 2.54124e-06, val loss: 3.24439e-06, min loss: 2.54009e-06\n",
      "Epoch: 387000, elapsed: 1.11e+01, train loss: 2.52975e-06, val loss: 3.24058e-06, min loss: 2.52975e-06\n",
      "Epoch: 387100, elapsed: 1.10e+01, train loss: 2.73734e-06, val loss: 3.38823e-06, min loss: 2.52975e-06\n",
      "Epoch: 387200, elapsed: 1.12e+01, train loss: 3.35371e-06, val loss: 3.65476e-06, min loss: 2.52975e-06\n",
      "Epoch: 387300, elapsed: 1.23e+01, train loss: 1.08300e-05, val loss: 7.51953e-06, min loss: 2.52975e-06\n",
      "Epoch: 387400, elapsed: 1.10e+01, train loss: 2.52682e-06, val loss: 3.24211e-06, min loss: 2.52682e-06\n",
      "Epoch: 387500, elapsed: 1.12e+01, train loss: 2.55604e-06, val loss: 3.24265e-06, min loss: 2.52682e-06\n",
      "Epoch: 387600, elapsed: 1.10e+01, train loss: 2.58774e-06, val loss: 3.27520e-06, min loss: 2.52682e-06\n",
      "Epoch: 387700, elapsed: 1.11e+01, train loss: 2.60063e-06, val loss: 3.26932e-06, min loss: 2.52682e-06\n",
      "Epoch: 387800, elapsed: 1.13e+01, train loss: 2.71137e-06, val loss: 3.35089e-06, min loss: 2.52682e-06\n",
      "Epoch: 387900, elapsed: 1.12e+01, train loss: 2.95198e-06, val loss: 3.69944e-06, min loss: 2.52682e-06\n",
      "Epoch: 388000, elapsed: 1.11e+01, train loss: 3.18107e-06, val loss: 4.17526e-06, min loss: 2.52682e-06\n",
      "Epoch: 388100, elapsed: 1.23e+01, train loss: 2.56142e-06, val loss: 3.27012e-06, min loss: 2.52682e-06\n",
      "Epoch: 388200, elapsed: 1.11e+01, train loss: 2.53633e-06, val loss: 3.27706e-06, min loss: 2.52682e-06\n",
      "Epoch: 388300, elapsed: 1.12e+01, train loss: 2.55258e-06, val loss: 3.27542e-06, min loss: 2.52682e-06\n",
      "Epoch: 388400, elapsed: 1.09e+01, train loss: 2.73600e-06, val loss: 3.43253e-06, min loss: 2.52682e-06\n",
      "Epoch: 388500, elapsed: 1.08e+01, train loss: 3.35734e-06, val loss: 4.27257e-06, min loss: 2.52682e-06\n",
      "Epoch: 388600, elapsed: 1.10e+01, train loss: 2.93555e-06, val loss: 3.27244e-06, min loss: 2.52682e-06\n",
      "Epoch: 388700, elapsed: 1.11e+01, train loss: 2.62068e-06, val loss: 3.32512e-06, min loss: 2.52682e-06\n",
      "Epoch: 388800, elapsed: 1.10e+01, train loss: 5.40137e-06, val loss: 6.20812e-06, min loss: 2.52682e-06\n",
      "Epoch: 388900, elapsed: 1.22e+01, train loss: 2.52494e-06, val loss: 3.24503e-06, min loss: 2.52494e-06\n",
      "Epoch: 389000, elapsed: 1.12e+01, train loss: 3.04951e-06, val loss: 3.61573e-06, min loss: 2.52494e-06\n",
      "Epoch: 389100, elapsed: 1.09e+01, train loss: 2.53550e-06, val loss: 3.25753e-06, min loss: 2.52494e-06\n",
      "Epoch: 389200, elapsed: 1.10e+01, train loss: 2.55032e-06, val loss: 3.25415e-06, min loss: 2.52494e-06\n",
      "Epoch: 389300, elapsed: 1.11e+01, train loss: 2.55741e-06, val loss: 3.25708e-06, min loss: 2.52494e-06\n",
      "Epoch: 389400, elapsed: 1.10e+01, train loss: 2.52050e-06, val loss: 3.24273e-06, min loss: 2.52050e-06\n",
      "Epoch: 389500, elapsed: 1.11e+01, train loss: 3.48594e-06, val loss: 4.39909e-06, min loss: 2.52050e-06\n",
      "Epoch: 389600, elapsed: 1.10e+01, train loss: 1.04267e-05, val loss: 1.13966e-05, min loss: 2.52050e-06\n",
      "Epoch: 389700, elapsed: 1.11e+01, train loss: 4.85100e-06, val loss: 5.97125e-06, min loss: 2.52050e-06\n",
      "Epoch: 389800, elapsed: 1.24e+01, train loss: 3.68598e-06, val loss: 4.43265e-06, min loss: 2.52050e-06\n",
      "Epoch: 389900, elapsed: 1.12e+01, train loss: 2.61287e-06, val loss: 3.52559e-06, min loss: 2.52050e-06\n",
      "Epoch: 390000, elapsed: 1.10e+01, train loss: 3.21155e-06, val loss: 4.10799e-06, min loss: 2.52050e-06\n",
      "Epoch: 390100, elapsed: 1.31e+01, train loss: 8.09077e-06, val loss: 6.74467e-06, min loss: 2.52050e-06\n",
      "Epoch: 390200, elapsed: 1.11e+01, train loss: 7.31407e-06, val loss: 6.45550e-06, min loss: 2.52050e-06\n",
      "Epoch: 390300, elapsed: 1.10e+01, train loss: 3.56356e-06, val loss: 4.10537e-06, min loss: 2.52050e-06\n",
      "Epoch: 390400, elapsed: 1.10e+01, train loss: 2.62097e-06, val loss: 3.30657e-06, min loss: 2.52050e-06\n",
      "Epoch: 390500, elapsed: 1.09e+01, train loss: 2.88694e-06, val loss: 3.75721e-06, min loss: 2.52050e-06\n",
      "Epoch: 390600, elapsed: 1.24e+01, train loss: 2.58213e-06, val loss: 3.32322e-06, min loss: 2.52050e-06\n",
      "Epoch: 390700, elapsed: 1.10e+01, train loss: 2.53255e-06, val loss: 3.29569e-06, min loss: 2.52050e-06\n",
      "Epoch: 390800, elapsed: 1.10e+01, train loss: 2.82895e-06, val loss: 3.56164e-06, min loss: 2.52050e-06\n",
      "Epoch: 390900, elapsed: 1.10e+01, train loss: 2.84953e-06, val loss: 3.71024e-06, min loss: 2.52050e-06\n",
      "Epoch: 391000, elapsed: 1.10e+01, train loss: 2.77223e-06, val loss: 3.56018e-06, min loss: 2.52050e-06\n",
      "Epoch: 391100, elapsed: 1.10e+01, train loss: 2.59270e-06, val loss: 3.33563e-06, min loss: 2.52050e-06\n",
      "Epoch: 391200, elapsed: 1.11e+01, train loss: 7.21620e-06, val loss: 7.37535e-06, min loss: 2.52050e-06\n",
      "Epoch: 391300, elapsed: 1.11e+01, train loss: 2.66235e-06, val loss: 3.57413e-06, min loss: 2.52050e-06\n",
      "Epoch: 391400, elapsed: 1.26e+01, train loss: 2.74368e-06, val loss: 3.58611e-06, min loss: 2.52050e-06\n",
      "Epoch: 391500, elapsed: 1.09e+01, train loss: 3.06059e-06, val loss: 3.75166e-06, min loss: 2.52050e-06\n",
      "Epoch: 391600, elapsed: 1.10e+01, train loss: 2.56859e-06, val loss: 3.25657e-06, min loss: 2.52050e-06\n",
      "Epoch: 391700, elapsed: 1.08e+01, train loss: 2.51444e-06, val loss: 3.22512e-06, min loss: 2.51444e-06\n",
      "Epoch: 391800, elapsed: 1.10e+01, train loss: 2.50130e-06, val loss: 3.22378e-06, min loss: 2.50130e-06\n",
      "Epoch: 391900, elapsed: 1.09e+01, train loss: 2.49727e-06, val loss: 3.21803e-06, min loss: 2.49727e-06\n",
      "Epoch: 392000, elapsed: 1.11e+01, train loss: 2.51135e-06, val loss: 3.31316e-06, min loss: 2.49727e-06\n",
      "Epoch: 392100, elapsed: 1.09e+01, train loss: 5.51789e-06, val loss: 5.60022e-06, min loss: 2.49727e-06\n",
      "Epoch: 392200, elapsed: 1.09e+01, train loss: 2.75953e-06, val loss: 3.29667e-06, min loss: 2.49727e-06\n",
      "Epoch: 392300, elapsed: 1.22e+01, train loss: 3.21622e-06, val loss: 3.90115e-06, min loss: 2.49727e-06\n",
      "Epoch: 392400, elapsed: 1.09e+01, train loss: 2.51277e-06, val loss: 3.23964e-06, min loss: 2.49727e-06\n",
      "Epoch: 392500, elapsed: 1.10e+01, train loss: 2.64668e-06, val loss: 3.34839e-06, min loss: 2.49727e-06\n",
      "Epoch: 392600, elapsed: 1.09e+01, train loss: 6.01834e-06, val loss: 6.08872e-06, min loss: 2.49727e-06\n",
      "Epoch: 392700, elapsed: 1.09e+01, train loss: 2.77362e-06, val loss: 3.44181e-06, min loss: 2.49727e-06\n",
      "Epoch: 392800, elapsed: 1.11e+01, train loss: 2.58104e-06, val loss: 3.24030e-06, min loss: 2.49727e-06\n",
      "Epoch: 392900, elapsed: 1.11e+01, train loss: 2.61979e-06, val loss: 3.29142e-06, min loss: 2.49727e-06\n",
      "Epoch: 393000, elapsed: 1.09e+01, train loss: 2.54844e-06, val loss: 3.25887e-06, min loss: 2.49727e-06\n",
      "Epoch: 393100, elapsed: 1.23e+01, train loss: 2.59611e-06, val loss: 3.28528e-06, min loss: 2.49727e-06\n",
      "Epoch: 393200, elapsed: 1.12e+01, train loss: 2.50559e-06, val loss: 3.20839e-06, min loss: 2.49727e-06\n",
      "Epoch: 393300, elapsed: 1.12e+01, train loss: 2.63340e-06, val loss: 3.36604e-06, min loss: 2.49727e-06\n",
      "Epoch: 393400, elapsed: 1.09e+01, train loss: 3.96288e-06, val loss: 4.40824e-06, min loss: 2.49727e-06\n",
      "Epoch: 393500, elapsed: 1.10e+01, train loss: 4.14006e-06, val loss: 4.01023e-06, min loss: 2.49727e-06\n",
      "Epoch: 393600, elapsed: 1.09e+01, train loss: 3.25563e-06, val loss: 3.65370e-06, min loss: 2.49727e-06\n",
      "Epoch: 393700, elapsed: 1.09e+01, train loss: 3.00332e-06, val loss: 3.58220e-06, min loss: 2.49727e-06\n",
      "Epoch: 393800, elapsed: 1.09e+01, train loss: 2.78451e-06, val loss: 3.55279e-06, min loss: 2.49727e-06\n",
      "Epoch: 393900, elapsed: 1.09e+01, train loss: 2.52539e-06, val loss: 3.27246e-06, min loss: 2.49727e-06\n",
      "Epoch: 394000, elapsed: 1.22e+01, train loss: 2.51968e-06, val loss: 3.22895e-06, min loss: 2.49727e-06\n",
      "Epoch: 394100, elapsed: 1.10e+01, train loss: 2.50275e-06, val loss: 3.23330e-06, min loss: 2.49727e-06\n",
      "Epoch: 394200, elapsed: 1.08e+01, train loss: 2.51331e-06, val loss: 3.21718e-06, min loss: 2.49727e-06\n",
      "Epoch: 394300, elapsed: 1.09e+01, train loss: 2.85722e-06, val loss: 3.44546e-06, min loss: 2.49727e-06\n",
      "Epoch: 394400, elapsed: 1.09e+01, train loss: 2.49779e-06, val loss: 3.21343e-06, min loss: 2.49727e-06\n",
      "Epoch: 394500, elapsed: 1.09e+01, train loss: 2.65189e-06, val loss: 3.56637e-06, min loss: 2.49727e-06\n",
      "Epoch: 394600, elapsed: 1.11e+01, train loss: 2.66744e-06, val loss: 3.33386e-06, min loss: 2.49727e-06\n",
      "Epoch: 394700, elapsed: 1.10e+01, train loss: 2.51087e-06, val loss: 3.32210e-06, min loss: 2.49727e-06\n",
      "Epoch: 394800, elapsed: 1.22e+01, train loss: 2.49698e-06, val loss: 3.19881e-06, min loss: 2.49698e-06\n",
      "Epoch: 394900, elapsed: 1.11e+01, train loss: 2.48447e-06, val loss: 3.19473e-06, min loss: 2.48447e-06\n",
      "Epoch: 395000, elapsed: 1.11e+01, train loss: 2.50283e-06, val loss: 3.19684e-06, min loss: 2.48447e-06\n",
      "Epoch: 395100, elapsed: 1.30e+01, train loss: 2.49010e-06, val loss: 3.19714e-06, min loss: 2.48447e-06\n",
      "Epoch: 395200, elapsed: 1.09e+01, train loss: 4.76175e-06, val loss: 6.23514e-06, min loss: 2.48447e-06\n",
      "Epoch: 395300, elapsed: 1.10e+01, train loss: 4.36696e-06, val loss: 5.10929e-06, min loss: 2.48447e-06\n",
      "Epoch: 395400, elapsed: 1.12e+01, train loss: 2.47977e-06, val loss: 3.19233e-06, min loss: 2.47977e-06\n",
      "Epoch: 395500, elapsed: 1.10e+01, train loss: 2.65111e-06, val loss: 3.27795e-06, min loss: 2.47977e-06\n",
      "Epoch: 395600, elapsed: 1.21e+01, train loss: 3.16712e-06, val loss: 3.71768e-06, min loss: 2.47977e-06\n",
      "Epoch: 395700, elapsed: 1.11e+01, train loss: 2.50681e-06, val loss: 3.22214e-06, min loss: 2.47977e-06\n",
      "Epoch: 395800, elapsed: 1.09e+01, train loss: 2.82426e-06, val loss: 3.49319e-06, min loss: 2.47977e-06\n",
      "Epoch: 395900, elapsed: 1.09e+01, train loss: 2.79975e-06, val loss: 3.33363e-06, min loss: 2.47977e-06\n",
      "Epoch: 396000, elapsed: 1.11e+01, train loss: 3.28949e-06, val loss: 3.90444e-06, min loss: 2.47977e-06\n",
      "Epoch: 396100, elapsed: 1.10e+01, train loss: 4.58276e-06, val loss: 5.32305e-06, min loss: 2.47977e-06\n",
      "Epoch: 396200, elapsed: 1.10e+01, train loss: 3.64450e-06, val loss: 3.95038e-06, min loss: 2.47977e-06\n",
      "Epoch: 396300, elapsed: 1.10e+01, train loss: 2.78813e-06, val loss: 3.26762e-06, min loss: 2.47977e-06\n",
      "Epoch: 396400, elapsed: 1.10e+01, train loss: 2.67743e-06, val loss: 3.27147e-06, min loss: 2.47977e-06\n",
      "Epoch: 396500, elapsed: 1.25e+01, train loss: 2.50194e-06, val loss: 3.26360e-06, min loss: 2.47977e-06\n",
      "Epoch: 396600, elapsed: 1.11e+01, train loss: 2.61358e-06, val loss: 3.35521e-06, min loss: 2.47977e-06\n",
      "Epoch: 396700, elapsed: 1.11e+01, train loss: 6.45445e-06, val loss: 5.58581e-06, min loss: 2.47977e-06\n",
      "Epoch: 396800, elapsed: 1.10e+01, train loss: 3.23989e-06, val loss: 3.49230e-06, min loss: 2.47977e-06\n",
      "Epoch: 396900, elapsed: 1.11e+01, train loss: 2.95311e-06, val loss: 3.53960e-06, min loss: 2.47977e-06\n",
      "Epoch: 397000, elapsed: 1.10e+01, train loss: 2.61453e-06, val loss: 3.32740e-06, min loss: 2.47977e-06\n",
      "Epoch: 397100, elapsed: 1.10e+01, train loss: 2.48555e-06, val loss: 3.17443e-06, min loss: 2.47977e-06\n",
      "Epoch: 397200, elapsed: 1.09e+01, train loss: 6.06820e-06, val loss: 5.95830e-06, min loss: 2.47977e-06\n",
      "Epoch: 397300, elapsed: 1.21e+01, train loss: 2.52836e-06, val loss: 3.28394e-06, min loss: 2.47977e-06\n",
      "Epoch: 397400, elapsed: 1.12e+01, train loss: 2.46435e-06, val loss: 3.17393e-06, min loss: 2.46435e-06\n",
      "Epoch: 397500, elapsed: 1.10e+01, train loss: 2.47892e-06, val loss: 3.17807e-06, min loss: 2.46435e-06\n",
      "Epoch: 397600, elapsed: 1.12e+01, train loss: 2.65560e-06, val loss: 3.42773e-06, min loss: 2.46435e-06\n",
      "Epoch: 397700, elapsed: 1.10e+01, train loss: 2.58326e-06, val loss: 3.32722e-06, min loss: 2.46435e-06\n",
      "Epoch: 397800, elapsed: 1.10e+01, train loss: 2.51112e-06, val loss: 3.28411e-06, min loss: 2.46435e-06\n",
      "Epoch: 397900, elapsed: 1.10e+01, train loss: 2.47000e-06, val loss: 3.17932e-06, min loss: 2.46435e-06\n",
      "Epoch: 398000, elapsed: 1.10e+01, train loss: 2.46672e-06, val loss: 3.18426e-06, min loss: 2.46435e-06\n",
      "Epoch: 398100, elapsed: 1.10e+01, train loss: 2.95192e-06, val loss: 3.78274e-06, min loss: 2.46435e-06\n",
      "Epoch: 398200, elapsed: 1.24e+01, train loss: 2.77095e-06, val loss: 3.50811e-06, min loss: 2.46435e-06\n",
      "Epoch: 398300, elapsed: 1.11e+01, train loss: 3.38939e-06, val loss: 4.39092e-06, min loss: 2.46435e-06\n",
      "Epoch: 398400, elapsed: 1.11e+01, train loss: 3.06274e-06, val loss: 3.53418e-06, min loss: 2.46435e-06\n",
      "Epoch: 398500, elapsed: 1.10e+01, train loss: 7.25133e-06, val loss: 7.60745e-06, min loss: 2.46435e-06\n",
      "Epoch: 398600, elapsed: 1.10e+01, train loss: 5.37096e-06, val loss: 5.30769e-06, min loss: 2.46435e-06\n",
      "Epoch: 398700, elapsed: 1.11e+01, train loss: 9.70950e-06, val loss: 8.40737e-06, min loss: 2.46435e-06\n",
      "Epoch: 398800, elapsed: 1.12e+01, train loss: 7.33580e-06, val loss: 7.81279e-06, min loss: 2.46435e-06\n",
      "Epoch: 398900, elapsed: 1.11e+01, train loss: 2.48813e-06, val loss: 3.35857e-06, min loss: 2.46435e-06\n",
      "Epoch: 399000, elapsed: 1.23e+01, train loss: 2.74165e-06, val loss: 3.53070e-06, min loss: 2.46435e-06\n",
      "Epoch: 399100, elapsed: 1.11e+01, train loss: 2.46198e-06, val loss: 3.15685e-06, min loss: 2.46198e-06\n",
      "Epoch: 399200, elapsed: 1.11e+01, train loss: 2.51122e-06, val loss: 3.22935e-06, min loss: 2.46198e-06\n",
      "Epoch: 399300, elapsed: 1.10e+01, train loss: 3.00137e-06, val loss: 3.39689e-06, min loss: 2.46198e-06\n",
      "Epoch: 399400, elapsed: 1.11e+01, train loss: 2.50206e-06, val loss: 3.23855e-06, min loss: 2.46198e-06\n",
      "Epoch: 399500, elapsed: 1.09e+01, train loss: 2.53243e-06, val loss: 3.25840e-06, min loss: 2.46198e-06\n",
      "Epoch: 399600, elapsed: 1.10e+01, train loss: 2.74265e-06, val loss: 3.68851e-06, min loss: 2.46198e-06\n",
      "Epoch: 399700, elapsed: 1.09e+01, train loss: 2.60377e-06, val loss: 3.35207e-06, min loss: 2.46198e-06\n",
      "Epoch: 399800, elapsed: 1.12e+01, train loss: 2.57333e-06, val loss: 3.31176e-06, min loss: 2.46198e-06\n",
      "Epoch: 399900, elapsed: 1.22e+01, train loss: 2.84492e-06, val loss: 3.60069e-06, min loss: 2.46198e-06\n",
      "Epoch: 400000, elapsed: 1.13e+01, train loss: 2.52421e-06, val loss: 3.24180e-06, min loss: 2.46198e-06\n",
      "Epoch: 400100, elapsed: 1.32e+01, train loss: 6.51434e-06, val loss: 7.45823e-06, min loss: 2.46198e-06\n",
      "Epoch: 400200, elapsed: 1.10e+01, train loss: 2.48450e-06, val loss: 3.15005e-06, min loss: 2.46198e-06\n",
      "Epoch: 400300, elapsed: 1.10e+01, train loss: 2.80668e-06, val loss: 3.46771e-06, min loss: 2.46198e-06\n",
      "Epoch: 400400, elapsed: 1.10e+01, train loss: 2.47924e-06, val loss: 3.21343e-06, min loss: 2.46198e-06\n",
      "Epoch: 400500, elapsed: 1.10e+01, train loss: 2.48450e-06, val loss: 3.16685e-06, min loss: 2.46198e-06\n",
      "Epoch: 400600, elapsed: 1.10e+01, train loss: 3.02540e-06, val loss: 3.82036e-06, min loss: 2.46198e-06\n",
      "Epoch: 400700, elapsed: 1.23e+01, train loss: 3.12682e-06, val loss: 3.46425e-06, min loss: 2.46198e-06\n",
      "Epoch: 400800, elapsed: 1.11e+01, train loss: 2.70730e-06, val loss: 3.48997e-06, min loss: 2.46198e-06\n",
      "Epoch: 400900, elapsed: 1.11e+01, train loss: 2.55012e-06, val loss: 3.20306e-06, min loss: 2.46198e-06\n",
      "Epoch: 401000, elapsed: 1.09e+01, train loss: 2.65745e-06, val loss: 3.38362e-06, min loss: 2.46198e-06\n",
      "Epoch: 401100, elapsed: 1.10e+01, train loss: 7.22723e-06, val loss: 7.53491e-06, min loss: 2.46198e-06\n",
      "Epoch: 401200, elapsed: 1.10e+01, train loss: 4.65079e-06, val loss: 3.96704e-06, min loss: 2.46198e-06\n",
      "Epoch: 401300, elapsed: 1.11e+01, train loss: 2.44295e-06, val loss: 3.15464e-06, min loss: 2.44295e-06\n",
      "Epoch: 401400, elapsed: 1.11e+01, train loss: 2.45516e-06, val loss: 3.15708e-06, min loss: 2.44295e-06\n",
      "Epoch: 401500, elapsed: 1.08e+01, train loss: 2.61314e-06, val loss: 3.48287e-06, min loss: 2.44295e-06\n",
      "Epoch: 401600, elapsed: 1.24e+01, train loss: 3.28599e-06, val loss: 3.87230e-06, min loss: 2.44295e-06\n",
      "Epoch: 401700, elapsed: 1.11e+01, train loss: 3.48223e-06, val loss: 5.21302e-06, min loss: 2.44295e-06\n",
      "Epoch: 401800, elapsed: 1.11e+01, train loss: 2.70693e-06, val loss: 3.51715e-06, min loss: 2.44295e-06\n",
      "Epoch: 401900, elapsed: 1.10e+01, train loss: 2.67527e-06, val loss: 3.38155e-06, min loss: 2.44295e-06\n",
      "Epoch: 402000, elapsed: 1.10e+01, train loss: 2.67584e-06, val loss: 3.44422e-06, min loss: 2.44295e-06\n",
      "Epoch: 402100, elapsed: 1.10e+01, train loss: 2.91615e-06, val loss: 3.62945e-06, min loss: 2.44295e-06\n",
      "Epoch: 402200, elapsed: 1.10e+01, train loss: 2.49811e-06, val loss: 3.18573e-06, min loss: 2.44295e-06\n",
      "Epoch: 402300, elapsed: 1.11e+01, train loss: 2.60802e-06, val loss: 3.32966e-06, min loss: 2.44295e-06\n",
      "Epoch: 402400, elapsed: 1.24e+01, train loss: 3.52761e-06, val loss: 4.19755e-06, min loss: 2.44295e-06\n",
      "Epoch: 402500, elapsed: 1.11e+01, train loss: 2.68109e-06, val loss: 3.32825e-06, min loss: 2.44295e-06\n",
      "Epoch: 402600, elapsed: 1.11e+01, train loss: 2.72030e-06, val loss: 3.62535e-06, min loss: 2.44295e-06\n",
      "Epoch: 402700, elapsed: 1.09e+01, train loss: 2.55641e-06, val loss: 3.31268e-06, min loss: 2.44295e-06\n",
      "Epoch: 402800, elapsed: 1.10e+01, train loss: 7.71678e-06, val loss: 9.40093e-06, min loss: 2.44295e-06\n",
      "Epoch: 402900, elapsed: 1.09e+01, train loss: 5.27380e-06, val loss: 6.09334e-06, min loss: 2.44295e-06\n",
      "Epoch: 403000, elapsed: 1.10e+01, train loss: 3.00216e-06, val loss: 3.69539e-06, min loss: 2.44295e-06\n",
      "Epoch: 403100, elapsed: 1.08e+01, train loss: 3.00422e-06, val loss: 3.76544e-06, min loss: 2.44295e-06\n",
      "Epoch: 403200, elapsed: 1.09e+01, train loss: 2.64654e-06, val loss: 3.40090e-06, min loss: 2.44295e-06\n",
      "Epoch: 403300, elapsed: 1.24e+01, train loss: 2.44755e-06, val loss: 3.17156e-06, min loss: 2.44295e-06\n",
      "Epoch: 403400, elapsed: 1.11e+01, train loss: 2.82751e-06, val loss: 3.37880e-06, min loss: 2.44295e-06\n",
      "Epoch: 403500, elapsed: 1.11e+01, train loss: 3.20321e-06, val loss: 4.21717e-06, min loss: 2.44295e-06\n",
      "Epoch: 403600, elapsed: 1.08e+01, train loss: 2.57914e-06, val loss: 3.30553e-06, min loss: 2.44295e-06\n",
      "Epoch: 403700, elapsed: 1.09e+01, train loss: 2.64559e-06, val loss: 3.39224e-06, min loss: 2.44295e-06\n",
      "Epoch: 403800, elapsed: 1.09e+01, train loss: 2.56839e-06, val loss: 3.24203e-06, min loss: 2.44295e-06\n",
      "Epoch: 403900, elapsed: 1.10e+01, train loss: 3.29818e-06, val loss: 3.89128e-06, min loss: 2.44295e-06\n",
      "Epoch: 404000, elapsed: 1.09e+01, train loss: 2.86363e-06, val loss: 3.64656e-06, min loss: 2.44295e-06\n",
      "Epoch: 404100, elapsed: 1.09e+01, train loss: 2.83796e-06, val loss: 3.85313e-06, min loss: 2.44295e-06\n",
      "Epoch: 404200, elapsed: 1.24e+01, train loss: 2.55627e-06, val loss: 3.19185e-06, min loss: 2.44295e-06\n",
      "Epoch: 404300, elapsed: 1.10e+01, train loss: 2.43820e-06, val loss: 3.14374e-06, min loss: 2.43820e-06\n",
      "Epoch: 404400, elapsed: 1.11e+01, train loss: 2.62893e-06, val loss: 3.22116e-06, min loss: 2.43820e-06\n",
      "Epoch: 404500, elapsed: 1.10e+01, train loss: 2.45128e-06, val loss: 3.18410e-06, min loss: 2.43820e-06\n",
      "Epoch: 404600, elapsed: 1.10e+01, train loss: 2.47103e-06, val loss: 3.17615e-06, min loss: 2.43820e-06\n",
      "Epoch: 404700, elapsed: 1.10e+01, train loss: 4.10588e-06, val loss: 4.66341e-06, min loss: 2.43820e-06\n",
      "Epoch: 404800, elapsed: 1.10e+01, train loss: 2.54302e-06, val loss: 3.28141e-06, min loss: 2.43820e-06\n",
      "Epoch: 404900, elapsed: 1.11e+01, train loss: 2.47047e-06, val loss: 3.15303e-06, min loss: 2.43820e-06\n",
      "Epoch: 405000, elapsed: 1.22e+01, train loss: 2.48111e-06, val loss: 3.19818e-06, min loss: 2.43820e-06\n",
      "Epoch: 405100, elapsed: 1.33e+01, train loss: 2.60251e-06, val loss: 3.15333e-06, min loss: 2.43820e-06\n",
      "Epoch: 405200, elapsed: 1.11e+01, train loss: 3.46318e-06, val loss: 4.05383e-06, min loss: 2.43820e-06\n",
      "Epoch: 405300, elapsed: 1.10e+01, train loss: 7.14736e-06, val loss: 8.63912e-06, min loss: 2.43820e-06\n",
      "Epoch: 405400, elapsed: 1.11e+01, train loss: 6.96903e-06, val loss: 8.56402e-06, min loss: 2.43820e-06\n",
      "Epoch: 405500, elapsed: 1.09e+01, train loss: 6.90208e-06, val loss: 8.24300e-06, min loss: 2.43820e-06\n",
      "Epoch: 405600, elapsed: 1.08e+01, train loss: 2.59767e-06, val loss: 3.31823e-06, min loss: 2.43820e-06\n",
      "Epoch: 405700, elapsed: 1.09e+01, train loss: 2.51549e-06, val loss: 3.20913e-06, min loss: 2.43820e-06\n",
      "Epoch: 405800, elapsed: 1.07e+01, train loss: 2.93178e-06, val loss: 3.77459e-06, min loss: 2.43820e-06\n",
      "Epoch: 405900, elapsed: 1.25e+01, train loss: 6.31938e-06, val loss: 6.24623e-06, min loss: 2.43820e-06\n",
      "Epoch: 406000, elapsed: 1.10e+01, train loss: 2.63614e-06, val loss: 3.33502e-06, min loss: 2.43820e-06\n",
      "Epoch: 406100, elapsed: 1.10e+01, train loss: 2.82642e-06, val loss: 3.46814e-06, min loss: 2.43820e-06\n",
      "Epoch: 406200, elapsed: 1.10e+01, train loss: 5.05350e-06, val loss: 6.19045e-06, min loss: 2.43820e-06\n",
      "Epoch: 406300, elapsed: 1.09e+01, train loss: 3.62981e-06, val loss: 3.80561e-06, min loss: 2.43820e-06\n",
      "Epoch: 406400, elapsed: 1.09e+01, train loss: 2.81256e-06, val loss: 3.39748e-06, min loss: 2.43820e-06\n",
      "Epoch: 406500, elapsed: 1.08e+01, train loss: 2.50938e-06, val loss: 3.21594e-06, min loss: 2.43820e-06\n",
      "Epoch: 406600, elapsed: 1.10e+01, train loss: 2.44607e-06, val loss: 3.17114e-06, min loss: 2.43820e-06\n",
      "Epoch: 406700, elapsed: 1.22e+01, train loss: 2.46239e-06, val loss: 3.13831e-06, min loss: 2.43820e-06\n",
      "Epoch: 406800, elapsed: 1.10e+01, train loss: 2.48326e-06, val loss: 3.19324e-06, min loss: 2.43820e-06\n",
      "Epoch: 406900, elapsed: 1.10e+01, train loss: 3.72379e-06, val loss: 4.54707e-06, min loss: 2.43820e-06\n",
      "Epoch: 407000, elapsed: 1.12e+01, train loss: 2.46950e-06, val loss: 3.15677e-06, min loss: 2.43820e-06\n",
      "Epoch: 407100, elapsed: 1.10e+01, train loss: 8.61757e-06, val loss: 7.85173e-06, min loss: 2.43820e-06\n",
      "Epoch: 407200, elapsed: 1.11e+01, train loss: 4.32777e-06, val loss: 4.08611e-06, min loss: 2.43820e-06\n",
      "Epoch: 407300, elapsed: 1.11e+01, train loss: 4.12547e-06, val loss: 3.67193e-06, min loss: 2.43820e-06\n",
      "Epoch: 407400, elapsed: 1.11e+01, train loss: 1.17605e-05, val loss: 1.13434e-05, min loss: 2.43820e-06\n",
      "Epoch: 407500, elapsed: 1.08e+01, train loss: 3.27636e-06, val loss: 4.17645e-06, min loss: 2.43820e-06\n",
      "Epoch: 407600, elapsed: 1.25e+01, train loss: 4.27585e-06, val loss: 5.21002e-06, min loss: 2.43820e-06\n",
      "Epoch: 407700, elapsed: 1.12e+01, train loss: 3.41131e-06, val loss: 4.23344e-06, min loss: 2.43820e-06\n",
      "Epoch: 407800, elapsed: 1.12e+01, train loss: 3.06743e-06, val loss: 3.72984e-06, min loss: 2.43820e-06\n",
      "Epoch: 407900, elapsed: 1.11e+01, train loss: 8.44529e-06, val loss: 7.22939e-06, min loss: 2.43820e-06\n",
      "Epoch: 408000, elapsed: 1.11e+01, train loss: 4.83093e-06, val loss: 4.06591e-06, min loss: 2.43820e-06\n",
      "Epoch: 408100, elapsed: 1.10e+01, train loss: 3.62070e-06, val loss: 3.78693e-06, min loss: 2.43820e-06\n",
      "Epoch: 408200, elapsed: 1.10e+01, train loss: 6.75210e-06, val loss: 6.40753e-06, min loss: 2.43820e-06\n",
      "Epoch: 408300, elapsed: 1.09e+01, train loss: 2.43896e-06, val loss: 3.15043e-06, min loss: 2.43820e-06\n",
      "Epoch: 408400, elapsed: 1.10e+01, train loss: 2.42583e-06, val loss: 3.14268e-06, min loss: 2.42583e-06\n",
      "Epoch: 408500, elapsed: 1.26e+01, train loss: 2.53551e-06, val loss: 3.23299e-06, min loss: 2.42583e-06\n",
      "Epoch: 408600, elapsed: 1.11e+01, train loss: 2.44827e-06, val loss: 3.13635e-06, min loss: 2.42583e-06\n",
      "Epoch: 408700, elapsed: 1.10e+01, train loss: 2.61611e-06, val loss: 3.36552e-06, min loss: 2.42583e-06\n",
      "Epoch: 408800, elapsed: 1.10e+01, train loss: 5.65621e-06, val loss: 6.26614e-06, min loss: 2.42583e-06\n",
      "Epoch: 408900, elapsed: 1.10e+01, train loss: 2.40929e-06, val loss: 3.13938e-06, min loss: 2.40929e-06\n",
      "Epoch: 409000, elapsed: 1.09e+01, train loss: 2.66234e-06, val loss: 3.46362e-06, min loss: 2.40929e-06\n",
      "Epoch: 409100, elapsed: 1.09e+01, train loss: 2.44092e-06, val loss: 3.12511e-06, min loss: 2.40929e-06\n",
      "Epoch: 409200, elapsed: 1.09e+01, train loss: 2.41531e-06, val loss: 3.13321e-06, min loss: 2.40929e-06\n",
      "Epoch: 409300, elapsed: 1.25e+01, train loss: 2.44324e-06, val loss: 3.21627e-06, min loss: 2.40929e-06\n",
      "Epoch: 409400, elapsed: 1.13e+01, train loss: 5.75542e-06, val loss: 6.09459e-06, min loss: 2.40929e-06\n",
      "Epoch: 409500, elapsed: 1.10e+01, train loss: 3.85996e-06, val loss: 4.96985e-06, min loss: 2.40929e-06\n",
      "Epoch: 409600, elapsed: 1.10e+01, train loss: 2.54815e-06, val loss: 3.21659e-06, min loss: 2.40929e-06\n",
      "Epoch: 409700, elapsed: 1.10e+01, train loss: 2.84281e-06, val loss: 3.60939e-06, min loss: 2.40929e-06\n",
      "Epoch: 409800, elapsed: 1.10e+01, train loss: 8.32737e-06, val loss: 9.60510e-06, min loss: 2.40929e-06\n",
      "Epoch: 409900, elapsed: 1.09e+01, train loss: 4.68264e-06, val loss: 5.98927e-06, min loss: 2.40929e-06\n",
      "Epoch: 410000, elapsed: 1.10e+01, train loss: 2.46636e-06, val loss: 3.15225e-06, min loss: 2.40929e-06\n",
      "Epoch: 410100, elapsed: 1.30e+01, train loss: 2.39218e-06, val loss: 3.08646e-06, min loss: 2.39218e-06\n",
      "Epoch: 410200, elapsed: 1.25e+01, train loss: 2.41975e-06, val loss: 3.10638e-06, min loss: 2.39218e-06\n",
      "Epoch: 410300, elapsed: 1.12e+01, train loss: 2.48862e-06, val loss: 3.18851e-06, min loss: 2.39218e-06\n",
      "Epoch: 410400, elapsed: 1.10e+01, train loss: 2.40100e-06, val loss: 3.10112e-06, min loss: 2.39218e-06\n",
      "Epoch: 410500, elapsed: 1.10e+01, train loss: 2.39692e-06, val loss: 3.10871e-06, min loss: 2.39218e-06\n",
      "Epoch: 410600, elapsed: 1.09e+01, train loss: 2.56264e-06, val loss: 3.17593e-06, min loss: 2.39218e-06\n",
      "Epoch: 410700, elapsed: 1.10e+01, train loss: 2.43827e-06, val loss: 3.14103e-06, min loss: 2.39218e-06\n",
      "Epoch: 410800, elapsed: 1.10e+01, train loss: 3.35572e-06, val loss: 4.20968e-06, min loss: 2.39218e-06\n",
      "Epoch: 410900, elapsed: 1.07e+01, train loss: 2.40447e-06, val loss: 3.11337e-06, min loss: 2.39218e-06\n",
      "Epoch: 411000, elapsed: 1.10e+01, train loss: 2.91119e-06, val loss: 3.61072e-06, min loss: 2.39218e-06\n",
      "Epoch: 411100, elapsed: 1.27e+01, train loss: 2.51434e-06, val loss: 3.22083e-06, min loss: 2.39218e-06\n",
      "Epoch: 411200, elapsed: 1.10e+01, train loss: 4.61622e-06, val loss: 4.73743e-06, min loss: 2.39218e-06\n",
      "Epoch: 411300, elapsed: 1.10e+01, train loss: 2.99873e-06, val loss: 3.48056e-06, min loss: 2.39218e-06\n",
      "Epoch: 411400, elapsed: 1.11e+01, train loss: 3.04700e-06, val loss: 3.79369e-06, min loss: 2.39218e-06\n",
      "Epoch: 411500, elapsed: 1.09e+01, train loss: 2.62398e-06, val loss: 3.38067e-06, min loss: 2.39218e-06\n",
      "Epoch: 411600, elapsed: 1.09e+01, train loss: 2.47207e-06, val loss: 3.14421e-06, min loss: 2.39218e-06\n",
      "Epoch: 411700, elapsed: 1.09e+01, train loss: 2.44054e-06, val loss: 3.20229e-06, min loss: 2.39218e-06\n",
      "Epoch: 411800, elapsed: 1.10e+01, train loss: 2.39853e-06, val loss: 3.08420e-06, min loss: 2.39218e-06\n",
      "Epoch: 411900, elapsed: 1.21e+01, train loss: 2.51739e-06, val loss: 3.23768e-06, min loss: 2.39218e-06\n",
      "Epoch: 412000, elapsed: 1.10e+01, train loss: 2.52885e-06, val loss: 3.11567e-06, min loss: 2.39218e-06\n",
      "Epoch: 412100, elapsed: 1.09e+01, train loss: 2.83162e-06, val loss: 3.51966e-06, min loss: 2.39218e-06\n",
      "Epoch: 412200, elapsed: 1.09e+01, train loss: 2.62615e-06, val loss: 3.19113e-06, min loss: 2.39218e-06\n",
      "Epoch: 412300, elapsed: 1.11e+01, train loss: 2.50045e-06, val loss: 3.17539e-06, min loss: 2.39218e-06\n",
      "Epoch: 412400, elapsed: 1.10e+01, train loss: 2.47277e-06, val loss: 3.15831e-06, min loss: 2.39218e-06\n",
      "Epoch: 412500, elapsed: 1.09e+01, train loss: 5.39096e-06, val loss: 6.25624e-06, min loss: 2.39218e-06\n",
      "Epoch: 412600, elapsed: 1.09e+01, train loss: 2.62131e-06, val loss: 3.28022e-06, min loss: 2.39218e-06\n",
      "Epoch: 412700, elapsed: 1.10e+01, train loss: 2.42569e-06, val loss: 3.10417e-06, min loss: 2.39218e-06\n",
      "Epoch: 412800, elapsed: 1.22e+01, train loss: 3.17645e-06, val loss: 4.35814e-06, min loss: 2.39218e-06\n",
      "Epoch: 412900, elapsed: 1.09e+01, train loss: 3.56627e-06, val loss: 4.92939e-06, min loss: 2.39218e-06\n",
      "Epoch: 413000, elapsed: 1.10e+01, train loss: 2.44062e-06, val loss: 3.14983e-06, min loss: 2.39218e-06\n",
      "Epoch: 413100, elapsed: 1.11e+01, train loss: 4.16411e-06, val loss: 4.94231e-06, min loss: 2.39218e-06\n",
      "Epoch: 413200, elapsed: 1.09e+01, train loss: 3.53006e-06, val loss: 3.96375e-06, min loss: 2.39218e-06\n",
      "Epoch: 413300, elapsed: 1.08e+01, train loss: 1.29612e-05, val loss: 1.23425e-05, min loss: 2.39218e-06\n",
      "Epoch: 413400, elapsed: 1.10e+01, train loss: 2.44487e-06, val loss: 3.20723e-06, min loss: 2.39218e-06\n",
      "Epoch: 413500, elapsed: 1.08e+01, train loss: 2.41519e-06, val loss: 3.11418e-06, min loss: 2.39218e-06\n",
      "Epoch: 413600, elapsed: 1.09e+01, train loss: 2.56847e-06, val loss: 3.30133e-06, min loss: 2.39218e-06\n",
      "Epoch: 413700, elapsed: 1.22e+01, train loss: 3.42407e-06, val loss: 4.62282e-06, min loss: 2.39218e-06\n",
      "Epoch: 413800, elapsed: 1.10e+01, train loss: 2.36974e-06, val loss: 3.07036e-06, min loss: 2.36974e-06\n",
      "Epoch: 413900, elapsed: 1.10e+01, train loss: 2.37835e-06, val loss: 3.06693e-06, min loss: 2.36974e-06\n",
      "Epoch: 414000, elapsed: 1.09e+01, train loss: 3.23144e-06, val loss: 3.92511e-06, min loss: 2.36974e-06\n",
      "Epoch: 414100, elapsed: 1.10e+01, train loss: 2.73242e-06, val loss: 3.37484e-06, min loss: 2.36974e-06\n",
      "Epoch: 414200, elapsed: 1.09e+01, train loss: 2.56881e-06, val loss: 3.19311e-06, min loss: 2.36974e-06\n",
      "Epoch: 414300, elapsed: 1.10e+01, train loss: 2.48418e-06, val loss: 3.21127e-06, min loss: 2.36974e-06\n",
      "Epoch: 414400, elapsed: 1.09e+01, train loss: 2.51678e-06, val loss: 3.27183e-06, min loss: 2.36974e-06\n",
      "Epoch: 414500, elapsed: 1.09e+01, train loss: 2.52289e-06, val loss: 3.12697e-06, min loss: 2.36974e-06\n",
      "Epoch: 414600, elapsed: 1.23e+01, train loss: 2.45974e-06, val loss: 3.18292e-06, min loss: 2.36974e-06\n",
      "Epoch: 414700, elapsed: 1.11e+01, train loss: 2.42096e-06, val loss: 3.08839e-06, min loss: 2.36974e-06\n",
      "Epoch: 414800, elapsed: 1.09e+01, train loss: 2.56849e-06, val loss: 3.22726e-06, min loss: 2.36974e-06\n",
      "Epoch: 414900, elapsed: 1.11e+01, train loss: 2.63082e-06, val loss: 3.30789e-06, min loss: 2.36974e-06\n",
      "Epoch: 415000, elapsed: 1.10e+01, train loss: 2.40387e-06, val loss: 3.14858e-06, min loss: 2.36974e-06\n",
      "Epoch: 415100, elapsed: 1.31e+01, train loss: 2.63121e-06, val loss: 3.28410e-06, min loss: 2.36974e-06\n",
      "Epoch: 415200, elapsed: 1.10e+01, train loss: 2.57490e-06, val loss: 3.33938e-06, min loss: 2.36974e-06\n",
      "Epoch: 415300, elapsed: 1.09e+01, train loss: 2.56166e-06, val loss: 3.13972e-06, min loss: 2.36974e-06\n",
      "Epoch: 415400, elapsed: 1.21e+01, train loss: 2.92812e-06, val loss: 3.38897e-06, min loss: 2.36974e-06\n",
      "Epoch: 415500, elapsed: 1.11e+01, train loss: 2.64304e-06, val loss: 3.26308e-06, min loss: 2.36974e-06\n",
      "Epoch: 415600, elapsed: 1.10e+01, train loss: 2.46478e-06, val loss: 3.13634e-06, min loss: 2.36974e-06\n",
      "Epoch: 415700, elapsed: 1.09e+01, train loss: 2.61900e-06, val loss: 3.41532e-06, min loss: 2.36974e-06\n",
      "Epoch: 415800, elapsed: 1.09e+01, train loss: 2.47239e-06, val loss: 3.39373e-06, min loss: 2.36974e-06\n",
      "Epoch: 415900, elapsed: 1.09e+01, train loss: 2.37745e-06, val loss: 3.07455e-06, min loss: 2.36974e-06\n",
      "Epoch: 416000, elapsed: 1.08e+01, train loss: 3.15725e-06, val loss: 3.80839e-06, min loss: 2.36974e-06\n",
      "Epoch: 416100, elapsed: 1.09e+01, train loss: 4.51609e-06, val loss: 5.59843e-06, min loss: 2.36974e-06\n",
      "Epoch: 416200, elapsed: 1.09e+01, train loss: 2.89434e-06, val loss: 3.52822e-06, min loss: 2.36974e-06\n",
      "Epoch: 416300, elapsed: 1.25e+01, train loss: 2.39808e-06, val loss: 3.14738e-06, min loss: 2.36974e-06\n",
      "Epoch: 416400, elapsed: 1.13e+01, train loss: 2.36667e-06, val loss: 3.07711e-06, min loss: 2.36667e-06\n",
      "Epoch: 416500, elapsed: 1.11e+01, train loss: 2.39161e-06, val loss: 3.07332e-06, min loss: 2.36667e-06\n",
      "Epoch: 416600, elapsed: 1.10e+01, train loss: 2.40656e-06, val loss: 3.09218e-06, min loss: 2.36667e-06\n",
      "Epoch: 416700, elapsed: 1.09e+01, train loss: 3.22944e-06, val loss: 3.95692e-06, min loss: 2.36667e-06\n",
      "Epoch: 416800, elapsed: 1.09e+01, train loss: 2.72085e-06, val loss: 3.30543e-06, min loss: 2.36667e-06\n",
      "Epoch: 416900, elapsed: 1.09e+01, train loss: 5.30695e-06, val loss: 5.79994e-06, min loss: 2.36667e-06\n",
      "Epoch: 417000, elapsed: 1.09e+01, train loss: 2.45448e-06, val loss: 3.11710e-06, min loss: 2.36667e-06\n",
      "Epoch: 417100, elapsed: 1.09e+01, train loss: 2.50567e-06, val loss: 3.25830e-06, min loss: 2.36667e-06\n",
      "Epoch: 417200, elapsed: 1.25e+01, train loss: 2.35569e-06, val loss: 3.05196e-06, min loss: 2.35569e-06\n",
      "Epoch: 417300, elapsed: 1.12e+01, train loss: 2.36699e-06, val loss: 3.06134e-06, min loss: 2.35569e-06\n",
      "Epoch: 417400, elapsed: 1.11e+01, train loss: 2.37726e-06, val loss: 3.06802e-06, min loss: 2.35569e-06\n",
      "Epoch: 417500, elapsed: 1.09e+01, train loss: 2.37448e-06, val loss: 3.07720e-06, min loss: 2.35569e-06\n",
      "Epoch: 417600, elapsed: 1.08e+01, train loss: 2.64399e-06, val loss: 3.33677e-06, min loss: 2.35569e-06\n",
      "Epoch: 417700, elapsed: 1.09e+01, train loss: 3.53881e-06, val loss: 4.17461e-06, min loss: 2.35569e-06\n",
      "Epoch: 417800, elapsed: 1.10e+01, train loss: 3.15206e-06, val loss: 4.10884e-06, min loss: 2.35569e-06\n",
      "Epoch: 417900, elapsed: 1.10e+01, train loss: 2.46085e-06, val loss: 3.13064e-06, min loss: 2.35569e-06\n",
      "Epoch: 418000, elapsed: 1.10e+01, train loss: 3.64547e-06, val loss: 4.22833e-06, min loss: 2.35569e-06\n",
      "Epoch: 418100, elapsed: 1.26e+01, train loss: 2.47780e-06, val loss: 3.14247e-06, min loss: 2.35569e-06\n",
      "Epoch: 418200, elapsed: 1.13e+01, train loss: 2.35098e-06, val loss: 3.04245e-06, min loss: 2.35098e-06\n",
      "Epoch: 418300, elapsed: 1.11e+01, train loss: 2.42829e-06, val loss: 3.15006e-06, min loss: 2.35098e-06\n",
      "Epoch: 418400, elapsed: 1.10e+01, train loss: 2.55855e-06, val loss: 3.29885e-06, min loss: 2.35098e-06\n",
      "Epoch: 418500, elapsed: 1.12e+01, train loss: 3.90618e-06, val loss: 4.96872e-06, min loss: 2.35098e-06\n",
      "Epoch: 418600, elapsed: 1.10e+01, train loss: 8.92360e-06, val loss: 9.32129e-06, min loss: 2.35098e-06\n",
      "Epoch: 418700, elapsed: 1.11e+01, train loss: 3.63725e-06, val loss: 4.42732e-06, min loss: 2.35098e-06\n",
      "Epoch: 418800, elapsed: 1.09e+01, train loss: 2.54137e-06, val loss: 3.13625e-06, min loss: 2.35098e-06\n",
      "Epoch: 418900, elapsed: 1.10e+01, train loss: 2.40378e-06, val loss: 3.10231e-06, min loss: 2.35098e-06\n",
      "Epoch: 419000, elapsed: 1.25e+01, train loss: 2.57477e-06, val loss: 3.35500e-06, min loss: 2.35098e-06\n",
      "Epoch: 419100, elapsed: 1.13e+01, train loss: 2.35597e-06, val loss: 3.06348e-06, min loss: 2.35098e-06\n",
      "Epoch: 419200, elapsed: 1.12e+01, train loss: 2.87008e-06, val loss: 3.26900e-06, min loss: 2.35098e-06\n",
      "Epoch: 419300, elapsed: 1.11e+01, train loss: 2.36835e-06, val loss: 3.05870e-06, min loss: 2.35098e-06\n",
      "Epoch: 419400, elapsed: 1.09e+01, train loss: 2.39177e-06, val loss: 3.12573e-06, min loss: 2.35098e-06\n",
      "Epoch: 419500, elapsed: 1.09e+01, train loss: 2.39678e-06, val loss: 3.05898e-06, min loss: 2.35098e-06\n",
      "Epoch: 419600, elapsed: 1.09e+01, train loss: 2.62527e-06, val loss: 3.47253e-06, min loss: 2.35098e-06\n",
      "Epoch: 419700, elapsed: 1.08e+01, train loss: 2.56853e-06, val loss: 3.41631e-06, min loss: 2.35098e-06\n",
      "Epoch: 419800, elapsed: 1.08e+01, train loss: 2.42503e-06, val loss: 3.20251e-06, min loss: 2.35098e-06\n",
      "Epoch: 419900, elapsed: 1.24e+01, train loss: 2.37951e-06, val loss: 3.14170e-06, min loss: 2.35098e-06\n",
      "Epoch: 420000, elapsed: 1.11e+01, train loss: 4.28865e-06, val loss: 4.30187e-06, min loss: 2.35098e-06\n",
      "Epoch: 420100, elapsed: 1.31e+01, train loss: 2.88462e-06, val loss: 3.56560e-06, min loss: 2.35098e-06\n",
      "Epoch: 420200, elapsed: 1.09e+01, train loss: 2.89806e-06, val loss: 3.46774e-06, min loss: 2.35098e-06\n",
      "Epoch: 420300, elapsed: 1.11e+01, train loss: 2.40546e-06, val loss: 3.10071e-06, min loss: 2.35098e-06\n",
      "Epoch: 420400, elapsed: 1.11e+01, train loss: 3.28841e-06, val loss: 3.55555e-06, min loss: 2.35098e-06\n",
      "Epoch: 420500, elapsed: 1.08e+01, train loss: 6.69278e-06, val loss: 9.04918e-06, min loss: 2.35098e-06\n",
      "Epoch: 420600, elapsed: 1.10e+01, train loss: 2.33078e-06, val loss: 3.02704e-06, min loss: 2.33078e-06\n",
      "Epoch: 420700, elapsed: 1.22e+01, train loss: 2.37953e-06, val loss: 3.04831e-06, min loss: 2.33078e-06\n",
      "Epoch: 420800, elapsed: 1.12e+01, train loss: 3.23162e-06, val loss: 4.19362e-06, min loss: 2.33078e-06\n",
      "Epoch: 420900, elapsed: 1.12e+01, train loss: 2.32882e-06, val loss: 3.02480e-06, min loss: 2.32882e-06\n",
      "Epoch: 421000, elapsed: 1.12e+01, train loss: 2.35366e-06, val loss: 3.03086e-06, min loss: 2.32882e-06\n",
      "Epoch: 421100, elapsed: 1.09e+01, train loss: 2.58382e-06, val loss: 3.42620e-06, min loss: 2.32882e-06\n",
      "Epoch: 421200, elapsed: 1.08e+01, train loss: 2.64215e-06, val loss: 3.41794e-06, min loss: 2.32882e-06\n",
      "Epoch: 421300, elapsed: 1.08e+01, train loss: 2.57510e-06, val loss: 3.53053e-06, min loss: 2.32882e-06\n",
      "Epoch: 421400, elapsed: 1.09e+01, train loss: 3.78765e-06, val loss: 6.48477e-06, min loss: 2.32882e-06\n",
      "Epoch: 421500, elapsed: 1.09e+01, train loss: 3.27569e-06, val loss: 4.15700e-06, min loss: 2.32882e-06\n",
      "Epoch: 421600, elapsed: 1.21e+01, train loss: 2.32636e-06, val loss: 3.02474e-06, min loss: 2.32636e-06\n",
      "Epoch: 421700, elapsed: 1.12e+01, train loss: 2.42117e-06, val loss: 3.10463e-06, min loss: 2.32636e-06\n",
      "Epoch: 421800, elapsed: 1.12e+01, train loss: 2.75281e-06, val loss: 3.08244e-06, min loss: 2.32636e-06\n",
      "Epoch: 421900, elapsed: 1.09e+01, train loss: 2.48392e-06, val loss: 3.59389e-06, min loss: 2.32636e-06\n",
      "Epoch: 422000, elapsed: 1.11e+01, train loss: 2.97871e-06, val loss: 3.34317e-06, min loss: 2.32636e-06\n",
      "Epoch: 422100, elapsed: 1.09e+01, train loss: 2.59782e-06, val loss: 3.22990e-06, min loss: 2.32636e-06\n",
      "Epoch: 422200, elapsed: 1.09e+01, train loss: 2.44692e-06, val loss: 3.16559e-06, min loss: 2.32636e-06\n",
      "Epoch: 422300, elapsed: 1.09e+01, train loss: 2.46559e-06, val loss: 3.15439e-06, min loss: 2.32636e-06\n",
      "Epoch: 422400, elapsed: 1.09e+01, train loss: 2.34275e-06, val loss: 3.02214e-06, min loss: 2.32636e-06\n",
      "Epoch: 422500, elapsed: 1.25e+01, train loss: 2.33996e-06, val loss: 3.03798e-06, min loss: 2.32636e-06\n",
      "Epoch: 422600, elapsed: 1.13e+01, train loss: 2.74766e-06, val loss: 3.57571e-06, min loss: 2.32636e-06\n",
      "Epoch: 422700, elapsed: 1.11e+01, train loss: 2.32144e-06, val loss: 3.01670e-06, min loss: 2.32144e-06\n",
      "Epoch: 422800, elapsed: 1.09e+01, train loss: 2.44220e-06, val loss: 3.22660e-06, min loss: 2.32144e-06\n",
      "Epoch: 422900, elapsed: 1.10e+01, train loss: 3.45703e-06, val loss: 4.22530e-06, min loss: 2.32144e-06\n",
      "Epoch: 423000, elapsed: 1.09e+01, train loss: 2.34433e-06, val loss: 3.02557e-06, min loss: 2.32144e-06\n",
      "Epoch: 423100, elapsed: 1.10e+01, train loss: 2.62839e-06, val loss: 3.41494e-06, min loss: 2.32144e-06\n",
      "Epoch: 423200, elapsed: 1.09e+01, train loss: 4.09226e-06, val loss: 4.22622e-06, min loss: 2.32144e-06\n",
      "Epoch: 423300, elapsed: 1.09e+01, train loss: 2.32773e-06, val loss: 3.01468e-06, min loss: 2.32144e-06\n",
      "Epoch: 423400, elapsed: 1.23e+01, train loss: 2.43078e-06, val loss: 3.14146e-06, min loss: 2.32144e-06\n",
      "Epoch: 423500, elapsed: 1.13e+01, train loss: 2.61903e-06, val loss: 3.28627e-06, min loss: 2.32144e-06\n",
      "Epoch: 423600, elapsed: 1.13e+01, train loss: 2.38696e-06, val loss: 3.14429e-06, min loss: 2.32144e-06\n",
      "Epoch: 423700, elapsed: 1.09e+01, train loss: 2.40047e-06, val loss: 3.17263e-06, min loss: 2.32144e-06\n",
      "Epoch: 423800, elapsed: 1.09e+01, train loss: 2.32820e-06, val loss: 3.01510e-06, min loss: 2.32144e-06\n",
      "Epoch: 423900, elapsed: 1.10e+01, train loss: 2.35135e-06, val loss: 3.04758e-06, min loss: 2.32144e-06\n",
      "Epoch: 424000, elapsed: 1.08e+01, train loss: 5.94842e-06, val loss: 5.24080e-06, min loss: 2.32144e-06\n",
      "Epoch: 424100, elapsed: 1.10e+01, train loss: 2.35455e-06, val loss: 3.07019e-06, min loss: 2.32144e-06\n",
      "Epoch: 424200, elapsed: 1.09e+01, train loss: 2.31222e-06, val loss: 3.00820e-06, min loss: 2.31222e-06\n",
      "Epoch: 424300, elapsed: 1.23e+01, train loss: 2.31490e-06, val loss: 3.00829e-06, min loss: 2.31222e-06\n",
      "Epoch: 424400, elapsed: 1.12e+01, train loss: 1.36620e-05, val loss: 1.37692e-05, min loss: 2.31222e-06\n",
      "Epoch: 424500, elapsed: 1.12e+01, train loss: 2.31976e-06, val loss: 3.02226e-06, min loss: 2.31222e-06\n",
      "Epoch: 424600, elapsed: 1.12e+01, train loss: 2.33557e-06, val loss: 3.02393e-06, min loss: 2.31222e-06\n",
      "Epoch: 424700, elapsed: 1.09e+01, train loss: 2.34201e-06, val loss: 3.05773e-06, min loss: 2.31222e-06\n",
      "Epoch: 424800, elapsed: 1.10e+01, train loss: 2.72722e-06, val loss: 3.36222e-06, min loss: 2.31222e-06\n",
      "Epoch: 424900, elapsed: 1.09e+01, train loss: 2.32346e-06, val loss: 3.00070e-06, min loss: 2.31222e-06\n",
      "Epoch: 425000, elapsed: 1.09e+01, train loss: 2.36208e-06, val loss: 3.06433e-06, min loss: 2.31222e-06\n",
      "Epoch: 425100, elapsed: 1.30e+01, train loss: 2.30652e-06, val loss: 2.99765e-06, min loss: 2.30652e-06\n",
      "Epoch: 425200, elapsed: 1.24e+01, train loss: 2.31239e-06, val loss: 2.99950e-06, min loss: 2.30652e-06\n",
      "Epoch: 425300, elapsed: 1.13e+01, train loss: 2.33937e-06, val loss: 3.03007e-06, min loss: 2.30652e-06\n",
      "Epoch: 425400, elapsed: 1.11e+01, train loss: 2.58306e-06, val loss: 3.01907e-06, min loss: 2.30652e-06\n",
      "Epoch: 425500, elapsed: 1.11e+01, train loss: 1.05539e-05, val loss: 8.47258e-06, min loss: 2.30652e-06\n",
      "Epoch: 425600, elapsed: 1.08e+01, train loss: 3.35224e-06, val loss: 4.50851e-06, min loss: 2.30652e-06\n",
      "Epoch: 425700, elapsed: 1.09e+01, train loss: 2.60916e-06, val loss: 3.43912e-06, min loss: 2.30652e-06\n",
      "Epoch: 425800, elapsed: 1.09e+01, train loss: 2.46904e-06, val loss: 3.23617e-06, min loss: 2.30652e-06\n",
      "Epoch: 425900, elapsed: 1.10e+01, train loss: 2.35985e-06, val loss: 3.10245e-06, min loss: 2.30652e-06\n",
      "Epoch: 426000, elapsed: 1.09e+01, train loss: 2.40554e-06, val loss: 3.15185e-06, min loss: 2.30652e-06\n",
      "Epoch: 426100, elapsed: 1.25e+01, train loss: 3.38102e-06, val loss: 3.99894e-06, min loss: 2.30652e-06\n",
      "Epoch: 426200, elapsed: 1.12e+01, train loss: 7.39984e-06, val loss: 8.17833e-06, min loss: 2.30652e-06\n",
      "Epoch: 426300, elapsed: 1.12e+01, train loss: 2.66602e-06, val loss: 3.47425e-06, min loss: 2.30652e-06\n",
      "Epoch: 426400, elapsed: 1.09e+01, train loss: 2.99549e-06, val loss: 3.80641e-06, min loss: 2.30652e-06\n",
      "Epoch: 426500, elapsed: 1.11e+01, train loss: 2.99316e-06, val loss: 3.84677e-06, min loss: 2.30652e-06\n",
      "Epoch: 426600, elapsed: 1.09e+01, train loss: 2.58415e-06, val loss: 3.51937e-06, min loss: 2.30652e-06\n",
      "Epoch: 426700, elapsed: 1.08e+01, train loss: 2.93581e-06, val loss: 3.40425e-06, min loss: 2.30652e-06\n",
      "Epoch: 426800, elapsed: 1.09e+01, train loss: 2.79073e-06, val loss: 3.60556e-06, min loss: 2.30652e-06\n",
      "Epoch: 426900, elapsed: 1.09e+01, train loss: 8.19340e-06, val loss: 8.95391e-06, min loss: 2.30652e-06\n",
      "Epoch: 427000, elapsed: 1.22e+01, train loss: 2.48353e-06, val loss: 3.23271e-06, min loss: 2.30652e-06\n",
      "Epoch: 427100, elapsed: 1.12e+01, train loss: 2.71132e-06, val loss: 3.47017e-06, min loss: 2.30652e-06\n",
      "Epoch: 427200, elapsed: 1.10e+01, train loss: 2.37178e-06, val loss: 3.07459e-06, min loss: 2.30652e-06\n",
      "Epoch: 427300, elapsed: 1.11e+01, train loss: 2.36213e-06, val loss: 3.06911e-06, min loss: 2.30652e-06\n",
      "Epoch: 427400, elapsed: 1.10e+01, train loss: 7.09827e-06, val loss: 7.96835e-06, min loss: 2.30652e-06\n",
      "Epoch: 427500, elapsed: 1.11e+01, train loss: 3.56797e-06, val loss: 3.99763e-06, min loss: 2.30652e-06\n",
      "Epoch: 427600, elapsed: 1.10e+01, train loss: 2.80330e-06, val loss: 3.42034e-06, min loss: 2.30652e-06\n",
      "Epoch: 427700, elapsed: 1.09e+01, train loss: 3.17824e-06, val loss: 3.91015e-06, min loss: 2.30652e-06\n",
      "Epoch: 427800, elapsed: 1.11e+01, train loss: 3.14352e-06, val loss: 3.74293e-06, min loss: 2.30652e-06\n",
      "Epoch: 427900, elapsed: 1.23e+01, train loss: 2.46299e-06, val loss: 3.15568e-06, min loss: 2.30652e-06\n",
      "Epoch: 428000, elapsed: 1.12e+01, train loss: 6.89126e-06, val loss: 8.72269e-06, min loss: 2.30652e-06\n",
      "Epoch: 428100, elapsed: 1.12e+01, train loss: 3.20854e-06, val loss: 3.97132e-06, min loss: 2.30652e-06\n",
      "Epoch: 428200, elapsed: 1.10e+01, train loss: 2.60935e-06, val loss: 3.32396e-06, min loss: 2.30652e-06\n",
      "Epoch: 428300, elapsed: 1.10e+01, train loss: 7.20433e-06, val loss: 7.35982e-06, min loss: 2.30652e-06\n",
      "Epoch: 428400, elapsed: 1.09e+01, train loss: 2.57439e-06, val loss: 3.42097e-06, min loss: 2.30652e-06\n",
      "Epoch: 428500, elapsed: 1.09e+01, train loss: 2.66328e-06, val loss: 3.73480e-06, min loss: 2.30652e-06\n",
      "Epoch: 428600, elapsed: 1.10e+01, train loss: 2.43866e-06, val loss: 3.14168e-06, min loss: 2.30652e-06\n",
      "Epoch: 428700, elapsed: 1.11e+01, train loss: 9.67713e-06, val loss: 1.18243e-05, min loss: 2.30652e-06\n",
      "Epoch: 428800, elapsed: 1.23e+01, train loss: 2.29234e-06, val loss: 2.99101e-06, min loss: 2.29234e-06\n",
      "Epoch: 428900, elapsed: 1.12e+01, train loss: 2.33928e-06, val loss: 3.00772e-06, min loss: 2.29234e-06\n",
      "Epoch: 429000, elapsed: 1.11e+01, train loss: 4.64123e-06, val loss: 4.90658e-06, min loss: 2.29234e-06\n",
      "Epoch: 429100, elapsed: 1.11e+01, train loss: 3.26640e-06, val loss: 3.91828e-06, min loss: 2.29234e-06\n",
      "Epoch: 429200, elapsed: 1.11e+01, train loss: 2.29176e-06, val loss: 2.98817e-06, min loss: 2.29176e-06\n",
      "Epoch: 429300, elapsed: 1.09e+01, train loss: 2.53267e-06, val loss: 3.08930e-06, min loss: 2.29176e-06\n",
      "Epoch: 429400, elapsed: 1.09e+01, train loss: 2.54468e-06, val loss: 3.27608e-06, min loss: 2.29176e-06\n",
      "Epoch: 429500, elapsed: 1.10e+01, train loss: 2.40986e-06, val loss: 3.08736e-06, min loss: 2.29176e-06\n",
      "Epoch: 429600, elapsed: 1.10e+01, train loss: 2.47951e-06, val loss: 3.02171e-06, min loss: 2.29176e-06\n",
      "Epoch: 429700, elapsed: 1.23e+01, train loss: 2.36262e-06, val loss: 3.04093e-06, min loss: 2.29176e-06\n",
      "Epoch: 429800, elapsed: 1.12e+01, train loss: 2.53069e-06, val loss: 3.24006e-06, min loss: 2.29176e-06\n",
      "Epoch: 429900, elapsed: 1.11e+01, train loss: 2.42039e-06, val loss: 3.06654e-06, min loss: 2.29176e-06\n",
      "Epoch: 430000, elapsed: 1.12e+01, train loss: 2.30559e-06, val loss: 3.01155e-06, min loss: 2.29176e-06\n",
      "Epoch: 430100, elapsed: 1.29e+01, train loss: 2.32690e-06, val loss: 3.05153e-06, min loss: 2.29176e-06\n",
      "Epoch: 430200, elapsed: 1.09e+01, train loss: 2.30766e-06, val loss: 2.99537e-06, min loss: 2.29176e-06\n",
      "Epoch: 430300, elapsed: 1.10e+01, train loss: 2.28990e-06, val loss: 2.98803e-06, min loss: 2.28990e-06\n",
      "Epoch: 430400, elapsed: 1.10e+01, train loss: 2.30948e-06, val loss: 2.97536e-06, min loss: 2.28990e-06\n",
      "Epoch: 430500, elapsed: 1.10e+01, train loss: 2.31266e-06, val loss: 2.96916e-06, min loss: 2.28990e-06\n",
      "Epoch: 430600, elapsed: 1.25e+01, train loss: 2.38763e-06, val loss: 3.10039e-06, min loss: 2.28990e-06\n",
      "Epoch: 430700, elapsed: 1.12e+01, train loss: 2.29379e-06, val loss: 2.98590e-06, min loss: 2.28990e-06\n",
      "Epoch: 430800, elapsed: 1.11e+01, train loss: 2.29162e-06, val loss: 2.98307e-06, min loss: 2.28990e-06\n",
      "Epoch: 430900, elapsed: 1.10e+01, train loss: 2.51610e-06, val loss: 3.20249e-06, min loss: 2.28990e-06\n",
      "Epoch: 431000, elapsed: 1.10e+01, train loss: 2.30517e-06, val loss: 2.99816e-06, min loss: 2.28990e-06\n",
      "Epoch: 431100, elapsed: 1.09e+01, train loss: 2.34324e-06, val loss: 3.00398e-06, min loss: 2.28990e-06\n",
      "Epoch: 431200, elapsed: 1.07e+01, train loss: 2.28323e-06, val loss: 2.96153e-06, min loss: 2.28323e-06\n",
      "Epoch: 431300, elapsed: 1.08e+01, train loss: 2.29370e-06, val loss: 2.99860e-06, min loss: 2.28323e-06\n",
      "Epoch: 431400, elapsed: 1.08e+01, train loss: 2.33369e-06, val loss: 3.06343e-06, min loss: 2.28323e-06\n",
      "Epoch: 431500, elapsed: 1.24e+01, train loss: 2.32175e-06, val loss: 3.08220e-06, min loss: 2.28323e-06\n",
      "Epoch: 431600, elapsed: 1.13e+01, train loss: 2.27195e-06, val loss: 2.96425e-06, min loss: 2.27195e-06\n",
      "Epoch: 431700, elapsed: 1.12e+01, train loss: 2.30395e-06, val loss: 3.02844e-06, min loss: 2.27195e-06\n",
      "Epoch: 431800, elapsed: 1.11e+01, train loss: 2.28204e-06, val loss: 2.98527e-06, min loss: 2.27195e-06\n",
      "Epoch: 431900, elapsed: 1.10e+01, train loss: 2.29442e-06, val loss: 2.97602e-06, min loss: 2.27195e-06\n",
      "Epoch: 432000, elapsed: 1.09e+01, train loss: 2.26962e-06, val loss: 2.96147e-06, min loss: 2.26962e-06\n",
      "Epoch: 432100, elapsed: 1.09e+01, train loss: 2.29897e-06, val loss: 2.98440e-06, min loss: 2.26962e-06\n",
      "Epoch: 432200, elapsed: 1.09e+01, train loss: 2.48670e-06, val loss: 3.12766e-06, min loss: 2.26962e-06\n",
      "Epoch: 432300, elapsed: 1.09e+01, train loss: 2.31403e-06, val loss: 3.02034e-06, min loss: 2.26962e-06\n",
      "Epoch: 432400, elapsed: 1.23e+01, train loss: 2.42379e-06, val loss: 3.08998e-06, min loss: 2.26962e-06\n",
      "Epoch: 432500, elapsed: 1.12e+01, train loss: 2.35148e-06, val loss: 3.03434e-06, min loss: 2.26962e-06\n",
      "Epoch: 432600, elapsed: 1.11e+01, train loss: 2.29487e-06, val loss: 3.00630e-06, min loss: 2.26962e-06\n",
      "Epoch: 432700, elapsed: 1.10e+01, train loss: 2.30839e-06, val loss: 3.04404e-06, min loss: 2.26962e-06\n",
      "Epoch: 432800, elapsed: 1.11e+01, train loss: 2.26534e-06, val loss: 2.96046e-06, min loss: 2.26534e-06\n",
      "Epoch: 432900, elapsed: 1.09e+01, train loss: 2.81551e-06, val loss: 3.48163e-06, min loss: 2.26534e-06\n",
      "Epoch: 433000, elapsed: 1.09e+01, train loss: 2.29803e-06, val loss: 3.03761e-06, min loss: 2.26534e-06\n",
      "Epoch: 433100, elapsed: 1.09e+01, train loss: 2.29893e-06, val loss: 2.97229e-06, min loss: 2.26534e-06\n",
      "Epoch: 433200, elapsed: 1.09e+01, train loss: 2.35714e-06, val loss: 3.06838e-06, min loss: 2.26534e-06\n",
      "Epoch: 433300, elapsed: 1.22e+01, train loss: 2.77876e-06, val loss: 3.02576e-06, min loss: 2.26534e-06\n",
      "Epoch: 433400, elapsed: 1.13e+01, train loss: 2.32194e-06, val loss: 3.10069e-06, min loss: 2.26534e-06\n",
      "Epoch: 433500, elapsed: 1.11e+01, train loss: 2.26370e-06, val loss: 2.98337e-06, min loss: 2.26370e-06\n",
      "Epoch: 433600, elapsed: 1.10e+01, train loss: 2.30269e-06, val loss: 2.98561e-06, min loss: 2.26370e-06\n",
      "Epoch: 433700, elapsed: 1.10e+01, train loss: 2.30283e-06, val loss: 3.16773e-06, min loss: 2.26370e-06\n",
      "Epoch: 433800, elapsed: 1.09e+01, train loss: 3.63320e-06, val loss: 4.43461e-06, min loss: 2.26370e-06\n",
      "Epoch: 433900, elapsed: 1.09e+01, train loss: 3.05264e-06, val loss: 3.81979e-06, min loss: 2.26370e-06\n",
      "Epoch: 434000, elapsed: 1.09e+01, train loss: 4.00686e-06, val loss: 5.38047e-06, min loss: 2.26370e-06\n",
      "Epoch: 434100, elapsed: 1.10e+01, train loss: 2.80789e-06, val loss: 3.65147e-06, min loss: 2.26370e-06\n",
      "Epoch: 434200, elapsed: 1.22e+01, train loss: 2.25806e-06, val loss: 2.94506e-06, min loss: 2.25806e-06\n",
      "Epoch: 434300, elapsed: 1.12e+01, train loss: 2.87536e-06, val loss: 3.63638e-06, min loss: 2.25806e-06\n",
      "Epoch: 434400, elapsed: 1.12e+01, train loss: 3.20593e-06, val loss: 3.93874e-06, min loss: 2.25806e-06\n",
      "Epoch: 434500, elapsed: 1.12e+01, train loss: 2.39366e-06, val loss: 3.04073e-06, min loss: 2.25806e-06\n",
      "Epoch: 434600, elapsed: 1.12e+01, train loss: 3.75622e-06, val loss: 3.94488e-06, min loss: 2.25806e-06\n",
      "Epoch: 434700, elapsed: 1.08e+01, train loss: 2.60494e-06, val loss: 3.27561e-06, min loss: 2.25806e-06\n",
      "Epoch: 434800, elapsed: 1.10e+01, train loss: 1.23251e-05, val loss: 1.30918e-05, min loss: 2.25806e-06\n",
      "Epoch: 434900, elapsed: 1.09e+01, train loss: 2.27628e-06, val loss: 2.94460e-06, min loss: 2.25806e-06\n",
      "Epoch: 435000, elapsed: 1.09e+01, train loss: 2.25636e-06, val loss: 2.94966e-06, min loss: 2.25636e-06\n",
      "Epoch: 435100, elapsed: 1.43e+01, train loss: 2.26287e-06, val loss: 2.95468e-06, min loss: 2.25636e-06\n",
      "Epoch: 435200, elapsed: 1.11e+01, train loss: 2.29590e-06, val loss: 2.99185e-06, min loss: 2.25636e-06\n",
      "Epoch: 435300, elapsed: 1.13e+01, train loss: 2.31347e-06, val loss: 3.00158e-06, min loss: 2.25636e-06\n",
      "Epoch: 435400, elapsed: 1.11e+01, train loss: 2.43213e-06, val loss: 3.27167e-06, min loss: 2.25636e-06\n",
      "Epoch: 435500, elapsed: 1.10e+01, train loss: 2.25521e-06, val loss: 2.94885e-06, min loss: 2.25521e-06\n",
      "Epoch: 435600, elapsed: 1.10e+01, train loss: 2.27610e-06, val loss: 2.98001e-06, min loss: 2.25521e-06\n",
      "Epoch: 435700, elapsed: 1.09e+01, train loss: 2.48827e-06, val loss: 2.98808e-06, min loss: 2.25521e-06\n",
      "Epoch: 435800, elapsed: 1.10e+01, train loss: 1.42744e-05, val loss: 1.44567e-05, min loss: 2.25521e-06\n",
      "Epoch: 435900, elapsed: 1.08e+01, train loss: 2.34072e-06, val loss: 3.01382e-06, min loss: 2.25521e-06\n",
      "Epoch: 436000, elapsed: 1.24e+01, train loss: 2.28219e-06, val loss: 2.98147e-06, min loss: 2.25521e-06\n",
      "Epoch: 436100, elapsed: 1.12e+01, train loss: 2.38170e-06, val loss: 3.07105e-06, min loss: 2.25521e-06\n",
      "Epoch: 436200, elapsed: 1.12e+01, train loss: 3.59493e-06, val loss: 4.37003e-06, min loss: 2.25521e-06\n",
      "Epoch: 436300, elapsed: 1.10e+01, train loss: 2.31738e-06, val loss: 2.96648e-06, min loss: 2.25521e-06\n",
      "Epoch: 436400, elapsed: 1.10e+01, train loss: 5.37164e-06, val loss: 5.70235e-06, min loss: 2.25521e-06\n",
      "Epoch: 436500, elapsed: 1.08e+01, train loss: 2.41827e-06, val loss: 3.06050e-06, min loss: 2.25521e-06\n",
      "Epoch: 436600, elapsed: 1.10e+01, train loss: 4.24300e-06, val loss: 5.44999e-06, min loss: 2.25521e-06\n",
      "Epoch: 436700, elapsed: 1.10e+01, train loss: 2.29728e-06, val loss: 2.99777e-06, min loss: 2.25521e-06\n",
      "Epoch: 436800, elapsed: 1.10e+01, train loss: 2.25678e-06, val loss: 2.94465e-06, min loss: 2.25521e-06\n",
      "Epoch: 436900, elapsed: 1.09e+01, train loss: 2.26545e-06, val loss: 2.96536e-06, min loss: 2.25521e-06\n",
      "Epoch: 437000, elapsed: 1.24e+01, train loss: 2.25365e-06, val loss: 2.93825e-06, min loss: 2.25365e-06\n",
      "Epoch: 437100, elapsed: 1.13e+01, train loss: 2.37731e-06, val loss: 3.10049e-06, min loss: 2.25365e-06\n",
      "Epoch: 437200, elapsed: 1.10e+01, train loss: 3.07159e-06, val loss: 3.74317e-06, min loss: 2.25365e-06\n",
      "Epoch: 437300, elapsed: 1.10e+01, train loss: 2.90276e-06, val loss: 3.52893e-06, min loss: 2.25365e-06\n",
      "Epoch: 437400, elapsed: 1.10e+01, train loss: 4.41932e-06, val loss: 5.29811e-06, min loss: 2.25365e-06\n",
      "Epoch: 437500, elapsed: 1.09e+01, train loss: 2.54899e-06, val loss: 3.25854e-06, min loss: 2.25365e-06\n",
      "Epoch: 437600, elapsed: 1.09e+01, train loss: 2.69797e-06, val loss: 3.79547e-06, min loss: 2.25365e-06\n",
      "Epoch: 437700, elapsed: 1.09e+01, train loss: 2.40824e-06, val loss: 3.09054e-06, min loss: 2.25365e-06\n",
      "Epoch: 437800, elapsed: 1.09e+01, train loss: 2.33425e-06, val loss: 2.99840e-06, min loss: 2.25365e-06\n",
      "Epoch: 437900, elapsed: 1.23e+01, train loss: 2.25583e-06, val loss: 2.96814e-06, min loss: 2.25365e-06\n",
      "Epoch: 438000, elapsed: 1.11e+01, train loss: 2.23885e-06, val loss: 2.93334e-06, min loss: 2.23885e-06\n",
      "Epoch: 438100, elapsed: 1.11e+01, train loss: 2.28616e-06, val loss: 2.96809e-06, min loss: 2.23885e-06\n",
      "Epoch: 438200, elapsed: 1.11e+01, train loss: 2.28134e-06, val loss: 3.03608e-06, min loss: 2.23885e-06\n",
      "Epoch: 438300, elapsed: 1.09e+01, train loss: 2.23877e-06, val loss: 2.94122e-06, min loss: 2.23877e-06\n",
      "Epoch: 438400, elapsed: 1.09e+01, train loss: 2.26986e-06, val loss: 2.92942e-06, min loss: 2.23877e-06\n",
      "Epoch: 438500, elapsed: 1.10e+01, train loss: 2.29804e-06, val loss: 2.95270e-06, min loss: 2.23877e-06\n",
      "Epoch: 438600, elapsed: 1.09e+01, train loss: 2.28709e-06, val loss: 2.96524e-06, min loss: 2.23877e-06\n",
      "Epoch: 438700, elapsed: 1.09e+01, train loss: 2.44803e-06, val loss: 3.17110e-06, min loss: 2.23877e-06\n",
      "Epoch: 438800, elapsed: 1.24e+01, train loss: 2.48441e-06, val loss: 3.10029e-06, min loss: 2.23877e-06\n",
      "Epoch: 438900, elapsed: 1.13e+01, train loss: 4.00011e-06, val loss: 4.16578e-06, min loss: 2.23877e-06\n",
      "Epoch: 439000, elapsed: 1.13e+01, train loss: 2.25493e-06, val loss: 2.95468e-06, min loss: 2.23877e-06\n",
      "Epoch: 439100, elapsed: 1.11e+01, train loss: 2.66998e-06, val loss: 3.05943e-06, min loss: 2.23877e-06\n",
      "Epoch: 439200, elapsed: 1.09e+01, train loss: 2.35311e-06, val loss: 3.13276e-06, min loss: 2.23877e-06\n",
      "Epoch: 439300, elapsed: 1.09e+01, train loss: 2.23934e-06, val loss: 2.94681e-06, min loss: 2.23877e-06\n",
      "Epoch: 439400, elapsed: 1.09e+01, train loss: 2.23471e-06, val loss: 2.91652e-06, min loss: 2.23471e-06\n",
      "Epoch: 439500, elapsed: 1.10e+01, train loss: 2.33219e-06, val loss: 3.00582e-06, min loss: 2.23471e-06\n",
      "Epoch: 439600, elapsed: 1.09e+01, train loss: 2.30277e-06, val loss: 3.02034e-06, min loss: 2.23471e-06\n",
      "Epoch: 439700, elapsed: 1.23e+01, train loss: 2.27108e-06, val loss: 2.96128e-06, min loss: 2.23471e-06\n",
      "Epoch: 439800, elapsed: 1.13e+01, train loss: 2.48646e-06, val loss: 3.19931e-06, min loss: 2.23471e-06\n",
      "Epoch: 439900, elapsed: 1.12e+01, train loss: 3.98414e-06, val loss: 4.66011e-06, min loss: 2.23471e-06\n",
      "Epoch: 440000, elapsed: 1.10e+01, train loss: 2.22536e-06, val loss: 2.92368e-06, min loss: 2.22536e-06\n",
      "Epoch: 440100, elapsed: 1.31e+01, train loss: 2.23350e-06, val loss: 2.92662e-06, min loss: 2.22536e-06\n",
      "Epoch: 440200, elapsed: 1.10e+01, train loss: 2.34269e-06, val loss: 3.11642e-06, min loss: 2.22536e-06\n",
      "Epoch: 440300, elapsed: 1.12e+01, train loss: 2.28935e-06, val loss: 3.01887e-06, min loss: 2.22536e-06\n",
      "Epoch: 440400, elapsed: 1.09e+01, train loss: 2.24269e-06, val loss: 2.94289e-06, min loss: 2.22536e-06\n",
      "Epoch: 440500, elapsed: 1.10e+01, train loss: 2.36465e-06, val loss: 3.10720e-06, min loss: 2.22536e-06\n",
      "Epoch: 440600, elapsed: 1.22e+01, train loss: 2.60548e-06, val loss: 3.43403e-06, min loss: 2.22536e-06\n",
      "Epoch: 440700, elapsed: 1.12e+01, train loss: 4.72608e-06, val loss: 5.85638e-06, min loss: 2.22536e-06\n",
      "Epoch: 440800, elapsed: 1.11e+01, train loss: 3.14207e-06, val loss: 3.16116e-06, min loss: 2.22536e-06\n",
      "Epoch: 440900, elapsed: 1.10e+01, train loss: 5.13577e-06, val loss: 4.99685e-06, min loss: 2.22536e-06\n",
      "Epoch: 441000, elapsed: 1.09e+01, train loss: 3.39962e-06, val loss: 3.71549e-06, min loss: 2.22536e-06\n",
      "Epoch: 441100, elapsed: 1.10e+01, train loss: 3.76057e-06, val loss: 4.89780e-06, min loss: 2.22536e-06\n",
      "Epoch: 441200, elapsed: 1.10e+01, train loss: 2.31730e-06, val loss: 3.09613e-06, min loss: 2.22536e-06\n",
      "Epoch: 441300, elapsed: 1.09e+01, train loss: 2.23877e-06, val loss: 2.94439e-06, min loss: 2.22536e-06\n",
      "Epoch: 441400, elapsed: 1.09e+01, train loss: 2.82189e-06, val loss: 3.46831e-06, min loss: 2.22536e-06\n",
      "Epoch: 441500, elapsed: 1.09e+01, train loss: 3.90827e-06, val loss: 4.76993e-06, min loss: 2.22536e-06\n",
      "Epoch: 441600, elapsed: 1.28e+01, train loss: 3.56714e-06, val loss: 4.60265e-06, min loss: 2.22536e-06\n",
      "Epoch: 441700, elapsed: 1.12e+01, train loss: 3.62453e-06, val loss: 3.62651e-06, min loss: 2.22536e-06\n",
      "Epoch: 441800, elapsed: 1.10e+01, train loss: 2.76702e-06, val loss: 3.39522e-06, min loss: 2.22536e-06\n",
      "Epoch: 441900, elapsed: 1.10e+01, train loss: 2.60946e-06, val loss: 3.37257e-06, min loss: 2.22536e-06\n",
      "Epoch: 442000, elapsed: 1.10e+01, train loss: 2.85548e-06, val loss: 3.95039e-06, min loss: 2.22536e-06\n",
      "Epoch: 442100, elapsed: 1.09e+01, train loss: 2.64076e-06, val loss: 3.06636e-06, min loss: 2.22536e-06\n",
      "Epoch: 442200, elapsed: 1.08e+01, train loss: 2.37732e-06, val loss: 3.03154e-06, min loss: 2.22536e-06\n",
      "Epoch: 442300, elapsed: 1.09e+01, train loss: 2.39342e-06, val loss: 3.23457e-06, min loss: 2.22536e-06\n",
      "Epoch: 442400, elapsed: 1.08e+01, train loss: 2.44595e-06, val loss: 3.17476e-06, min loss: 2.22536e-06\n",
      "Epoch: 442500, elapsed: 1.23e+01, train loss: 2.34690e-06, val loss: 3.16862e-06, min loss: 2.22536e-06\n",
      "Epoch: 442600, elapsed: 1.12e+01, train loss: 3.23232e-06, val loss: 3.58266e-06, min loss: 2.22536e-06\n",
      "Epoch: 442700, elapsed: 1.11e+01, train loss: 2.23872e-06, val loss: 2.92517e-06, min loss: 2.22536e-06\n",
      "Epoch: 442800, elapsed: 1.09e+01, train loss: 2.21178e-06, val loss: 2.91183e-06, min loss: 2.21178e-06\n",
      "Epoch: 442900, elapsed: 1.10e+01, train loss: 2.29652e-06, val loss: 2.94693e-06, min loss: 2.21178e-06\n",
      "Epoch: 443000, elapsed: 1.12e+01, train loss: 8.15248e-06, val loss: 9.97669e-06, min loss: 2.21178e-06\n",
      "Epoch: 443100, elapsed: 1.08e+01, train loss: 2.42078e-06, val loss: 3.15018e-06, min loss: 2.21178e-06\n",
      "Epoch: 443200, elapsed: 1.09e+01, train loss: 2.32615e-06, val loss: 3.10370e-06, min loss: 2.21178e-06\n",
      "Epoch: 443300, elapsed: 1.09e+01, train loss: 2.21281e-06, val loss: 2.91124e-06, min loss: 2.21178e-06\n",
      "Epoch: 443400, elapsed: 1.23e+01, train loss: 2.22048e-06, val loss: 2.90342e-06, min loss: 2.21178e-06\n",
      "Epoch: 443500, elapsed: 1.13e+01, train loss: 2.30654e-06, val loss: 3.02867e-06, min loss: 2.21178e-06\n",
      "Epoch: 443600, elapsed: 1.13e+01, train loss: 5.83488e-06, val loss: 6.28993e-06, min loss: 2.21178e-06\n",
      "Epoch: 443700, elapsed: 1.11e+01, train loss: 2.26562e-06, val loss: 2.96821e-06, min loss: 2.21178e-06\n",
      "Epoch: 443800, elapsed: 1.11e+01, train loss: 2.32627e-06, val loss: 3.00294e-06, min loss: 2.21178e-06\n",
      "Epoch: 443900, elapsed: 1.11e+01, train loss: 2.30909e-06, val loss: 3.39069e-06, min loss: 2.21178e-06\n",
      "Epoch: 444000, elapsed: 1.11e+01, train loss: 2.24930e-06, val loss: 3.01420e-06, min loss: 2.21178e-06\n",
      "Epoch: 444100, elapsed: 1.09e+01, train loss: 6.47421e-06, val loss: 6.91326e-06, min loss: 2.21178e-06\n",
      "Epoch: 444200, elapsed: 1.09e+01, train loss: 2.61472e-06, val loss: 3.50535e-06, min loss: 2.21178e-06\n",
      "Epoch: 444300, elapsed: 1.21e+01, train loss: 4.28462e-06, val loss: 4.25764e-06, min loss: 2.21178e-06\n",
      "Epoch: 444400, elapsed: 1.11e+01, train loss: 6.91794e-06, val loss: 5.82618e-06, min loss: 2.21178e-06\n",
      "Epoch: 444500, elapsed: 1.12e+01, train loss: 3.36927e-06, val loss: 4.24503e-06, min loss: 2.21178e-06\n",
      "Epoch: 444600, elapsed: 1.11e+01, train loss: 7.52319e-06, val loss: 8.03003e-06, min loss: 2.21178e-06\n",
      "Epoch: 444700, elapsed: 1.09e+01, train loss: 2.28878e-06, val loss: 3.03675e-06, min loss: 2.21178e-06\n",
      "Epoch: 444800, elapsed: 1.10e+01, train loss: 2.82042e-06, val loss: 3.57422e-06, min loss: 2.21178e-06\n",
      "Epoch: 444900, elapsed: 1.08e+01, train loss: 2.83929e-06, val loss: 3.70501e-06, min loss: 2.21178e-06\n",
      "Epoch: 445000, elapsed: 1.10e+01, train loss: 2.40668e-06, val loss: 3.05205e-06, min loss: 2.21178e-06\n",
      "Epoch: 445100, elapsed: 1.30e+01, train loss: 2.51598e-06, val loss: 3.26955e-06, min loss: 2.21178e-06\n",
      "Epoch: 445200, elapsed: 1.22e+01, train loss: 3.03709e-06, val loss: 3.81387e-06, min loss: 2.21178e-06\n",
      "Epoch: 445300, elapsed: 1.12e+01, train loss: 2.23514e-06, val loss: 2.92629e-06, min loss: 2.21178e-06\n",
      "Epoch: 445400, elapsed: 1.12e+01, train loss: 2.25030e-06, val loss: 2.97167e-06, min loss: 2.21178e-06\n",
      "Epoch: 445500, elapsed: 1.11e+01, train loss: 4.33249e-06, val loss: 5.35340e-06, min loss: 2.21178e-06\n",
      "Epoch: 445600, elapsed: 1.08e+01, train loss: 2.39884e-06, val loss: 3.17545e-06, min loss: 2.21178e-06\n",
      "Epoch: 445700, elapsed: 1.09e+01, train loss: 2.67557e-06, val loss: 3.59696e-06, min loss: 2.21178e-06\n",
      "Epoch: 445800, elapsed: 1.09e+01, train loss: 2.34305e-06, val loss: 3.12180e-06, min loss: 2.21178e-06\n",
      "Epoch: 445900, elapsed: 1.10e+01, train loss: 2.23318e-06, val loss: 2.91386e-06, min loss: 2.21178e-06\n",
      "Epoch: 446000, elapsed: 1.08e+01, train loss: 2.28033e-06, val loss: 3.02358e-06, min loss: 2.21178e-06\n",
      "Epoch: 446100, elapsed: 1.10e+01, train loss: 2.19862e-06, val loss: 2.90194e-06, min loss: 2.19862e-06\n",
      "Epoch: 446200, elapsed: 1.22e+01, train loss: 2.19811e-06, val loss: 2.88830e-06, min loss: 2.19811e-06\n",
      "Epoch: 446300, elapsed: 1.11e+01, train loss: 2.26334e-06, val loss: 2.94096e-06, min loss: 2.19811e-06\n",
      "Epoch: 446400, elapsed: 1.09e+01, train loss: 2.31139e-06, val loss: 3.05289e-06, min loss: 2.19811e-06\n",
      "Epoch: 446500, elapsed: 1.10e+01, train loss: 2.19372e-06, val loss: 2.89678e-06, min loss: 2.19372e-06\n",
      "Epoch: 446600, elapsed: 1.09e+01, train loss: 2.19401e-06, val loss: 2.88864e-06, min loss: 2.19372e-06\n",
      "Epoch: 446700, elapsed: 1.09e+01, train loss: 2.28943e-06, val loss: 3.02866e-06, min loss: 2.19372e-06\n",
      "Epoch: 446800, elapsed: 1.09e+01, train loss: 2.20470e-06, val loss: 2.91049e-06, min loss: 2.19372e-06\n",
      "Epoch: 446900, elapsed: 1.11e+01, train loss: 2.25892e-06, val loss: 2.95153e-06, min loss: 2.19372e-06\n",
      "Epoch: 447000, elapsed: 1.07e+01, train loss: 2.39679e-06, val loss: 3.00696e-06, min loss: 2.19372e-06\n",
      "Epoch: 447100, elapsed: 1.23e+01, train loss: 2.31639e-06, val loss: 3.14179e-06, min loss: 2.19372e-06\n",
      "Epoch: 447200, elapsed: 1.11e+01, train loss: 2.25256e-06, val loss: 2.95198e-06, min loss: 2.19372e-06\n",
      "Epoch: 447300, elapsed: 1.11e+01, train loss: 4.67674e-06, val loss: 5.96544e-06, min loss: 2.19372e-06\n",
      "Epoch: 447400, elapsed: 1.11e+01, train loss: 2.64608e-06, val loss: 3.32455e-06, min loss: 2.19372e-06\n",
      "Epoch: 447500, elapsed: 1.10e+01, train loss: 2.68855e-06, val loss: 3.28521e-06, min loss: 2.19372e-06\n",
      "Epoch: 447600, elapsed: 1.10e+01, train loss: 2.28377e-06, val loss: 2.96971e-06, min loss: 2.19372e-06\n",
      "Epoch: 447700, elapsed: 1.08e+01, train loss: 2.39093e-06, val loss: 3.09230e-06, min loss: 2.19372e-06\n",
      "Epoch: 447800, elapsed: 1.10e+01, train loss: 2.22676e-06, val loss: 2.95930e-06, min loss: 2.19372e-06\n",
      "Epoch: 447900, elapsed: 1.09e+01, train loss: 2.33323e-06, val loss: 3.01420e-06, min loss: 2.19372e-06\n",
      "Epoch: 448000, elapsed: 1.09e+01, train loss: 2.21945e-06, val loss: 2.92508e-06, min loss: 2.19372e-06\n",
      "Epoch: 448100, elapsed: 1.25e+01, train loss: 2.23283e-06, val loss: 2.91641e-06, min loss: 2.19372e-06\n",
      "Epoch: 448200, elapsed: 1.11e+01, train loss: 2.28623e-06, val loss: 2.97028e-06, min loss: 2.19372e-06\n",
      "Epoch: 448300, elapsed: 1.11e+01, train loss: 2.46493e-06, val loss: 3.25672e-06, min loss: 2.19372e-06\n",
      "Epoch: 448400, elapsed: 1.09e+01, train loss: 5.72767e-06, val loss: 6.00461e-06, min loss: 2.19372e-06\n",
      "Epoch: 448500, elapsed: 1.08e+01, train loss: 2.42647e-06, val loss: 3.06690e-06, min loss: 2.19372e-06\n",
      "Epoch: 448600, elapsed: 1.09e+01, train loss: 2.18870e-06, val loss: 2.90185e-06, min loss: 2.18870e-06\n",
      "Epoch: 448700, elapsed: 1.09e+01, train loss: 2.19390e-06, val loss: 2.88842e-06, min loss: 2.18870e-06\n",
      "Epoch: 448800, elapsed: 1.10e+01, train loss: 2.18524e-06, val loss: 2.90773e-06, min loss: 2.18524e-06\n",
      "Epoch: 448900, elapsed: 1.10e+01, train loss: 2.23703e-06, val loss: 2.93774e-06, min loss: 2.18524e-06\n",
      "Epoch: 449000, elapsed: 1.24e+01, train loss: 2.19895e-06, val loss: 2.88984e-06, min loss: 2.18524e-06\n",
      "Epoch: 449100, elapsed: 1.14e+01, train loss: 2.18490e-06, val loss: 2.87884e-06, min loss: 2.18490e-06\n",
      "Epoch: 449200, elapsed: 1.13e+01, train loss: 2.66224e-06, val loss: 3.41213e-06, min loss: 2.18490e-06\n",
      "Epoch: 449300, elapsed: 1.10e+01, train loss: 3.40440e-06, val loss: 4.14164e-06, min loss: 2.18490e-06\n",
      "Epoch: 449400, elapsed: 1.10e+01, train loss: 2.31915e-06, val loss: 3.04463e-06, min loss: 2.18490e-06\n",
      "Epoch: 449500, elapsed: 1.09e+01, train loss: 3.87792e-06, val loss: 4.61985e-06, min loss: 2.18490e-06\n",
      "Epoch: 449600, elapsed: 1.09e+01, train loss: 5.15617e-06, val loss: 6.03443e-06, min loss: 2.18490e-06\n",
      "Epoch: 449700, elapsed: 1.09e+01, train loss: 4.21716e-06, val loss: 4.33336e-06, min loss: 2.18490e-06\n",
      "Epoch: 449800, elapsed: 1.10e+01, train loss: 2.38810e-06, val loss: 3.17217e-06, min loss: 2.18490e-06\n",
      "Epoch: 449900, elapsed: 1.24e+01, train loss: 2.47572e-06, val loss: 3.16673e-06, min loss: 2.18490e-06\n",
      "Epoch: 450000, elapsed: 1.12e+01, train loss: 2.45122e-06, val loss: 3.11424e-06, min loss: 2.18490e-06\n",
      "Epoch: 450100, elapsed: 1.32e+01, train loss: 2.18844e-06, val loss: 2.89476e-06, min loss: 2.18490e-06\n",
      "Epoch: 450200, elapsed: 1.12e+01, train loss: 2.19269e-06, val loss: 2.89358e-06, min loss: 2.18490e-06\n",
      "Epoch: 450300, elapsed: 1.09e+01, train loss: 2.19898e-06, val loss: 2.90848e-06, min loss: 2.18490e-06\n",
      "Epoch: 450400, elapsed: 1.10e+01, train loss: 2.78888e-06, val loss: 3.47692e-06, min loss: 2.18490e-06\n",
      "Epoch: 450500, elapsed: 1.10e+01, train loss: 6.04793e-06, val loss: 5.97626e-06, min loss: 2.18490e-06\n",
      "Epoch: 450600, elapsed: 1.10e+01, train loss: 2.85997e-06, val loss: 3.58851e-06, min loss: 2.18490e-06\n",
      "Epoch: 450700, elapsed: 1.09e+01, train loss: 3.28403e-06, val loss: 3.59861e-06, min loss: 2.18490e-06\n",
      "Epoch: 450800, elapsed: 1.23e+01, train loss: 2.57172e-06, val loss: 2.90972e-06, min loss: 2.18490e-06\n",
      "Epoch: 450900, elapsed: 1.13e+01, train loss: 6.15369e-06, val loss: 4.39546e-06, min loss: 2.18490e-06\n",
      "Epoch: 451000, elapsed: 1.13e+01, train loss: 2.40590e-06, val loss: 3.08242e-06, min loss: 2.18490e-06\n",
      "Epoch: 451100, elapsed: 1.12e+01, train loss: 2.17845e-06, val loss: 2.88425e-06, min loss: 2.17845e-06\n",
      "Epoch: 451200, elapsed: 1.11e+01, train loss: 4.09727e-06, val loss: 3.48450e-06, min loss: 2.17845e-06\n",
      "Epoch: 451300, elapsed: 1.08e+01, train loss: 2.33761e-06, val loss: 2.94291e-06, min loss: 2.17845e-06\n",
      "Epoch: 451400, elapsed: 1.09e+01, train loss: 2.18313e-06, val loss: 2.88974e-06, min loss: 2.17845e-06\n",
      "Epoch: 451500, elapsed: 1.10e+01, train loss: 2.96559e-06, val loss: 4.13465e-06, min loss: 2.17845e-06\n",
      "Epoch: 451600, elapsed: 1.09e+01, train loss: 2.68419e-06, val loss: 3.30458e-06, min loss: 2.17845e-06\n",
      "Epoch: 451700, elapsed: 1.10e+01, train loss: 2.61888e-06, val loss: 3.19363e-06, min loss: 2.17845e-06\n",
      "Epoch: 451800, elapsed: 1.25e+01, train loss: 2.20707e-06, val loss: 2.89739e-06, min loss: 2.17845e-06\n",
      "Epoch: 451900, elapsed: 1.11e+01, train loss: 2.22781e-06, val loss: 2.89313e-06, min loss: 2.17845e-06\n",
      "Epoch: 452000, elapsed: 1.12e+01, train loss: 2.18050e-06, val loss: 2.88313e-06, min loss: 2.17845e-06\n",
      "Epoch: 452100, elapsed: 1.12e+01, train loss: 2.22221e-06, val loss: 2.97648e-06, min loss: 2.17845e-06\n",
      "Epoch: 452200, elapsed: 1.10e+01, train loss: 3.10506e-06, val loss: 4.06880e-06, min loss: 2.17845e-06\n",
      "Epoch: 452300, elapsed: 1.09e+01, train loss: 9.00869e-06, val loss: 7.74468e-06, min loss: 2.17845e-06\n",
      "Epoch: 452400, elapsed: 1.09e+01, train loss: 3.41640e-06, val loss: 4.52854e-06, min loss: 2.17845e-06\n",
      "Epoch: 452500, elapsed: 1.09e+01, train loss: 2.89479e-06, val loss: 3.87875e-06, min loss: 2.17845e-06\n",
      "Epoch: 452600, elapsed: 1.10e+01, train loss: 2.16603e-06, val loss: 2.86606e-06, min loss: 2.16603e-06\n",
      "Epoch: 452700, elapsed: 1.24e+01, train loss: 2.26943e-06, val loss: 2.96684e-06, min loss: 2.16603e-06\n",
      "Epoch: 452800, elapsed: 1.14e+01, train loss: 2.18880e-06, val loss: 2.87521e-06, min loss: 2.16603e-06\n",
      "Epoch: 452900, elapsed: 1.12e+01, train loss: 3.01399e-06, val loss: 3.57225e-06, min loss: 2.16603e-06\n",
      "Epoch: 453000, elapsed: 1.11e+01, train loss: 2.32023e-06, val loss: 3.14630e-06, min loss: 2.16603e-06\n",
      "Epoch: 453100, elapsed: 1.09e+01, train loss: 2.45530e-06, val loss: 3.24594e-06, min loss: 2.16603e-06\n",
      "Epoch: 453200, elapsed: 1.10e+01, train loss: 2.25374e-06, val loss: 3.00057e-06, min loss: 2.16603e-06\n",
      "Epoch: 453300, elapsed: 1.09e+01, train loss: 2.25334e-06, val loss: 3.00318e-06, min loss: 2.16603e-06\n",
      "Epoch: 453400, elapsed: 1.09e+01, train loss: 2.25542e-06, val loss: 2.93372e-06, min loss: 2.16603e-06\n",
      "Epoch: 453500, elapsed: 1.09e+01, train loss: 2.16472e-06, val loss: 2.87011e-06, min loss: 2.16472e-06\n",
      "Epoch: 453600, elapsed: 1.11e+01, train loss: 2.31953e-06, val loss: 3.03922e-06, min loss: 2.16472e-06\n",
      "Epoch: 453700, elapsed: 1.24e+01, train loss: 2.73908e-06, val loss: 3.30479e-06, min loss: 2.16472e-06\n",
      "Epoch: 453800, elapsed: 1.12e+01, train loss: 2.78905e-06, val loss: 3.52853e-06, min loss: 2.16472e-06\n",
      "Epoch: 453900, elapsed: 1.12e+01, train loss: 2.53771e-06, val loss: 3.19755e-06, min loss: 2.16472e-06\n",
      "Epoch: 454000, elapsed: 1.11e+01, train loss: 2.28957e-06, val loss: 3.01156e-06, min loss: 2.16472e-06\n",
      "Epoch: 454100, elapsed: 1.10e+01, train loss: 2.82140e-06, val loss: 3.07708e-06, min loss: 2.16472e-06\n",
      "Epoch: 454200, elapsed: 1.09e+01, train loss: 2.23158e-06, val loss: 2.86672e-06, min loss: 2.16472e-06\n",
      "Epoch: 454300, elapsed: 1.11e+01, train loss: 2.16352e-06, val loss: 2.87451e-06, min loss: 2.16352e-06\n",
      "Epoch: 454400, elapsed: 1.09e+01, train loss: 2.22945e-06, val loss: 2.87492e-06, min loss: 2.16352e-06\n",
      "Epoch: 454500, elapsed: 1.09e+01, train loss: 2.56090e-06, val loss: 3.41695e-06, min loss: 2.16352e-06\n",
      "Epoch: 454600, elapsed: 1.23e+01, train loss: 3.57841e-06, val loss: 3.54183e-06, min loss: 2.16352e-06\n",
      "Epoch: 454700, elapsed: 1.11e+01, train loss: 6.69084e-06, val loss: 5.13349e-06, min loss: 2.16352e-06\n",
      "Epoch: 454800, elapsed: 1.13e+01, train loss: 2.27594e-06, val loss: 2.98288e-06, min loss: 2.16352e-06\n",
      "Epoch: 454900, elapsed: 1.11e+01, train loss: 2.15339e-06, val loss: 2.86095e-06, min loss: 2.15339e-06\n",
      "Epoch: 455000, elapsed: 1.09e+01, train loss: 2.16240e-06, val loss: 2.85998e-06, min loss: 2.15339e-06\n",
      "Epoch: 455100, elapsed: 1.31e+01, train loss: 2.16133e-06, val loss: 2.84991e-06, min loss: 2.15339e-06\n",
      "Epoch: 455200, elapsed: 1.10e+01, train loss: 2.36340e-06, val loss: 3.06415e-06, min loss: 2.15339e-06\n",
      "Epoch: 455300, elapsed: 1.08e+01, train loss: 5.81747e-06, val loss: 4.47058e-06, min loss: 2.15339e-06\n",
      "Epoch: 455400, elapsed: 1.09e+01, train loss: 5.18421e-06, val loss: 6.24678e-06, min loss: 2.15339e-06\n",
      "Epoch: 455500, elapsed: 1.08e+01, train loss: 5.95565e-06, val loss: 5.50592e-06, min loss: 2.15339e-06\n",
      "Epoch: 455600, elapsed: 1.24e+01, train loss: 2.14932e-06, val loss: 2.86100e-06, min loss: 2.14932e-06\n",
      "Epoch: 455700, elapsed: 1.12e+01, train loss: 2.15211e-06, val loss: 2.86468e-06, min loss: 2.14932e-06\n",
      "Epoch: 455800, elapsed: 1.10e+01, train loss: 2.14965e-06, val loss: 2.85649e-06, min loss: 2.14932e-06\n",
      "Epoch: 455900, elapsed: 1.10e+01, train loss: 2.18109e-06, val loss: 2.87051e-06, min loss: 2.14932e-06\n",
      "Epoch: 456000, elapsed: 1.10e+01, train loss: 4.10300e-06, val loss: 3.90392e-06, min loss: 2.14932e-06\n",
      "Epoch: 456100, elapsed: 1.09e+01, train loss: 2.14829e-06, val loss: 2.88555e-06, min loss: 2.14829e-06\n",
      "Epoch: 456200, elapsed: 1.09e+01, train loss: 2.20522e-06, val loss: 2.93667e-06, min loss: 2.14829e-06\n",
      "Epoch: 456300, elapsed: 1.10e+01, train loss: 2.57559e-06, val loss: 3.37049e-06, min loss: 2.14829e-06\n",
      "Epoch: 456400, elapsed: 1.09e+01, train loss: 2.46889e-06, val loss: 3.32103e-06, min loss: 2.14829e-06\n",
      "Epoch: 456500, elapsed: 1.25e+01, train loss: 4.43554e-06, val loss: 5.42162e-06, min loss: 2.14829e-06\n",
      "Epoch: 456600, elapsed: 1.12e+01, train loss: 2.15764e-06, val loss: 2.84994e-06, min loss: 2.14829e-06\n",
      "Epoch: 456700, elapsed: 1.12e+01, train loss: 2.19589e-06, val loss: 2.87608e-06, min loss: 2.14829e-06\n",
      "Epoch: 456800, elapsed: 1.11e+01, train loss: 2.19490e-06, val loss: 2.89357e-06, min loss: 2.14829e-06\n",
      "Epoch: 456900, elapsed: 1.10e+01, train loss: 2.25425e-06, val loss: 2.94918e-06, min loss: 2.14829e-06\n",
      "Epoch: 457000, elapsed: 1.09e+01, train loss: 2.15966e-06, val loss: 2.86472e-06, min loss: 2.14829e-06\n",
      "Epoch: 457100, elapsed: 1.08e+01, train loss: 2.18283e-06, val loss: 2.84963e-06, min loss: 2.14829e-06\n",
      "Epoch: 457200, elapsed: 1.10e+01, train loss: 2.23445e-06, val loss: 2.91139e-06, min loss: 2.14829e-06\n",
      "Epoch: 457300, elapsed: 1.08e+01, train loss: 4.43392e-06, val loss: 5.37128e-06, min loss: 2.14829e-06\n",
      "Epoch: 457400, elapsed: 1.08e+01, train loss: 2.56538e-06, val loss: 3.20758e-06, min loss: 2.14829e-06\n",
      "Epoch: 457500, elapsed: 1.24e+01, train loss: 2.56194e-06, val loss: 3.18450e-06, min loss: 2.14829e-06\n",
      "Epoch: 457600, elapsed: 1.12e+01, train loss: 2.35034e-06, val loss: 3.07764e-06, min loss: 2.14829e-06\n",
      "Epoch: 457700, elapsed: 1.12e+01, train loss: 2.17166e-06, val loss: 2.88778e-06, min loss: 2.14829e-06\n",
      "Epoch: 457800, elapsed: 1.11e+01, train loss: 3.85135e-06, val loss: 4.72412e-06, min loss: 2.14829e-06\n",
      "Epoch: 457900, elapsed: 1.09e+01, train loss: 2.70320e-06, val loss: 2.98152e-06, min loss: 2.14829e-06\n",
      "Epoch: 458000, elapsed: 1.10e+01, train loss: 2.50347e-06, val loss: 3.26186e-06, min loss: 2.14829e-06\n",
      "Epoch: 458100, elapsed: 1.09e+01, train loss: 4.22300e-06, val loss: 5.18655e-06, min loss: 2.14829e-06\n",
      "Epoch: 458200, elapsed: 1.11e+01, train loss: 2.23807e-06, val loss: 2.87946e-06, min loss: 2.14829e-06\n",
      "Epoch: 458300, elapsed: 1.10e+01, train loss: 2.19312e-06, val loss: 2.92258e-06, min loss: 2.14829e-06\n",
      "Epoch: 458400, elapsed: 1.22e+01, train loss: 4.16387e-06, val loss: 5.53609e-06, min loss: 2.14829e-06\n",
      "Epoch: 458500, elapsed: 1.12e+01, train loss: 2.20440e-06, val loss: 2.97478e-06, min loss: 2.14829e-06\n",
      "Epoch: 458600, elapsed: 1.13e+01, train loss: 2.13925e-06, val loss: 2.84399e-06, min loss: 2.13925e-06\n",
      "Epoch: 458700, elapsed: 1.11e+01, train loss: 2.13899e-06, val loss: 2.83832e-06, min loss: 2.13899e-06\n",
      "Epoch: 458800, elapsed: 1.10e+01, train loss: 2.16453e-06, val loss: 2.85678e-06, min loss: 2.13899e-06\n",
      "Epoch: 458900, elapsed: 1.09e+01, train loss: 2.17469e-06, val loss: 2.87918e-06, min loss: 2.13899e-06\n",
      "Epoch: 459000, elapsed: 1.09e+01, train loss: 3.33448e-06, val loss: 4.53881e-06, min loss: 2.13899e-06\n",
      "Epoch: 459100, elapsed: 1.09e+01, train loss: 3.27112e-06, val loss: 4.32227e-06, min loss: 2.13899e-06\n",
      "Epoch: 459200, elapsed: 1.09e+01, train loss: 9.73560e-06, val loss: 9.39301e-06, min loss: 2.13899e-06\n",
      "Epoch: 459300, elapsed: 1.08e+01, train loss: 5.10757e-06, val loss: 6.28034e-06, min loss: 2.13899e-06\n",
      "Epoch: 459400, elapsed: 1.24e+01, train loss: 3.18439e-06, val loss: 3.84280e-06, min loss: 2.13899e-06\n",
      "Epoch: 459500, elapsed: 1.13e+01, train loss: 2.42489e-06, val loss: 3.18263e-06, min loss: 2.13899e-06\n",
      "Epoch: 459600, elapsed: 1.13e+01, train loss: 2.28408e-06, val loss: 3.07287e-06, min loss: 2.13899e-06\n",
      "Epoch: 459700, elapsed: 1.12e+01, train loss: 2.22012e-06, val loss: 2.90097e-06, min loss: 2.13899e-06\n",
      "Epoch: 459800, elapsed: 1.10e+01, train loss: 5.57951e-06, val loss: 5.44376e-06, min loss: 2.13899e-06\n",
      "Epoch: 459900, elapsed: 1.10e+01, train loss: 2.58563e-06, val loss: 3.40005e-06, min loss: 2.13899e-06\n",
      "Epoch: 460000, elapsed: 1.09e+01, train loss: 2.85451e-06, val loss: 3.43229e-06, min loss: 2.13899e-06\n",
      "Epoch: 460100, elapsed: 1.30e+01, train loss: 2.36239e-06, val loss: 3.11423e-06, min loss: 2.13899e-06\n",
      "Epoch: 460200, elapsed: 1.10e+01, train loss: 5.09634e-06, val loss: 5.05857e-06, min loss: 2.13899e-06\n",
      "Epoch: 460300, elapsed: 1.25e+01, train loss: 2.32133e-06, val loss: 2.94195e-06, min loss: 2.13899e-06\n",
      "Epoch: 460400, elapsed: 1.14e+01, train loss: 2.45772e-06, val loss: 3.13601e-06, min loss: 2.13899e-06\n",
      "Epoch: 460500, elapsed: 1.12e+01, train loss: 2.13229e-06, val loss: 2.84999e-06, min loss: 2.13229e-06\n",
      "Epoch: 460600, elapsed: 1.12e+01, train loss: 2.23055e-06, val loss: 2.90776e-06, min loss: 2.13229e-06\n",
      "Epoch: 460700, elapsed: 1.11e+01, train loss: 2.62268e-06, val loss: 3.15844e-06, min loss: 2.13229e-06\n",
      "Epoch: 460800, elapsed: 1.09e+01, train loss: 2.24737e-06, val loss: 2.91508e-06, min loss: 2.13229e-06\n",
      "Epoch: 460900, elapsed: 1.08e+01, train loss: 2.17109e-06, val loss: 2.87700e-06, min loss: 2.13229e-06\n",
      "Epoch: 461000, elapsed: 1.09e+01, train loss: 2.31171e-06, val loss: 3.03919e-06, min loss: 2.13229e-06\n",
      "Epoch: 461100, elapsed: 1.11e+01, train loss: 2.58632e-06, val loss: 3.00203e-06, min loss: 2.13229e-06\n",
      "Epoch: 461200, elapsed: 1.09e+01, train loss: 2.42032e-06, val loss: 2.87488e-06, min loss: 2.13229e-06\n",
      "Epoch: 461300, elapsed: 1.25e+01, train loss: 6.62303e-06, val loss: 5.29089e-06, min loss: 2.13229e-06\n",
      "Epoch: 461400, elapsed: 1.12e+01, train loss: 3.62536e-06, val loss: 4.38018e-06, min loss: 2.13229e-06\n",
      "Epoch: 461500, elapsed: 1.11e+01, train loss: 2.25277e-06, val loss: 3.10334e-06, min loss: 2.13229e-06\n",
      "Epoch: 461600, elapsed: 1.11e+01, train loss: 2.38369e-06, val loss: 3.09039e-06, min loss: 2.13229e-06\n",
      "Epoch: 461700, elapsed: 1.10e+01, train loss: 4.80302e-06, val loss: 4.92248e-06, min loss: 2.13229e-06\n",
      "Epoch: 461800, elapsed: 1.08e+01, train loss: 2.65438e-06, val loss: 3.35573e-06, min loss: 2.13229e-06\n",
      "Epoch: 461900, elapsed: 1.08e+01, train loss: 2.22938e-06, val loss: 3.01773e-06, min loss: 2.13229e-06\n",
      "Epoch: 462000, elapsed: 1.10e+01, train loss: 2.12685e-06, val loss: 2.84749e-06, min loss: 2.12685e-06\n",
      "Epoch: 462100, elapsed: 1.10e+01, train loss: 5.40742e-06, val loss: 5.33880e-06, min loss: 2.12685e-06\n",
      "Epoch: 462200, elapsed: 1.23e+01, train loss: 2.17937e-06, val loss: 2.88383e-06, min loss: 2.12685e-06\n",
      "Epoch: 462300, elapsed: 1.12e+01, train loss: 7.26407e-06, val loss: 8.43332e-06, min loss: 2.12685e-06\n",
      "Epoch: 462400, elapsed: 1.12e+01, train loss: 2.48589e-06, val loss: 3.17531e-06, min loss: 2.12685e-06\n",
      "Epoch: 462500, elapsed: 1.11e+01, train loss: 2.30783e-06, val loss: 3.05352e-06, min loss: 2.12685e-06\n",
      "Epoch: 462600, elapsed: 1.09e+01, train loss: 3.22032e-06, val loss: 3.80416e-06, min loss: 2.12685e-06\n",
      "Epoch: 462700, elapsed: 1.10e+01, train loss: 2.31180e-06, val loss: 3.06815e-06, min loss: 2.12685e-06\n",
      "Epoch: 462800, elapsed: 1.10e+01, train loss: 2.76762e-06, val loss: 3.37450e-06, min loss: 2.12685e-06\n",
      "Epoch: 462900, elapsed: 1.08e+01, train loss: 2.35060e-06, val loss: 3.05228e-06, min loss: 2.12685e-06\n",
      "Epoch: 463000, elapsed: 1.10e+01, train loss: 2.18970e-06, val loss: 2.91286e-06, min loss: 2.12685e-06\n",
      "Epoch: 463100, elapsed: 1.09e+01, train loss: 2.99832e-06, val loss: 3.57218e-06, min loss: 2.12685e-06\n",
      "Epoch: 463200, elapsed: 1.25e+01, train loss: 2.63346e-06, val loss: 3.26567e-06, min loss: 2.12685e-06\n",
      "Epoch: 463300, elapsed: 1.12e+01, train loss: 2.12050e-06, val loss: 2.84436e-06, min loss: 2.12050e-06\n",
      "Epoch: 463400, elapsed: 1.13e+01, train loss: 2.16843e-06, val loss: 2.87742e-06, min loss: 2.12050e-06\n",
      "Epoch: 463500, elapsed: 1.10e+01, train loss: 2.17637e-06, val loss: 2.91215e-06, min loss: 2.12050e-06\n",
      "Epoch: 463600, elapsed: 1.11e+01, train loss: 4.17561e-06, val loss: 5.18623e-06, min loss: 2.12050e-06\n",
      "Epoch: 463700, elapsed: 1.10e+01, train loss: 2.47911e-06, val loss: 3.19757e-06, min loss: 2.12050e-06\n",
      "Epoch: 463800, elapsed: 1.08e+01, train loss: 2.24025e-06, val loss: 2.93148e-06, min loss: 2.12050e-06\n",
      "Epoch: 463900, elapsed: 1.08e+01, train loss: 2.12623e-06, val loss: 2.83270e-06, min loss: 2.12050e-06\n",
      "Epoch: 464000, elapsed: 1.09e+01, train loss: 2.13823e-06, val loss: 2.86326e-06, min loss: 2.12050e-06\n",
      "Epoch: 464100, elapsed: 1.08e+01, train loss: 2.16117e-06, val loss: 2.88014e-06, min loss: 2.12050e-06\n",
      "Epoch: 464200, elapsed: 1.25e+01, train loss: 7.92471e-06, val loss: 9.11751e-06, min loss: 2.12050e-06\n",
      "Epoch: 464300, elapsed: 1.12e+01, train loss: 5.13928e-06, val loss: 6.37912e-06, min loss: 2.12050e-06\n",
      "Epoch: 464400, elapsed: 1.12e+01, train loss: 2.24428e-06, val loss: 3.11590e-06, min loss: 2.12050e-06\n",
      "Epoch: 464500, elapsed: 1.12e+01, train loss: 2.30757e-06, val loss: 3.02459e-06, min loss: 2.12050e-06\n",
      "Epoch: 464600, elapsed: 1.09e+01, train loss: 2.14647e-06, val loss: 2.85808e-06, min loss: 2.12050e-06\n",
      "Epoch: 464700, elapsed: 1.08e+01, train loss: 2.13934e-06, val loss: 2.84987e-06, min loss: 2.12050e-06\n",
      "Epoch: 464800, elapsed: 1.09e+01, train loss: 2.79857e-06, val loss: 3.54936e-06, min loss: 2.12050e-06\n",
      "Epoch: 464900, elapsed: 1.08e+01, train loss: 3.49835e-06, val loss: 4.99953e-06, min loss: 2.12050e-06\n",
      "Epoch: 465000, elapsed: 1.08e+01, train loss: 2.19125e-06, val loss: 2.99836e-06, min loss: 2.12050e-06\n",
      "Epoch: 465100, elapsed: 1.46e+01, train loss: 2.38820e-06, val loss: 3.07076e-06, min loss: 2.12050e-06\n",
      "Epoch: 465200, elapsed: 1.11e+01, train loss: 2.51014e-06, val loss: 3.08180e-06, min loss: 2.12050e-06\n",
      "Epoch: 465300, elapsed: 1.12e+01, train loss: 2.14298e-06, val loss: 2.82894e-06, min loss: 2.12050e-06\n",
      "Epoch: 465400, elapsed: 1.12e+01, train loss: 2.23391e-06, val loss: 2.94910e-06, min loss: 2.12050e-06\n",
      "Epoch: 465500, elapsed: 1.11e+01, train loss: 4.39144e-06, val loss: 5.31404e-06, min loss: 2.12050e-06\n",
      "Epoch: 465600, elapsed: 1.09e+01, train loss: 2.14135e-06, val loss: 3.02652e-06, min loss: 2.12050e-06\n",
      "Epoch: 465700, elapsed: 1.10e+01, train loss: 2.54427e-06, val loss: 3.21660e-06, min loss: 2.12050e-06\n",
      "Epoch: 465800, elapsed: 1.08e+01, train loss: 2.71624e-06, val loss: 3.22200e-06, min loss: 2.12050e-06\n",
      "Epoch: 465900, elapsed: 1.08e+01, train loss: 2.24111e-06, val loss: 3.05875e-06, min loss: 2.12050e-06\n",
      "Epoch: 466000, elapsed: 1.08e+01, train loss: 2.12079e-06, val loss: 2.82599e-06, min loss: 2.12050e-06\n",
      "Epoch: 466100, elapsed: 1.24e+01, train loss: 2.13511e-06, val loss: 2.84048e-06, min loss: 2.12050e-06\n",
      "Epoch: 466200, elapsed: 1.13e+01, train loss: 2.24165e-06, val loss: 3.04077e-06, min loss: 2.12050e-06\n",
      "Epoch: 466300, elapsed: 1.11e+01, train loss: 2.13656e-06, val loss: 2.82434e-06, min loss: 2.12050e-06\n",
      "Epoch: 466400, elapsed: 1.10e+01, train loss: 2.22246e-06, val loss: 3.02565e-06, min loss: 2.12050e-06\n",
      "Epoch: 466500, elapsed: 1.10e+01, train loss: 2.74152e-06, val loss: 3.58916e-06, min loss: 2.12050e-06\n",
      "Epoch: 466600, elapsed: 1.10e+01, train loss: 4.07895e-06, val loss: 5.09417e-06, min loss: 2.12050e-06\n",
      "Epoch: 466700, elapsed: 1.11e+01, train loss: 6.34239e-06, val loss: 5.72513e-06, min loss: 2.12050e-06\n",
      "Epoch: 466800, elapsed: 1.10e+01, train loss: 3.27077e-06, val loss: 3.54012e-06, min loss: 2.12050e-06\n",
      "Epoch: 466900, elapsed: 1.10e+01, train loss: 2.25566e-06, val loss: 2.87475e-06, min loss: 2.12050e-06\n",
      "Epoch: 467000, elapsed: 1.09e+01, train loss: 2.40617e-06, val loss: 3.00892e-06, min loss: 2.12050e-06\n",
      "Epoch: 467100, elapsed: 1.26e+01, train loss: 2.13119e-06, val loss: 2.82773e-06, min loss: 2.12050e-06\n",
      "Epoch: 467200, elapsed: 1.12e+01, train loss: 2.16531e-06, val loss: 2.92263e-06, min loss: 2.12050e-06\n",
      "Epoch: 467300, elapsed: 1.11e+01, train loss: 2.16133e-06, val loss: 2.86224e-06, min loss: 2.12050e-06\n",
      "Epoch: 467400, elapsed: 1.12e+01, train loss: 2.32937e-06, val loss: 3.05872e-06, min loss: 2.12050e-06\n",
      "Epoch: 467500, elapsed: 1.11e+01, train loss: 2.41189e-06, val loss: 3.05894e-06, min loss: 2.12050e-06\n",
      "Epoch: 467600, elapsed: 1.10e+01, train loss: 2.36882e-06, val loss: 3.04583e-06, min loss: 2.12050e-06\n",
      "Epoch: 467700, elapsed: 1.10e+01, train loss: 2.13659e-06, val loss: 2.83843e-06, min loss: 2.12050e-06\n",
      "Epoch: 467800, elapsed: 1.07e+01, train loss: 2.49614e-06, val loss: 2.97430e-06, min loss: 2.12050e-06\n",
      "Epoch: 467900, elapsed: 1.10e+01, train loss: 2.45345e-06, val loss: 3.14533e-06, min loss: 2.12050e-06\n",
      "Epoch: 468000, elapsed: 1.24e+01, train loss: 2.10754e-06, val loss: 2.82775e-06, min loss: 2.10754e-06\n",
      "Epoch: 468100, elapsed: 1.13e+01, train loss: 2.13638e-06, val loss: 2.81534e-06, min loss: 2.10754e-06\n",
      "Epoch: 468200, elapsed: 1.13e+01, train loss: 2.10205e-06, val loss: 2.81357e-06, min loss: 2.10205e-06\n",
      "Epoch: 468300, elapsed: 1.12e+01, train loss: 2.12576e-06, val loss: 2.86136e-06, min loss: 2.10205e-06\n",
      "Epoch: 468400, elapsed: 1.09e+01, train loss: 2.13120e-06, val loss: 2.84086e-06, min loss: 2.10205e-06\n",
      "Epoch: 468500, elapsed: 1.09e+01, train loss: 4.41418e-06, val loss: 5.44914e-06, min loss: 2.10205e-06\n",
      "Epoch: 468600, elapsed: 1.09e+01, train loss: 2.52954e-06, val loss: 3.13353e-06, min loss: 2.10205e-06\n",
      "Epoch: 468700, elapsed: 1.10e+01, train loss: 2.12418e-06, val loss: 2.99884e-06, min loss: 2.10205e-06\n",
      "Epoch: 468800, elapsed: 1.10e+01, train loss: 2.14234e-06, val loss: 2.93404e-06, min loss: 2.10205e-06\n",
      "Epoch: 468900, elapsed: 1.09e+01, train loss: 2.16938e-06, val loss: 2.85053e-06, min loss: 2.10205e-06\n",
      "Epoch: 469000, elapsed: 1.25e+01, train loss: 2.14073e-06, val loss: 2.85195e-06, min loss: 2.10205e-06\n",
      "Epoch: 469100, elapsed: 1.13e+01, train loss: 2.23139e-06, val loss: 2.94989e-06, min loss: 2.10205e-06\n",
      "Epoch: 469200, elapsed: 1.12e+01, train loss: 2.13812e-06, val loss: 2.84300e-06, min loss: 2.10205e-06\n",
      "Epoch: 469300, elapsed: 1.10e+01, train loss: 2.69746e-06, val loss: 3.60210e-06, min loss: 2.10205e-06\n",
      "Epoch: 469400, elapsed: 1.10e+01, train loss: 3.82406e-06, val loss: 5.24282e-06, min loss: 2.10205e-06\n",
      "Epoch: 469500, elapsed: 1.09e+01, train loss: 2.14242e-06, val loss: 2.84364e-06, min loss: 2.10205e-06\n",
      "Epoch: 469600, elapsed: 1.09e+01, train loss: 2.10051e-06, val loss: 2.81456e-06, min loss: 2.10051e-06\n",
      "Epoch: 469700, elapsed: 1.09e+01, train loss: 2.09741e-06, val loss: 2.81799e-06, min loss: 2.09741e-06\n",
      "Epoch: 469800, elapsed: 1.10e+01, train loss: 2.21081e-06, val loss: 2.86485e-06, min loss: 2.09741e-06\n",
      "Epoch: 469900, elapsed: 1.11e+01, train loss: 2.78283e-06, val loss: 3.62866e-06, min loss: 2.09741e-06\n",
      "Epoch: 470000, elapsed: 1.25e+01, train loss: 3.05525e-06, val loss: 3.55244e-06, min loss: 2.09741e-06\n",
      "Epoch: 470100, elapsed: 1.34e+01, train loss: 2.20504e-06, val loss: 2.83358e-06, min loss: 2.09741e-06\n",
      "Epoch: 470200, elapsed: 1.11e+01, train loss: 5.83254e-06, val loss: 4.55438e-06, min loss: 2.09741e-06\n",
      "Epoch: 470300, elapsed: 1.11e+01, train loss: 6.54781e-06, val loss: 7.17193e-06, min loss: 2.09741e-06\n",
      "Epoch: 470400, elapsed: 1.10e+01, train loss: 2.85202e-06, val loss: 3.27586e-06, min loss: 2.09741e-06\n",
      "Epoch: 470500, elapsed: 1.11e+01, train loss: 2.26918e-06, val loss: 2.93193e-06, min loss: 2.09741e-06\n",
      "Epoch: 470600, elapsed: 1.10e+01, train loss: 2.14684e-06, val loss: 2.86895e-06, min loss: 2.09741e-06\n",
      "Epoch: 470700, elapsed: 1.10e+01, train loss: 2.13611e-06, val loss: 2.88822e-06, min loss: 2.09741e-06\n",
      "Epoch: 470800, elapsed: 1.09e+01, train loss: 2.12973e-06, val loss: 2.80368e-06, min loss: 2.09741e-06\n",
      "Epoch: 470900, elapsed: 1.26e+01, train loss: 2.74475e-06, val loss: 4.04338e-06, min loss: 2.09741e-06\n",
      "Epoch: 471000, elapsed: 1.15e+01, train loss: 4.50207e-06, val loss: 5.87211e-06, min loss: 2.09741e-06\n",
      "Epoch: 471100, elapsed: 1.12e+01, train loss: 3.79960e-06, val loss: 4.82949e-06, min loss: 2.09741e-06\n",
      "Epoch: 471200, elapsed: 1.14e+01, train loss: 6.27488e-06, val loss: 6.65657e-06, min loss: 2.09741e-06\n",
      "Epoch: 471300, elapsed: 1.13e+01, train loss: 2.62528e-06, val loss: 3.55501e-06, min loss: 2.09741e-06\n",
      "Epoch: 471400, elapsed: 1.10e+01, train loss: 2.17791e-06, val loss: 3.01673e-06, min loss: 2.09741e-06\n",
      "Epoch: 471500, elapsed: 1.08e+01, train loss: 2.29661e-06, val loss: 3.09749e-06, min loss: 2.09741e-06\n",
      "Epoch: 471600, elapsed: 1.08e+01, train loss: 2.14187e-06, val loss: 2.84833e-06, min loss: 2.09741e-06\n",
      "Epoch: 471700, elapsed: 1.09e+01, train loss: 2.13758e-06, val loss: 2.84352e-06, min loss: 2.09741e-06\n",
      "Epoch: 471800, elapsed: 1.10e+01, train loss: 2.08600e-06, val loss: 2.81749e-06, min loss: 2.08600e-06\n",
      "Epoch: 471900, elapsed: 1.24e+01, train loss: 2.14929e-06, val loss: 2.90545e-06, min loss: 2.08600e-06\n",
      "Epoch: 472000, elapsed: 1.12e+01, train loss: 3.48459e-06, val loss: 4.53638e-06, min loss: 2.08600e-06\n",
      "Epoch: 472100, elapsed: 1.12e+01, train loss: 2.76763e-06, val loss: 4.26021e-06, min loss: 2.08600e-06\n",
      "Epoch: 472200, elapsed: 1.13e+01, train loss: 2.08363e-06, val loss: 2.80905e-06, min loss: 2.08363e-06\n",
      "Epoch: 472300, elapsed: 1.10e+01, train loss: 2.08637e-06, val loss: 2.80884e-06, min loss: 2.08363e-06\n",
      "Epoch: 472400, elapsed: 1.12e+01, train loss: 2.08485e-06, val loss: 2.79727e-06, min loss: 2.08363e-06\n",
      "Epoch: 472500, elapsed: 1.10e+01, train loss: 2.17926e-06, val loss: 2.87415e-06, min loss: 2.08363e-06\n",
      "Epoch: 472600, elapsed: 1.09e+01, train loss: 5.81504e-06, val loss: 6.73777e-06, min loss: 2.08363e-06\n",
      "Epoch: 472700, elapsed: 1.09e+01, train loss: 2.35209e-06, val loss: 3.06448e-06, min loss: 2.08363e-06\n",
      "Epoch: 472800, elapsed: 1.09e+01, train loss: 2.12035e-06, val loss: 2.82375e-06, min loss: 2.08363e-06\n",
      "Epoch: 472900, elapsed: 1.25e+01, train loss: 2.09317e-06, val loss: 2.81022e-06, min loss: 2.08363e-06\n",
      "Epoch: 473000, elapsed: 1.12e+01, train loss: 3.44139e-06, val loss: 4.56849e-06, min loss: 2.08363e-06\n",
      "Epoch: 473100, elapsed: 1.13e+01, train loss: 6.71191e-06, val loss: 7.18393e-06, min loss: 2.08363e-06\n",
      "Epoch: 473200, elapsed: 1.12e+01, train loss: 2.83202e-06, val loss: 3.44938e-06, min loss: 2.08363e-06\n",
      "Epoch: 473300, elapsed: 1.11e+01, train loss: 3.84406e-06, val loss: 4.62406e-06, min loss: 2.08363e-06\n",
      "Epoch: 473400, elapsed: 1.09e+01, train loss: 2.09570e-06, val loss: 2.79651e-06, min loss: 2.08363e-06\n",
      "Epoch: 473500, elapsed: 1.10e+01, train loss: 2.09957e-06, val loss: 2.79822e-06, min loss: 2.08363e-06\n",
      "Epoch: 473600, elapsed: 1.09e+01, train loss: 2.11292e-06, val loss: 2.83354e-06, min loss: 2.08363e-06\n",
      "Epoch: 473700, elapsed: 1.09e+01, train loss: 2.12395e-06, val loss: 2.82654e-06, min loss: 2.08363e-06\n",
      "Epoch: 473800, elapsed: 1.10e+01, train loss: 2.48376e-06, val loss: 3.18686e-06, min loss: 2.08363e-06\n",
      "Epoch: 473900, elapsed: 1.27e+01, train loss: 3.63603e-06, val loss: 4.75819e-06, min loss: 2.08363e-06\n",
      "Epoch: 474000, elapsed: 1.10e+01, train loss: 8.79440e-06, val loss: 8.41568e-06, min loss: 2.08363e-06\n",
      "Epoch: 474100, elapsed: 1.10e+01, train loss: 7.93319e-06, val loss: 8.51189e-06, min loss: 2.08363e-06\n",
      "Epoch: 474200, elapsed: 1.10e+01, train loss: 2.70302e-06, val loss: 3.70024e-06, min loss: 2.08363e-06\n",
      "Epoch: 474300, elapsed: 1.10e+01, train loss: 2.82056e-06, val loss: 3.60483e-06, min loss: 2.08363e-06\n",
      "Epoch: 474400, elapsed: 1.12e+01, train loss: 2.71909e-06, val loss: 3.16836e-06, min loss: 2.08363e-06\n",
      "Epoch: 474500, elapsed: 1.09e+01, train loss: 2.55783e-06, val loss: 3.21042e-06, min loss: 2.08363e-06\n",
      "Epoch: 474600, elapsed: 1.10e+01, train loss: 2.16157e-06, val loss: 2.93241e-06, min loss: 2.08363e-06\n",
      "Epoch: 474700, elapsed: 1.08e+01, train loss: 2.10087e-06, val loss: 2.81278e-06, min loss: 2.08363e-06\n",
      "Epoch: 474800, elapsed: 1.23e+01, train loss: 2.08837e-06, val loss: 2.80716e-06, min loss: 2.08363e-06\n",
      "Epoch: 474900, elapsed: 1.13e+01, train loss: 2.08276e-06, val loss: 2.81725e-06, min loss: 2.08276e-06\n",
      "Epoch: 475000, elapsed: 1.11e+01, train loss: 2.08881e-06, val loss: 2.80100e-06, min loss: 2.08276e-06\n",
      "Epoch: 475100, elapsed: 1.32e+01, train loss: 2.14075e-06, val loss: 2.86284e-06, min loss: 2.08276e-06\n",
      "Epoch: 475200, elapsed: 1.11e+01, train loss: 2.12044e-06, val loss: 2.83779e-06, min loss: 2.08276e-06\n",
      "Epoch: 475300, elapsed: 1.11e+01, train loss: 2.08709e-06, val loss: 2.84231e-06, min loss: 2.08276e-06\n",
      "Epoch: 475400, elapsed: 1.11e+01, train loss: 2.15776e-06, val loss: 2.90497e-06, min loss: 2.08276e-06\n",
      "Epoch: 475500, elapsed: 1.11e+01, train loss: 6.60069e-06, val loss: 6.87592e-06, min loss: 2.08276e-06\n",
      "Epoch: 475600, elapsed: 1.09e+01, train loss: 2.07300e-06, val loss: 2.80531e-06, min loss: 2.07300e-06\n",
      "Epoch: 475700, elapsed: 1.09e+01, train loss: 2.08150e-06, val loss: 2.80389e-06, min loss: 2.07300e-06\n",
      "Epoch: 475800, elapsed: 1.25e+01, train loss: 2.14976e-06, val loss: 2.85581e-06, min loss: 2.07300e-06\n",
      "Epoch: 475900, elapsed: 1.12e+01, train loss: 7.66066e-06, val loss: 8.66627e-06, min loss: 2.07300e-06\n",
      "Epoch: 476000, elapsed: 1.13e+01, train loss: 4.47913e-06, val loss: 5.92150e-06, min loss: 2.07300e-06\n",
      "Epoch: 476100, elapsed: 1.11e+01, train loss: 4.16194e-06, val loss: 5.40922e-06, min loss: 2.07300e-06\n",
      "Epoch: 476200, elapsed: 1.11e+01, train loss: 9.09604e-06, val loss: 7.69044e-06, min loss: 2.07300e-06\n",
      "Epoch: 476300, elapsed: 1.10e+01, train loss: 2.36702e-06, val loss: 3.22385e-06, min loss: 2.07300e-06\n",
      "Epoch: 476400, elapsed: 1.09e+01, train loss: 2.30745e-06, val loss: 3.04601e-06, min loss: 2.07300e-06\n",
      "Epoch: 476500, elapsed: 1.11e+01, train loss: 2.70946e-06, val loss: 3.41674e-06, min loss: 2.07300e-06\n",
      "Epoch: 476600, elapsed: 1.10e+01, train loss: 2.07286e-06, val loss: 2.78982e-06, min loss: 2.07286e-06\n",
      "Epoch: 476700, elapsed: 1.10e+01, train loss: 2.11026e-06, val loss: 2.81489e-06, min loss: 2.07286e-06\n",
      "Epoch: 476800, elapsed: 1.25e+01, train loss: 2.07870e-06, val loss: 2.78796e-06, min loss: 2.07286e-06\n",
      "Epoch: 476900, elapsed: 1.12e+01, train loss: 2.31139e-06, val loss: 3.11364e-06, min loss: 2.07286e-06\n",
      "Epoch: 477000, elapsed: 1.12e+01, train loss: 2.16350e-06, val loss: 2.91942e-06, min loss: 2.07286e-06\n",
      "Epoch: 477100, elapsed: 1.11e+01, train loss: 9.22107e-06, val loss: 8.91988e-06, min loss: 2.07286e-06\n",
      "Epoch: 477200, elapsed: 1.11e+01, train loss: 2.57957e-06, val loss: 3.42671e-06, min loss: 2.07286e-06\n",
      "Epoch: 477300, elapsed: 1.11e+01, train loss: 2.12128e-06, val loss: 2.82638e-06, min loss: 2.07286e-06\n",
      "Epoch: 477400, elapsed: 1.10e+01, train loss: 2.07910e-06, val loss: 2.77863e-06, min loss: 2.07286e-06\n",
      "Epoch: 477500, elapsed: 1.10e+01, train loss: 2.07354e-06, val loss: 2.80059e-06, min loss: 2.07286e-06\n",
      "Epoch: 477600, elapsed: 1.10e+01, train loss: 2.09610e-06, val loss: 2.79556e-06, min loss: 2.07286e-06\n",
      "Epoch: 477700, elapsed: 1.10e+01, train loss: 2.55895e-06, val loss: 3.02788e-06, min loss: 2.07286e-06\n",
      "Epoch: 477800, elapsed: 1.27e+01, train loss: 2.60213e-06, val loss: 3.22704e-06, min loss: 2.07286e-06\n",
      "Epoch: 477900, elapsed: 1.13e+01, train loss: 2.67180e-06, val loss: 3.31107e-06, min loss: 2.07286e-06\n",
      "Epoch: 478000, elapsed: 1.13e+01, train loss: 2.08869e-06, val loss: 2.81597e-06, min loss: 2.07286e-06\n",
      "Epoch: 478100, elapsed: 1.12e+01, train loss: 2.95030e-06, val loss: 3.10371e-06, min loss: 2.07286e-06\n",
      "Epoch: 478200, elapsed: 1.12e+01, train loss: 4.61693e-06, val loss: 4.25723e-06, min loss: 2.07286e-06\n",
      "Epoch: 478300, elapsed: 1.10e+01, train loss: 2.88564e-06, val loss: 3.94694e-06, min loss: 2.07286e-06\n",
      "Epoch: 478400, elapsed: 1.09e+01, train loss: 2.08674e-06, val loss: 2.83124e-06, min loss: 2.07286e-06\n",
      "Epoch: 478500, elapsed: 1.08e+01, train loss: 2.16005e-06, val loss: 2.87127e-06, min loss: 2.07286e-06\n",
      "Epoch: 478600, elapsed: 1.10e+01, train loss: 2.61580e-06, val loss: 3.38087e-06, min loss: 2.07286e-06\n",
      "Epoch: 478700, elapsed: 1.07e+01, train loss: 2.06670e-06, val loss: 2.78196e-06, min loss: 2.06670e-06\n",
      "Epoch: 478800, elapsed: 1.27e+01, train loss: 2.25301e-06, val loss: 3.15548e-06, min loss: 2.06670e-06\n",
      "Epoch: 478900, elapsed: 1.12e+01, train loss: 2.49773e-06, val loss: 3.06143e-06, min loss: 2.06670e-06\n",
      "Epoch: 479000, elapsed: 1.11e+01, train loss: 2.09395e-06, val loss: 2.81818e-06, min loss: 2.06670e-06\n",
      "Epoch: 479100, elapsed: 1.11e+01, train loss: 2.14410e-06, val loss: 2.88435e-06, min loss: 2.06670e-06\n",
      "Epoch: 479200, elapsed: 1.12e+01, train loss: 2.13342e-06, val loss: 2.87553e-06, min loss: 2.06670e-06\n",
      "Epoch: 479300, elapsed: 1.13e+01, train loss: 2.67277e-06, val loss: 3.70137e-06, min loss: 2.06670e-06\n",
      "Epoch: 479400, elapsed: 1.09e+01, train loss: 7.36578e-06, val loss: 8.11814e-06, min loss: 2.06670e-06\n",
      "Epoch: 479500, elapsed: 1.12e+01, train loss: 2.97709e-06, val loss: 3.68341e-06, min loss: 2.06670e-06\n",
      "Epoch: 479600, elapsed: 1.09e+01, train loss: 2.16028e-06, val loss: 2.86418e-06, min loss: 2.06670e-06\n",
      "Epoch: 479700, elapsed: 1.09e+01, train loss: 2.25302e-06, val loss: 2.95143e-06, min loss: 2.06670e-06\n",
      "Epoch: 479800, elapsed: 1.28e+01, train loss: 3.97021e-06, val loss: 4.98182e-06, min loss: 2.06670e-06\n",
      "Epoch: 479900, elapsed: 1.13e+01, train loss: 2.20923e-06, val loss: 2.88797e-06, min loss: 2.06670e-06\n",
      "Epoch: 480000, elapsed: 1.12e+01, train loss: 2.15253e-06, val loss: 2.89154e-06, min loss: 2.06670e-06\n",
      "Epoch: 480100, elapsed: 1.32e+01, train loss: 2.11100e-06, val loss: 2.79937e-06, min loss: 2.06670e-06\n",
      "Epoch: 480200, elapsed: 1.10e+01, train loss: 2.36683e-06, val loss: 3.23150e-06, min loss: 2.06670e-06\n",
      "Epoch: 480300, elapsed: 1.11e+01, train loss: 6.30312e-06, val loss: 7.32277e-06, min loss: 2.06670e-06\n",
      "Epoch: 480400, elapsed: 1.09e+01, train loss: 2.08379e-06, val loss: 2.82958e-06, min loss: 2.06670e-06\n",
      "Epoch: 480500, elapsed: 1.09e+01, train loss: 2.33741e-06, val loss: 2.95703e-06, min loss: 2.06670e-06\n",
      "Epoch: 480600, elapsed: 1.09e+01, train loss: 2.06983e-06, val loss: 2.81084e-06, min loss: 2.06670e-06\n",
      "Epoch: 480700, elapsed: 1.24e+01, train loss: 2.46401e-06, val loss: 2.85265e-06, min loss: 2.06670e-06\n",
      "Epoch: 480800, elapsed: 1.13e+01, train loss: 2.29107e-06, val loss: 3.00369e-06, min loss: 2.06670e-06\n",
      "Epoch: 480900, elapsed: 1.12e+01, train loss: 2.06469e-06, val loss: 2.78814e-06, min loss: 2.06469e-06\n",
      "Epoch: 481000, elapsed: 1.11e+01, train loss: 2.58705e-06, val loss: 3.18718e-06, min loss: 2.06469e-06\n",
      "Epoch: 481100, elapsed: 1.12e+01, train loss: 2.17281e-06, val loss: 2.82134e-06, min loss: 2.06469e-06\n",
      "Epoch: 481200, elapsed: 1.10e+01, train loss: 2.07047e-06, val loss: 2.78020e-06, min loss: 2.06469e-06\n",
      "Epoch: 481300, elapsed: 1.10e+01, train loss: 2.11042e-06, val loss: 2.82063e-06, min loss: 2.06469e-06\n",
      "Epoch: 481400, elapsed: 1.09e+01, train loss: 2.17272e-06, val loss: 2.83086e-06, min loss: 2.06469e-06\n",
      "Epoch: 481500, elapsed: 1.09e+01, train loss: 2.06934e-06, val loss: 2.78333e-06, min loss: 2.06469e-06\n",
      "Epoch: 481600, elapsed: 1.09e+01, train loss: 2.11011e-06, val loss: 2.82022e-06, min loss: 2.06469e-06\n",
      "Epoch: 481700, elapsed: 1.24e+01, train loss: 4.75056e-06, val loss: 5.75467e-06, min loss: 2.06469e-06\n",
      "Epoch: 481800, elapsed: 1.13e+01, train loss: 3.13909e-06, val loss: 3.26902e-06, min loss: 2.06469e-06\n",
      "Epoch: 481900, elapsed: 1.12e+01, train loss: 3.04884e-06, val loss: 3.35607e-06, min loss: 2.06469e-06\n",
      "Epoch: 482000, elapsed: 1.11e+01, train loss: 2.58932e-06, val loss: 3.37834e-06, min loss: 2.06469e-06\n",
      "Epoch: 482100, elapsed: 1.10e+01, train loss: 2.13569e-06, val loss: 2.84553e-06, min loss: 2.06469e-06\n",
      "Epoch: 482200, elapsed: 1.11e+01, train loss: 2.28189e-06, val loss: 2.96946e-06, min loss: 2.06469e-06\n",
      "Epoch: 482300, elapsed: 1.09e+01, train loss: 2.94254e-06, val loss: 3.61052e-06, min loss: 2.06469e-06\n",
      "Epoch: 482400, elapsed: 1.09e+01, train loss: 2.26422e-06, val loss: 2.94821e-06, min loss: 2.06469e-06\n",
      "Epoch: 482500, elapsed: 1.09e+01, train loss: 2.15994e-06, val loss: 2.94519e-06, min loss: 2.06469e-06\n",
      "Epoch: 482600, elapsed: 1.10e+01, train loss: 2.09738e-06, val loss: 2.91376e-06, min loss: 2.06469e-06\n",
      "Epoch: 482700, elapsed: 1.25e+01, train loss: 4.95854e-06, val loss: 5.95671e-06, min loss: 2.06469e-06\n",
      "Epoch: 482800, elapsed: 1.12e+01, train loss: 2.15995e-06, val loss: 3.08796e-06, min loss: 2.06469e-06\n",
      "Epoch: 482900, elapsed: 1.11e+01, train loss: 2.32678e-06, val loss: 3.13962e-06, min loss: 2.06469e-06\n",
      "Epoch: 483000, elapsed: 1.10e+01, train loss: 2.48470e-06, val loss: 3.28456e-06, min loss: 2.06469e-06\n",
      "Epoch: 483100, elapsed: 1.11e+01, train loss: 2.14474e-06, val loss: 2.88188e-06, min loss: 2.06469e-06\n",
      "Epoch: 483200, elapsed: 1.09e+01, train loss: 2.10407e-06, val loss: 2.77197e-06, min loss: 2.06469e-06\n",
      "Epoch: 483300, elapsed: 1.10e+01, train loss: 2.14003e-06, val loss: 2.82972e-06, min loss: 2.06469e-06\n",
      "Epoch: 483400, elapsed: 1.09e+01, train loss: 3.03767e-06, val loss: 3.90749e-06, min loss: 2.06469e-06\n",
      "Epoch: 483500, elapsed: 1.09e+01, train loss: 2.59516e-06, val loss: 3.16991e-06, min loss: 2.06469e-06\n",
      "Epoch: 483600, elapsed: 1.09e+01, train loss: 2.30720e-06, val loss: 3.10506e-06, min loss: 2.06469e-06\n",
      "Epoch: 483700, elapsed: 1.25e+01, train loss: 8.84464e-06, val loss: 9.21713e-06, min loss: 2.06469e-06\n",
      "Epoch: 483800, elapsed: 1.12e+01, train loss: 6.32139e-06, val loss: 7.57699e-06, min loss: 2.06469e-06\n",
      "Epoch: 483900, elapsed: 1.11e+01, train loss: 2.63151e-06, val loss: 3.71896e-06, min loss: 2.06469e-06\n",
      "Epoch: 484000, elapsed: 1.12e+01, train loss: 2.04753e-06, val loss: 2.76928e-06, min loss: 2.04753e-06\n",
      "Epoch: 484100, elapsed: 1.10e+01, train loss: 2.06209e-06, val loss: 2.81913e-06, min loss: 2.04753e-06\n",
      "Epoch: 484200, elapsed: 1.12e+01, train loss: 2.05389e-06, val loss: 2.77102e-06, min loss: 2.04753e-06\n",
      "Epoch: 484300, elapsed: 1.09e+01, train loss: 2.04290e-06, val loss: 2.76465e-06, min loss: 2.04290e-06\n",
      "Epoch: 484400, elapsed: 1.10e+01, train loss: 2.04848e-06, val loss: 2.78326e-06, min loss: 2.04290e-06\n",
      "Epoch: 484500, elapsed: 1.10e+01, train loss: 2.20271e-06, val loss: 2.91736e-06, min loss: 2.04290e-06\n",
      "Epoch: 484600, elapsed: 1.09e+01, train loss: 2.59887e-06, val loss: 3.29694e-06, min loss: 2.04290e-06\n",
      "Epoch: 484700, elapsed: 1.26e+01, train loss: 5.73446e-06, val loss: 7.65872e-06, min loss: 2.04290e-06\n",
      "Epoch: 484800, elapsed: 1.13e+01, train loss: 2.40509e-06, val loss: 3.20036e-06, min loss: 2.04290e-06\n",
      "Epoch: 484900, elapsed: 1.12e+01, train loss: 8.70557e-06, val loss: 7.92162e-06, min loss: 2.04290e-06\n",
      "Epoch: 485000, elapsed: 1.12e+01, train loss: 3.64253e-06, val loss: 4.13145e-06, min loss: 2.04290e-06\n",
      "Epoch: 485100, elapsed: 1.31e+01, train loss: 2.30296e-06, val loss: 3.14951e-06, min loss: 2.04290e-06\n",
      "Epoch: 485200, elapsed: 1.10e+01, train loss: 3.67796e-06, val loss: 3.91778e-06, min loss: 2.04290e-06\n",
      "Epoch: 485300, elapsed: 1.09e+01, train loss: 2.11274e-06, val loss: 2.84202e-06, min loss: 2.04290e-06\n",
      "Epoch: 485400, elapsed: 1.09e+01, train loss: 2.08886e-06, val loss: 2.84728e-06, min loss: 2.04290e-06\n",
      "Epoch: 485500, elapsed: 1.10e+01, train loss: 2.06288e-06, val loss: 2.82532e-06, min loss: 2.04290e-06\n",
      "Epoch: 485600, elapsed: 1.09e+01, train loss: 2.04104e-06, val loss: 2.77045e-06, min loss: 2.04104e-06\n",
      "Epoch: 485700, elapsed: 1.26e+01, train loss: 2.04123e-06, val loss: 2.76172e-06, min loss: 2.04104e-06\n",
      "Epoch: 485800, elapsed: 1.11e+01, train loss: 2.04134e-06, val loss: 2.75812e-06, min loss: 2.04104e-06\n",
      "Epoch: 485900, elapsed: 1.11e+01, train loss: 2.05003e-06, val loss: 2.78725e-06, min loss: 2.04104e-06\n",
      "Epoch: 486000, elapsed: 1.12e+01, train loss: 2.07067e-06, val loss: 2.81078e-06, min loss: 2.04104e-06\n",
      "Epoch: 486100, elapsed: 1.10e+01, train loss: 2.49789e-06, val loss: 3.09663e-06, min loss: 2.04104e-06\n",
      "Epoch: 486200, elapsed: 1.12e+01, train loss: 2.07218e-06, val loss: 2.79953e-06, min loss: 2.04104e-06\n",
      "Epoch: 486300, elapsed: 1.10e+01, train loss: 2.12587e-06, val loss: 2.83607e-06, min loss: 2.04104e-06\n",
      "Epoch: 486400, elapsed: 1.10e+01, train loss: 2.13969e-06, val loss: 2.94243e-06, min loss: 2.04104e-06\n",
      "Epoch: 486500, elapsed: 1.10e+01, train loss: 2.36398e-06, val loss: 3.15710e-06, min loss: 2.04104e-06\n",
      "Epoch: 486600, elapsed: 1.09e+01, train loss: 4.87140e-06, val loss: 4.29749e-06, min loss: 2.04104e-06\n",
      "Epoch: 486700, elapsed: 1.27e+01, train loss: 2.21680e-06, val loss: 2.79978e-06, min loss: 2.04104e-06\n",
      "Epoch: 486800, elapsed: 1.13e+01, train loss: 2.06673e-06, val loss: 2.86348e-06, min loss: 2.04104e-06\n",
      "Epoch: 486900, elapsed: 1.13e+01, train loss: 2.27419e-06, val loss: 2.96544e-06, min loss: 2.04104e-06\n",
      "Epoch: 487000, elapsed: 1.11e+01, train loss: 2.16983e-06, val loss: 2.83883e-06, min loss: 2.04104e-06\n",
      "Epoch: 487100, elapsed: 1.11e+01, train loss: 2.12644e-06, val loss: 2.81851e-06, min loss: 2.04104e-06\n",
      "Epoch: 487200, elapsed: 1.09e+01, train loss: 2.36018e-06, val loss: 2.91589e-06, min loss: 2.04104e-06\n",
      "Epoch: 487300, elapsed: 1.10e+01, train loss: 2.10069e-06, val loss: 2.85005e-06, min loss: 2.04104e-06\n",
      "Epoch: 487400, elapsed: 1.09e+01, train loss: 2.06237e-06, val loss: 2.79783e-06, min loss: 2.04104e-06\n",
      "Epoch: 487500, elapsed: 1.08e+01, train loss: 2.49727e-06, val loss: 3.40094e-06, min loss: 2.04104e-06\n",
      "Epoch: 487600, elapsed: 1.09e+01, train loss: 1.06225e-05, val loss: 1.04315e-05, min loss: 2.04104e-06\n",
      "Epoch: 487700, elapsed: 1.26e+01, train loss: 2.08386e-06, val loss: 2.87178e-06, min loss: 2.04104e-06\n",
      "Epoch: 487800, elapsed: 1.13e+01, train loss: 2.06897e-06, val loss: 2.77402e-06, min loss: 2.04104e-06\n",
      "Epoch: 487900, elapsed: 1.12e+01, train loss: 2.04914e-06, val loss: 2.76396e-06, min loss: 2.04104e-06\n",
      "Epoch: 488000, elapsed: 1.10e+01, train loss: 2.19342e-06, val loss: 2.98492e-06, min loss: 2.04104e-06\n",
      "Epoch: 488100, elapsed: 1.11e+01, train loss: 4.67946e-06, val loss: 4.29264e-06, min loss: 2.04104e-06\n",
      "Epoch: 488200, elapsed: 1.10e+01, train loss: 2.27167e-06, val loss: 2.93373e-06, min loss: 2.04104e-06\n",
      "Epoch: 488300, elapsed: 1.09e+01, train loss: 2.23883e-06, val loss: 3.11970e-06, min loss: 2.04104e-06\n",
      "Epoch: 488400, elapsed: 1.09e+01, train loss: 3.65068e-06, val loss: 4.37459e-06, min loss: 2.04104e-06\n",
      "Epoch: 488500, elapsed: 1.10e+01, train loss: 2.23141e-06, val loss: 2.95488e-06, min loss: 2.04104e-06\n",
      "Epoch: 488600, elapsed: 1.10e+01, train loss: 2.07019e-06, val loss: 2.83045e-06, min loss: 2.04104e-06\n",
      "Epoch: 488700, elapsed: 1.25e+01, train loss: 2.06443e-06, val loss: 2.80266e-06, min loss: 2.04104e-06\n",
      "Epoch: 488800, elapsed: 1.13e+01, train loss: 2.84026e-06, val loss: 3.59738e-06, min loss: 2.04104e-06\n",
      "Epoch: 488900, elapsed: 1.13e+01, train loss: 2.48517e-06, val loss: 3.28954e-06, min loss: 2.04104e-06\n",
      "Epoch: 489000, elapsed: 1.13e+01, train loss: 2.07745e-06, val loss: 2.81999e-06, min loss: 2.04104e-06\n",
      "Epoch: 489100, elapsed: 1.12e+01, train loss: 2.18277e-06, val loss: 2.88495e-06, min loss: 2.04104e-06\n",
      "Epoch: 489200, elapsed: 1.11e+01, train loss: 2.04311e-06, val loss: 2.79116e-06, min loss: 2.04104e-06\n",
      "Epoch: 489300, elapsed: 1.10e+01, train loss: 3.34976e-06, val loss: 4.11174e-06, min loss: 2.04104e-06\n",
      "Epoch: 489400, elapsed: 1.08e+01, train loss: 2.46863e-06, val loss: 3.38426e-06, min loss: 2.04104e-06\n",
      "Epoch: 489500, elapsed: 1.09e+01, train loss: 2.16442e-06, val loss: 2.96182e-06, min loss: 2.04104e-06\n",
      "Epoch: 489600, elapsed: 1.10e+01, train loss: 2.06346e-06, val loss: 2.77668e-06, min loss: 2.04104e-06\n",
      "Epoch: 489700, elapsed: 1.27e+01, train loss: 2.09980e-06, val loss: 2.81824e-06, min loss: 2.04104e-06\n",
      "Epoch: 489800, elapsed: 1.15e+01, train loss: 2.07518e-06, val loss: 2.85612e-06, min loss: 2.04104e-06\n",
      "Epoch: 489900, elapsed: 1.14e+01, train loss: 2.04431e-06, val loss: 2.74493e-06, min loss: 2.04104e-06\n",
      "Epoch: 490000, elapsed: 1.13e+01, train loss: 2.03486e-06, val loss: 2.74514e-06, min loss: 2.03486e-06\n",
      "Epoch: 490100, elapsed: 1.31e+01, train loss: 2.03318e-06, val loss: 2.77761e-06, min loss: 2.03318e-06\n",
      "Epoch: 490200, elapsed: 1.11e+01, train loss: 2.03711e-06, val loss: 2.76361e-06, min loss: 2.03318e-06\n",
      "Epoch: 490300, elapsed: 1.10e+01, train loss: 2.09796e-06, val loss: 2.81181e-06, min loss: 2.03318e-06\n",
      "Epoch: 490400, elapsed: 1.08e+01, train loss: 2.03587e-06, val loss: 2.77536e-06, min loss: 2.03318e-06\n",
      "Epoch: 490500, elapsed: 1.07e+01, train loss: 2.06005e-06, val loss: 2.77596e-06, min loss: 2.03318e-06\n",
      "Epoch: 490600, elapsed: 1.09e+01, train loss: 2.76167e-06, val loss: 3.44606e-06, min loss: 2.03318e-06\n",
      "Epoch: 490700, elapsed: 1.26e+01, train loss: 2.58527e-06, val loss: 3.09183e-06, min loss: 2.03318e-06\n",
      "Epoch: 490800, elapsed: 1.13e+01, train loss: 3.55474e-06, val loss: 3.02771e-06, min loss: 2.03318e-06\n",
      "Epoch: 490900, elapsed: 1.13e+01, train loss: 4.42322e-06, val loss: 5.47380e-06, min loss: 2.03318e-06\n",
      "Epoch: 491000, elapsed: 1.10e+01, train loss: 7.57619e-06, val loss: 7.78380e-06, min loss: 2.03318e-06\n",
      "Epoch: 491100, elapsed: 1.11e+01, train loss: 2.71977e-06, val loss: 3.25748e-06, min loss: 2.03318e-06\n",
      "Epoch: 491200, elapsed: 1.10e+01, train loss: 2.04304e-06, val loss: 2.94242e-06, min loss: 2.03318e-06\n",
      "Epoch: 491300, elapsed: 1.09e+01, train loss: 2.34569e-06, val loss: 2.97744e-06, min loss: 2.03318e-06\n",
      "Epoch: 491400, elapsed: 1.10e+01, train loss: 2.07970e-06, val loss: 2.79741e-06, min loss: 2.03318e-06\n",
      "Epoch: 491500, elapsed: 1.11e+01, train loss: 2.17450e-06, val loss: 2.94558e-06, min loss: 2.03318e-06\n",
      "Epoch: 491600, elapsed: 1.10e+01, train loss: 2.03650e-06, val loss: 2.79433e-06, min loss: 2.03318e-06\n",
      "Epoch: 491700, elapsed: 1.27e+01, train loss: 2.22432e-06, val loss: 3.03271e-06, min loss: 2.03318e-06\n",
      "Epoch: 491800, elapsed: 1.13e+01, train loss: 2.30839e-06, val loss: 2.94886e-06, min loss: 2.03318e-06\n",
      "Epoch: 491900, elapsed: 1.12e+01, train loss: 2.03441e-06, val loss: 2.77455e-06, min loss: 2.03318e-06\n",
      "Epoch: 492000, elapsed: 1.12e+01, train loss: 2.04039e-06, val loss: 2.75973e-06, min loss: 2.03318e-06\n",
      "Epoch: 492100, elapsed: 1.13e+01, train loss: 2.02048e-06, val loss: 2.78699e-06, min loss: 2.02048e-06\n",
      "Epoch: 492200, elapsed: 1.10e+01, train loss: 2.29324e-06, val loss: 3.15180e-06, min loss: 2.02048e-06\n",
      "Epoch: 492300, elapsed: 1.10e+01, train loss: 5.90218e-06, val loss: 6.11511e-06, min loss: 2.02048e-06\n",
      "Epoch: 492400, elapsed: 1.10e+01, train loss: 6.38171e-06, val loss: 6.56741e-06, min loss: 2.02048e-06\n",
      "Epoch: 492500, elapsed: 1.10e+01, train loss: 2.01656e-06, val loss: 2.74543e-06, min loss: 2.01656e-06\n",
      "Epoch: 492600, elapsed: 1.09e+01, train loss: 2.02933e-06, val loss: 2.74678e-06, min loss: 2.01656e-06\n",
      "Epoch: 492700, elapsed: 1.26e+01, train loss: 2.05429e-06, val loss: 2.78556e-06, min loss: 2.01656e-06\n",
      "Epoch: 492800, elapsed: 1.13e+01, train loss: 2.19079e-06, val loss: 2.89077e-06, min loss: 2.01656e-06\n",
      "Epoch: 492900, elapsed: 1.14e+01, train loss: 4.25367e-06, val loss: 4.55994e-06, min loss: 2.01656e-06\n",
      "Epoch: 493000, elapsed: 1.12e+01, train loss: 2.35695e-06, val loss: 3.71701e-06, min loss: 2.01656e-06\n",
      "Epoch: 493100, elapsed: 1.11e+01, train loss: 2.02959e-06, val loss: 2.74817e-06, min loss: 2.01656e-06\n",
      "Epoch: 493200, elapsed: 1.10e+01, train loss: 2.01394e-06, val loss: 2.74547e-06, min loss: 2.01394e-06\n",
      "Epoch: 493300, elapsed: 1.08e+01, train loss: 2.04401e-06, val loss: 2.85425e-06, min loss: 2.01394e-06\n",
      "Epoch: 493400, elapsed: 1.09e+01, train loss: 7.92107e-06, val loss: 1.00097e-05, min loss: 2.01394e-06\n",
      "Epoch: 493500, elapsed: 1.10e+01, train loss: 2.01438e-06, val loss: 2.73890e-06, min loss: 2.01394e-06\n",
      "Epoch: 493600, elapsed: 1.09e+01, train loss: 2.03401e-06, val loss: 2.77673e-06, min loss: 2.01394e-06\n",
      "Epoch: 493700, elapsed: 1.24e+01, train loss: 2.03642e-06, val loss: 2.79628e-06, min loss: 2.01394e-06\n",
      "Epoch: 493800, elapsed: 1.15e+01, train loss: 2.07122e-06, val loss: 2.80372e-06, min loss: 2.01394e-06\n",
      "Epoch: 493900, elapsed: 1.13e+01, train loss: 2.05264e-06, val loss: 2.88519e-06, min loss: 2.01394e-06\n",
      "Epoch: 494000, elapsed: 1.13e+01, train loss: 2.05249e-06, val loss: 2.79246e-06, min loss: 2.01394e-06\n",
      "Epoch: 494100, elapsed: 1.10e+01, train loss: 2.03390e-06, val loss: 2.77399e-06, min loss: 2.01394e-06\n",
      "Epoch: 494200, elapsed: 1.10e+01, train loss: 2.01856e-06, val loss: 2.74735e-06, min loss: 2.01394e-06\n",
      "Epoch: 494300, elapsed: 1.09e+01, train loss: 2.04189e-06, val loss: 2.79239e-06, min loss: 2.01394e-06\n",
      "Epoch: 494400, elapsed: 1.09e+01, train loss: 2.05917e-06, val loss: 2.80556e-06, min loss: 2.01394e-06\n",
      "Epoch: 494500, elapsed: 1.10e+01, train loss: 2.05211e-06, val loss: 2.77201e-06, min loss: 2.01394e-06\n",
      "Epoch: 494600, elapsed: 1.09e+01, train loss: 2.01615e-06, val loss: 2.74216e-06, min loss: 2.01394e-06\n",
      "Epoch: 494700, elapsed: 1.09e+01, train loss: 2.01210e-06, val loss: 2.74441e-06, min loss: 2.01210e-06\n",
      "Epoch: 494800, elapsed: 1.27e+01, train loss: 2.17090e-06, val loss: 2.96119e-06, min loss: 2.01210e-06\n",
      "Epoch: 494900, elapsed: 1.12e+01, train loss: 2.08012e-06, val loss: 2.77709e-06, min loss: 2.01210e-06\n",
      "Epoch: 495000, elapsed: 1.12e+01, train loss: 2.03831e-06, val loss: 2.76332e-06, min loss: 2.01210e-06\n",
      "Epoch: 495100, elapsed: 1.33e+01, train loss: 2.02972e-06, val loss: 2.78953e-06, min loss: 2.01210e-06\n",
      "Epoch: 495200, elapsed: 1.11e+01, train loss: 2.05426e-06, val loss: 2.76191e-06, min loss: 2.01210e-06\n",
      "Epoch: 495300, elapsed: 1.11e+01, train loss: 2.20539e-06, val loss: 2.96460e-06, min loss: 2.01210e-06\n",
      "Epoch: 495400, elapsed: 1.09e+01, train loss: 2.01195e-06, val loss: 2.73876e-06, min loss: 2.01195e-06\n",
      "Epoch: 495500, elapsed: 1.08e+01, train loss: 5.24890e-06, val loss: 6.75720e-06, min loss: 2.01195e-06\n",
      "Epoch: 495600, elapsed: 1.10e+01, train loss: 2.09700e-06, val loss: 2.84511e-06, min loss: 2.01195e-06\n",
      "Epoch: 495700, elapsed: 1.24e+01, train loss: 2.00743e-06, val loss: 2.74582e-06, min loss: 2.00743e-06\n",
      "Epoch: 495800, elapsed: 1.13e+01, train loss: 2.48074e-06, val loss: 2.84497e-06, min loss: 2.00743e-06\n",
      "Epoch: 495900, elapsed: 1.13e+01, train loss: 2.12211e-06, val loss: 2.97709e-06, min loss: 2.00743e-06\n",
      "Epoch: 496000, elapsed: 1.12e+01, train loss: 2.00545e-06, val loss: 2.74021e-06, min loss: 2.00545e-06\n",
      "Epoch: 496100, elapsed: 1.11e+01, train loss: 5.39733e-06, val loss: 9.16683e-06, min loss: 2.00545e-06\n",
      "Epoch: 496200, elapsed: 1.10e+01, train loss: 2.00481e-06, val loss: 2.73823e-06, min loss: 2.00481e-06\n",
      "Epoch: 496300, elapsed: 1.10e+01, train loss: 2.08925e-06, val loss: 2.87544e-06, min loss: 2.00481e-06\n",
      "Epoch: 496400, elapsed: 1.09e+01, train loss: 2.27314e-06, val loss: 2.87458e-06, min loss: 2.00481e-06\n",
      "Epoch: 496500, elapsed: 1.09e+01, train loss: 2.05361e-06, val loss: 2.79888e-06, min loss: 2.00481e-06\n",
      "Epoch: 496600, elapsed: 1.08e+01, train loss: 2.03743e-06, val loss: 2.79225e-06, min loss: 2.00481e-06\n",
      "Epoch: 496700, elapsed: 1.08e+01, train loss: 2.43858e-06, val loss: 3.29896e-06, min loss: 2.00481e-06\n",
      "Epoch: 496800, elapsed: 1.27e+01, train loss: 2.14758e-06, val loss: 2.86072e-06, min loss: 2.00481e-06\n",
      "Epoch: 496900, elapsed: 1.12e+01, train loss: 2.02973e-06, val loss: 2.74756e-06, min loss: 2.00481e-06\n",
      "Epoch: 497000, elapsed: 1.13e+01, train loss: 2.02466e-06, val loss: 2.79084e-06, min loss: 2.00481e-06\n",
      "Epoch: 497100, elapsed: 1.12e+01, train loss: 2.12249e-06, val loss: 3.23661e-06, min loss: 2.00481e-06\n",
      "Epoch: 497200, elapsed: 1.12e+01, train loss: 2.40994e-06, val loss: 3.26774e-06, min loss: 2.00481e-06\n",
      "Epoch: 497300, elapsed: 1.10e+01, train loss: 2.04840e-06, val loss: 2.81564e-06, min loss: 2.00481e-06\n",
      "Epoch: 497400, elapsed: 1.10e+01, train loss: 2.43594e-06, val loss: 3.21363e-06, min loss: 2.00481e-06\n",
      "Epoch: 497500, elapsed: 1.09e+01, train loss: 6.93265e-06, val loss: 6.92762e-06, min loss: 2.00481e-06\n",
      "Epoch: 497600, elapsed: 1.09e+01, train loss: 2.41238e-06, val loss: 3.09251e-06, min loss: 2.00481e-06\n",
      "Epoch: 497700, elapsed: 1.10e+01, train loss: 2.28411e-06, val loss: 2.95751e-06, min loss: 2.00481e-06\n",
      "Epoch: 497800, elapsed: 1.26e+01, train loss: 2.20273e-06, val loss: 2.96494e-06, min loss: 2.00481e-06\n",
      "Epoch: 497900, elapsed: 1.13e+01, train loss: 4.47220e-06, val loss: 5.43074e-06, min loss: 2.00481e-06\n",
      "Epoch: 498000, elapsed: 1.14e+01, train loss: 2.10087e-06, val loss: 2.79887e-06, min loss: 2.00481e-06\n",
      "Epoch: 498100, elapsed: 1.11e+01, train loss: 2.09812e-06, val loss: 2.84704e-06, min loss: 2.00481e-06\n",
      "Epoch: 498200, elapsed: 1.10e+01, train loss: 2.00482e-06, val loss: 2.74211e-06, min loss: 2.00481e-06\n",
      "Epoch: 498300, elapsed: 1.11e+01, train loss: 2.05599e-06, val loss: 2.85696e-06, min loss: 2.00481e-06\n",
      "Epoch: 498400, elapsed: 1.10e+01, train loss: 2.79280e-06, val loss: 3.47104e-06, min loss: 2.00481e-06\n",
      "Epoch: 498500, elapsed: 1.10e+01, train loss: 2.68727e-06, val loss: 3.45225e-06, min loss: 2.00481e-06\n",
      "Epoch: 498600, elapsed: 1.10e+01, train loss: 2.02268e-06, val loss: 2.79282e-06, min loss: 2.00481e-06\n",
      "Epoch: 498700, elapsed: 1.10e+01, train loss: 2.04760e-06, val loss: 2.76842e-06, min loss: 2.00481e-06\n",
      "Epoch: 498800, elapsed: 1.27e+01, train loss: 2.03406e-06, val loss: 2.75458e-06, min loss: 2.00481e-06\n",
      "Epoch: 498900, elapsed: 1.13e+01, train loss: 2.05224e-06, val loss: 2.81302e-06, min loss: 2.00481e-06\n",
      "Epoch: 499000, elapsed: 1.16e+01, train loss: 2.01049e-06, val loss: 2.75165e-06, min loss: 2.00481e-06\n",
      "Epoch: 499100, elapsed: 1.13e+01, train loss: 2.05371e-06, val loss: 2.77511e-06, min loss: 2.00481e-06\n",
      "Epoch: 499200, elapsed: 1.12e+01, train loss: 2.81878e-06, val loss: 3.31319e-06, min loss: 2.00481e-06\n",
      "Epoch: 499300, elapsed: 1.11e+01, train loss: 3.21946e-06, val loss: 3.19742e-06, min loss: 2.00481e-06\n",
      "Epoch: 499400, elapsed: 1.10e+01, train loss: 2.19441e-06, val loss: 2.75165e-06, min loss: 2.00481e-06\n",
      "Epoch: 499500, elapsed: 1.08e+01, train loss: 2.19303e-06, val loss: 2.88090e-06, min loss: 2.00481e-06\n",
      "Epoch: 499600, elapsed: 1.09e+01, train loss: 2.27503e-06, val loss: 2.98571e-06, min loss: 2.00481e-06\n",
      "Epoch: 499700, elapsed: 1.09e+01, train loss: 2.20429e-06, val loss: 3.17731e-06, min loss: 2.00481e-06\n",
      "Epoch: 499800, elapsed: 1.23e+01, train loss: 2.51215e-06, val loss: 3.13185e-06, min loss: 2.00481e-06\n",
      "Epoch: 499900, elapsed: 1.13e+01, train loss: 1.10782e-05, val loss: 1.11922e-05, min loss: 2.00481e-06\n",
      "Epoch: 500000, elapsed: 1.12e+01, train loss: 3.36692e-06, val loss: 3.93808e-06, min loss: 2.00481e-06\n",
      "Epoch: 500100, elapsed: 1.32e+01, train loss: 3.34114e-06, val loss: 4.06987e-06, min loss: 2.00481e-06\n",
      "Epoch: 500200, elapsed: 1.11e+01, train loss: 2.31188e-06, val loss: 3.03091e-06, min loss: 2.00481e-06\n",
      "Epoch: 500300, elapsed: 1.11e+01, train loss: 2.00856e-06, val loss: 2.75287e-06, min loss: 2.00481e-06\n",
      "Epoch: 500400, elapsed: 1.11e+01, train loss: 2.00562e-06, val loss: 2.76561e-06, min loss: 2.00481e-06\n",
      "Epoch: 500500, elapsed: 1.09e+01, train loss: 2.01253e-06, val loss: 2.74040e-06, min loss: 2.00481e-06\n",
      "Epoch: 500600, elapsed: 1.09e+01, train loss: 2.00102e-06, val loss: 2.73738e-06, min loss: 2.00102e-06\n",
      "Epoch: 500700, elapsed: 1.09e+01, train loss: 1.99569e-06, val loss: 2.73627e-06, min loss: 1.99569e-06\n",
      "Epoch: 500800, elapsed: 1.24e+01, train loss: 2.15868e-06, val loss: 2.79928e-06, min loss: 1.99569e-06\n",
      "Epoch: 500900, elapsed: 1.13e+01, train loss: 2.07962e-06, val loss: 2.74247e-06, min loss: 1.99569e-06\n",
      "Epoch: 501000, elapsed: 1.12e+01, train loss: 2.03888e-06, val loss: 2.82869e-06, min loss: 1.99569e-06\n",
      "Epoch: 501100, elapsed: 1.13e+01, train loss: 2.71323e-06, val loss: 3.54817e-06, min loss: 1.99569e-06\n",
      "Epoch: 501200, elapsed: 1.13e+01, train loss: 2.04446e-06, val loss: 2.74987e-06, min loss: 1.99569e-06\n",
      "Epoch: 501300, elapsed: 1.11e+01, train loss: 1.99836e-06, val loss: 2.74140e-06, min loss: 1.99569e-06\n",
      "Epoch: 501400, elapsed: 1.10e+01, train loss: 2.04466e-06, val loss: 2.91984e-06, min loss: 1.99569e-06\n",
      "Epoch: 501500, elapsed: 1.10e+01, train loss: 3.86369e-06, val loss: 3.66487e-06, min loss: 1.99569e-06\n",
      "Epoch: 501600, elapsed: 1.09e+01, train loss: 2.97435e-06, val loss: 3.27141e-06, min loss: 1.99569e-06\n",
      "Epoch: 501700, elapsed: 1.11e+01, train loss: 2.55558e-06, val loss: 3.43174e-06, min loss: 1.99569e-06\n",
      "Epoch: 501800, elapsed: 1.09e+01, train loss: 2.49741e-06, val loss: 3.41199e-06, min loss: 1.99569e-06\n",
      "Epoch: 501900, elapsed: 1.28e+01, train loss: 2.07472e-06, val loss: 2.92439e-06, min loss: 1.99569e-06\n",
      "Epoch: 502000, elapsed: 1.13e+01, train loss: 2.60256e-06, val loss: 3.53097e-06, min loss: 1.99569e-06\n",
      "Epoch: 502100, elapsed: 1.13e+01, train loss: 2.15380e-06, val loss: 3.09238e-06, min loss: 1.99569e-06\n",
      "Epoch: 502200, elapsed: 1.13e+01, train loss: 2.41667e-06, val loss: 3.05132e-06, min loss: 1.99569e-06\n",
      "Epoch: 502300, elapsed: 1.11e+01, train loss: 2.38139e-06, val loss: 3.05339e-06, min loss: 1.99569e-06\n",
      "Epoch: 502400, elapsed: 1.10e+01, train loss: 2.60289e-06, val loss: 3.54826e-06, min loss: 1.99569e-06\n",
      "Epoch: 502500, elapsed: 1.10e+01, train loss: 2.41291e-06, val loss: 3.78646e-06, min loss: 1.99569e-06\n",
      "Epoch: 502600, elapsed: 1.10e+01, train loss: 9.99765e-06, val loss: 1.11573e-05, min loss: 1.99569e-06\n",
      "Epoch: 502700, elapsed: 1.09e+01, train loss: 1.98550e-06, val loss: 2.72110e-06, min loss: 1.98550e-06\n",
      "Epoch: 502800, elapsed: 1.10e+01, train loss: 2.00409e-06, val loss: 2.74463e-06, min loss: 1.98550e-06\n",
      "Epoch: 502900, elapsed: 1.27e+01, train loss: 3.86158e-06, val loss: 4.73568e-06, min loss: 1.98550e-06\n",
      "Epoch: 503000, elapsed: 1.13e+01, train loss: 1.98481e-06, val loss: 2.71951e-06, min loss: 1.98481e-06\n",
      "Epoch: 503100, elapsed: 1.10e+01, train loss: 1.98877e-06, val loss: 2.73619e-06, min loss: 1.98481e-06\n",
      "Epoch: 503200, elapsed: 1.12e+01, train loss: 1.39667e-05, val loss: 1.14688e-05, min loss: 1.98481e-06\n",
      "Epoch: 503300, elapsed: 1.11e+01, train loss: 1.98425e-06, val loss: 2.72827e-06, min loss: 1.98425e-06\n",
      "Epoch: 503400, elapsed: 1.09e+01, train loss: 2.00448e-06, val loss: 2.72729e-06, min loss: 1.98425e-06\n",
      "Epoch: 503500, elapsed: 1.11e+01, train loss: 2.50443e-06, val loss: 3.56649e-06, min loss: 1.98425e-06\n",
      "Epoch: 503600, elapsed: 1.10e+01, train loss: 2.15274e-06, val loss: 2.88295e-06, min loss: 1.98425e-06\n",
      "Epoch: 503700, elapsed: 1.10e+01, train loss: 2.82005e-06, val loss: 3.69541e-06, min loss: 1.98425e-06\n",
      "Epoch: 503800, elapsed: 1.10e+01, train loss: 4.96793e-06, val loss: 5.69968e-06, min loss: 1.98425e-06\n",
      "Epoch: 503900, elapsed: 1.26e+01, train loss: 2.01866e-06, val loss: 2.71808e-06, min loss: 1.98425e-06\n",
      "Epoch: 504000, elapsed: 1.13e+01, train loss: 2.02936e-06, val loss: 2.71763e-06, min loss: 1.98425e-06\n",
      "Epoch: 504100, elapsed: 1.13e+01, train loss: 2.01158e-06, val loss: 2.73606e-06, min loss: 1.98425e-06\n",
      "Epoch: 504200, elapsed: 1.10e+01, train loss: 4.04665e-06, val loss: 4.69713e-06, min loss: 1.98425e-06\n",
      "Epoch: 504300, elapsed: 1.12e+01, train loss: 6.44256e-06, val loss: 8.11318e-06, min loss: 1.98425e-06\n",
      "Epoch: 504400, elapsed: 1.12e+01, train loss: 2.59187e-06, val loss: 3.41041e-06, min loss: 1.98425e-06\n",
      "Epoch: 504500, elapsed: 1.12e+01, train loss: 3.36341e-06, val loss: 4.52893e-06, min loss: 1.98425e-06\n",
      "Epoch: 504600, elapsed: 1.09e+01, train loss: 2.35184e-06, val loss: 3.43840e-06, min loss: 1.98425e-06\n",
      "Epoch: 504700, elapsed: 1.10e+01, train loss: 2.08005e-06, val loss: 2.77319e-06, min loss: 1.98425e-06\n",
      "Epoch: 504800, elapsed: 1.10e+01, train loss: 2.01210e-06, val loss: 2.75637e-06, min loss: 1.98425e-06\n",
      "Epoch: 504900, elapsed: 1.11e+01, train loss: 1.98330e-06, val loss: 2.71378e-06, min loss: 1.98330e-06\n",
      "Epoch: 505000, elapsed: 1.29e+01, train loss: 1.99748e-06, val loss: 2.73995e-06, min loss: 1.98330e-06\n",
      "Epoch: 505100, elapsed: 1.35e+01, train loss: 2.02160e-06, val loss: 2.80820e-06, min loss: 1.98330e-06\n",
      "Epoch: 505200, elapsed: 1.13e+01, train loss: 1.98152e-06, val loss: 2.72505e-06, min loss: 1.98152e-06\n",
      "Epoch: 505300, elapsed: 1.14e+01, train loss: 1.99105e-06, val loss: 2.73873e-06, min loss: 1.98152e-06\n",
      "Epoch: 505400, elapsed: 1.12e+01, train loss: 1.99470e-06, val loss: 2.72510e-06, min loss: 1.98152e-06\n",
      "Epoch: 505500, elapsed: 1.10e+01, train loss: 2.01368e-06, val loss: 2.72712e-06, min loss: 1.98152e-06\n",
      "Epoch: 505600, elapsed: 1.09e+01, train loss: 2.02447e-06, val loss: 2.70885e-06, min loss: 1.98152e-06\n",
      "Epoch: 505700, elapsed: 1.10e+01, train loss: 1.97848e-06, val loss: 2.71144e-06, min loss: 1.97848e-06\n",
      "Epoch: 505800, elapsed: 1.10e+01, train loss: 1.98338e-06, val loss: 2.70853e-06, min loss: 1.97848e-06\n",
      "Epoch: 505900, elapsed: 1.10e+01, train loss: 1.99229e-06, val loss: 2.73480e-06, min loss: 1.97848e-06\n",
      "Epoch: 506000, elapsed: 1.27e+01, train loss: 1.99543e-06, val loss: 2.71873e-06, min loss: 1.97848e-06\n",
      "Epoch: 506100, elapsed: 1.13e+01, train loss: 2.04370e-06, val loss: 2.79912e-06, min loss: 1.97848e-06\n",
      "Epoch: 506200, elapsed: 1.09e+01, train loss: 1.99781e-06, val loss: 2.70117e-06, min loss: 1.97848e-06\n",
      "Epoch: 506300, elapsed: 1.10e+01, train loss: 2.02902e-06, val loss: 2.75508e-06, min loss: 1.97848e-06\n",
      "Epoch: 506400, elapsed: 1.11e+01, train loss: 2.64040e-06, val loss: 3.32636e-06, min loss: 1.97848e-06\n",
      "Epoch: 506500, elapsed: 1.09e+01, train loss: 3.35190e-06, val loss: 3.24236e-06, min loss: 1.97848e-06\n",
      "Epoch: 506600, elapsed: 1.09e+01, train loss: 2.12919e-06, val loss: 2.96572e-06, min loss: 1.97848e-06\n",
      "Epoch: 506700, elapsed: 1.08e+01, train loss: 1.98591e-06, val loss: 2.75640e-06, min loss: 1.97848e-06\n",
      "Epoch: 506800, elapsed: 1.11e+01, train loss: 2.01439e-06, val loss: 2.82184e-06, min loss: 1.97848e-06\n",
      "Epoch: 506900, elapsed: 1.10e+01, train loss: 3.22525e-06, val loss: 3.09819e-06, min loss: 1.97848e-06\n",
      "Epoch: 507000, elapsed: 1.27e+01, train loss: 3.92216e-06, val loss: 4.42369e-06, min loss: 1.97848e-06\n",
      "Epoch: 507100, elapsed: 1.14e+01, train loss: 3.18322e-06, val loss: 4.60965e-06, min loss: 1.97848e-06\n",
      "Epoch: 507200, elapsed: 1.13e+01, train loss: 2.19998e-06, val loss: 2.90281e-06, min loss: 1.97848e-06\n",
      "Epoch: 507300, elapsed: 1.11e+01, train loss: 1.97530e-06, val loss: 2.71505e-06, min loss: 1.97530e-06\n",
      "Epoch: 507400, elapsed: 1.12e+01, train loss: 2.66536e-06, val loss: 3.42916e-06, min loss: 1.97530e-06\n",
      "Epoch: 507500, elapsed: 1.10e+01, train loss: 2.01583e-06, val loss: 2.75548e-06, min loss: 1.97530e-06\n",
      "Epoch: 507600, elapsed: 1.10e+01, train loss: 2.23143e-06, val loss: 2.80884e-06, min loss: 1.97530e-06\n",
      "Epoch: 507700, elapsed: 1.10e+01, train loss: 2.07997e-06, val loss: 2.87305e-06, min loss: 1.97530e-06\n",
      "Epoch: 507800, elapsed: 1.11e+01, train loss: 1.98007e-06, val loss: 2.71087e-06, min loss: 1.97530e-06\n",
      "Epoch: 507900, elapsed: 1.12e+01, train loss: 2.00282e-06, val loss: 2.77130e-06, min loss: 1.97530e-06\n",
      "Epoch: 508000, elapsed: 1.26e+01, train loss: 2.78738e-06, val loss: 3.23095e-06, min loss: 1.97530e-06\n",
      "Epoch: 508100, elapsed: 1.14e+01, train loss: 2.19831e-06, val loss: 3.00729e-06, min loss: 1.97530e-06\n",
      "Epoch: 508200, elapsed: 1.12e+01, train loss: 2.03555e-06, val loss: 2.78930e-06, min loss: 1.97530e-06\n",
      "Epoch: 508300, elapsed: 1.13e+01, train loss: 4.46074e-06, val loss: 4.92487e-06, min loss: 1.97530e-06\n",
      "Epoch: 508400, elapsed: 1.12e+01, train loss: 2.00878e-06, val loss: 2.70278e-06, min loss: 1.97530e-06\n",
      "Epoch: 508500, elapsed: 1.10e+01, train loss: 1.98176e-06, val loss: 2.69178e-06, min loss: 1.97530e-06\n",
      "Epoch: 508600, elapsed: 1.11e+01, train loss: 2.15153e-06, val loss: 2.79363e-06, min loss: 1.97530e-06\n",
      "Epoch: 508700, elapsed: 1.12e+01, train loss: 2.18873e-06, val loss: 3.01130e-06, min loss: 1.97530e-06\n",
      "Epoch: 508800, elapsed: 1.10e+01, train loss: 2.02396e-06, val loss: 2.75696e-06, min loss: 1.97530e-06\n",
      "Epoch: 508900, elapsed: 1.09e+01, train loss: 2.00322e-06, val loss: 2.73867e-06, min loss: 1.97530e-06\n",
      "Epoch: 509000, elapsed: 1.08e+01, train loss: 2.11683e-06, val loss: 2.90017e-06, min loss: 1.97530e-06\n",
      "Epoch: 509100, elapsed: 1.27e+01, train loss: 2.45013e-06, val loss: 3.54061e-06, min loss: 1.97530e-06\n",
      "Epoch: 509200, elapsed: 1.13e+01, train loss: 5.38077e-06, val loss: 6.70209e-06, min loss: 1.97530e-06\n",
      "Epoch: 509300, elapsed: 1.12e+01, train loss: 2.62839e-06, val loss: 3.49954e-06, min loss: 1.97530e-06\n",
      "Epoch: 509400, elapsed: 1.12e+01, train loss: 1.97849e-06, val loss: 2.72909e-06, min loss: 1.97530e-06\n",
      "Epoch: 509500, elapsed: 1.12e+01, train loss: 1.98683e-06, val loss: 2.71390e-06, min loss: 1.97530e-06\n",
      "Epoch: 509600, elapsed: 1.10e+01, train loss: 2.62893e-06, val loss: 3.71193e-06, min loss: 1.97530e-06\n",
      "Epoch: 509700, elapsed: 1.09e+01, train loss: 7.35829e-06, val loss: 7.22380e-06, min loss: 1.97530e-06\n",
      "Epoch: 509800, elapsed: 1.10e+01, train loss: 2.57967e-06, val loss: 3.60662e-06, min loss: 1.97530e-06\n",
      "Epoch: 509900, elapsed: 1.10e+01, train loss: 2.09206e-06, val loss: 2.86316e-06, min loss: 1.97530e-06\n",
      "Epoch: 510000, elapsed: 1.09e+01, train loss: 2.04173e-06, val loss: 2.75204e-06, min loss: 1.97530e-06\n",
      "Epoch: 510100, elapsed: 1.47e+01, train loss: 2.01106e-06, val loss: 2.70118e-06, min loss: 1.97530e-06\n",
      "Epoch: 510200, elapsed: 1.12e+01, train loss: 2.31097e-06, val loss: 3.09484e-06, min loss: 1.97530e-06\n",
      "Epoch: 510300, elapsed: 1.14e+01, train loss: 2.10168e-06, val loss: 2.81288e-06, min loss: 1.97530e-06\n",
      "Epoch: 510400, elapsed: 1.12e+01, train loss: 2.00613e-06, val loss: 2.73941e-06, min loss: 1.97530e-06\n",
      "Epoch: 510500, elapsed: 1.12e+01, train loss: 1.99549e-06, val loss: 2.74526e-06, min loss: 1.97530e-06\n",
      "Epoch: 510600, elapsed: 1.12e+01, train loss: 2.02001e-06, val loss: 2.79162e-06, min loss: 1.97530e-06\n",
      "Epoch: 510700, elapsed: 1.10e+01, train loss: 1.97969e-06, val loss: 2.70419e-06, min loss: 1.97530e-06\n",
      "Epoch: 510800, elapsed: 1.11e+01, train loss: 1.98216e-06, val loss: 2.73043e-06, min loss: 1.97530e-06\n",
      "Epoch: 510900, elapsed: 1.08e+01, train loss: 2.08408e-06, val loss: 2.70078e-06, min loss: 1.97530e-06\n",
      "Epoch: 511000, elapsed: 1.09e+01, train loss: 3.39359e-06, val loss: 3.89247e-06, min loss: 1.97530e-06\n",
      "Epoch: 511100, elapsed: 1.26e+01, train loss: 2.70559e-06, val loss: 3.14038e-06, min loss: 1.97530e-06\n",
      "Epoch: 511200, elapsed: 1.14e+01, train loss: 2.36998e-06, val loss: 3.23496e-06, min loss: 1.97530e-06\n",
      "Epoch: 511300, elapsed: 1.14e+01, train loss: 1.97558e-06, val loss: 2.70275e-06, min loss: 1.97530e-06\n",
      "Epoch: 511400, elapsed: 1.14e+01, train loss: 1.98071e-06, val loss: 2.73979e-06, min loss: 1.97530e-06\n",
      "Epoch: 511500, elapsed: 1.11e+01, train loss: 2.07417e-06, val loss: 2.81001e-06, min loss: 1.97530e-06\n",
      "Epoch: 511600, elapsed: 1.14e+01, train loss: 2.23389e-06, val loss: 2.97666e-06, min loss: 1.97530e-06\n",
      "Epoch: 511700, elapsed: 1.12e+01, train loss: 2.47920e-06, val loss: 2.80943e-06, min loss: 1.97530e-06\n",
      "Epoch: 511800, elapsed: 1.11e+01, train loss: 2.05956e-06, val loss: 2.85548e-06, min loss: 1.97530e-06\n",
      "Epoch: 511900, elapsed: 1.10e+01, train loss: 1.98128e-06, val loss: 2.71353e-06, min loss: 1.97530e-06\n",
      "Epoch: 512000, elapsed: 1.08e+01, train loss: 1.96927e-06, val loss: 2.69602e-06, min loss: 1.96927e-06\n",
      "Epoch: 512100, elapsed: 1.11e+01, train loss: 1.96758e-06, val loss: 2.70006e-06, min loss: 1.96758e-06\n",
      "Epoch: 512200, elapsed: 1.29e+01, train loss: 2.53620e-06, val loss: 3.37096e-06, min loss: 1.96758e-06\n",
      "Epoch: 512300, elapsed: 1.12e+01, train loss: 2.90931e-06, val loss: 3.59291e-06, min loss: 1.96758e-06\n",
      "Epoch: 512400, elapsed: 1.11e+01, train loss: 2.13823e-06, val loss: 2.81588e-06, min loss: 1.96758e-06\n",
      "Epoch: 512500, elapsed: 1.11e+01, train loss: 2.37935e-06, val loss: 3.08538e-06, min loss: 1.96758e-06\n",
      "Epoch: 512600, elapsed: 1.12e+01, train loss: 4.70474e-06, val loss: 5.68603e-06, min loss: 1.96758e-06\n",
      "Epoch: 512700, elapsed: 1.12e+01, train loss: 2.23349e-06, val loss: 3.10366e-06, min loss: 1.96758e-06\n",
      "Epoch: 512800, elapsed: 1.11e+01, train loss: 2.20157e-06, val loss: 2.87829e-06, min loss: 1.96758e-06\n",
      "Epoch: 512900, elapsed: 1.10e+01, train loss: 2.04013e-06, val loss: 2.73246e-06, min loss: 1.96758e-06\n",
      "Epoch: 513000, elapsed: 1.10e+01, train loss: 2.92421e-06, val loss: 4.02626e-06, min loss: 1.96758e-06\n",
      "Epoch: 513100, elapsed: 1.09e+01, train loss: 2.28781e-06, val loss: 3.36771e-06, min loss: 1.96758e-06\n",
      "Epoch: 513200, elapsed: 1.26e+01, train loss: 1.99237e-06, val loss: 2.75451e-06, min loss: 1.96758e-06\n",
      "Epoch: 513300, elapsed: 1.13e+01, train loss: 2.50042e-06, val loss: 3.50783e-06, min loss: 1.96758e-06\n",
      "Epoch: 513400, elapsed: 1.12e+01, train loss: 5.49079e-06, val loss: 6.42089e-06, min loss: 1.96758e-06\n",
      "Epoch: 513500, elapsed: 1.12e+01, train loss: 2.65104e-06, val loss: 3.90824e-06, min loss: 1.96758e-06\n",
      "Epoch: 513600, elapsed: 1.12e+01, train loss: 9.45796e-06, val loss: 1.15148e-05, min loss: 1.96758e-06\n",
      "Epoch: 513700, elapsed: 1.12e+01, train loss: 1.95535e-06, val loss: 2.69562e-06, min loss: 1.95535e-06\n",
      "Epoch: 513800, elapsed: 1.12e+01, train loss: 2.01651e-06, val loss: 2.71082e-06, min loss: 1.95535e-06\n",
      "Epoch: 513900, elapsed: 1.10e+01, train loss: 2.17436e-06, val loss: 3.10530e-06, min loss: 1.95535e-06\n",
      "Epoch: 514000, elapsed: 1.09e+01, train loss: 5.02587e-06, val loss: 4.26648e-06, min loss: 1.95535e-06\n",
      "Epoch: 514100, elapsed: 1.08e+01, train loss: 2.05335e-06, val loss: 2.87512e-06, min loss: 1.95535e-06\n",
      "Epoch: 514200, elapsed: 1.07e+01, train loss: 1.96229e-06, val loss: 2.71796e-06, min loss: 1.95535e-06\n",
      "Epoch: 514300, elapsed: 1.26e+01, train loss: 2.02971e-06, val loss: 2.81369e-06, min loss: 1.95535e-06\n",
      "Epoch: 514400, elapsed: 1.13e+01, train loss: 5.04361e-06, val loss: 5.03477e-06, min loss: 1.95535e-06\n",
      "Epoch: 514500, elapsed: 1.11e+01, train loss: 2.15970e-06, val loss: 2.84324e-06, min loss: 1.95535e-06\n",
      "Epoch: 514600, elapsed: 1.11e+01, train loss: 2.02818e-06, val loss: 2.73259e-06, min loss: 1.95535e-06\n",
      "Epoch: 514700, elapsed: 1.10e+01, train loss: 2.11989e-06, val loss: 2.93986e-06, min loss: 1.95535e-06\n",
      "Epoch: 514800, elapsed: 1.13e+01, train loss: 2.04513e-06, val loss: 2.83574e-06, min loss: 1.95535e-06\n",
      "Epoch: 514900, elapsed: 1.10e+01, train loss: 2.03527e-06, val loss: 2.73742e-06, min loss: 1.95535e-06\n",
      "Epoch: 515000, elapsed: 1.10e+01, train loss: 1.95892e-06, val loss: 2.69080e-06, min loss: 1.95535e-06\n",
      "Epoch: 515100, elapsed: 1.29e+01, train loss: 2.00322e-06, val loss: 2.72419e-06, min loss: 1.95535e-06\n",
      "Epoch: 515200, elapsed: 1.08e+01, train loss: 2.11629e-06, val loss: 3.07247e-06, min loss: 1.95535e-06\n",
      "Epoch: 515300, elapsed: 1.27e+01, train loss: 2.10031e-06, val loss: 2.94479e-06, min loss: 1.95535e-06\n",
      "Epoch: 515400, elapsed: 1.12e+01, train loss: 2.36864e-06, val loss: 2.79518e-06, min loss: 1.95535e-06\n",
      "Epoch: 515500, elapsed: 1.12e+01, train loss: 7.66020e-06, val loss: 7.58355e-06, min loss: 1.95535e-06\n",
      "Epoch: 515600, elapsed: 1.11e+01, train loss: 8.86758e-06, val loss: 1.03578e-05, min loss: 1.95535e-06\n",
      "Epoch: 515700, elapsed: 1.12e+01, train loss: 2.01726e-06, val loss: 2.72792e-06, min loss: 1.95535e-06\n",
      "Epoch: 515800, elapsed: 1.14e+01, train loss: 3.25959e-06, val loss: 4.17468e-06, min loss: 1.95535e-06\n",
      "Epoch: 515900, elapsed: 1.11e+01, train loss: 3.04357e-06, val loss: 3.57253e-06, min loss: 1.95535e-06\n",
      "Epoch: 516000, elapsed: 1.08e+01, train loss: 2.32452e-06, val loss: 2.86249e-06, min loss: 1.95535e-06\n",
      "Epoch: 516100, elapsed: 1.10e+01, train loss: 2.24972e-06, val loss: 2.96671e-06, min loss: 1.95535e-06\n",
      "Epoch: 516200, elapsed: 1.09e+01, train loss: 2.05430e-06, val loss: 2.80837e-06, min loss: 1.95535e-06\n",
      "Epoch: 516300, elapsed: 1.09e+01, train loss: 2.03090e-06, val loss: 2.81096e-06, min loss: 1.95535e-06\n",
      "Epoch: 516400, elapsed: 1.26e+01, train loss: 2.16231e-06, val loss: 2.96682e-06, min loss: 1.95535e-06\n",
      "Epoch: 516500, elapsed: 1.14e+01, train loss: 2.26715e-06, val loss: 3.01132e-06, min loss: 1.95535e-06\n",
      "Epoch: 516600, elapsed: 1.12e+01, train loss: 2.72611e-06, val loss: 3.34353e-06, min loss: 1.95535e-06\n",
      "Epoch: 516700, elapsed: 1.13e+01, train loss: 3.00071e-06, val loss: 4.54047e-06, min loss: 1.95535e-06\n",
      "Epoch: 516800, elapsed: 1.11e+01, train loss: 1.95723e-06, val loss: 2.70060e-06, min loss: 1.95535e-06\n",
      "Epoch: 516900, elapsed: 1.10e+01, train loss: 1.96326e-06, val loss: 2.69077e-06, min loss: 1.95535e-06\n",
      "Epoch: 517000, elapsed: 1.09e+01, train loss: 1.95791e-06, val loss: 2.69749e-06, min loss: 1.95535e-06\n",
      "Epoch: 517100, elapsed: 1.09e+01, train loss: 1.94891e-06, val loss: 2.68982e-06, min loss: 1.94891e-06\n",
      "Epoch: 517200, elapsed: 1.10e+01, train loss: 2.10288e-06, val loss: 2.81986e-06, min loss: 1.94891e-06\n",
      "Epoch: 517300, elapsed: 1.11e+01, train loss: 2.24258e-06, val loss: 2.94872e-06, min loss: 1.94891e-06\n",
      "Epoch: 517400, elapsed: 1.26e+01, train loss: 3.48145e-06, val loss: 3.66776e-06, min loss: 1.94891e-06\n",
      "Epoch: 517500, elapsed: 1.11e+01, train loss: 2.12763e-06, val loss: 2.98395e-06, min loss: 1.94891e-06\n",
      "Epoch: 517600, elapsed: 1.14e+01, train loss: 2.47771e-06, val loss: 3.12470e-06, min loss: 1.94891e-06\n",
      "Epoch: 517700, elapsed: 1.11e+01, train loss: 2.74369e-06, val loss: 4.10443e-06, min loss: 1.94891e-06\n",
      "Epoch: 517800, elapsed: 1.12e+01, train loss: 2.03283e-06, val loss: 2.75399e-06, min loss: 1.94891e-06\n",
      "Epoch: 517900, elapsed: 1.12e+01, train loss: 1.94663e-06, val loss: 2.68091e-06, min loss: 1.94663e-06\n",
      "Epoch: 518000, elapsed: 1.11e+01, train loss: 1.94465e-06, val loss: 2.68019e-06, min loss: 1.94465e-06\n",
      "Epoch: 518100, elapsed: 1.11e+01, train loss: 1.99405e-06, val loss: 2.69556e-06, min loss: 1.94465e-06\n",
      "Epoch: 518200, elapsed: 1.10e+01, train loss: 2.00286e-06, val loss: 2.71346e-06, min loss: 1.94465e-06\n",
      "Epoch: 518300, elapsed: 1.10e+01, train loss: 2.04929e-06, val loss: 2.77492e-06, min loss: 1.94465e-06\n",
      "Epoch: 518400, elapsed: 1.08e+01, train loss: 1.94633e-06, val loss: 2.69893e-06, min loss: 1.94465e-06\n",
      "Epoch: 518500, elapsed: 1.28e+01, train loss: 2.04114e-06, val loss: 2.69739e-06, min loss: 1.94465e-06\n",
      "Epoch: 518600, elapsed: 1.15e+01, train loss: 3.37413e-06, val loss: 4.09502e-06, min loss: 1.94465e-06\n",
      "Epoch: 518700, elapsed: 1.12e+01, train loss: 2.00124e-06, val loss: 2.80764e-06, min loss: 1.94465e-06\n",
      "Epoch: 518800, elapsed: 1.12e+01, train loss: 1.94536e-06, val loss: 2.67407e-06, min loss: 1.94465e-06\n",
      "Epoch: 518900, elapsed: 1.12e+01, train loss: 1.97127e-06, val loss: 2.69251e-06, min loss: 1.94465e-06\n",
      "Epoch: 519000, elapsed: 1.11e+01, train loss: 1.96087e-06, val loss: 2.71389e-06, min loss: 1.94465e-06\n",
      "Epoch: 519100, elapsed: 1.11e+01, train loss: 2.19549e-06, val loss: 2.90960e-06, min loss: 1.94465e-06\n",
      "Epoch: 519200, elapsed: 1.09e+01, train loss: 2.88546e-06, val loss: 3.46383e-06, min loss: 1.94465e-06\n",
      "Epoch: 519300, elapsed: 1.10e+01, train loss: 1.95387e-06, val loss: 2.72396e-06, min loss: 1.94465e-06\n",
      "Epoch: 519400, elapsed: 1.07e+01, train loss: 2.18819e-06, val loss: 3.01052e-06, min loss: 1.94465e-06\n",
      "Epoch: 519500, elapsed: 1.25e+01, train loss: 3.39708e-06, val loss: 4.64897e-06, min loss: 1.94465e-06\n",
      "Epoch: 519600, elapsed: 1.11e+01, train loss: 2.44180e-06, val loss: 3.25043e-06, min loss: 1.94465e-06\n",
      "Epoch: 519700, elapsed: 1.12e+01, train loss: 3.07174e-06, val loss: 3.80753e-06, min loss: 1.94465e-06\n",
      "Epoch: 519800, elapsed: 1.11e+01, train loss: 1.95049e-06, val loss: 2.70019e-06, min loss: 1.94465e-06\n",
      "Epoch: 519900, elapsed: 1.11e+01, train loss: 2.06985e-06, val loss: 2.73794e-06, min loss: 1.94465e-06\n",
      "Epoch: 520000, elapsed: 1.12e+01, train loss: 1.98676e-06, val loss: 2.74295e-06, min loss: 1.94465e-06\n",
      "Epoch: 520100, elapsed: 1.31e+01, train loss: 3.69316e-06, val loss: 5.12814e-06, min loss: 1.94465e-06\n",
      "Epoch: 520200, elapsed: 1.10e+01, train loss: 2.03174e-06, val loss: 2.77823e-06, min loss: 1.94465e-06\n",
      "Epoch: 520300, elapsed: 1.08e+01, train loss: 1.95308e-06, val loss: 2.68206e-06, min loss: 1.94465e-06\n",
      "Epoch: 520400, elapsed: 1.09e+01, train loss: 2.00867e-06, val loss: 2.70303e-06, min loss: 1.94465e-06\n",
      "Epoch: 520500, elapsed: 1.10e+01, train loss: 1.95910e-06, val loss: 2.73985e-06, min loss: 1.94465e-06\n",
      "Epoch: 520600, elapsed: 1.26e+01, train loss: 1.95782e-06, val loss: 2.72038e-06, min loss: 1.94465e-06\n",
      "Epoch: 520700, elapsed: 1.13e+01, train loss: 1.98070e-06, val loss: 2.74624e-06, min loss: 1.94465e-06\n",
      "Epoch: 520800, elapsed: 1.11e+01, train loss: 2.58492e-06, val loss: 3.58712e-06, min loss: 1.94465e-06\n",
      "Epoch: 520900, elapsed: 1.11e+01, train loss: 4.95766e-06, val loss: 6.21485e-06, min loss: 1.94465e-06\n",
      "Epoch: 521000, elapsed: 1.12e+01, train loss: 2.67833e-06, val loss: 3.10572e-06, min loss: 1.94465e-06\n",
      "Epoch: 521100, elapsed: 1.12e+01, train loss: 2.08238e-06, val loss: 2.72737e-06, min loss: 1.94465e-06\n",
      "Epoch: 521200, elapsed: 1.10e+01, train loss: 2.13344e-06, val loss: 2.76529e-06, min loss: 1.94465e-06\n",
      "Epoch: 521300, elapsed: 1.09e+01, train loss: 2.11923e-06, val loss: 2.95180e-06, min loss: 1.94465e-06\n",
      "Epoch: 521400, elapsed: 1.08e+01, train loss: 2.19012e-06, val loss: 3.00616e-06, min loss: 1.94465e-06\n",
      "Epoch: 521500, elapsed: 1.08e+01, train loss: 3.53297e-06, val loss: 4.18729e-06, min loss: 1.94465e-06\n",
      "Epoch: 521600, elapsed: 1.27e+01, train loss: 3.73254e-06, val loss: 4.52768e-06, min loss: 1.94465e-06\n",
      "Epoch: 521700, elapsed: 1.14e+01, train loss: 2.47666e-06, val loss: 3.30837e-06, min loss: 1.94465e-06\n",
      "Epoch: 521800, elapsed: 1.13e+01, train loss: 2.65078e-06, val loss: 3.25011e-06, min loss: 1.94465e-06\n",
      "Epoch: 521900, elapsed: 1.12e+01, train loss: 4.14566e-06, val loss: 4.23744e-06, min loss: 1.94465e-06\n",
      "Epoch: 522000, elapsed: 1.13e+01, train loss: 2.96365e-06, val loss: 3.61590e-06, min loss: 1.94465e-06\n",
      "Epoch: 522100, elapsed: 1.13e+01, train loss: 2.14646e-06, val loss: 2.80118e-06, min loss: 1.94465e-06\n",
      "Epoch: 522200, elapsed: 1.11e+01, train loss: 2.14597e-06, val loss: 2.79050e-06, min loss: 1.94465e-06\n",
      "Epoch: 522300, elapsed: 1.09e+01, train loss: 2.82889e-06, val loss: 3.66883e-06, min loss: 1.94465e-06\n",
      "Epoch: 522400, elapsed: 1.09e+01, train loss: 5.06214e-06, val loss: 5.33401e-06, min loss: 1.94465e-06\n",
      "Epoch: 522500, elapsed: 1.08e+01, train loss: 1.95120e-06, val loss: 2.68483e-06, min loss: 1.94465e-06\n",
      "Epoch: 522600, elapsed: 1.10e+01, train loss: 2.13296e-06, val loss: 2.73805e-06, min loss: 1.94465e-06\n",
      "Epoch: 522700, elapsed: 1.27e+01, train loss: 2.00731e-06, val loss: 2.67072e-06, min loss: 1.94465e-06\n",
      "Epoch: 522800, elapsed: 1.13e+01, train loss: 2.38560e-06, val loss: 2.95605e-06, min loss: 1.94465e-06\n",
      "Epoch: 522900, elapsed: 1.13e+01, train loss: 1.93726e-06, val loss: 2.67382e-06, min loss: 1.93726e-06\n",
      "Epoch: 523000, elapsed: 1.12e+01, train loss: 1.96381e-06, val loss: 2.67131e-06, min loss: 1.93726e-06\n",
      "Epoch: 523100, elapsed: 1.11e+01, train loss: 2.95659e-06, val loss: 3.78439e-06, min loss: 1.93726e-06\n",
      "Epoch: 523200, elapsed: 1.11e+01, train loss: 2.39116e-06, val loss: 3.22337e-06, min loss: 1.93726e-06\n",
      "Epoch: 523300, elapsed: 1.12e+01, train loss: 2.08672e-06, val loss: 2.79114e-06, min loss: 1.93726e-06\n",
      "Epoch: 523400, elapsed: 1.11e+01, train loss: 1.97861e-06, val loss: 2.72482e-06, min loss: 1.93726e-06\n",
      "Epoch: 523500, elapsed: 1.10e+01, train loss: 1.93335e-06, val loss: 2.67407e-06, min loss: 1.93335e-06\n",
      "Epoch: 523600, elapsed: 1.11e+01, train loss: 1.93112e-06, val loss: 2.67426e-06, min loss: 1.93112e-06\n",
      "Epoch: 523700, elapsed: 1.10e+01, train loss: 2.03515e-06, val loss: 2.68714e-06, min loss: 1.93112e-06\n",
      "Epoch: 523800, elapsed: 1.27e+01, train loss: 2.16212e-06, val loss: 2.99724e-06, min loss: 1.93112e-06\n",
      "Epoch: 523900, elapsed: 1.13e+01, train loss: 2.51290e-06, val loss: 3.17292e-06, min loss: 1.93112e-06\n",
      "Epoch: 524000, elapsed: 1.14e+01, train loss: 2.19338e-06, val loss: 2.82494e-06, min loss: 1.93112e-06\n",
      "Epoch: 524100, elapsed: 1.11e+01, train loss: 1.94914e-06, val loss: 2.67402e-06, min loss: 1.93112e-06\n",
      "Epoch: 524200, elapsed: 1.11e+01, train loss: 2.59627e-06, val loss: 3.52707e-06, min loss: 1.93112e-06\n",
      "Epoch: 524300, elapsed: 1.11e+01, train loss: 1.95820e-06, val loss: 2.71380e-06, min loss: 1.93112e-06\n",
      "Epoch: 524400, elapsed: 1.10e+01, train loss: 1.92764e-06, val loss: 2.67259e-06, min loss: 1.92764e-06\n",
      "Epoch: 524500, elapsed: 1.10e+01, train loss: 2.00309e-06, val loss: 2.76120e-06, min loss: 1.92764e-06\n",
      "Epoch: 524600, elapsed: 1.10e+01, train loss: 1.93430e-06, val loss: 2.70315e-06, min loss: 1.92764e-06\n",
      "Epoch: 524700, elapsed: 1.09e+01, train loss: 1.93674e-06, val loss: 2.65907e-06, min loss: 1.92764e-06\n",
      "Epoch: 524800, elapsed: 1.26e+01, train loss: 1.94345e-06, val loss: 2.65880e-06, min loss: 1.92764e-06\n",
      "Epoch: 524900, elapsed: 1.12e+01, train loss: 2.19060e-06, val loss: 2.82159e-06, min loss: 1.92764e-06\n",
      "Epoch: 525000, elapsed: 1.13e+01, train loss: 2.65140e-06, val loss: 3.12083e-06, min loss: 1.92764e-06\n",
      "Epoch: 525100, elapsed: 1.33e+01, train loss: 7.73642e-06, val loss: 4.96286e-06, min loss: 1.92764e-06\n",
      "Epoch: 525200, elapsed: 1.13e+01, train loss: 1.92455e-06, val loss: 2.66535e-06, min loss: 1.92455e-06\n",
      "Epoch: 525300, elapsed: 1.13e+01, train loss: 1.93285e-06, val loss: 2.69146e-06, min loss: 1.92455e-06\n",
      "Epoch: 525400, elapsed: 1.13e+01, train loss: 1.96476e-06, val loss: 2.66283e-06, min loss: 1.92455e-06\n",
      "Epoch: 525500, elapsed: 1.11e+01, train loss: 2.28888e-06, val loss: 2.97316e-06, min loss: 1.92455e-06\n",
      "Epoch: 525600, elapsed: 1.09e+01, train loss: 1.93586e-06, val loss: 2.68529e-06, min loss: 1.92455e-06\n",
      "Epoch: 525700, elapsed: 1.10e+01, train loss: 2.38931e-06, val loss: 3.18964e-06, min loss: 1.92455e-06\n",
      "Epoch: 525800, elapsed: 1.08e+01, train loss: 1.92785e-06, val loss: 2.67330e-06, min loss: 1.92455e-06\n",
      "Epoch: 525900, elapsed: 1.27e+01, train loss: 1.99617e-06, val loss: 2.71183e-06, min loss: 1.92455e-06\n",
      "Epoch: 526000, elapsed: 1.14e+01, train loss: 2.09478e-06, val loss: 2.74719e-06, min loss: 1.92455e-06\n",
      "Epoch: 526100, elapsed: 1.12e+01, train loss: 2.16706e-06, val loss: 2.94119e-06, min loss: 1.92455e-06\n",
      "Epoch: 526200, elapsed: 1.13e+01, train loss: 1.97540e-06, val loss: 2.70659e-06, min loss: 1.92455e-06\n",
      "Epoch: 526300, elapsed: 1.12e+01, train loss: 7.59735e-06, val loss: 8.50574e-06, min loss: 1.92455e-06\n",
      "Epoch: 526400, elapsed: 1.12e+01, train loss: 4.52985e-06, val loss: 5.80895e-06, min loss: 1.92455e-06\n",
      "Epoch: 526500, elapsed: 1.10e+01, train loss: 2.44379e-06, val loss: 3.57000e-06, min loss: 1.92455e-06\n",
      "Epoch: 526600, elapsed: 1.09e+01, train loss: 2.01012e-06, val loss: 2.74941e-06, min loss: 1.92455e-06\n",
      "Epoch: 526700, elapsed: 1.10e+01, train loss: 2.03086e-06, val loss: 2.83954e-06, min loss: 1.92455e-06\n",
      "Epoch: 526800, elapsed: 1.08e+01, train loss: 2.44875e-06, val loss: 3.35597e-06, min loss: 1.92455e-06\n",
      "Epoch: 526900, elapsed: 1.24e+01, train loss: 2.10935e-06, val loss: 2.77334e-06, min loss: 1.92455e-06\n",
      "Epoch: 527000, elapsed: 1.12e+01, train loss: 2.03695e-06, val loss: 2.72307e-06, min loss: 1.92455e-06\n",
      "Epoch: 527100, elapsed: 1.12e+01, train loss: 1.91947e-06, val loss: 2.66548e-06, min loss: 1.91947e-06\n",
      "Epoch: 527200, elapsed: 1.13e+01, train loss: 1.92287e-06, val loss: 2.68548e-06, min loss: 1.91947e-06\n",
      "Epoch: 527300, elapsed: 1.12e+01, train loss: 1.92153e-06, val loss: 2.67528e-06, min loss: 1.91947e-06\n",
      "Epoch: 527400, elapsed: 1.11e+01, train loss: 1.93974e-06, val loss: 2.69129e-06, min loss: 1.91947e-06\n",
      "Epoch: 527500, elapsed: 1.13e+01, train loss: 1.97811e-06, val loss: 2.73674e-06, min loss: 1.91947e-06\n",
      "Epoch: 527600, elapsed: 1.09e+01, train loss: 2.44160e-06, val loss: 2.96592e-06, min loss: 1.91947e-06\n",
      "Epoch: 527700, elapsed: 1.08e+01, train loss: 4.63154e-06, val loss: 4.71296e-06, min loss: 1.91947e-06\n",
      "Epoch: 527800, elapsed: 1.08e+01, train loss: 2.34666e-06, val loss: 2.96684e-06, min loss: 1.91947e-06\n",
      "Epoch: 527900, elapsed: 1.08e+01, train loss: 1.92443e-06, val loss: 2.70937e-06, min loss: 1.91947e-06\n",
      "Epoch: 528000, elapsed: 1.26e+01, train loss: 1.99095e-06, val loss: 2.77579e-06, min loss: 1.91947e-06\n",
      "Epoch: 528100, elapsed: 1.12e+01, train loss: 1.99138e-06, val loss: 2.68833e-06, min loss: 1.91947e-06\n",
      "Epoch: 528200, elapsed: 1.13e+01, train loss: 1.93983e-06, val loss: 2.66692e-06, min loss: 1.91947e-06\n",
      "Epoch: 528300, elapsed: 1.11e+01, train loss: 1.92708e-06, val loss: 2.69771e-06, min loss: 1.91947e-06\n",
      "Epoch: 528400, elapsed: 1.12e+01, train loss: 1.92652e-06, val loss: 2.68791e-06, min loss: 1.91947e-06\n",
      "Epoch: 528500, elapsed: 1.11e+01, train loss: 1.96396e-06, val loss: 2.69290e-06, min loss: 1.91947e-06\n",
      "Epoch: 528600, elapsed: 1.11e+01, train loss: 1.94756e-06, val loss: 2.72059e-06, min loss: 1.91947e-06\n",
      "Epoch: 528700, elapsed: 1.09e+01, train loss: 1.91740e-06, val loss: 2.66822e-06, min loss: 1.91740e-06\n",
      "Epoch: 528800, elapsed: 1.09e+01, train loss: 1.97463e-06, val loss: 2.68655e-06, min loss: 1.91740e-06\n",
      "Epoch: 528900, elapsed: 1.09e+01, train loss: 1.96741e-06, val loss: 2.66730e-06, min loss: 1.91740e-06\n",
      "Epoch: 529000, elapsed: 1.09e+01, train loss: 1.97294e-06, val loss: 2.69730e-06, min loss: 1.91740e-06\n",
      "Epoch: 529100, elapsed: 1.29e+01, train loss: 2.88222e-06, val loss: 3.95307e-06, min loss: 1.91740e-06\n",
      "Epoch: 529200, elapsed: 1.12e+01, train loss: 4.04953e-06, val loss: 5.31884e-06, min loss: 1.91740e-06\n",
      "Epoch: 529300, elapsed: 1.12e+01, train loss: 9.66103e-06, val loss: 1.03385e-05, min loss: 1.91740e-06\n",
      "Epoch: 529400, elapsed: 1.12e+01, train loss: 2.65204e-06, val loss: 3.58573e-06, min loss: 1.91740e-06\n",
      "Epoch: 529500, elapsed: 1.15e+01, train loss: 4.42931e-06, val loss: 5.66390e-06, min loss: 1.91740e-06\n",
      "Epoch: 529600, elapsed: 1.12e+01, train loss: 2.06445e-06, val loss: 2.95775e-06, min loss: 1.91740e-06\n",
      "Epoch: 529700, elapsed: 1.11e+01, train loss: 1.96456e-06, val loss: 2.73455e-06, min loss: 1.91740e-06\n",
      "Epoch: 529800, elapsed: 1.11e+01, train loss: 2.65637e-06, val loss: 3.49932e-06, min loss: 1.91740e-06\n",
      "Epoch: 529900, elapsed: 1.10e+01, train loss: 2.25259e-06, val loss: 3.12382e-06, min loss: 1.91740e-06\n",
      "Epoch: 530000, elapsed: 1.09e+01, train loss: 2.01798e-06, val loss: 2.71227e-06, min loss: 1.91740e-06\n",
      "Epoch: 530100, elapsed: 1.47e+01, train loss: 2.76391e-06, val loss: 3.22138e-06, min loss: 1.91740e-06\n",
      "Epoch: 530200, elapsed: 1.15e+01, train loss: 2.30898e-06, val loss: 3.13446e-06, min loss: 1.91740e-06\n",
      "Epoch: 530300, elapsed: 1.12e+01, train loss: 2.31330e-06, val loss: 2.84541e-06, min loss: 1.91740e-06\n",
      "Epoch: 530400, elapsed: 1.13e+01, train loss: 2.44248e-06, val loss: 3.13793e-06, min loss: 1.91740e-06\n",
      "Epoch: 530500, elapsed: 1.11e+01, train loss: 2.06969e-06, val loss: 2.92067e-06, min loss: 1.91740e-06\n",
      "Epoch: 530600, elapsed: 1.12e+01, train loss: 5.76359e-06, val loss: 6.03397e-06, min loss: 1.91740e-06\n",
      "Epoch: 530700, elapsed: 1.12e+01, train loss: 2.51566e-06, val loss: 3.19770e-06, min loss: 1.91740e-06\n",
      "Epoch: 530800, elapsed: 1.10e+01, train loss: 2.17279e-06, val loss: 3.03541e-06, min loss: 1.91740e-06\n",
      "Epoch: 530900, elapsed: 1.10e+01, train loss: 4.00658e-06, val loss: 5.30528e-06, min loss: 1.91740e-06\n",
      "Epoch: 531000, elapsed: 1.09e+01, train loss: 2.57762e-06, val loss: 3.10178e-06, min loss: 1.91740e-06\n",
      "Epoch: 531100, elapsed: 1.11e+01, train loss: 3.10503e-06, val loss: 3.19101e-06, min loss: 1.91740e-06\n",
      "Epoch: 531200, elapsed: 1.26e+01, train loss: 1.92180e-06, val loss: 2.66570e-06, min loss: 1.91740e-06\n",
      "Epoch: 531300, elapsed: 1.13e+01, train loss: 1.91183e-06, val loss: 2.66373e-06, min loss: 1.91183e-06\n",
      "Epoch: 531400, elapsed: 1.14e+01, train loss: 1.92992e-06, val loss: 2.69793e-06, min loss: 1.91183e-06\n",
      "Epoch: 531500, elapsed: 1.12e+01, train loss: 2.85356e-06, val loss: 3.49579e-06, min loss: 1.91183e-06\n",
      "Epoch: 531600, elapsed: 1.14e+01, train loss: 1.98910e-06, val loss: 2.77226e-06, min loss: 1.91183e-06\n",
      "Epoch: 531700, elapsed: 1.14e+01, train loss: 2.66601e-06, val loss: 3.20189e-06, min loss: 1.91183e-06\n",
      "Epoch: 531800, elapsed: 1.13e+01, train loss: 1.91591e-06, val loss: 2.66756e-06, min loss: 1.91183e-06\n",
      "Epoch: 531900, elapsed: 1.12e+01, train loss: 1.93981e-06, val loss: 2.68964e-06, min loss: 1.91183e-06\n",
      "Epoch: 532000, elapsed: 1.11e+01, train loss: 5.91012e-06, val loss: 6.17809e-06, min loss: 1.91183e-06\n",
      "Epoch: 532100, elapsed: 1.12e+01, train loss: 2.21672e-06, val loss: 2.91507e-06, min loss: 1.91183e-06\n",
      "Epoch: 532200, elapsed: 1.10e+01, train loss: 1.93101e-06, val loss: 2.69577e-06, min loss: 1.91183e-06\n",
      "Epoch: 532300, elapsed: 1.27e+01, train loss: 2.40463e-06, val loss: 3.13086e-06, min loss: 1.91183e-06\n",
      "Epoch: 532400, elapsed: 1.12e+01, train loss: 2.18176e-06, val loss: 3.00122e-06, min loss: 1.91183e-06\n",
      "Epoch: 532500, elapsed: 1.12e+01, train loss: 2.08041e-06, val loss: 2.85581e-06, min loss: 1.91183e-06\n",
      "Epoch: 532600, elapsed: 1.11e+01, train loss: 4.49533e-06, val loss: 4.83389e-06, min loss: 1.91183e-06\n",
      "Epoch: 532700, elapsed: 1.11e+01, train loss: 3.90192e-06, val loss: 5.51499e-06, min loss: 1.91183e-06\n",
      "Epoch: 532800, elapsed: 1.14e+01, train loss: 1.91824e-06, val loss: 2.69966e-06, min loss: 1.91183e-06\n",
      "Epoch: 532900, elapsed: 1.12e+01, train loss: 1.23459e-05, val loss: 1.01739e-05, min loss: 1.91183e-06\n",
      "Epoch: 533000, elapsed: 1.11e+01, train loss: 1.91649e-06, val loss: 2.65399e-06, min loss: 1.91183e-06\n",
      "Epoch: 533100, elapsed: 1.10e+01, train loss: 3.71777e-06, val loss: 4.84272e-06, min loss: 1.91183e-06\n",
      "Epoch: 533200, elapsed: 1.09e+01, train loss: 2.40477e-06, val loss: 2.89100e-06, min loss: 1.91183e-06\n",
      "Epoch: 533300, elapsed: 1.09e+01, train loss: 2.35588e-06, val loss: 2.72040e-06, min loss: 1.91183e-06\n",
      "Epoch: 533400, elapsed: 1.27e+01, train loss: 1.97204e-06, val loss: 2.76088e-06, min loss: 1.91183e-06\n",
      "Epoch: 533500, elapsed: 1.12e+01, train loss: 1.90491e-06, val loss: 2.65525e-06, min loss: 1.90491e-06\n",
      "Epoch: 533600, elapsed: 1.12e+01, train loss: 1.90379e-06, val loss: 2.65476e-06, min loss: 1.90379e-06\n",
      "Epoch: 533700, elapsed: 1.11e+01, train loss: 1.97658e-06, val loss: 2.69969e-06, min loss: 1.90379e-06\n",
      "Epoch: 533800, elapsed: 1.12e+01, train loss: 1.90177e-06, val loss: 2.64724e-06, min loss: 1.90177e-06\n",
      "Epoch: 533900, elapsed: 1.11e+01, train loss: 1.91814e-06, val loss: 2.64723e-06, min loss: 1.90177e-06\n",
      "Epoch: 534000, elapsed: 1.11e+01, train loss: 1.90118e-06, val loss: 2.64835e-06, min loss: 1.90118e-06\n",
      "Epoch: 534100, elapsed: 1.10e+01, train loss: 1.91349e-06, val loss: 2.64987e-06, min loss: 1.90118e-06\n",
      "Epoch: 534200, elapsed: 1.08e+01, train loss: 2.22739e-06, val loss: 2.73683e-06, min loss: 1.90118e-06\n",
      "Epoch: 534300, elapsed: 1.09e+01, train loss: 2.41336e-06, val loss: 2.95926e-06, min loss: 1.90118e-06\n",
      "Epoch: 534400, elapsed: 1.08e+01, train loss: 2.32056e-06, val loss: 3.04360e-06, min loss: 1.90118e-06\n",
      "Epoch: 534500, elapsed: 1.27e+01, train loss: 3.91171e-06, val loss: 5.16471e-06, min loss: 1.90118e-06\n",
      "Epoch: 534600, elapsed: 1.15e+01, train loss: 2.91682e-06, val loss: 3.73168e-06, min loss: 1.90118e-06\n",
      "Epoch: 534700, elapsed: 1.13e+01, train loss: 2.05679e-06, val loss: 2.77091e-06, min loss: 1.90118e-06\n",
      "Epoch: 534800, elapsed: 1.13e+01, train loss: 1.98084e-06, val loss: 2.66663e-06, min loss: 1.90118e-06\n",
      "Epoch: 534900, elapsed: 1.13e+01, train loss: 1.90224e-06, val loss: 2.64195e-06, min loss: 1.90118e-06\n",
      "Epoch: 535000, elapsed: 1.11e+01, train loss: 2.65091e-06, val loss: 3.47757e-06, min loss: 1.90118e-06\n",
      "Epoch: 535100, elapsed: 1.31e+01, train loss: 1.93977e-06, val loss: 2.70843e-06, min loss: 1.90118e-06\n",
      "Epoch: 535200, elapsed: 1.11e+01, train loss: 4.23476e-06, val loss: 3.97737e-06, min loss: 1.90118e-06\n",
      "Epoch: 535300, elapsed: 1.10e+01, train loss: 2.12520e-06, val loss: 2.93022e-06, min loss: 1.90118e-06\n",
      "Epoch: 535400, elapsed: 1.10e+01, train loss: 2.24637e-06, val loss: 3.26881e-06, min loss: 1.90118e-06\n",
      "Epoch: 535500, elapsed: 1.27e+01, train loss: 1.96472e-06, val loss: 2.76046e-06, min loss: 1.90118e-06\n",
      "Epoch: 535600, elapsed: 1.14e+01, train loss: 1.91168e-06, val loss: 2.69170e-06, min loss: 1.90118e-06\n",
      "Epoch: 535700, elapsed: 1.13e+01, train loss: 1.89939e-06, val loss: 2.63864e-06, min loss: 1.89939e-06\n",
      "Epoch: 535800, elapsed: 1.12e+01, train loss: 2.64780e-06, val loss: 3.17577e-06, min loss: 1.89939e-06\n",
      "Epoch: 535900, elapsed: 1.13e+01, train loss: 1.90241e-06, val loss: 2.65926e-06, min loss: 1.89939e-06\n",
      "Epoch: 536000, elapsed: 1.12e+01, train loss: 1.91724e-06, val loss: 2.63350e-06, min loss: 1.89939e-06\n",
      "Epoch: 536100, elapsed: 1.12e+01, train loss: 2.24420e-06, val loss: 3.15954e-06, min loss: 1.89939e-06\n",
      "Epoch: 536200, elapsed: 1.10e+01, train loss: 2.39678e-06, val loss: 2.89736e-06, min loss: 1.89939e-06\n",
      "Epoch: 536300, elapsed: 1.09e+01, train loss: 9.78860e-06, val loss: 9.92671e-06, min loss: 1.89939e-06\n",
      "Epoch: 536400, elapsed: 1.10e+01, train loss: 1.96733e-06, val loss: 2.70066e-06, min loss: 1.89939e-06\n",
      "Epoch: 536500, elapsed: 1.08e+01, train loss: 1.89636e-06, val loss: 2.63987e-06, min loss: 1.89636e-06\n",
      "Epoch: 536600, elapsed: 1.27e+01, train loss: 1.91388e-06, val loss: 2.66824e-06, min loss: 1.89636e-06\n",
      "Epoch: 536700, elapsed: 1.12e+01, train loss: 2.41177e-06, val loss: 3.16221e-06, min loss: 1.89636e-06\n",
      "Epoch: 536800, elapsed: 1.12e+01, train loss: 1.99822e-06, val loss: 2.84030e-06, min loss: 1.89636e-06\n",
      "Epoch: 536900, elapsed: 1.12e+01, train loss: 2.27219e-06, val loss: 2.74114e-06, min loss: 1.89636e-06\n",
      "Epoch: 537000, elapsed: 1.12e+01, train loss: 7.62434e-06, val loss: 5.77285e-06, min loss: 1.89636e-06\n",
      "Epoch: 537100, elapsed: 1.10e+01, train loss: 1.90130e-06, val loss: 2.66710e-06, min loss: 1.89636e-06\n",
      "Epoch: 537200, elapsed: 1.11e+01, train loss: 1.89596e-06, val loss: 2.65336e-06, min loss: 1.89596e-06\n",
      "Epoch: 537300, elapsed: 1.11e+01, train loss: 1.90849e-06, val loss: 2.66206e-06, min loss: 1.89596e-06\n",
      "Epoch: 537400, elapsed: 1.10e+01, train loss: 1.98439e-06, val loss: 2.77151e-06, min loss: 1.89596e-06\n",
      "Epoch: 537500, elapsed: 1.09e+01, train loss: 3.11980e-06, val loss: 3.63936e-06, min loss: 1.89596e-06\n",
      "Epoch: 537600, elapsed: 1.09e+01, train loss: 4.74149e-06, val loss: 4.12314e-06, min loss: 1.89596e-06\n",
      "Epoch: 537700, elapsed: 1.27e+01, train loss: 1.93284e-06, val loss: 2.65590e-06, min loss: 1.89596e-06\n",
      "Epoch: 537800, elapsed: 1.13e+01, train loss: 1.89946e-06, val loss: 2.66952e-06, min loss: 1.89596e-06\n",
      "Epoch: 537900, elapsed: 1.16e+01, train loss: 1.91265e-06, val loss: 2.64057e-06, min loss: 1.89596e-06\n",
      "Epoch: 538000, elapsed: 1.11e+01, train loss: 1.89953e-06, val loss: 2.65104e-06, min loss: 1.89596e-06\n",
      "Epoch: 538100, elapsed: 1.13e+01, train loss: 1.90534e-06, val loss: 2.62542e-06, min loss: 1.89596e-06\n",
      "Epoch: 538200, elapsed: 1.11e+01, train loss: 1.93974e-06, val loss: 2.68068e-06, min loss: 1.89596e-06\n",
      "Epoch: 538300, elapsed: 1.13e+01, train loss: 2.01413e-06, val loss: 2.79850e-06, min loss: 1.89596e-06\n",
      "Epoch: 538400, elapsed: 1.10e+01, train loss: 2.24257e-06, val loss: 2.70051e-06, min loss: 1.89596e-06\n",
      "Epoch: 538500, elapsed: 1.10e+01, train loss: 2.31463e-06, val loss: 3.25029e-06, min loss: 1.89596e-06\n",
      "Epoch: 538600, elapsed: 1.10e+01, train loss: 2.04843e-06, val loss: 2.74576e-06, min loss: 1.89596e-06\n",
      "Epoch: 538700, elapsed: 1.10e+01, train loss: 1.97328e-06, val loss: 2.63219e-06, min loss: 1.89596e-06\n",
      "Epoch: 538800, elapsed: 1.28e+01, train loss: 2.01752e-06, val loss: 3.05718e-06, min loss: 1.89596e-06\n",
      "Epoch: 538900, elapsed: 1.12e+01, train loss: 2.49473e-06, val loss: 3.10329e-06, min loss: 1.89596e-06\n",
      "Epoch: 539000, elapsed: 1.11e+01, train loss: 1.95572e-06, val loss: 2.64723e-06, min loss: 1.89596e-06\n",
      "Epoch: 539100, elapsed: 1.10e+01, train loss: 1.88972e-06, val loss: 2.65008e-06, min loss: 1.88972e-06\n",
      "Epoch: 539200, elapsed: 1.11e+01, train loss: 1.89316e-06, val loss: 2.65449e-06, min loss: 1.88972e-06\n",
      "Epoch: 539300, elapsed: 1.12e+01, train loss: 1.97356e-06, val loss: 2.74361e-06, min loss: 1.88972e-06\n",
      "Epoch: 539400, elapsed: 1.12e+01, train loss: 1.95253e-06, val loss: 2.72978e-06, min loss: 1.88972e-06\n",
      "Epoch: 539500, elapsed: 1.10e+01, train loss: 1.94316e-06, val loss: 2.66868e-06, min loss: 1.88972e-06\n",
      "Epoch: 539600, elapsed: 1.10e+01, train loss: 1.88949e-06, val loss: 2.64189e-06, min loss: 1.88949e-06\n",
      "Epoch: 539700, elapsed: 1.09e+01, train loss: 2.04171e-06, val loss: 2.64905e-06, min loss: 1.88949e-06\n",
      "Epoch: 539800, elapsed: 1.07e+01, train loss: 7.31734e-06, val loss: 9.48527e-06, min loss: 1.88949e-06\n",
      "Epoch: 539900, elapsed: 1.27e+01, train loss: 1.88719e-06, val loss: 2.63134e-06, min loss: 1.88719e-06\n",
      "Epoch: 540000, elapsed: 1.14e+01, train loss: 3.88079e-06, val loss: 4.51277e-06, min loss: 1.88719e-06\n",
      "Epoch: 540100, elapsed: 1.31e+01, train loss: 1.90751e-06, val loss: 2.76297e-06, min loss: 1.88719e-06\n",
      "Epoch: 540200, elapsed: 1.11e+01, train loss: 3.41572e-06, val loss: 3.77076e-06, min loss: 1.88719e-06\n",
      "Epoch: 540300, elapsed: 1.13e+01, train loss: 3.58710e-06, val loss: 3.53752e-06, min loss: 1.88719e-06\n",
      "Epoch: 540400, elapsed: 1.12e+01, train loss: 2.12487e-06, val loss: 3.09153e-06, min loss: 1.88719e-06\n",
      "Epoch: 540500, elapsed: 1.11e+01, train loss: 2.96562e-06, val loss: 4.02597e-06, min loss: 1.88719e-06\n",
      "Epoch: 540600, elapsed: 1.10e+01, train loss: 1.88818e-06, val loss: 2.62870e-06, min loss: 1.88719e-06\n",
      "Epoch: 540700, elapsed: 1.09e+01, train loss: 1.88980e-06, val loss: 2.62571e-06, min loss: 1.88719e-06\n",
      "Epoch: 540800, elapsed: 1.08e+01, train loss: 1.90591e-06, val loss: 2.62835e-06, min loss: 1.88719e-06\n",
      "Epoch: 540900, elapsed: 1.09e+01, train loss: 1.89369e-06, val loss: 2.63459e-06, min loss: 1.88719e-06\n",
      "Epoch: 541000, elapsed: 1.27e+01, train loss: 2.66779e-06, val loss: 3.42712e-06, min loss: 1.88719e-06\n",
      "Epoch: 541100, elapsed: 1.12e+01, train loss: 2.45104e-06, val loss: 2.94660e-06, min loss: 1.88719e-06\n",
      "Epoch: 541200, elapsed: 1.12e+01, train loss: 9.66294e-06, val loss: 9.12276e-06, min loss: 1.88719e-06\n",
      "Epoch: 541300, elapsed: 1.12e+01, train loss: 2.44632e-06, val loss: 3.40687e-06, min loss: 1.88719e-06\n",
      "Epoch: 541400, elapsed: 1.12e+01, train loss: 3.51730e-06, val loss: 5.03136e-06, min loss: 1.88719e-06\n",
      "Epoch: 541500, elapsed: 1.11e+01, train loss: 2.27517e-06, val loss: 3.12359e-06, min loss: 1.88719e-06\n",
      "Epoch: 541600, elapsed: 1.10e+01, train loss: 2.49416e-06, val loss: 3.19069e-06, min loss: 1.88719e-06\n",
      "Epoch: 541700, elapsed: 1.09e+01, train loss: 1.95948e-06, val loss: 2.75183e-06, min loss: 1.88719e-06\n",
      "Epoch: 541800, elapsed: 1.09e+01, train loss: 1.89009e-06, val loss: 2.65100e-06, min loss: 1.88719e-06\n",
      "Epoch: 541900, elapsed: 1.09e+01, train loss: 1.90025e-06, val loss: 2.62802e-06, min loss: 1.88719e-06\n",
      "Epoch: 542000, elapsed: 1.09e+01, train loss: 1.90755e-06, val loss: 2.67438e-06, min loss: 1.88719e-06\n",
      "Epoch: 542100, elapsed: 1.29e+01, train loss: 1.92033e-06, val loss: 2.65478e-06, min loss: 1.88719e-06\n",
      "Epoch: 542200, elapsed: 1.13e+01, train loss: 1.98594e-06, val loss: 2.69847e-06, min loss: 1.88719e-06\n",
      "Epoch: 542300, elapsed: 1.12e+01, train loss: 1.89633e-06, val loss: 2.65931e-06, min loss: 1.88719e-06\n",
      "Epoch: 542400, elapsed: 1.11e+01, train loss: 1.91251e-06, val loss: 2.63776e-06, min loss: 1.88719e-06\n",
      "Epoch: 542500, elapsed: 1.11e+01, train loss: 1.93912e-06, val loss: 2.67698e-06, min loss: 1.88719e-06\n",
      "Epoch: 542600, elapsed: 1.10e+01, train loss: 1.88445e-06, val loss: 2.63798e-06, min loss: 1.88445e-06\n",
      "Epoch: 542700, elapsed: 1.10e+01, train loss: 3.61268e-06, val loss: 3.91673e-06, min loss: 1.88445e-06\n",
      "Epoch: 542800, elapsed: 1.11e+01, train loss: 1.91440e-06, val loss: 2.74626e-06, min loss: 1.88445e-06\n",
      "Epoch: 542900, elapsed: 1.08e+01, train loss: 1.96922e-06, val loss: 2.69300e-06, min loss: 1.88445e-06\n",
      "Epoch: 543000, elapsed: 1.09e+01, train loss: 5.42464e-06, val loss: 5.41231e-06, min loss: 1.88445e-06\n",
      "Epoch: 543100, elapsed: 1.10e+01, train loss: 2.32575e-06, val loss: 3.24156e-06, min loss: 1.88445e-06\n",
      "Epoch: 543200, elapsed: 1.28e+01, train loss: 2.04495e-06, val loss: 2.76628e-06, min loss: 1.88445e-06\n",
      "Epoch: 543300, elapsed: 1.14e+01, train loss: 2.23795e-06, val loss: 2.92792e-06, min loss: 1.88445e-06\n",
      "Epoch: 543400, elapsed: 1.11e+01, train loss: 2.20558e-06, val loss: 2.69901e-06, min loss: 1.88445e-06\n",
      "Epoch: 543500, elapsed: 1.11e+01, train loss: 2.41363e-06, val loss: 3.11973e-06, min loss: 1.88445e-06\n",
      "Epoch: 543600, elapsed: 1.10e+01, train loss: 2.82882e-06, val loss: 3.10882e-06, min loss: 1.88445e-06\n",
      "Epoch: 543700, elapsed: 1.12e+01, train loss: 2.55545e-06, val loss: 3.51333e-06, min loss: 1.88445e-06\n",
      "Epoch: 543800, elapsed: 1.11e+01, train loss: 2.23368e-06, val loss: 3.24590e-06, min loss: 1.88445e-06\n",
      "Epoch: 543900, elapsed: 1.10e+01, train loss: 2.90106e-06, val loss: 3.35801e-06, min loss: 1.88445e-06\n",
      "Epoch: 544000, elapsed: 1.09e+01, train loss: 1.94988e-06, val loss: 2.75113e-06, min loss: 1.88445e-06\n",
      "Epoch: 544100, elapsed: 1.10e+01, train loss: 4.73625e-06, val loss: 5.77960e-06, min loss: 1.88445e-06\n",
      "Epoch: 544200, elapsed: 1.08e+01, train loss: 2.20092e-06, val loss: 3.09052e-06, min loss: 1.88445e-06\n",
      "Epoch: 544300, elapsed: 1.28e+01, train loss: 1.90050e-06, val loss: 2.62502e-06, min loss: 1.88445e-06\n",
      "Epoch: 544400, elapsed: 1.13e+01, train loss: 1.87828e-06, val loss: 2.64215e-06, min loss: 1.87828e-06\n",
      "Epoch: 544500, elapsed: 1.12e+01, train loss: 1.92375e-06, val loss: 2.72248e-06, min loss: 1.87828e-06\n",
      "Epoch: 544600, elapsed: 1.11e+01, train loss: 1.95623e-06, val loss: 2.70666e-06, min loss: 1.87828e-06\n",
      "Epoch: 544700, elapsed: 1.11e+01, train loss: 2.12377e-06, val loss: 3.01409e-06, min loss: 1.87828e-06\n",
      "Epoch: 544800, elapsed: 1.12e+01, train loss: 2.88302e-06, val loss: 3.49788e-06, min loss: 1.87828e-06\n",
      "Epoch: 544900, elapsed: 1.11e+01, train loss: 1.90771e-06, val loss: 2.62413e-06, min loss: 1.87828e-06\n",
      "Epoch: 545000, elapsed: 1.10e+01, train loss: 1.87716e-06, val loss: 2.63362e-06, min loss: 1.87716e-06\n",
      "Epoch: 545100, elapsed: 1.29e+01, train loss: 1.88637e-06, val loss: 2.63228e-06, min loss: 1.87716e-06\n",
      "Epoch: 545200, elapsed: 1.09e+01, train loss: 5.77386e-06, val loss: 7.58322e-06, min loss: 1.87716e-06\n",
      "Epoch: 545300, elapsed: 1.25e+01, train loss: 1.89996e-06, val loss: 2.64902e-06, min loss: 1.87716e-06\n",
      "Epoch: 545400, elapsed: 1.16e+01, train loss: 1.90920e-06, val loss: 2.67791e-06, min loss: 1.87716e-06\n",
      "Epoch: 545500, elapsed: 1.11e+01, train loss: 1.90574e-06, val loss: 2.61527e-06, min loss: 1.87716e-06\n",
      "Epoch: 545600, elapsed: 1.13e+01, train loss: 1.91778e-06, val loss: 2.67708e-06, min loss: 1.87716e-06\n",
      "Epoch: 545700, elapsed: 1.12e+01, train loss: 1.90363e-06, val loss: 2.63858e-06, min loss: 1.87716e-06\n",
      "Epoch: 545800, elapsed: 1.11e+01, train loss: 1.92272e-06, val loss: 2.69601e-06, min loss: 1.87716e-06\n",
      "Epoch: 545900, elapsed: 1.11e+01, train loss: 1.98633e-06, val loss: 2.75525e-06, min loss: 1.87716e-06\n",
      "Epoch: 546000, elapsed: 1.10e+01, train loss: 1.96769e-06, val loss: 2.72922e-06, min loss: 1.87716e-06\n",
      "Epoch: 546100, elapsed: 1.11e+01, train loss: 2.09998e-06, val loss: 2.67772e-06, min loss: 1.87716e-06\n",
      "Epoch: 546200, elapsed: 1.10e+01, train loss: 1.90881e-06, val loss: 2.63998e-06, min loss: 1.87716e-06\n",
      "Epoch: 546300, elapsed: 1.10e+01, train loss: 1.90344e-06, val loss: 2.68545e-06, min loss: 1.87716e-06\n",
      "Epoch: 546400, elapsed: 1.26e+01, train loss: 2.13299e-06, val loss: 2.80742e-06, min loss: 1.87716e-06\n",
      "Epoch: 546500, elapsed: 1.13e+01, train loss: 1.99515e-06, val loss: 2.94539e-06, min loss: 1.87716e-06\n",
      "Epoch: 546600, elapsed: 1.12e+01, train loss: 3.88818e-06, val loss: 4.30589e-06, min loss: 1.87716e-06\n",
      "Epoch: 546700, elapsed: 1.13e+01, train loss: 6.42342e-06, val loss: 5.64377e-06, min loss: 1.87716e-06\n",
      "Epoch: 546800, elapsed: 1.11e+01, train loss: 6.25061e-06, val loss: 7.20762e-06, min loss: 1.87716e-06\n",
      "Epoch: 546900, elapsed: 1.11e+01, train loss: 2.70040e-06, val loss: 3.16434e-06, min loss: 1.87716e-06\n",
      "Epoch: 547000, elapsed: 1.10e+01, train loss: 1.87077e-06, val loss: 2.61661e-06, min loss: 1.87077e-06\n",
      "Epoch: 547100, elapsed: 1.12e+01, train loss: 1.90677e-06, val loss: 2.67774e-06, min loss: 1.87077e-06\n",
      "Epoch: 547200, elapsed: 1.09e+01, train loss: 2.04651e-06, val loss: 2.70509e-06, min loss: 1.87077e-06\n",
      "Epoch: 547300, elapsed: 1.09e+01, train loss: 2.00868e-06, val loss: 2.80956e-06, min loss: 1.87077e-06\n",
      "Epoch: 547400, elapsed: 1.09e+01, train loss: 2.03560e-06, val loss: 2.71511e-06, min loss: 1.87077e-06\n",
      "Epoch: 547500, elapsed: 1.26e+01, train loss: 2.17875e-06, val loss: 3.19432e-06, min loss: 1.87077e-06\n",
      "Epoch: 547600, elapsed: 1.11e+01, train loss: 2.06803e-06, val loss: 2.68326e-06, min loss: 1.87077e-06\n",
      "Epoch: 547700, elapsed: 1.11e+01, train loss: 2.92425e-06, val loss: 3.52780e-06, min loss: 1.87077e-06\n",
      "Epoch: 547800, elapsed: 1.12e+01, train loss: 1.94996e-06, val loss: 2.68038e-06, min loss: 1.87077e-06\n",
      "Epoch: 547900, elapsed: 1.12e+01, train loss: 1.87568e-06, val loss: 2.61928e-06, min loss: 1.87077e-06\n",
      "Epoch: 548000, elapsed: 1.12e+01, train loss: 1.86874e-06, val loss: 2.62100e-06, min loss: 1.86874e-06\n",
      "Epoch: 548100, elapsed: 1.11e+01, train loss: 1.88074e-06, val loss: 2.63136e-06, min loss: 1.86874e-06\n",
      "Epoch: 548200, elapsed: 1.13e+01, train loss: 1.91937e-06, val loss: 2.62100e-06, min loss: 1.86874e-06\n",
      "Epoch: 548300, elapsed: 1.09e+01, train loss: 1.91680e-06, val loss: 2.67516e-06, min loss: 1.86874e-06\n",
      "Epoch: 548400, elapsed: 1.09e+01, train loss: 2.12759e-06, val loss: 3.06472e-06, min loss: 1.86874e-06\n",
      "Epoch: 548500, elapsed: 1.09e+01, train loss: 1.92481e-06, val loss: 2.83418e-06, min loss: 1.86874e-06\n",
      "Epoch: 548600, elapsed: 1.08e+01, train loss: 1.87801e-06, val loss: 2.65255e-06, min loss: 1.86874e-06\n",
      "Epoch: 548700, elapsed: 1.27e+01, train loss: 1.86714e-06, val loss: 2.62306e-06, min loss: 1.86714e-06\n",
      "Epoch: 548800, elapsed: 1.11e+01, train loss: 1.99714e-06, val loss: 2.65691e-06, min loss: 1.86714e-06\n",
      "Epoch: 548900, elapsed: 1.12e+01, train loss: 2.09272e-06, val loss: 2.94783e-06, min loss: 1.86714e-06\n",
      "Epoch: 549000, elapsed: 1.12e+01, train loss: 2.83348e-06, val loss: 3.28818e-06, min loss: 1.86714e-06\n",
      "Epoch: 549100, elapsed: 1.12e+01, train loss: 4.13253e-06, val loss: 5.61543e-06, min loss: 1.86714e-06\n",
      "Epoch: 549200, elapsed: 1.11e+01, train loss: 2.08057e-06, val loss: 2.79284e-06, min loss: 1.86714e-06\n",
      "Epoch: 549300, elapsed: 1.11e+01, train loss: 4.45691e-06, val loss: 4.60935e-06, min loss: 1.86714e-06\n",
      "Epoch: 549400, elapsed: 1.10e+01, train loss: 1.96463e-06, val loss: 2.70052e-06, min loss: 1.86714e-06\n",
      "Epoch: 549500, elapsed: 1.10e+01, train loss: 5.66156e-06, val loss: 6.45596e-06, min loss: 1.86714e-06\n",
      "Epoch: 549600, elapsed: 1.10e+01, train loss: 2.19855e-06, val loss: 2.86876e-06, min loss: 1.86714e-06\n",
      "Epoch: 549700, elapsed: 1.09e+01, train loss: 1.87347e-06, val loss: 2.61273e-06, min loss: 1.86714e-06\n",
      "Epoch: 549800, elapsed: 1.27e+01, train loss: 1.87311e-06, val loss: 2.64107e-06, min loss: 1.86714e-06\n",
      "Epoch: 549900, elapsed: 1.14e+01, train loss: 1.87858e-06, val loss: 2.60829e-06, min loss: 1.86714e-06\n",
      "Epoch: 550000, elapsed: 1.12e+01, train loss: 1.90693e-06, val loss: 2.66459e-06, min loss: 1.86714e-06\n",
      "Epoch: 550100, elapsed: 1.32e+01, train loss: 1.90300e-06, val loss: 2.67874e-06, min loss: 1.86714e-06\n",
      "Epoch: 550200, elapsed: 1.12e+01, train loss: 1.86571e-06, val loss: 2.62470e-06, min loss: 1.86571e-06\n",
      "Epoch: 550300, elapsed: 1.11e+01, train loss: 1.89193e-06, val loss: 2.63151e-06, min loss: 1.86571e-06\n",
      "Epoch: 550400, elapsed: 1.13e+01, train loss: 1.87456e-06, val loss: 2.62168e-06, min loss: 1.86571e-06\n",
      "Epoch: 550500, elapsed: 1.10e+01, train loss: 1.89048e-06, val loss: 2.63691e-06, min loss: 1.86571e-06\n",
      "Epoch: 550600, elapsed: 1.10e+01, train loss: 1.90104e-06, val loss: 2.62503e-06, min loss: 1.86571e-06\n",
      "Epoch: 550700, elapsed: 1.10e+01, train loss: 1.99213e-06, val loss: 2.80760e-06, min loss: 1.86571e-06\n",
      "Epoch: 550800, elapsed: 1.26e+01, train loss: 1.87812e-06, val loss: 2.61779e-06, min loss: 1.86571e-06\n",
      "Epoch: 550900, elapsed: 1.12e+01, train loss: 2.13098e-06, val loss: 2.99285e-06, min loss: 1.86571e-06\n",
      "Epoch: 551000, elapsed: 1.11e+01, train loss: 2.53280e-06, val loss: 3.11759e-06, min loss: 1.86571e-06\n",
      "Epoch: 551100, elapsed: 1.12e+01, train loss: 2.25378e-06, val loss: 3.08859e-06, min loss: 1.86571e-06\n",
      "Epoch: 551200, elapsed: 1.13e+01, train loss: 3.21716e-06, val loss: 3.89059e-06, min loss: 1.86571e-06\n",
      "Epoch: 551300, elapsed: 1.12e+01, train loss: 2.64143e-06, val loss: 2.98952e-06, min loss: 1.86571e-06\n",
      "Epoch: 551400, elapsed: 1.11e+01, train loss: 1.97498e-06, val loss: 2.65657e-06, min loss: 1.86571e-06\n",
      "Epoch: 551500, elapsed: 1.11e+01, train loss: 1.90550e-06, val loss: 2.62964e-06, min loss: 1.86571e-06\n",
      "Epoch: 551600, elapsed: 1.11e+01, train loss: 2.57567e-06, val loss: 3.63925e-06, min loss: 1.86571e-06\n",
      "Epoch: 551700, elapsed: 1.11e+01, train loss: 2.60679e-06, val loss: 2.83094e-06, min loss: 1.86571e-06\n",
      "Epoch: 551800, elapsed: 1.10e+01, train loss: 2.02844e-06, val loss: 2.87205e-06, min loss: 1.86571e-06\n",
      "Epoch: 551900, elapsed: 1.09e+01, train loss: 1.96182e-06, val loss: 2.62894e-06, min loss: 1.86571e-06\n",
      "Epoch: 552000, elapsed: 1.30e+01, train loss: 1.89240e-06, val loss: 2.61491e-06, min loss: 1.86571e-06\n",
      "Epoch: 552100, elapsed: 1.13e+01, train loss: 1.86748e-06, val loss: 2.62448e-06, min loss: 1.86571e-06\n",
      "Epoch: 552200, elapsed: 1.11e+01, train loss: 1.86554e-06, val loss: 2.62771e-06, min loss: 1.86554e-06\n",
      "Epoch: 552300, elapsed: 1.12e+01, train loss: 1.10777e-05, val loss: 1.17384e-05, min loss: 1.86554e-06\n",
      "Epoch: 552400, elapsed: 1.14e+01, train loss: 1.85764e-06, val loss: 2.60782e-06, min loss: 1.85764e-06\n",
      "Epoch: 552500, elapsed: 1.11e+01, train loss: 1.86272e-06, val loss: 2.62453e-06, min loss: 1.85764e-06\n",
      "Epoch: 552600, elapsed: 1.11e+01, train loss: 2.02386e-06, val loss: 2.69255e-06, min loss: 1.85764e-06\n",
      "Epoch: 552700, elapsed: 1.11e+01, train loss: 1.90064e-06, val loss: 2.63150e-06, min loss: 1.85764e-06\n",
      "Epoch: 552800, elapsed: 1.09e+01, train loss: 2.49084e-06, val loss: 4.62254e-06, min loss: 1.85764e-06\n",
      "Epoch: 552900, elapsed: 1.09e+01, train loss: 1.85765e-06, val loss: 2.61540e-06, min loss: 1.85764e-06\n",
      "Epoch: 553000, elapsed: 1.10e+01, train loss: 1.86555e-06, val loss: 2.59870e-06, min loss: 1.85764e-06\n",
      "Epoch: 553100, elapsed: 1.27e+01, train loss: 1.88671e-06, val loss: 2.63795e-06, min loss: 1.85764e-06\n",
      "Epoch: 553200, elapsed: 1.12e+01, train loss: 1.93353e-06, val loss: 2.68348e-06, min loss: 1.85764e-06\n",
      "Epoch: 553300, elapsed: 1.13e+01, train loss: 1.91855e-06, val loss: 2.72022e-06, min loss: 1.85764e-06\n",
      "Epoch: 553400, elapsed: 1.13e+01, train loss: 2.00951e-06, val loss: 2.70249e-06, min loss: 1.85764e-06\n",
      "Epoch: 553500, elapsed: 1.11e+01, train loss: 2.40502e-06, val loss: 3.52149e-06, min loss: 1.85764e-06\n",
      "Epoch: 553600, elapsed: 1.12e+01, train loss: 3.43763e-06, val loss: 3.84278e-06, min loss: 1.85764e-06\n",
      "Epoch: 553700, elapsed: 1.12e+01, train loss: 2.02060e-06, val loss: 2.81343e-06, min loss: 1.85764e-06\n",
      "Epoch: 553800, elapsed: 1.10e+01, train loss: 5.67779e-06, val loss: 5.47927e-06, min loss: 1.85764e-06\n",
      "Epoch: 553900, elapsed: 1.11e+01, train loss: 7.21994e-06, val loss: 5.99137e-06, min loss: 1.85764e-06\n",
      "Epoch: 554000, elapsed: 1.10e+01, train loss: 1.90718e-06, val loss: 2.75997e-06, min loss: 1.85764e-06\n",
      "Epoch: 554100, elapsed: 1.10e+01, train loss: 1.85868e-06, val loss: 2.62232e-06, min loss: 1.85764e-06\n",
      "Epoch: 554200, elapsed: 1.29e+01, train loss: 1.85698e-06, val loss: 2.62160e-06, min loss: 1.85698e-06\n",
      "Epoch: 554300, elapsed: 1.15e+01, train loss: 1.86596e-06, val loss: 2.62961e-06, min loss: 1.85698e-06\n",
      "Epoch: 554400, elapsed: 1.13e+01, train loss: 2.47256e-06, val loss: 2.86040e-06, min loss: 1.85698e-06\n",
      "Epoch: 554500, elapsed: 1.12e+01, train loss: 1.87765e-06, val loss: 2.60334e-06, min loss: 1.85698e-06\n",
      "Epoch: 554600, elapsed: 1.12e+01, train loss: 2.08626e-06, val loss: 2.73580e-06, min loss: 1.85698e-06\n",
      "Epoch: 554700, elapsed: 1.11e+01, train loss: 2.00545e-06, val loss: 2.69544e-06, min loss: 1.85698e-06\n",
      "Epoch: 554800, elapsed: 1.12e+01, train loss: 1.87026e-06, val loss: 2.64008e-06, min loss: 1.85698e-06\n",
      "Epoch: 554900, elapsed: 1.11e+01, train loss: 7.02629e-06, val loss: 6.00673e-06, min loss: 1.85698e-06\n",
      "Epoch: 555000, elapsed: 1.10e+01, train loss: 5.90401e-06, val loss: 7.60546e-06, min loss: 1.85698e-06\n",
      "Epoch: 555100, elapsed: 1.30e+01, train loss: 2.15973e-06, val loss: 2.83592e-06, min loss: 1.85698e-06\n",
      "Epoch: 555200, elapsed: 1.09e+01, train loss: 1.98848e-06, val loss: 2.72716e-06, min loss: 1.85698e-06\n",
      "Epoch: 555300, elapsed: 1.28e+01, train loss: 2.22444e-06, val loss: 2.99176e-06, min loss: 1.85698e-06\n",
      "Epoch: 555400, elapsed: 1.13e+01, train loss: 1.98156e-06, val loss: 2.68170e-06, min loss: 1.85698e-06\n",
      "Epoch: 555500, elapsed: 1.12e+01, train loss: 1.84980e-06, val loss: 2.60266e-06, min loss: 1.84980e-06\n",
      "Epoch: 555600, elapsed: 1.12e+01, train loss: 1.85453e-06, val loss: 2.61483e-06, min loss: 1.84980e-06\n",
      "Epoch: 555700, elapsed: 1.11e+01, train loss: 1.86935e-06, val loss: 2.61161e-06, min loss: 1.84980e-06\n",
      "Epoch: 555800, elapsed: 1.11e+01, train loss: 2.46211e-06, val loss: 3.06174e-06, min loss: 1.84980e-06\n",
      "Epoch: 555900, elapsed: 1.12e+01, train loss: 2.85537e-06, val loss: 3.42910e-06, min loss: 1.84980e-06\n",
      "Epoch: 556000, elapsed: 1.11e+01, train loss: 3.97040e-06, val loss: 4.29633e-06, min loss: 1.84980e-06\n",
      "Epoch: 556100, elapsed: 1.10e+01, train loss: 9.51203e-06, val loss: 1.16181e-05, min loss: 1.84980e-06\n",
      "Epoch: 556200, elapsed: 1.10e+01, train loss: 3.40322e-06, val loss: 3.67122e-06, min loss: 1.84980e-06\n",
      "Epoch: 556300, elapsed: 1.10e+01, train loss: 4.12207e-06, val loss: 4.16173e-06, min loss: 1.84980e-06\n",
      "Epoch: 556400, elapsed: 1.29e+01, train loss: 1.87864e-06, val loss: 2.62305e-06, min loss: 1.84980e-06\n",
      "Epoch: 556500, elapsed: 1.15e+01, train loss: 1.85669e-06, val loss: 2.58928e-06, min loss: 1.84980e-06\n",
      "Epoch: 556600, elapsed: 1.13e+01, train loss: 1.87986e-06, val loss: 2.61158e-06, min loss: 1.84980e-06\n",
      "Epoch: 556700, elapsed: 1.14e+01, train loss: 1.85646e-06, val loss: 2.58511e-06, min loss: 1.84980e-06\n",
      "Epoch: 556800, elapsed: 1.11e+01, train loss: 1.90489e-06, val loss: 2.63932e-06, min loss: 1.84980e-06\n",
      "Epoch: 556900, elapsed: 1.14e+01, train loss: 2.38283e-06, val loss: 3.12520e-06, min loss: 1.84980e-06\n",
      "Epoch: 557000, elapsed: 1.12e+01, train loss: 4.08682e-06, val loss: 4.93549e-06, min loss: 1.84980e-06\n",
      "Epoch: 557100, elapsed: 1.10e+01, train loss: 8.21640e-06, val loss: 8.28933e-06, min loss: 1.84980e-06\n",
      "Epoch: 557200, elapsed: 1.10e+01, train loss: 2.48363e-06, val loss: 3.53836e-06, min loss: 1.84980e-06\n",
      "Epoch: 557300, elapsed: 1.09e+01, train loss: 5.48446e-06, val loss: 7.40067e-06, min loss: 1.84980e-06\n",
      "Epoch: 557400, elapsed: 1.11e+01, train loss: 2.04944e-06, val loss: 2.86254e-06, min loss: 1.84980e-06\n",
      "Epoch: 557500, elapsed: 1.27e+01, train loss: 1.85466e-06, val loss: 2.57983e-06, min loss: 1.84980e-06\n",
      "Epoch: 557600, elapsed: 1.14e+01, train loss: 2.15299e-06, val loss: 2.82437e-06, min loss: 1.84980e-06\n",
      "Epoch: 557700, elapsed: 1.14e+01, train loss: 2.88791e-06, val loss: 3.62154e-06, min loss: 1.84980e-06\n",
      "Epoch: 557800, elapsed: 1.11e+01, train loss: 2.26976e-06, val loss: 2.97833e-06, min loss: 1.84980e-06\n",
      "Epoch: 557900, elapsed: 1.13e+01, train loss: 1.84844e-06, val loss: 2.62813e-06, min loss: 1.84844e-06\n",
      "Epoch: 558000, elapsed: 1.12e+01, train loss: 1.86724e-06, val loss: 2.60131e-06, min loss: 1.84844e-06\n",
      "Epoch: 558100, elapsed: 1.13e+01, train loss: 1.86243e-06, val loss: 2.62807e-06, min loss: 1.84844e-06\n",
      "Epoch: 558200, elapsed: 1.13e+01, train loss: 1.88411e-06, val loss: 2.62745e-06, min loss: 1.84844e-06\n",
      "Epoch: 558300, elapsed: 1.11e+01, train loss: 3.70321e-06, val loss: 4.16172e-06, min loss: 1.84844e-06\n",
      "Epoch: 558400, elapsed: 1.09e+01, train loss: 1.84300e-06, val loss: 2.60217e-06, min loss: 1.84300e-06\n",
      "Epoch: 558500, elapsed: 1.09e+01, train loss: 1.85650e-06, val loss: 2.62324e-06, min loss: 1.84300e-06\n",
      "Epoch: 558600, elapsed: 1.11e+01, train loss: 2.18961e-06, val loss: 2.94969e-06, min loss: 1.84300e-06\n",
      "Epoch: 558700, elapsed: 1.29e+01, train loss: 2.89344e-06, val loss: 3.68911e-06, min loss: 1.84300e-06\n",
      "Epoch: 558800, elapsed: 1.13e+01, train loss: 1.84310e-06, val loss: 2.58906e-06, min loss: 1.84300e-06\n",
      "Epoch: 558900, elapsed: 1.13e+01, train loss: 1.86164e-06, val loss: 2.59588e-06, min loss: 1.84300e-06\n",
      "Epoch: 559000, elapsed: 1.13e+01, train loss: 2.41042e-06, val loss: 3.14307e-06, min loss: 1.84300e-06\n",
      "Epoch: 559100, elapsed: 1.12e+01, train loss: 1.84589e-06, val loss: 2.59537e-06, min loss: 1.84300e-06\n",
      "Epoch: 559200, elapsed: 1.13e+01, train loss: 1.84476e-06, val loss: 2.60753e-06, min loss: 1.84300e-06\n",
      "Epoch: 559300, elapsed: 1.12e+01, train loss: 1.85365e-06, val loss: 2.59247e-06, min loss: 1.84300e-06\n",
      "Epoch: 559400, elapsed: 1.12e+01, train loss: 5.83920e-06, val loss: 7.59394e-06, min loss: 1.84300e-06\n",
      "Epoch: 559500, elapsed: 1.10e+01, train loss: 1.84021e-06, val loss: 2.59473e-06, min loss: 1.84021e-06\n",
      "Epoch: 559600, elapsed: 1.09e+01, train loss: 1.85728e-06, val loss: 2.58663e-06, min loss: 1.84021e-06\n",
      "Epoch: 559700, elapsed: 1.09e+01, train loss: 1.84486e-06, val loss: 2.60013e-06, min loss: 1.84021e-06\n",
      "Epoch: 559800, elapsed: 1.29e+01, train loss: 1.87142e-06, val loss: 2.58187e-06, min loss: 1.84021e-06\n",
      "Epoch: 559900, elapsed: 1.12e+01, train loss: 1.86267e-06, val loss: 2.65786e-06, min loss: 1.84021e-06\n",
      "Epoch: 560000, elapsed: 1.12e+01, train loss: 1.89449e-06, val loss: 2.61538e-06, min loss: 1.84021e-06\n",
      "Epoch: 560100, elapsed: 1.32e+01, train loss: 3.00106e-06, val loss: 3.79341e-06, min loss: 1.84021e-06\n",
      "Epoch: 560200, elapsed: 1.12e+01, train loss: 2.15680e-06, val loss: 2.86603e-06, min loss: 1.84021e-06\n",
      "Epoch: 560300, elapsed: 1.12e+01, train loss: 1.90883e-06, val loss: 2.66069e-06, min loss: 1.84021e-06\n",
      "Epoch: 560400, elapsed: 1.11e+01, train loss: 1.84510e-06, val loss: 2.60917e-06, min loss: 1.84021e-06\n",
      "Epoch: 560500, elapsed: 1.11e+01, train loss: 1.86756e-06, val loss: 2.62715e-06, min loss: 1.84021e-06\n",
      "Epoch: 560600, elapsed: 1.09e+01, train loss: 4.12320e-06, val loss: 5.51480e-06, min loss: 1.84021e-06\n",
      "Epoch: 560700, elapsed: 1.09e+01, train loss: 1.96644e-06, val loss: 2.73035e-06, min loss: 1.84021e-06\n",
      "Epoch: 560800, elapsed: 1.11e+01, train loss: 2.01433e-06, val loss: 2.77323e-06, min loss: 1.84021e-06\n",
      "Epoch: 560900, elapsed: 1.30e+01, train loss: 2.20536e-06, val loss: 3.87937e-06, min loss: 1.84021e-06\n",
      "Epoch: 561000, elapsed: 1.13e+01, train loss: 1.84015e-06, val loss: 2.58064e-06, min loss: 1.84015e-06\n",
      "Epoch: 561100, elapsed: 1.11e+01, train loss: 1.84221e-06, val loss: 2.61054e-06, min loss: 1.84015e-06\n",
      "Epoch: 561200, elapsed: 1.11e+01, train loss: 1.99056e-06, val loss: 2.84309e-06, min loss: 1.84015e-06\n",
      "Epoch: 561300, elapsed: 1.14e+01, train loss: 2.48112e-06, val loss: 3.11543e-06, min loss: 1.84015e-06\n",
      "Epoch: 561400, elapsed: 1.13e+01, train loss: 1.84549e-06, val loss: 2.58683e-06, min loss: 1.84015e-06\n",
      "Epoch: 561500, elapsed: 1.12e+01, train loss: 1.90151e-06, val loss: 2.65248e-06, min loss: 1.84015e-06\n",
      "Epoch: 561600, elapsed: 1.11e+01, train loss: 1.85972e-06, val loss: 2.58550e-06, min loss: 1.84015e-06\n",
      "Epoch: 561700, elapsed: 1.10e+01, train loss: 2.43763e-06, val loss: 3.43345e-06, min loss: 1.84015e-06\n",
      "Epoch: 561800, elapsed: 1.09e+01, train loss: 2.01668e-06, val loss: 2.87553e-06, min loss: 1.84015e-06\n",
      "Epoch: 561900, elapsed: 1.09e+01, train loss: 1.91832e-06, val loss: 2.73831e-06, min loss: 1.84015e-06\n",
      "Epoch: 562000, elapsed: 1.28e+01, train loss: 2.06618e-06, val loss: 2.85771e-06, min loss: 1.84015e-06\n",
      "Epoch: 562100, elapsed: 1.12e+01, train loss: 1.92928e-06, val loss: 2.71229e-06, min loss: 1.84015e-06\n",
      "Epoch: 562200, elapsed: 1.13e+01, train loss: 4.48997e-06, val loss: 5.33658e-06, min loss: 1.84015e-06\n",
      "Epoch: 562300, elapsed: 1.12e+01, train loss: 1.87683e-06, val loss: 2.61391e-06, min loss: 1.84015e-06\n",
      "Epoch: 562400, elapsed: 1.11e+01, train loss: 1.84309e-06, val loss: 2.57268e-06, min loss: 1.84015e-06\n",
      "Epoch: 562500, elapsed: 1.12e+01, train loss: 1.90002e-06, val loss: 2.60934e-06, min loss: 1.84015e-06\n",
      "Epoch: 562600, elapsed: 1.11e+01, train loss: 1.90807e-06, val loss: 2.60955e-06, min loss: 1.84015e-06\n",
      "Epoch: 562700, elapsed: 1.10e+01, train loss: 1.84599e-06, val loss: 2.62763e-06, min loss: 1.84015e-06\n",
      "Epoch: 562800, elapsed: 1.10e+01, train loss: 1.84669e-06, val loss: 2.62969e-06, min loss: 1.84015e-06\n",
      "Epoch: 562900, elapsed: 1.10e+01, train loss: 1.94663e-06, val loss: 2.61876e-06, min loss: 1.84015e-06\n",
      "Epoch: 563000, elapsed: 1.09e+01, train loss: 2.47825e-06, val loss: 2.88309e-06, min loss: 1.84015e-06\n",
      "Epoch: 563100, elapsed: 1.09e+01, train loss: 2.30943e-06, val loss: 3.05325e-06, min loss: 1.84015e-06\n",
      "Epoch: 563200, elapsed: 1.29e+01, train loss: 2.00360e-06, val loss: 2.72268e-06, min loss: 1.84015e-06\n",
      "Epoch: 563300, elapsed: 1.13e+01, train loss: 1.90493e-06, val loss: 2.63270e-06, min loss: 1.84015e-06\n",
      "Epoch: 563400, elapsed: 1.12e+01, train loss: 2.02431e-06, val loss: 2.75899e-06, min loss: 1.84015e-06\n",
      "Epoch: 563500, elapsed: 1.12e+01, train loss: 1.93292e-06, val loss: 2.73174e-06, min loss: 1.84015e-06\n",
      "Epoch: 563600, elapsed: 1.13e+01, train loss: 1.83821e-06, val loss: 2.58710e-06, min loss: 1.83821e-06\n",
      "Epoch: 563700, elapsed: 1.14e+01, train loss: 2.33687e-06, val loss: 3.15942e-06, min loss: 1.83821e-06\n",
      "Epoch: 563800, elapsed: 1.12e+01, train loss: 1.83836e-06, val loss: 2.59849e-06, min loss: 1.83821e-06\n",
      "Epoch: 563900, elapsed: 1.10e+01, train loss: 2.46657e-06, val loss: 3.40971e-06, min loss: 1.83821e-06\n",
      "Epoch: 564000, elapsed: 1.10e+01, train loss: 2.20893e-06, val loss: 3.04766e-06, min loss: 1.83821e-06\n",
      "Epoch: 564100, elapsed: 1.08e+01, train loss: 1.83682e-06, val loss: 2.59406e-06, min loss: 1.83682e-06\n",
      "Epoch: 564200, elapsed: 1.09e+01, train loss: 2.07690e-06, val loss: 2.90579e-06, min loss: 1.83682e-06\n",
      "Epoch: 564300, elapsed: 1.27e+01, train loss: 2.89708e-06, val loss: 4.50723e-06, min loss: 1.83682e-06\n",
      "Epoch: 564400, elapsed: 1.12e+01, train loss: 2.32635e-06, val loss: 3.13573e-06, min loss: 1.83682e-06\n",
      "Epoch: 564500, elapsed: 1.13e+01, train loss: 1.86642e-06, val loss: 2.78945e-06, min loss: 1.83682e-06\n",
      "Epoch: 564600, elapsed: 1.13e+01, train loss: 2.00791e-06, val loss: 2.76616e-06, min loss: 1.83682e-06\n",
      "Epoch: 564700, elapsed: 1.11e+01, train loss: 1.87924e-06, val loss: 2.66759e-06, min loss: 1.83682e-06\n",
      "Epoch: 564800, elapsed: 1.11e+01, train loss: 3.46362e-06, val loss: 4.44679e-06, min loss: 1.83682e-06\n",
      "Epoch: 564900, elapsed: 1.10e+01, train loss: 1.93234e-06, val loss: 2.59603e-06, min loss: 1.83682e-06\n",
      "Epoch: 565000, elapsed: 1.11e+01, train loss: 2.11408e-06, val loss: 2.81579e-06, min loss: 1.83682e-06\n",
      "Epoch: 565100, elapsed: 1.31e+01, train loss: 1.90045e-06, val loss: 2.71595e-06, min loss: 1.83682e-06\n",
      "Epoch: 565200, elapsed: 1.09e+01, train loss: 1.83888e-06, val loss: 2.61803e-06, min loss: 1.83682e-06\n",
      "Epoch: 565300, elapsed: 1.10e+01, train loss: 1.85291e-06, val loss: 2.62183e-06, min loss: 1.83682e-06\n",
      "Epoch: 565400, elapsed: 1.28e+01, train loss: 1.87920e-06, val loss: 2.64221e-06, min loss: 1.83682e-06\n",
      "Epoch: 565500, elapsed: 1.14e+01, train loss: 2.03685e-06, val loss: 2.77750e-06, min loss: 1.83682e-06\n",
      "Epoch: 565600, elapsed: 1.14e+01, train loss: 2.39972e-06, val loss: 2.90268e-06, min loss: 1.83682e-06\n",
      "Epoch: 565700, elapsed: 1.13e+01, train loss: 2.50064e-06, val loss: 2.89249e-06, min loss: 1.83682e-06\n",
      "Epoch: 565800, elapsed: 1.11e+01, train loss: 2.47930e-06, val loss: 2.89561e-06, min loss: 1.83682e-06\n",
      "Epoch: 565900, elapsed: 1.13e+01, train loss: 3.02692e-06, val loss: 3.55216e-06, min loss: 1.83682e-06\n",
      "Epoch: 566000, elapsed: 1.12e+01, train loss: 2.11684e-06, val loss: 2.69438e-06, min loss: 1.83682e-06\n",
      "Epoch: 566100, elapsed: 1.11e+01, train loss: 2.24127e-06, val loss: 3.00168e-06, min loss: 1.83682e-06\n",
      "Epoch: 566200, elapsed: 1.10e+01, train loss: 2.09661e-06, val loss: 2.72940e-06, min loss: 1.83682e-06\n",
      "Epoch: 566300, elapsed: 1.10e+01, train loss: 3.77780e-06, val loss: 3.63986e-06, min loss: 1.83682e-06\n",
      "Epoch: 566400, elapsed: 1.10e+01, train loss: 7.86879e-06, val loss: 8.52260e-06, min loss: 1.83682e-06\n",
      "Epoch: 566500, elapsed: 1.25e+01, train loss: 8.52103e-06, val loss: 8.09478e-06, min loss: 1.83682e-06\n",
      "Epoch: 566600, elapsed: 1.13e+01, train loss: 1.88972e-06, val loss: 2.70717e-06, min loss: 1.83682e-06\n",
      "Epoch: 566700, elapsed: 1.12e+01, train loss: 1.82538e-06, val loss: 2.57014e-06, min loss: 1.82538e-06\n",
      "Epoch: 566800, elapsed: 1.12e+01, train loss: 2.26260e-06, val loss: 2.96069e-06, min loss: 1.82538e-06\n",
      "Epoch: 566900, elapsed: 1.13e+01, train loss: 2.01014e-06, val loss: 2.79613e-06, min loss: 1.82538e-06\n",
      "Epoch: 567000, elapsed: 1.12e+01, train loss: 5.20919e-06, val loss: 7.05284e-06, min loss: 1.82538e-06\n",
      "Epoch: 567100, elapsed: 1.11e+01, train loss: 1.83816e-06, val loss: 2.58008e-06, min loss: 1.82538e-06\n",
      "Epoch: 567200, elapsed: 1.12e+01, train loss: 1.86288e-06, val loss: 2.61496e-06, min loss: 1.82538e-06\n",
      "Epoch: 567300, elapsed: 1.11e+01, train loss: 1.91564e-06, val loss: 2.65001e-06, min loss: 1.82538e-06\n",
      "Epoch: 567400, elapsed: 1.08e+01, train loss: 1.82502e-06, val loss: 2.58435e-06, min loss: 1.82502e-06\n",
      "Epoch: 567500, elapsed: 1.10e+01, train loss: 1.82521e-06, val loss: 2.55764e-06, min loss: 1.82502e-06\n",
      "Epoch: 567600, elapsed: 1.10e+01, train loss: 1.82910e-06, val loss: 2.56666e-06, min loss: 1.82502e-06\n",
      "Epoch: 567700, elapsed: 1.30e+01, train loss: 1.86874e-06, val loss: 2.59496e-06, min loss: 1.82502e-06\n",
      "Epoch: 567800, elapsed: 1.13e+01, train loss: 1.90184e-06, val loss: 2.63473e-06, min loss: 1.82502e-06\n",
      "Epoch: 567900, elapsed: 1.13e+01, train loss: 2.53876e-06, val loss: 3.38024e-06, min loss: 1.82502e-06\n",
      "Epoch: 568000, elapsed: 1.11e+01, train loss: 1.85252e-06, val loss: 2.57641e-06, min loss: 1.82502e-06\n",
      "Epoch: 568100, elapsed: 1.13e+01, train loss: 2.05277e-06, val loss: 2.73096e-06, min loss: 1.82502e-06\n",
      "Epoch: 568200, elapsed: 1.12e+01, train loss: 1.97200e-06, val loss: 2.75109e-06, min loss: 1.82502e-06\n",
      "Epoch: 568300, elapsed: 1.12e+01, train loss: 3.80613e-06, val loss: 4.93986e-06, min loss: 1.82502e-06\n",
      "Epoch: 568400, elapsed: 1.14e+01, train loss: 2.52990e-06, val loss: 3.58890e-06, min loss: 1.82502e-06\n",
      "Epoch: 568500, elapsed: 1.11e+01, train loss: 1.95983e-06, val loss: 2.74553e-06, min loss: 1.82502e-06\n",
      "Epoch: 568600, elapsed: 1.09e+01, train loss: 1.82572e-06, val loss: 2.56755e-06, min loss: 1.82502e-06\n",
      "Epoch: 568700, elapsed: 1.09e+01, train loss: 1.87727e-06, val loss: 2.71069e-06, min loss: 1.82502e-06\n",
      "Epoch: 568800, elapsed: 1.28e+01, train loss: 1.83309e-06, val loss: 2.57631e-06, min loss: 1.82502e-06\n",
      "Epoch: 568900, elapsed: 1.12e+01, train loss: 1.94044e-06, val loss: 2.64034e-06, min loss: 1.82502e-06\n",
      "Epoch: 569000, elapsed: 1.11e+01, train loss: 1.84461e-06, val loss: 2.60310e-06, min loss: 1.82502e-06\n",
      "Epoch: 569100, elapsed: 1.12e+01, train loss: 1.82196e-06, val loss: 2.57889e-06, min loss: 1.82196e-06\n",
      "Epoch: 569200, elapsed: 1.13e+01, train loss: 1.92465e-06, val loss: 2.77546e-06, min loss: 1.82196e-06\n",
      "Epoch: 569300, elapsed: 1.12e+01, train loss: 3.60080e-06, val loss: 4.58065e-06, min loss: 1.82196e-06\n",
      "Epoch: 569400, elapsed: 1.10e+01, train loss: 6.68270e-06, val loss: 7.78172e-06, min loss: 1.82196e-06\n",
      "Epoch: 569500, elapsed: 1.11e+01, train loss: 5.13781e-06, val loss: 4.79659e-06, min loss: 1.82196e-06\n",
      "Epoch: 569600, elapsed: 1.11e+01, train loss: 3.84117e-06, val loss: 3.09243e-06, min loss: 1.82196e-06\n",
      "Epoch: 569700, elapsed: 1.09e+01, train loss: 1.81722e-06, val loss: 2.56846e-06, min loss: 1.81722e-06\n",
      "Epoch: 569800, elapsed: 1.08e+01, train loss: 1.85319e-06, val loss: 2.55454e-06, min loss: 1.81722e-06\n",
      "Epoch: 569900, elapsed: 1.09e+01, train loss: 3.01073e-06, val loss: 4.24301e-06, min loss: 1.81722e-06\n",
      "Epoch: 570000, elapsed: 1.29e+01, train loss: 1.81787e-06, val loss: 2.56894e-06, min loss: 1.81722e-06\n",
      "Epoch: 570100, elapsed: 1.35e+01, train loss: 1.87874e-06, val loss: 2.63065e-06, min loss: 1.81722e-06\n",
      "Epoch: 570200, elapsed: 1.14e+01, train loss: 1.97671e-06, val loss: 2.76298e-06, min loss: 1.81722e-06\n",
      "Epoch: 570300, elapsed: 1.12e+01, train loss: 1.82915e-06, val loss: 2.55212e-06, min loss: 1.81722e-06\n",
      "Epoch: 570400, elapsed: 1.12e+01, train loss: 1.85038e-06, val loss: 2.63445e-06, min loss: 1.81722e-06\n",
      "Epoch: 570500, elapsed: 1.11e+01, train loss: 1.98866e-06, val loss: 2.83530e-06, min loss: 1.81722e-06\n",
      "Epoch: 570600, elapsed: 1.13e+01, train loss: 6.33436e-06, val loss: 6.75490e-06, min loss: 1.81722e-06\n",
      "Epoch: 570700, elapsed: 1.11e+01, train loss: 1.95327e-06, val loss: 2.98658e-06, min loss: 1.81722e-06\n",
      "Epoch: 570800, elapsed: 1.11e+01, train loss: 2.18797e-06, val loss: 3.05635e-06, min loss: 1.81722e-06\n",
      "Epoch: 570900, elapsed: 1.10e+01, train loss: 2.64554e-06, val loss: 3.62590e-06, min loss: 1.81722e-06\n",
      "Epoch: 571000, elapsed: 1.08e+01, train loss: 1.92298e-06, val loss: 2.65601e-06, min loss: 1.81722e-06\n",
      "Epoch: 571100, elapsed: 1.29e+01, train loss: 1.83255e-06, val loss: 2.62767e-06, min loss: 1.81722e-06\n",
      "Epoch: 571200, elapsed: 1.12e+01, train loss: 1.81533e-06, val loss: 2.57143e-06, min loss: 1.81533e-06\n",
      "Epoch: 571300, elapsed: 1.13e+01, train loss: 2.10690e-06, val loss: 2.68692e-06, min loss: 1.81533e-06\n",
      "Epoch: 571400, elapsed: 1.11e+01, train loss: 2.01383e-06, val loss: 2.97402e-06, min loss: 1.81533e-06\n",
      "Epoch: 571500, elapsed: 1.12e+01, train loss: 1.09692e-05, val loss: 1.06303e-05, min loss: 1.81533e-06\n",
      "Epoch: 571600, elapsed: 1.11e+01, train loss: 1.91686e-06, val loss: 2.64415e-06, min loss: 1.81533e-06\n",
      "Epoch: 571700, elapsed: 1.12e+01, train loss: 1.82565e-06, val loss: 2.55749e-06, min loss: 1.81533e-06\n",
      "Epoch: 571800, elapsed: 1.11e+01, train loss: 1.85798e-06, val loss: 2.57220e-06, min loss: 1.81533e-06\n",
      "Epoch: 571900, elapsed: 1.12e+01, train loss: 1.97978e-06, val loss: 2.80161e-06, min loss: 1.81533e-06\n",
      "Epoch: 572000, elapsed: 1.09e+01, train loss: 2.02227e-06, val loss: 2.70857e-06, min loss: 1.81533e-06\n",
      "Epoch: 572100, elapsed: 1.09e+01, train loss: 1.99040e-06, val loss: 2.80415e-06, min loss: 1.81533e-06\n",
      "Epoch: 572200, elapsed: 1.27e+01, train loss: 2.04948e-06, val loss: 2.83239e-06, min loss: 1.81533e-06\n",
      "Epoch: 572300, elapsed: 1.14e+01, train loss: 1.82568e-06, val loss: 2.55920e-06, min loss: 1.81533e-06\n",
      "Epoch: 572400, elapsed: 1.14e+01, train loss: 1.82958e-06, val loss: 2.56510e-06, min loss: 1.81533e-06\n",
      "Epoch: 572500, elapsed: 1.13e+01, train loss: 1.82658e-06, val loss: 2.56562e-06, min loss: 1.81533e-06\n",
      "Epoch: 572600, elapsed: 1.14e+01, train loss: 1.98263e-06, val loss: 2.69135e-06, min loss: 1.81533e-06\n",
      "Epoch: 572700, elapsed: 1.11e+01, train loss: 3.78295e-06, val loss: 4.38827e-06, min loss: 1.81533e-06\n",
      "Epoch: 572800, elapsed: 1.11e+01, train loss: 2.71733e-06, val loss: 2.91553e-06, min loss: 1.81533e-06\n",
      "Epoch: 572900, elapsed: 1.11e+01, train loss: 1.85478e-06, val loss: 3.14075e-06, min loss: 1.81533e-06\n",
      "Epoch: 573000, elapsed: 1.11e+01, train loss: 2.02932e-06, val loss: 2.79453e-06, min loss: 1.81533e-06\n",
      "Epoch: 573100, elapsed: 1.10e+01, train loss: 2.06927e-06, val loss: 2.95586e-06, min loss: 1.81533e-06\n",
      "Epoch: 573200, elapsed: 1.08e+01, train loss: 2.04757e-06, val loss: 2.77562e-06, min loss: 1.81533e-06\n",
      "Epoch: 573300, elapsed: 1.08e+01, train loss: 2.14094e-06, val loss: 2.88559e-06, min loss: 1.81533e-06\n",
      "Epoch: 573400, elapsed: 1.29e+01, train loss: 1.81115e-06, val loss: 2.56943e-06, min loss: 1.81115e-06\n",
      "Epoch: 573500, elapsed: 1.13e+01, train loss: 1.81483e-06, val loss: 2.57827e-06, min loss: 1.81115e-06\n",
      "Epoch: 573600, elapsed: 1.12e+01, train loss: 1.98414e-06, val loss: 2.70248e-06, min loss: 1.81115e-06\n",
      "Epoch: 573700, elapsed: 1.12e+01, train loss: 2.22573e-06, val loss: 3.11850e-06, min loss: 1.81115e-06\n",
      "Epoch: 573800, elapsed: 1.12e+01, train loss: 1.82649e-06, val loss: 2.58551e-06, min loss: 1.81115e-06\n",
      "Epoch: 573900, elapsed: 1.12e+01, train loss: 2.02228e-06, val loss: 2.77445e-06, min loss: 1.81115e-06\n",
      "Epoch: 574000, elapsed: 1.11e+01, train loss: 1.96175e-06, val loss: 2.66252e-06, min loss: 1.81115e-06\n",
      "Epoch: 574100, elapsed: 1.13e+01, train loss: 2.02184e-06, val loss: 3.12353e-06, min loss: 1.81115e-06\n",
      "Epoch: 574200, elapsed: 1.11e+01, train loss: 2.03409e-06, val loss: 2.73085e-06, min loss: 1.81115e-06\n",
      "Epoch: 574300, elapsed: 1.10e+01, train loss: 1.84222e-06, val loss: 2.62407e-06, min loss: 1.81115e-06\n",
      "Epoch: 574400, elapsed: 1.10e+01, train loss: 2.02638e-06, val loss: 2.87972e-06, min loss: 1.81115e-06\n",
      "Epoch: 574500, elapsed: 1.26e+01, train loss: 3.89427e-06, val loss: 5.31911e-06, min loss: 1.81115e-06\n",
      "Epoch: 574600, elapsed: 1.12e+01, train loss: 3.23497e-06, val loss: 3.45012e-06, min loss: 1.81115e-06\n",
      "Epoch: 574700, elapsed: 1.14e+01, train loss: 3.36637e-06, val loss: 4.03628e-06, min loss: 1.81115e-06\n",
      "Epoch: 574800, elapsed: 1.12e+01, train loss: 2.55334e-06, val loss: 3.51105e-06, min loss: 1.81115e-06\n",
      "Epoch: 574900, elapsed: 1.12e+01, train loss: 2.22290e-06, val loss: 2.94096e-06, min loss: 1.81115e-06\n",
      "Epoch: 575000, elapsed: 1.12e+01, train loss: 1.90403e-06, val loss: 2.68101e-06, min loss: 1.81115e-06\n",
      "Epoch: 575100, elapsed: 1.32e+01, train loss: 1.85876e-06, val loss: 2.64649e-06, min loss: 1.81115e-06\n",
      "Epoch: 575200, elapsed: 1.11e+01, train loss: 4.57601e-06, val loss: 5.34033e-06, min loss: 1.81115e-06\n",
      "Epoch: 575300, elapsed: 1.13e+01, train loss: 2.64708e-06, val loss: 3.11952e-06, min loss: 1.81115e-06\n",
      "Epoch: 575400, elapsed: 1.10e+01, train loss: 1.90782e-06, val loss: 2.73479e-06, min loss: 1.81115e-06\n",
      "Epoch: 575500, elapsed: 1.09e+01, train loss: 1.88935e-06, val loss: 2.59194e-06, min loss: 1.81115e-06\n",
      "Epoch: 575600, elapsed: 1.25e+01, train loss: 2.09412e-06, val loss: 2.97159e-06, min loss: 1.81115e-06\n",
      "Epoch: 575700, elapsed: 1.13e+01, train loss: 3.38135e-06, val loss: 3.99091e-06, min loss: 1.81115e-06\n",
      "Epoch: 575800, elapsed: 1.12e+01, train loss: 2.69659e-06, val loss: 3.63348e-06, min loss: 1.81115e-06\n",
      "Epoch: 575900, elapsed: 1.14e+01, train loss: 1.86276e-06, val loss: 2.66032e-06, min loss: 1.81115e-06\n",
      "Epoch: 576000, elapsed: 1.12e+01, train loss: 1.80407e-06, val loss: 2.56065e-06, min loss: 1.80407e-06\n",
      "Epoch: 576100, elapsed: 1.12e+01, train loss: 1.83392e-06, val loss: 2.60012e-06, min loss: 1.80407e-06\n",
      "Epoch: 576200, elapsed: 1.12e+01, train loss: 2.16726e-06, val loss: 2.66550e-06, min loss: 1.80407e-06\n",
      "Epoch: 576300, elapsed: 1.12e+01, train loss: 1.84381e-06, val loss: 2.69053e-06, min loss: 1.80407e-06\n",
      "Epoch: 576400, elapsed: 1.12e+01, train loss: 1.80493e-06, val loss: 2.54319e-06, min loss: 1.80407e-06\n",
      "Epoch: 576500, elapsed: 1.11e+01, train loss: 2.90288e-06, val loss: 3.29232e-06, min loss: 1.80407e-06\n",
      "Epoch: 576600, elapsed: 1.09e+01, train loss: 1.82243e-06, val loss: 2.56129e-06, min loss: 1.80407e-06\n",
      "Epoch: 576700, elapsed: 1.09e+01, train loss: 1.81321e-06, val loss: 2.55728e-06, min loss: 1.80407e-06\n",
      "Epoch: 576800, elapsed: 1.30e+01, train loss: 1.82829e-06, val loss: 2.57943e-06, min loss: 1.80407e-06\n",
      "Epoch: 576900, elapsed: 1.13e+01, train loss: 1.91204e-06, val loss: 2.61358e-06, min loss: 1.80407e-06\n",
      "Epoch: 577000, elapsed: 1.12e+01, train loss: 3.05241e-06, val loss: 4.08320e-06, min loss: 1.80407e-06\n",
      "Epoch: 577100, elapsed: 1.12e+01, train loss: 1.90687e-06, val loss: 2.62733e-06, min loss: 1.80407e-06\n",
      "Epoch: 577200, elapsed: 1.11e+01, train loss: 1.80957e-06, val loss: 2.58843e-06, min loss: 1.80407e-06\n",
      "Epoch: 577300, elapsed: 1.12e+01, train loss: 2.01068e-06, val loss: 2.69556e-06, min loss: 1.80407e-06\n",
      "Epoch: 577400, elapsed: 1.11e+01, train loss: 1.96160e-06, val loss: 2.56742e-06, min loss: 1.80407e-06\n",
      "Epoch: 577500, elapsed: 1.11e+01, train loss: 1.01502e-05, val loss: 6.55934e-06, min loss: 1.80407e-06\n",
      "Epoch: 577600, elapsed: 1.11e+01, train loss: 1.89883e-06, val loss: 2.66224e-06, min loss: 1.80407e-06\n",
      "Epoch: 577700, elapsed: 1.11e+01, train loss: 1.90691e-06, val loss: 2.71855e-06, min loss: 1.80407e-06\n",
      "Epoch: 577800, elapsed: 1.10e+01, train loss: 2.09062e-06, val loss: 2.73194e-06, min loss: 1.80407e-06\n",
      "Epoch: 577900, elapsed: 1.09e+01, train loss: 1.88006e-06, val loss: 2.61490e-06, min loss: 1.80407e-06\n",
      "Epoch: 578000, elapsed: 1.29e+01, train loss: 4.31448e-06, val loss: 5.15308e-06, min loss: 1.80407e-06\n",
      "Epoch: 578100, elapsed: 1.12e+01, train loss: 2.71704e-06, val loss: 3.63014e-06, min loss: 1.80407e-06\n",
      "Epoch: 578200, elapsed: 1.11e+01, train loss: 3.17516e-06, val loss: 3.64629e-06, min loss: 1.80407e-06\n",
      "Epoch: 578300, elapsed: 1.12e+01, train loss: 2.05454e-06, val loss: 2.87396e-06, min loss: 1.80407e-06\n",
      "Epoch: 578400, elapsed: 1.12e+01, train loss: 1.91271e-06, val loss: 2.74231e-06, min loss: 1.80407e-06\n",
      "Epoch: 578500, elapsed: 1.13e+01, train loss: 1.97489e-06, val loss: 2.56212e-06, min loss: 1.80407e-06\n",
      "Epoch: 578600, elapsed: 1.10e+01, train loss: 5.01814e-06, val loss: 3.92954e-06, min loss: 1.80407e-06\n",
      "Epoch: 578700, elapsed: 1.12e+01, train loss: 2.66601e-06, val loss: 3.54834e-06, min loss: 1.80407e-06\n",
      "Epoch: 578800, elapsed: 1.12e+01, train loss: 2.17290e-06, val loss: 2.81918e-06, min loss: 1.80407e-06\n",
      "Epoch: 578900, elapsed: 1.10e+01, train loss: 1.81393e-06, val loss: 2.53983e-06, min loss: 1.80407e-06\n",
      "Epoch: 579000, elapsed: 1.10e+01, train loss: 2.41755e-06, val loss: 3.53557e-06, min loss: 1.80407e-06\n",
      "Epoch: 579100, elapsed: 1.27e+01, train loss: 1.80325e-06, val loss: 2.57432e-06, min loss: 1.80325e-06\n",
      "Epoch: 579200, elapsed: 1.13e+01, train loss: 1.79833e-06, val loss: 2.54012e-06, min loss: 1.79833e-06\n",
      "Epoch: 579300, elapsed: 1.12e+01, train loss: 1.81701e-06, val loss: 2.55983e-06, min loss: 1.79833e-06\n",
      "Epoch: 579400, elapsed: 1.11e+01, train loss: 1.85048e-06, val loss: 2.58342e-06, min loss: 1.79833e-06\n",
      "Epoch: 579500, elapsed: 1.12e+01, train loss: 4.18547e-06, val loss: 4.15752e-06, min loss: 1.79833e-06\n",
      "Epoch: 579600, elapsed: 1.11e+01, train loss: 2.88094e-06, val loss: 3.78287e-06, min loss: 1.79833e-06\n",
      "Epoch: 579700, elapsed: 1.11e+01, train loss: 2.15337e-06, val loss: 2.92988e-06, min loss: 1.79833e-06\n",
      "Epoch: 579800, elapsed: 1.13e+01, train loss: 1.80177e-06, val loss: 2.55872e-06, min loss: 1.79833e-06\n",
      "Epoch: 579900, elapsed: 1.11e+01, train loss: 1.86314e-06, val loss: 2.69095e-06, min loss: 1.79833e-06\n",
      "Epoch: 580000, elapsed: 1.12e+01, train loss: 2.22856e-06, val loss: 3.28099e-06, min loss: 1.79833e-06\n",
      "Epoch: 580100, elapsed: 1.30e+01, train loss: 3.22428e-06, val loss: 3.64283e-06, min loss: 1.79833e-06\n",
      "Epoch: 580200, elapsed: 1.08e+01, train loss: 3.47963e-06, val loss: 3.94043e-06, min loss: 1.79833e-06\n",
      "Epoch: 580300, elapsed: 1.30e+01, train loss: 2.70709e-06, val loss: 2.99180e-06, min loss: 1.79833e-06\n",
      "Epoch: 580400, elapsed: 1.13e+01, train loss: 1.82947e-06, val loss: 2.60154e-06, min loss: 1.79833e-06\n",
      "Epoch: 580500, elapsed: 1.13e+01, train loss: 1.85221e-06, val loss: 2.62675e-06, min loss: 1.79833e-06\n",
      "Epoch: 580600, elapsed: 1.13e+01, train loss: 2.34958e-06, val loss: 3.21932e-06, min loss: 1.79833e-06\n",
      "Epoch: 580700, elapsed: 1.13e+01, train loss: 1.80930e-06, val loss: 2.58653e-06, min loss: 1.79833e-06\n",
      "Epoch: 580800, elapsed: 1.12e+01, train loss: 1.82843e-06, val loss: 2.58827e-06, min loss: 1.79833e-06\n",
      "Epoch: 580900, elapsed: 1.12e+01, train loss: 1.92289e-06, val loss: 2.57328e-06, min loss: 1.79833e-06\n",
      "Epoch: 581000, elapsed: 1.10e+01, train loss: 3.05736e-06, val loss: 3.31003e-06, min loss: 1.79833e-06\n",
      "Epoch: 581100, elapsed: 1.12e+01, train loss: 2.05283e-06, val loss: 2.91971e-06, min loss: 1.79833e-06\n",
      "Epoch: 581200, elapsed: 1.10e+01, train loss: 1.89125e-06, val loss: 2.58902e-06, min loss: 1.79833e-06\n",
      "Epoch: 581300, elapsed: 1.09e+01, train loss: 1.90755e-06, val loss: 2.65560e-06, min loss: 1.79833e-06\n",
      "Epoch: 581400, elapsed: 1.30e+01, train loss: 1.87954e-06, val loss: 2.63535e-06, min loss: 1.79833e-06\n",
      "Epoch: 581500, elapsed: 1.13e+01, train loss: 2.00985e-06, val loss: 2.76857e-06, min loss: 1.79833e-06\n",
      "Epoch: 581600, elapsed: 1.13e+01, train loss: 1.93821e-06, val loss: 2.71561e-06, min loss: 1.79833e-06\n",
      "Epoch: 581700, elapsed: 1.12e+01, train loss: 1.84337e-06, val loss: 2.52407e-06, min loss: 1.79833e-06\n",
      "Epoch: 581800, elapsed: 1.12e+01, train loss: 1.81296e-06, val loss: 2.55162e-06, min loss: 1.79833e-06\n",
      "Epoch: 581900, elapsed: 1.12e+01, train loss: 1.81555e-06, val loss: 2.53728e-06, min loss: 1.79833e-06\n",
      "Epoch: 582000, elapsed: 1.12e+01, train loss: 2.33331e-06, val loss: 2.85789e-06, min loss: 1.79833e-06\n",
      "Epoch: 582100, elapsed: 1.12e+01, train loss: 1.84401e-06, val loss: 2.93690e-06, min loss: 1.79833e-06\n",
      "Epoch: 582200, elapsed: 1.12e+01, train loss: 3.03005e-06, val loss: 3.86154e-06, min loss: 1.79833e-06\n",
      "Epoch: 582300, elapsed: 1.11e+01, train loss: 2.24421e-06, val loss: 3.22584e-06, min loss: 1.79833e-06\n",
      "Epoch: 582400, elapsed: 1.09e+01, train loss: 2.05043e-06, val loss: 2.86886e-06, min loss: 1.79833e-06\n",
      "Epoch: 582500, elapsed: 1.11e+01, train loss: 2.09273e-06, val loss: 2.61753e-06, min loss: 1.79833e-06\n",
      "Epoch: 582600, elapsed: 1.29e+01, train loss: 1.94840e-06, val loss: 2.53464e-06, min loss: 1.79833e-06\n",
      "Epoch: 582700, elapsed: 1.13e+01, train loss: 2.12984e-06, val loss: 2.71924e-06, min loss: 1.79833e-06\n",
      "Epoch: 582800, elapsed: 1.13e+01, train loss: 2.00036e-06, val loss: 2.63933e-06, min loss: 1.79833e-06\n",
      "Epoch: 582900, elapsed: 1.13e+01, train loss: 2.04218e-06, val loss: 2.81763e-06, min loss: 1.79833e-06\n",
      "Epoch: 583000, elapsed: 1.12e+01, train loss: 1.87068e-06, val loss: 2.57791e-06, min loss: 1.79833e-06\n",
      "Epoch: 583100, elapsed: 1.11e+01, train loss: 1.89724e-06, val loss: 2.60565e-06, min loss: 1.79833e-06\n",
      "Epoch: 583200, elapsed: 1.13e+01, train loss: 1.84897e-06, val loss: 2.55622e-06, min loss: 1.79833e-06\n",
      "Epoch: 583300, elapsed: 1.12e+01, train loss: 1.82021e-06, val loss: 2.52386e-06, min loss: 1.79833e-06\n",
      "Epoch: 583400, elapsed: 1.11e+01, train loss: 2.24086e-06, val loss: 3.18513e-06, min loss: 1.79833e-06\n",
      "Epoch: 583500, elapsed: 1.11e+01, train loss: 4.57468e-06, val loss: 4.45902e-06, min loss: 1.79833e-06\n",
      "Epoch: 583600, elapsed: 1.09e+01, train loss: 2.52564e-06, val loss: 3.44731e-06, min loss: 1.79833e-06\n",
      "Epoch: 583700, elapsed: 1.10e+01, train loss: 3.49241e-06, val loss: 4.41483e-06, min loss: 1.79833e-06\n",
      "Epoch: 583800, elapsed: 1.28e+01, train loss: 2.14465e-06, val loss: 2.98540e-06, min loss: 1.79833e-06\n",
      "Epoch: 583900, elapsed: 1.13e+01, train loss: 1.91987e-06, val loss: 2.65275e-06, min loss: 1.79833e-06\n",
      "Epoch: 584000, elapsed: 1.13e+01, train loss: 2.07356e-06, val loss: 2.67309e-06, min loss: 1.79833e-06\n",
      "Epoch: 584100, elapsed: 1.11e+01, train loss: 1.85417e-06, val loss: 2.55868e-06, min loss: 1.79833e-06\n",
      "Epoch: 584200, elapsed: 1.11e+01, train loss: 3.19996e-06, val loss: 4.48077e-06, min loss: 1.79833e-06\n",
      "Epoch: 584300, elapsed: 1.11e+01, train loss: 2.61804e-06, val loss: 2.76786e-06, min loss: 1.79833e-06\n",
      "Epoch: 584400, elapsed: 1.13e+01, train loss: 2.54944e-06, val loss: 3.90506e-06, min loss: 1.79833e-06\n",
      "Epoch: 584500, elapsed: 1.13e+01, train loss: 2.69711e-06, val loss: 2.72149e-06, min loss: 1.79833e-06\n",
      "Epoch: 584600, elapsed: 1.11e+01, train loss: 1.80531e-06, val loss: 2.54027e-06, min loss: 1.79833e-06\n",
      "Epoch: 584700, elapsed: 1.10e+01, train loss: 1.78858e-06, val loss: 2.53614e-06, min loss: 1.78858e-06\n",
      "Epoch: 584800, elapsed: 1.09e+01, train loss: 2.09318e-06, val loss: 3.05176e-06, min loss: 1.78858e-06\n",
      "Epoch: 584900, elapsed: 1.27e+01, train loss: 2.22451e-06, val loss: 3.05234e-06, min loss: 1.78858e-06\n",
      "Epoch: 585000, elapsed: 1.14e+01, train loss: 2.00009e-06, val loss: 2.56720e-06, min loss: 1.78858e-06\n",
      "Epoch: 585100, elapsed: 1.34e+01, train loss: 8.48248e-06, val loss: 9.63744e-06, min loss: 1.78858e-06\n",
      "Epoch: 585200, elapsed: 1.13e+01, train loss: 1.85675e-06, val loss: 2.67410e-06, min loss: 1.78858e-06\n",
      "Epoch: 585300, elapsed: 1.12e+01, train loss: 1.82437e-06, val loss: 2.54792e-06, min loss: 1.78858e-06\n",
      "Epoch: 585400, elapsed: 1.12e+01, train loss: 3.94831e-06, val loss: 4.69594e-06, min loss: 1.78858e-06\n",
      "Epoch: 585500, elapsed: 1.14e+01, train loss: 2.68510e-06, val loss: 2.92177e-06, min loss: 1.78858e-06\n",
      "Epoch: 585600, elapsed: 1.12e+01, train loss: 2.13383e-06, val loss: 2.85057e-06, min loss: 1.78858e-06\n",
      "Epoch: 585700, elapsed: 1.11e+01, train loss: 2.05289e-06, val loss: 2.60266e-06, min loss: 1.78858e-06\n",
      "Epoch: 585800, elapsed: 1.11e+01, train loss: 1.78329e-06, val loss: 2.53020e-06, min loss: 1.78329e-06\n",
      "Epoch: 585900, elapsed: 1.11e+01, train loss: 1.79316e-06, val loss: 2.53372e-06, min loss: 1.78329e-06\n",
      "Epoch: 586000, elapsed: 1.10e+01, train loss: 2.12394e-06, val loss: 2.82660e-06, min loss: 1.78329e-06\n",
      "Epoch: 586100, elapsed: 1.29e+01, train loss: 2.35776e-06, val loss: 2.83174e-06, min loss: 1.78329e-06\n",
      "Epoch: 586200, elapsed: 1.12e+01, train loss: 2.18425e-06, val loss: 2.75516e-06, min loss: 1.78329e-06\n",
      "Epoch: 586300, elapsed: 1.13e+01, train loss: 1.80903e-06, val loss: 2.62417e-06, min loss: 1.78329e-06\n",
      "Epoch: 586400, elapsed: 1.09e+01, train loss: 2.13499e-06, val loss: 2.72422e-06, min loss: 1.78329e-06\n",
      "Epoch: 586500, elapsed: 1.11e+01, train loss: 1.80312e-06, val loss: 2.58448e-06, min loss: 1.78329e-06\n",
      "Epoch: 586600, elapsed: 1.11e+01, train loss: 1.87130e-06, val loss: 2.68220e-06, min loss: 1.78329e-06\n",
      "Epoch: 586700, elapsed: 1.11e+01, train loss: 1.87440e-06, val loss: 2.57043e-06, min loss: 1.78329e-06\n",
      "Epoch: 586800, elapsed: 1.10e+01, train loss: 2.29048e-06, val loss: 3.29239e-06, min loss: 1.78329e-06\n",
      "Epoch: 586900, elapsed: 1.10e+01, train loss: 2.28198e-06, val loss: 3.33574e-06, min loss: 1.78329e-06\n",
      "Epoch: 587000, elapsed: 1.11e+01, train loss: 1.78333e-06, val loss: 2.54157e-06, min loss: 1.78329e-06\n",
      "Epoch: 587100, elapsed: 1.10e+01, train loss: 1.81324e-06, val loss: 2.54859e-06, min loss: 1.78329e-06\n",
      "Epoch: 587200, elapsed: 1.08e+01, train loss: 2.63052e-06, val loss: 3.02580e-06, min loss: 1.78329e-06\n",
      "Epoch: 587300, elapsed: 1.28e+01, train loss: 2.17522e-06, val loss: 2.73421e-06, min loss: 1.78329e-06\n",
      "Epoch: 587400, elapsed: 1.13e+01, train loss: 1.88825e-06, val loss: 2.53346e-06, min loss: 1.78329e-06\n",
      "Epoch: 587500, elapsed: 1.13e+01, train loss: 1.91524e-06, val loss: 2.69309e-06, min loss: 1.78329e-06\n",
      "Epoch: 587600, elapsed: 1.13e+01, train loss: 3.60143e-06, val loss: 5.22200e-06, min loss: 1.78329e-06\n",
      "Epoch: 587700, elapsed: 1.11e+01, train loss: 2.57624e-06, val loss: 3.36978e-06, min loss: 1.78329e-06\n",
      "Epoch: 587800, elapsed: 1.11e+01, train loss: 3.62024e-06, val loss: 4.60742e-06, min loss: 1.78329e-06\n",
      "Epoch: 587900, elapsed: 1.12e+01, train loss: 3.92830e-06, val loss: 5.32431e-06, min loss: 1.78329e-06\n",
      "Epoch: 588000, elapsed: 1.12e+01, train loss: 2.55838e-06, val loss: 3.32623e-06, min loss: 1.78329e-06\n",
      "Epoch: 588100, elapsed: 1.11e+01, train loss: 1.91424e-06, val loss: 2.70928e-06, min loss: 1.78329e-06\n",
      "Epoch: 588200, elapsed: 1.10e+01, train loss: 2.04227e-06, val loss: 2.83394e-06, min loss: 1.78329e-06\n",
      "Epoch: 588300, elapsed: 1.10e+01, train loss: 1.95919e-06, val loss: 2.72862e-06, min loss: 1.78329e-06\n",
      "Epoch: 588400, elapsed: 1.27e+01, train loss: 2.25571e-06, val loss: 2.82019e-06, min loss: 1.78329e-06\n",
      "Epoch: 588500, elapsed: 1.12e+01, train loss: 2.20009e-06, val loss: 2.59635e-06, min loss: 1.78329e-06\n",
      "Epoch: 588600, elapsed: 1.12e+01, train loss: 1.79138e-06, val loss: 2.51778e-06, min loss: 1.78329e-06\n",
      "Epoch: 588700, elapsed: 1.11e+01, train loss: 1.78103e-06, val loss: 2.52992e-06, min loss: 1.78103e-06\n",
      "Epoch: 588800, elapsed: 1.12e+01, train loss: 1.92425e-06, val loss: 2.71790e-06, min loss: 1.78103e-06\n",
      "Epoch: 588900, elapsed: 1.12e+01, train loss: 4.46397e-06, val loss: 6.05992e-06, min loss: 1.78103e-06\n",
      "Epoch: 589000, elapsed: 1.11e+01, train loss: 1.81508e-06, val loss: 2.55876e-06, min loss: 1.78103e-06\n",
      "Epoch: 589100, elapsed: 1.12e+01, train loss: 1.77757e-06, val loss: 2.52840e-06, min loss: 1.77757e-06\n",
      "Epoch: 589200, elapsed: 1.11e+01, train loss: 1.79296e-06, val loss: 2.52060e-06, min loss: 1.77757e-06\n",
      "Epoch: 589300, elapsed: 1.11e+01, train loss: 1.79590e-06, val loss: 2.52527e-06, min loss: 1.77757e-06\n",
      "Epoch: 589400, elapsed: 1.09e+01, train loss: 1.90115e-06, val loss: 2.72931e-06, min loss: 1.77757e-06\n",
      "Epoch: 589500, elapsed: 1.09e+01, train loss: 2.39888e-06, val loss: 2.88090e-06, min loss: 1.77757e-06\n",
      "Epoch: 589600, elapsed: 1.27e+01, train loss: 3.57767e-06, val loss: 4.66996e-06, min loss: 1.77757e-06\n",
      "Epoch: 589700, elapsed: 1.13e+01, train loss: 1.83557e-06, val loss: 2.53501e-06, min loss: 1.77757e-06\n",
      "Epoch: 589800, elapsed: 1.13e+01, train loss: 4.51894e-06, val loss: 5.54156e-06, min loss: 1.77757e-06\n",
      "Epoch: 589900, elapsed: 1.10e+01, train loss: 1.98956e-06, val loss: 2.91991e-06, min loss: 1.77757e-06\n",
      "Epoch: 590000, elapsed: 1.11e+01, train loss: 1.79945e-06, val loss: 2.51871e-06, min loss: 1.77757e-06\n",
      "Epoch: 590100, elapsed: 1.33e+01, train loss: 3.29432e-06, val loss: 4.29903e-06, min loss: 1.77757e-06\n",
      "Epoch: 590200, elapsed: 1.13e+01, train loss: 2.41366e-06, val loss: 3.01157e-06, min loss: 1.77757e-06\n",
      "Epoch: 590300, elapsed: 1.13e+01, train loss: 3.59809e-06, val loss: 3.69616e-06, min loss: 1.77757e-06\n",
      "Epoch: 590400, elapsed: 1.13e+01, train loss: 1.97199e-06, val loss: 2.72387e-06, min loss: 1.77757e-06\n",
      "Epoch: 590500, elapsed: 1.11e+01, train loss: 1.94581e-06, val loss: 2.79150e-06, min loss: 1.77757e-06\n",
      "Epoch: 590600, elapsed: 1.11e+01, train loss: 1.82005e-06, val loss: 2.63554e-06, min loss: 1.77757e-06\n",
      "Epoch: 590700, elapsed: 1.09e+01, train loss: 1.77513e-06, val loss: 2.52128e-06, min loss: 1.77513e-06\n",
      "Epoch: 590800, elapsed: 1.30e+01, train loss: 1.85250e-06, val loss: 2.60170e-06, min loss: 1.77513e-06\n",
      "Epoch: 590900, elapsed: 1.15e+01, train loss: 1.82861e-06, val loss: 2.58955e-06, min loss: 1.77513e-06\n",
      "Epoch: 591000, elapsed: 1.16e+01, train loss: 2.92298e-06, val loss: 3.66873e-06, min loss: 1.77513e-06\n",
      "Epoch: 591100, elapsed: 1.11e+01, train loss: 1.84556e-06, val loss: 2.73770e-06, min loss: 1.77513e-06\n",
      "Epoch: 591200, elapsed: 1.12e+01, train loss: 1.88851e-06, val loss: 2.60842e-06, min loss: 1.77513e-06\n",
      "Epoch: 591300, elapsed: 1.11e+01, train loss: 1.96503e-06, val loss: 2.69809e-06, min loss: 1.77513e-06\n",
      "Epoch: 591400, elapsed: 1.14e+01, train loss: 1.83848e-06, val loss: 2.62184e-06, min loss: 1.77513e-06\n",
      "Epoch: 591500, elapsed: 1.13e+01, train loss: 1.78163e-06, val loss: 2.52148e-06, min loss: 1.77513e-06\n",
      "Epoch: 591600, elapsed: 1.11e+01, train loss: 1.84184e-06, val loss: 2.65835e-06, min loss: 1.77513e-06\n",
      "Epoch: 591700, elapsed: 1.11e+01, train loss: 1.83094e-06, val loss: 2.60853e-06, min loss: 1.77513e-06\n",
      "Epoch: 591800, elapsed: 1.11e+01, train loss: 1.79140e-06, val loss: 2.56093e-06, min loss: 1.77513e-06\n",
      "Epoch: 591900, elapsed: 1.29e+01, train loss: 1.78031e-06, val loss: 2.54542e-06, min loss: 1.77513e-06\n",
      "Epoch: 592000, elapsed: 1.15e+01, train loss: 1.96994e-06, val loss: 2.81137e-06, min loss: 1.77513e-06\n",
      "Epoch: 592100, elapsed: 1.14e+01, train loss: 1.83291e-06, val loss: 2.66176e-06, min loss: 1.77513e-06\n",
      "Epoch: 592200, elapsed: 1.15e+01, train loss: 2.05931e-06, val loss: 2.73273e-06, min loss: 1.77513e-06\n",
      "Epoch: 592300, elapsed: 1.14e+01, train loss: 4.47186e-06, val loss: 4.30817e-06, min loss: 1.77513e-06\n",
      "Epoch: 592400, elapsed: 1.12e+01, train loss: 1.83982e-06, val loss: 2.56494e-06, min loss: 1.77513e-06\n",
      "Epoch: 592500, elapsed: 1.12e+01, train loss: 1.77226e-06, val loss: 2.52642e-06, min loss: 1.77226e-06\n",
      "Epoch: 592600, elapsed: 1.12e+01, train loss: 1.77953e-06, val loss: 2.52263e-06, min loss: 1.77226e-06\n",
      "Epoch: 592700, elapsed: 1.14e+01, train loss: 1.81353e-06, val loss: 2.54760e-06, min loss: 1.77226e-06\n",
      "Epoch: 592800, elapsed: 1.13e+01, train loss: 1.89355e-06, val loss: 2.64261e-06, min loss: 1.77226e-06\n",
      "Epoch: 592900, elapsed: 1.12e+01, train loss: 1.85475e-06, val loss: 2.50865e-06, min loss: 1.77226e-06\n",
      "Epoch: 593000, elapsed: 1.09e+01, train loss: 2.88605e-06, val loss: 3.13787e-06, min loss: 1.77226e-06\n",
      "Epoch: 593100, elapsed: 1.28e+01, train loss: 2.32608e-06, val loss: 3.25661e-06, min loss: 1.77226e-06\n",
      "Epoch: 593200, elapsed: 1.13e+01, train loss: 1.82326e-06, val loss: 2.53542e-06, min loss: 1.77226e-06\n",
      "Epoch: 593300, elapsed: 1.12e+01, train loss: 1.82742e-06, val loss: 2.55872e-06, min loss: 1.77226e-06\n",
      "Epoch: 593400, elapsed: 1.13e+01, train loss: 1.77170e-06, val loss: 2.51619e-06, min loss: 1.77170e-06\n",
      "Epoch: 593500, elapsed: 1.14e+01, train loss: 1.77270e-06, val loss: 2.50426e-06, min loss: 1.77170e-06\n",
      "Epoch: 593600, elapsed: 1.12e+01, train loss: 1.98091e-06, val loss: 2.82608e-06, min loss: 1.77170e-06\n",
      "Epoch: 593700, elapsed: 1.13e+01, train loss: 4.25401e-06, val loss: 4.19139e-06, min loss: 1.77170e-06\n",
      "Epoch: 593800, elapsed: 1.12e+01, train loss: 1.80751e-06, val loss: 2.54366e-06, min loss: 1.77170e-06\n",
      "Epoch: 593900, elapsed: 1.13e+01, train loss: 1.77184e-06, val loss: 2.54298e-06, min loss: 1.77170e-06\n",
      "Epoch: 594000, elapsed: 1.11e+01, train loss: 1.77947e-06, val loss: 2.51111e-06, min loss: 1.77170e-06\n",
      "Epoch: 594100, elapsed: 1.09e+01, train loss: 1.78596e-06, val loss: 2.56063e-06, min loss: 1.77170e-06\n",
      "Epoch: 594200, elapsed: 1.08e+01, train loss: 1.80968e-06, val loss: 2.54862e-06, min loss: 1.77170e-06\n",
      "Epoch: 594300, elapsed: 1.28e+01, train loss: 1.77134e-06, val loss: 2.52386e-06, min loss: 1.77134e-06\n",
      "Epoch: 594400, elapsed: 1.12e+01, train loss: 1.97670e-06, val loss: 2.63371e-06, min loss: 1.77134e-06\n",
      "Epoch: 594500, elapsed: 1.12e+01, train loss: 2.52763e-06, val loss: 3.10193e-06, min loss: 1.77134e-06\n",
      "Epoch: 594600, elapsed: 1.13e+01, train loss: 1.79541e-06, val loss: 2.51881e-06, min loss: 1.77134e-06\n",
      "Epoch: 594700, elapsed: 1.11e+01, train loss: 2.63853e-06, val loss: 3.44373e-06, min loss: 1.77134e-06\n",
      "Epoch: 594800, elapsed: 1.12e+01, train loss: 1.84802e-06, val loss: 2.54212e-06, min loss: 1.77134e-06\n",
      "Epoch: 594900, elapsed: 1.11e+01, train loss: 3.21483e-06, val loss: 3.97074e-06, min loss: 1.77134e-06\n",
      "Epoch: 595000, elapsed: 1.12e+01, train loss: 6.18433e-06, val loss: 5.97575e-06, min loss: 1.77134e-06\n",
      "Epoch: 595100, elapsed: 1.32e+01, train loss: 9.87123e-06, val loss: 9.82759e-06, min loss: 1.77134e-06\n",
      "Epoch: 595200, elapsed: 1.11e+01, train loss: 1.78508e-06, val loss: 2.51878e-06, min loss: 1.77134e-06\n",
      "Epoch: 595300, elapsed: 1.11e+01, train loss: 1.76872e-06, val loss: 2.52069e-06, min loss: 1.76872e-06\n",
      "Epoch: 595400, elapsed: 1.08e+01, train loss: 1.92803e-06, val loss: 2.62203e-06, min loss: 1.76872e-06\n",
      "Epoch: 595500, elapsed: 1.29e+01, train loss: 2.51028e-06, val loss: 3.72329e-06, min loss: 1.76872e-06\n",
      "Epoch: 595600, elapsed: 1.13e+01, train loss: 1.76430e-06, val loss: 2.51577e-06, min loss: 1.76430e-06\n",
      "Epoch: 595700, elapsed: 1.12e+01, train loss: 1.77333e-06, val loss: 2.51605e-06, min loss: 1.76430e-06\n",
      "Epoch: 595800, elapsed: 1.12e+01, train loss: 2.03162e-06, val loss: 2.63420e-06, min loss: 1.76430e-06\n",
      "Epoch: 595900, elapsed: 1.12e+01, train loss: 2.01328e-06, val loss: 2.87566e-06, min loss: 1.76430e-06\n",
      "Epoch: 596000, elapsed: 1.11e+01, train loss: 1.94186e-06, val loss: 2.68308e-06, min loss: 1.76430e-06\n",
      "Epoch: 596100, elapsed: 1.11e+01, train loss: 1.82507e-06, val loss: 2.52471e-06, min loss: 1.76430e-06\n",
      "Epoch: 596200, elapsed: 1.11e+01, train loss: 2.19445e-06, val loss: 3.10722e-06, min loss: 1.76430e-06\n",
      "Epoch: 596300, elapsed: 1.12e+01, train loss: 1.76305e-06, val loss: 2.50755e-06, min loss: 1.76305e-06\n",
      "Epoch: 596400, elapsed: 1.10e+01, train loss: 1.83656e-06, val loss: 2.53863e-06, min loss: 1.76305e-06\n",
      "Epoch: 596500, elapsed: 1.10e+01, train loss: 1.96122e-06, val loss: 2.80064e-06, min loss: 1.76305e-06\n",
      "Epoch: 596600, elapsed: 1.10e+01, train loss: 2.16231e-06, val loss: 2.98355e-06, min loss: 1.76305e-06\n",
      "Epoch: 596700, elapsed: 1.31e+01, train loss: 1.80311e-06, val loss: 2.49622e-06, min loss: 1.76305e-06\n",
      "Epoch: 596800, elapsed: 1.14e+01, train loss: 1.88471e-06, val loss: 2.58762e-06, min loss: 1.76305e-06\n",
      "Epoch: 596900, elapsed: 1.12e+01, train loss: 2.21962e-06, val loss: 3.11816e-06, min loss: 1.76305e-06\n",
      "Epoch: 597000, elapsed: 1.11e+01, train loss: 2.71115e-06, val loss: 2.84149e-06, min loss: 1.76305e-06\n",
      "Epoch: 597100, elapsed: 1.11e+01, train loss: 3.07525e-06, val loss: 4.04913e-06, min loss: 1.76305e-06\n",
      "Epoch: 597200, elapsed: 1.13e+01, train loss: 2.14559e-06, val loss: 2.59193e-06, min loss: 1.76305e-06\n",
      "Epoch: 597300, elapsed: 1.13e+01, train loss: 2.13601e-06, val loss: 2.95494e-06, min loss: 1.76305e-06\n",
      "Epoch: 597400, elapsed: 1.13e+01, train loss: 1.79396e-06, val loss: 2.57059e-06, min loss: 1.76305e-06\n",
      "Epoch: 597500, elapsed: 1.12e+01, train loss: 1.86997e-06, val loss: 2.62804e-06, min loss: 1.76305e-06\n",
      "Epoch: 597600, elapsed: 1.12e+01, train loss: 3.53165e-06, val loss: 3.46583e-06, min loss: 1.76305e-06\n",
      "Epoch: 597700, elapsed: 1.10e+01, train loss: 4.94706e-06, val loss: 5.72856e-06, min loss: 1.76305e-06\n",
      "Epoch: 597800, elapsed: 1.09e+01, train loss: 3.14775e-06, val loss: 3.71808e-06, min loss: 1.76305e-06\n",
      "Epoch: 597900, elapsed: 1.30e+01, train loss: 3.37802e-06, val loss: 4.20248e-06, min loss: 1.76305e-06\n",
      "Epoch: 598000, elapsed: 1.12e+01, train loss: 6.78343e-06, val loss: 8.70616e-06, min loss: 1.76305e-06\n",
      "Epoch: 598100, elapsed: 1.11e+01, train loss: 1.76642e-06, val loss: 2.50012e-06, min loss: 1.76305e-06\n",
      "Epoch: 598200, elapsed: 1.12e+01, train loss: 1.75954e-06, val loss: 2.51025e-06, min loss: 1.75954e-06\n",
      "Epoch: 598300, elapsed: 1.12e+01, train loss: 1.79493e-06, val loss: 2.57395e-06, min loss: 1.75954e-06\n",
      "Epoch: 598400, elapsed: 1.12e+01, train loss: 1.79416e-06, val loss: 2.55899e-06, min loss: 1.75954e-06\n",
      "Epoch: 598500, elapsed: 1.11e+01, train loss: 2.12325e-06, val loss: 3.01550e-06, min loss: 1.75954e-06\n",
      "Epoch: 598600, elapsed: 1.11e+01, train loss: 4.89303e-06, val loss: 5.84051e-06, min loss: 1.75954e-06\n",
      "Epoch: 598700, elapsed: 1.11e+01, train loss: 2.01571e-06, val loss: 2.71828e-06, min loss: 1.75954e-06\n",
      "Epoch: 598800, elapsed: 1.13e+01, train loss: 1.93591e-06, val loss: 2.90493e-06, min loss: 1.75954e-06\n",
      "Epoch: 598900, elapsed: 1.09e+01, train loss: 2.99010e-06, val loss: 3.03602e-06, min loss: 1.75954e-06\n",
      "Epoch: 599000, elapsed: 1.09e+01, train loss: 2.19460e-06, val loss: 2.87850e-06, min loss: 1.75954e-06\n",
      "Epoch: 599100, elapsed: 1.30e+01, train loss: 2.89557e-06, val loss: 3.21048e-06, min loss: 1.75954e-06\n",
      "Epoch: 599200, elapsed: 1.12e+01, train loss: 3.14064e-06, val loss: 3.99032e-06, min loss: 1.75954e-06\n",
      "Epoch: 599300, elapsed: 1.14e+01, train loss: 9.81861e-06, val loss: 1.05287e-05, min loss: 1.75954e-06\n",
      "Epoch: 599400, elapsed: 1.11e+01, train loss: 2.01233e-06, val loss: 2.84515e-06, min loss: 1.75954e-06\n",
      "Epoch: 599500, elapsed: 1.11e+01, train loss: 1.76720e-06, val loss: 2.51173e-06, min loss: 1.75954e-06\n",
      "Epoch: 599600, elapsed: 1.12e+01, train loss: 2.05210e-06, val loss: 2.69845e-06, min loss: 1.75954e-06\n",
      "Epoch: 599700, elapsed: 1.11e+01, train loss: 1.75744e-06, val loss: 2.51462e-06, min loss: 1.75744e-06\n",
      "Epoch: 599800, elapsed: 1.09e+01, train loss: 1.76174e-06, val loss: 2.52857e-06, min loss: 1.75744e-06\n",
      "Epoch: 599900, elapsed: 1.11e+01, train loss: 2.01000e-06, val loss: 2.85721e-06, min loss: 1.75744e-06\n",
      "Epoch: 600000, elapsed: 1.12e+01, train loss: 1.55343e-05, val loss: 1.41806e-05, min loss: 1.75744e-06\n",
      "Epoch: 600100, elapsed: 1.30e+01, train loss: 1.75386e-06, val loss: 2.50080e-06, min loss: 1.75386e-06\n",
      "Epoch: 600200, elapsed: 1.27e+01, train loss: 1.85600e-06, val loss: 2.64490e-06, min loss: 1.75386e-06\n",
      "Epoch: 600300, elapsed: 1.13e+01, train loss: 1.84408e-06, val loss: 2.71408e-06, min loss: 1.75386e-06\n",
      "Epoch: 600400, elapsed: 1.13e+01, train loss: 1.75580e-06, val loss: 2.50827e-06, min loss: 1.75386e-06\n",
      "Epoch: 600500, elapsed: 1.13e+01, train loss: 2.11688e-06, val loss: 2.60643e-06, min loss: 1.75386e-06\n",
      "Epoch: 600600, elapsed: 1.11e+01, train loss: 2.45777e-06, val loss: 2.89616e-06, min loss: 1.75386e-06\n",
      "Epoch: 600700, elapsed: 1.12e+01, train loss: 2.71821e-06, val loss: 3.32611e-06, min loss: 1.75386e-06\n",
      "Epoch: 600800, elapsed: 1.11e+01, train loss: 2.90752e-06, val loss: 3.72286e-06, min loss: 1.75386e-06\n",
      "Epoch: 600900, elapsed: 1.12e+01, train loss: 6.07408e-06, val loss: 6.69195e-06, min loss: 1.75386e-06\n",
      "Epoch: 601000, elapsed: 1.13e+01, train loss: 1.93159e-06, val loss: 2.63688e-06, min loss: 1.75386e-06\n",
      "Epoch: 601100, elapsed: 1.13e+01, train loss: 2.17847e-06, val loss: 3.06833e-06, min loss: 1.75386e-06\n",
      "Epoch: 601200, elapsed: 1.10e+01, train loss: 2.27133e-06, val loss: 3.05952e-06, min loss: 1.75386e-06\n",
      "Epoch: 601300, elapsed: 1.09e+01, train loss: 2.33807e-06, val loss: 2.70233e-06, min loss: 1.75386e-06\n",
      "Epoch: 601400, elapsed: 1.29e+01, train loss: 7.79366e-06, val loss: 7.44126e-06, min loss: 1.75386e-06\n",
      "Epoch: 601500, elapsed: 1.13e+01, train loss: 2.50951e-06, val loss: 3.67791e-06, min loss: 1.75386e-06\n",
      "Epoch: 601600, elapsed: 1.12e+01, train loss: 1.76793e-06, val loss: 2.54490e-06, min loss: 1.75386e-06\n",
      "Epoch: 601700, elapsed: 1.11e+01, train loss: 1.98685e-06, val loss: 2.67443e-06, min loss: 1.75386e-06\n",
      "Epoch: 601800, elapsed: 1.13e+01, train loss: 1.77218e-06, val loss: 2.54370e-06, min loss: 1.75386e-06\n",
      "Epoch: 601900, elapsed: 1.12e+01, train loss: 1.75228e-06, val loss: 2.49419e-06, min loss: 1.75228e-06\n",
      "Epoch: 602000, elapsed: 1.12e+01, train loss: 1.75377e-06, val loss: 2.52072e-06, min loss: 1.75228e-06\n",
      "Epoch: 602100, elapsed: 1.11e+01, train loss: 1.77453e-06, val loss: 2.51869e-06, min loss: 1.75228e-06\n",
      "Epoch: 602200, elapsed: 1.13e+01, train loss: 7.46424e-06, val loss: 9.28132e-06, min loss: 1.75228e-06\n",
      "Epoch: 602300, elapsed: 1.11e+01, train loss: 1.76183e-06, val loss: 2.48392e-06, min loss: 1.75228e-06\n",
      "Epoch: 602400, elapsed: 1.12e+01, train loss: 1.75849e-06, val loss: 2.47809e-06, min loss: 1.75228e-06\n",
      "Epoch: 602500, elapsed: 1.08e+01, train loss: 1.75369e-06, val loss: 2.48711e-06, min loss: 1.75228e-06\n",
      "Epoch: 602600, elapsed: 1.29e+01, train loss: 1.77567e-06, val loss: 2.53164e-06, min loss: 1.75228e-06\n",
      "Epoch: 602700, elapsed: 1.13e+01, train loss: 1.83944e-06, val loss: 2.54875e-06, min loss: 1.75228e-06\n",
      "Epoch: 602800, elapsed: 1.14e+01, train loss: 1.83199e-06, val loss: 2.56518e-06, min loss: 1.75228e-06\n",
      "Epoch: 602900, elapsed: 1.12e+01, train loss: 1.75605e-06, val loss: 2.50691e-06, min loss: 1.75228e-06\n",
      "Epoch: 603000, elapsed: 1.10e+01, train loss: 2.34065e-06, val loss: 2.78203e-06, min loss: 1.75228e-06\n",
      "Epoch: 603100, elapsed: 1.12e+01, train loss: 2.03520e-06, val loss: 2.64660e-06, min loss: 1.75228e-06\n",
      "Epoch: 603200, elapsed: 1.13e+01, train loss: 1.80275e-06, val loss: 2.58942e-06, min loss: 1.75228e-06\n",
      "Epoch: 603300, elapsed: 1.13e+01, train loss: 1.91591e-06, val loss: 2.74598e-06, min loss: 1.75228e-06\n",
      "Epoch: 603400, elapsed: 1.11e+01, train loss: 2.69291e-06, val loss: 2.80878e-06, min loss: 1.75228e-06\n",
      "Epoch: 603500, elapsed: 1.11e+01, train loss: 1.84960e-06, val loss: 2.79356e-06, min loss: 1.75228e-06\n",
      "Epoch: 603600, elapsed: 1.11e+01, train loss: 1.84992e-06, val loss: 2.83111e-06, min loss: 1.75228e-06\n",
      "Epoch: 603700, elapsed: 1.10e+01, train loss: 4.96350e-06, val loss: 6.16164e-06, min loss: 1.75228e-06\n",
      "Epoch: 603800, elapsed: 1.27e+01, train loss: 5.63957e-06, val loss: 4.67916e-06, min loss: 1.75228e-06\n",
      "Epoch: 603900, elapsed: 1.13e+01, train loss: 2.02989e-06, val loss: 2.92556e-06, min loss: 1.75228e-06\n",
      "Epoch: 604000, elapsed: 1.12e+01, train loss: 1.85452e-06, val loss: 2.55961e-06, min loss: 1.75228e-06\n",
      "Epoch: 604100, elapsed: 1.11e+01, train loss: 1.75559e-06, val loss: 2.51309e-06, min loss: 1.75228e-06\n",
      "Epoch: 604200, elapsed: 1.11e+01, train loss: 2.13935e-06, val loss: 2.99730e-06, min loss: 1.75228e-06\n",
      "Epoch: 604300, elapsed: 1.11e+01, train loss: 1.99233e-06, val loss: 3.11727e-06, min loss: 1.75228e-06\n",
      "Epoch: 604400, elapsed: 1.13e+01, train loss: 1.81957e-06, val loss: 2.87361e-06, min loss: 1.75228e-06\n",
      "Epoch: 604500, elapsed: 1.11e+01, train loss: 5.30980e-06, val loss: 6.65861e-06, min loss: 1.75228e-06\n",
      "Epoch: 604600, elapsed: 1.10e+01, train loss: 7.20457e-06, val loss: 7.19168e-06, min loss: 1.75228e-06\n",
      "Epoch: 604700, elapsed: 1.10e+01, train loss: 3.91547e-06, val loss: 5.06505e-06, min loss: 1.75228e-06\n",
      "Epoch: 604800, elapsed: 1.10e+01, train loss: 2.02174e-06, val loss: 2.70824e-06, min loss: 1.75228e-06\n",
      "Epoch: 604900, elapsed: 1.10e+01, train loss: 2.78934e-06, val loss: 3.75910e-06, min loss: 1.75228e-06\n",
      "Epoch: 605000, elapsed: 1.26e+01, train loss: 1.74808e-06, val loss: 2.48675e-06, min loss: 1.74808e-06\n",
      "Epoch: 605100, elapsed: 1.35e+01, train loss: 1.79840e-06, val loss: 2.58719e-06, min loss: 1.74808e-06\n",
      "Epoch: 605200, elapsed: 1.12e+01, train loss: 1.83133e-06, val loss: 2.58554e-06, min loss: 1.74808e-06\n",
      "Epoch: 605300, elapsed: 1.12e+01, train loss: 1.82386e-06, val loss: 2.62354e-06, min loss: 1.74808e-06\n",
      "Epoch: 605400, elapsed: 1.13e+01, train loss: 2.41289e-06, val loss: 3.41373e-06, min loss: 1.74808e-06\n",
      "Epoch: 605500, elapsed: 1.12e+01, train loss: 1.93130e-06, val loss: 2.68021e-06, min loss: 1.74808e-06\n",
      "Epoch: 605600, elapsed: 1.12e+01, train loss: 1.74395e-06, val loss: 2.50019e-06, min loss: 1.74395e-06\n",
      "Epoch: 605700, elapsed: 1.13e+01, train loss: 1.09744e-05, val loss: 6.96336e-06, min loss: 1.74395e-06\n",
      "Epoch: 605800, elapsed: 1.11e+01, train loss: 1.74255e-06, val loss: 2.49066e-06, min loss: 1.74255e-06\n",
      "Epoch: 605900, elapsed: 1.13e+01, train loss: 1.76662e-06, val loss: 2.49773e-06, min loss: 1.74255e-06\n",
      "Epoch: 606000, elapsed: 1.09e+01, train loss: 1.82144e-06, val loss: 2.50056e-06, min loss: 1.74255e-06\n",
      "Epoch: 606100, elapsed: 1.09e+01, train loss: 1.86524e-06, val loss: 2.68489e-06, min loss: 1.74255e-06\n",
      "Epoch: 606200, elapsed: 1.29e+01, train loss: 1.74470e-06, val loss: 2.50586e-06, min loss: 1.74255e-06\n",
      "Epoch: 606300, elapsed: 1.11e+01, train loss: 1.76634e-06, val loss: 2.52795e-06, min loss: 1.74255e-06\n",
      "Epoch: 606400, elapsed: 1.13e+01, train loss: 1.13438e-05, val loss: 8.80456e-06, min loss: 1.74255e-06\n",
      "Epoch: 606500, elapsed: 1.11e+01, train loss: 2.52979e-06, val loss: 3.81080e-06, min loss: 1.74255e-06\n",
      "Epoch: 606600, elapsed: 1.12e+01, train loss: 1.94148e-06, val loss: 2.55486e-06, min loss: 1.74255e-06\n",
      "Epoch: 606700, elapsed: 1.12e+01, train loss: 4.79000e-06, val loss: 6.23868e-06, min loss: 1.74255e-06\n",
      "Epoch: 606800, elapsed: 1.10e+01, train loss: 1.87795e-06, val loss: 2.57612e-06, min loss: 1.74255e-06\n",
      "Epoch: 606900, elapsed: 1.12e+01, train loss: 1.95948e-06, val loss: 2.64131e-06, min loss: 1.74255e-06\n",
      "Epoch: 607000, elapsed: 1.12e+01, train loss: 2.45417e-06, val loss: 3.28666e-06, min loss: 1.74255e-06\n",
      "Epoch: 607100, elapsed: 1.11e+01, train loss: 4.71980e-06, val loss: 5.36288e-06, min loss: 1.74255e-06\n",
      "Epoch: 607200, elapsed: 1.11e+01, train loss: 1.94391e-06, val loss: 2.60293e-06, min loss: 1.74255e-06\n",
      "Epoch: 607300, elapsed: 1.09e+01, train loss: 1.74946e-06, val loss: 2.49596e-06, min loss: 1.74255e-06\n",
      "Epoch: 607400, elapsed: 1.28e+01, train loss: 1.79904e-06, val loss: 2.50865e-06, min loss: 1.74255e-06\n",
      "Epoch: 607500, elapsed: 1.13e+01, train loss: 1.77264e-06, val loss: 2.48474e-06, min loss: 1.74255e-06\n",
      "Epoch: 607600, elapsed: 1.14e+01, train loss: 1.75998e-06, val loss: 2.53303e-06, min loss: 1.74255e-06\n",
      "Epoch: 607700, elapsed: 1.15e+01, train loss: 1.76239e-06, val loss: 2.51801e-06, min loss: 1.74255e-06\n",
      "Epoch: 607800, elapsed: 1.12e+01, train loss: 1.92186e-06, val loss: 2.66927e-06, min loss: 1.74255e-06\n",
      "Epoch: 607900, elapsed: 1.14e+01, train loss: 1.88396e-06, val loss: 2.60500e-06, min loss: 1.74255e-06\n",
      "Epoch: 608000, elapsed: 1.13e+01, train loss: 2.06756e-06, val loss: 2.84796e-06, min loss: 1.74255e-06\n",
      "Epoch: 608100, elapsed: 1.13e+01, train loss: 3.12548e-06, val loss: 3.46010e-06, min loss: 1.74255e-06\n",
      "Epoch: 608200, elapsed: 1.13e+01, train loss: 9.49803e-06, val loss: 9.72580e-06, min loss: 1.74255e-06\n",
      "Epoch: 608300, elapsed: 1.12e+01, train loss: 1.76752e-06, val loss: 2.57063e-06, min loss: 1.74255e-06\n",
      "Epoch: 608400, elapsed: 1.10e+01, train loss: 1.74530e-06, val loss: 2.50445e-06, min loss: 1.74255e-06\n",
      "Epoch: 608500, elapsed: 1.10e+01, train loss: 1.78303e-06, val loss: 2.58787e-06, min loss: 1.74255e-06\n",
      "Epoch: 608600, elapsed: 1.30e+01, train loss: 3.86881e-06, val loss: 5.66385e-06, min loss: 1.74255e-06\n",
      "Epoch: 608700, elapsed: 1.12e+01, train loss: 1.73772e-06, val loss: 2.48220e-06, min loss: 1.73772e-06\n",
      "Epoch: 608800, elapsed: 1.13e+01, train loss: 1.76341e-06, val loss: 2.51623e-06, min loss: 1.73772e-06\n",
      "Epoch: 608900, elapsed: 1.13e+01, train loss: 4.84934e-06, val loss: 4.61767e-06, min loss: 1.73772e-06\n",
      "Epoch: 609000, elapsed: 1.13e+01, train loss: 1.75854e-06, val loss: 2.54200e-06, min loss: 1.73772e-06\n",
      "Epoch: 609100, elapsed: 1.12e+01, train loss: 1.81384e-06, val loss: 2.56994e-06, min loss: 1.73772e-06\n",
      "Epoch: 609200, elapsed: 1.13e+01, train loss: 1.75344e-06, val loss: 2.48041e-06, min loss: 1.73772e-06\n",
      "Epoch: 609300, elapsed: 1.12e+01, train loss: 1.81434e-06, val loss: 2.58133e-06, min loss: 1.73772e-06\n",
      "Epoch: 609400, elapsed: 1.12e+01, train loss: 3.98089e-06, val loss: 4.48699e-06, min loss: 1.73772e-06\n",
      "Epoch: 609500, elapsed: 1.13e+01, train loss: 1.78495e-06, val loss: 2.46603e-06, min loss: 1.73772e-06\n",
      "Epoch: 609600, elapsed: 1.12e+01, train loss: 1.94568e-06, val loss: 3.21041e-06, min loss: 1.73772e-06\n",
      "Epoch: 609700, elapsed: 1.09e+01, train loss: 1.99602e-06, val loss: 2.75919e-06, min loss: 1.73772e-06\n",
      "Epoch: 609800, elapsed: 1.27e+01, train loss: 1.76602e-06, val loss: 2.56755e-06, min loss: 1.73772e-06\n",
      "Epoch: 609900, elapsed: 1.13e+01, train loss: 2.26341e-06, val loss: 3.02077e-06, min loss: 1.73772e-06\n",
      "Epoch: 610000, elapsed: 1.14e+01, train loss: 1.90135e-06, val loss: 2.64494e-06, min loss: 1.73772e-06\n",
      "Epoch: 610100, elapsed: 1.34e+01, train loss: 1.79545e-06, val loss: 2.66726e-06, min loss: 1.73772e-06\n",
      "Epoch: 610200, elapsed: 1.13e+01, train loss: 1.74297e-06, val loss: 2.48065e-06, min loss: 1.73772e-06\n",
      "Epoch: 610300, elapsed: 1.14e+01, train loss: 6.01918e-06, val loss: 6.84485e-06, min loss: 1.73772e-06\n",
      "Epoch: 610400, elapsed: 1.12e+01, train loss: 5.10757e-06, val loss: 6.20186e-06, min loss: 1.73772e-06\n",
      "Epoch: 610500, elapsed: 1.12e+01, train loss: 1.98282e-06, val loss: 2.59017e-06, min loss: 1.73772e-06\n",
      "Epoch: 610600, elapsed: 1.13e+01, train loss: 2.01629e-06, val loss: 2.67681e-06, min loss: 1.73772e-06\n",
      "Epoch: 610700, elapsed: 1.15e+01, train loss: 1.88867e-06, val loss: 2.70483e-06, min loss: 1.73772e-06\n",
      "Epoch: 610800, elapsed: 1.12e+01, train loss: 2.23114e-06, val loss: 2.88764e-06, min loss: 1.73772e-06\n",
      "Epoch: 610900, elapsed: 1.10e+01, train loss: 2.27554e-06, val loss: 3.07860e-06, min loss: 1.73772e-06\n",
      "Epoch: 611000, elapsed: 1.28e+01, train loss: 4.71883e-06, val loss: 5.24105e-06, min loss: 1.73772e-06\n",
      "Epoch: 611100, elapsed: 1.13e+01, train loss: 2.14059e-06, val loss: 2.74557e-06, min loss: 1.73772e-06\n",
      "Epoch: 611200, elapsed: 1.14e+01, train loss: 1.85931e-06, val loss: 2.66107e-06, min loss: 1.73772e-06\n",
      "Epoch: 611300, elapsed: 1.13e+01, train loss: 2.02163e-06, val loss: 2.64121e-06, min loss: 1.73772e-06\n",
      "Epoch: 611400, elapsed: 1.14e+01, train loss: 3.03296e-06, val loss: 3.29759e-06, min loss: 1.73772e-06\n",
      "Epoch: 611500, elapsed: 1.14e+01, train loss: 3.96543e-06, val loss: 4.39381e-06, min loss: 1.73772e-06\n",
      "Epoch: 611600, elapsed: 1.12e+01, train loss: 4.42496e-06, val loss: 6.01940e-06, min loss: 1.73772e-06\n",
      "Epoch: 611700, elapsed: 1.14e+01, train loss: 2.71903e-06, val loss: 4.04884e-06, min loss: 1.73772e-06\n",
      "Epoch: 611800, elapsed: 1.13e+01, train loss: 2.69171e-06, val loss: 3.93286e-06, min loss: 1.73772e-06\n",
      "Epoch: 611900, elapsed: 1.12e+01, train loss: 2.94150e-06, val loss: 3.70098e-06, min loss: 1.73772e-06\n",
      "Epoch: 612000, elapsed: 1.12e+01, train loss: 2.65065e-06, val loss: 3.09862e-06, min loss: 1.73772e-06\n",
      "Epoch: 612100, elapsed: 1.11e+01, train loss: 1.79126e-06, val loss: 2.50429e-06, min loss: 1.73772e-06\n",
      "Epoch: 612200, elapsed: 1.29e+01, train loss: 1.77522e-06, val loss: 2.46284e-06, min loss: 1.73772e-06\n",
      "Epoch: 612300, elapsed: 1.16e+01, train loss: 1.74212e-06, val loss: 2.50603e-06, min loss: 1.73772e-06\n",
      "Epoch: 612400, elapsed: 1.15e+01, train loss: 1.87974e-06, val loss: 2.73265e-06, min loss: 1.73772e-06\n",
      "Epoch: 612500, elapsed: 1.15e+01, train loss: 3.57411e-06, val loss: 3.71451e-06, min loss: 1.73772e-06\n",
      "Epoch: 612600, elapsed: 1.12e+01, train loss: 2.89761e-06, val loss: 3.42579e-06, min loss: 1.73772e-06\n",
      "Epoch: 612700, elapsed: 1.14e+01, train loss: 4.21348e-06, val loss: 3.97935e-06, min loss: 1.73772e-06\n",
      "Epoch: 612800, elapsed: 1.14e+01, train loss: 1.76351e-06, val loss: 2.53646e-06, min loss: 1.73772e-06\n",
      "Epoch: 612900, elapsed: 1.14e+01, train loss: 1.74317e-06, val loss: 2.48241e-06, min loss: 1.73772e-06\n",
      "Epoch: 613000, elapsed: 1.12e+01, train loss: 1.73709e-06, val loss: 2.48471e-06, min loss: 1.73709e-06\n",
      "Epoch: 613100, elapsed: 1.13e+01, train loss: 2.02660e-06, val loss: 2.66138e-06, min loss: 1.73709e-06\n",
      "Epoch: 613200, elapsed: 1.12e+01, train loss: 2.82259e-06, val loss: 3.33916e-06, min loss: 1.73709e-06\n",
      "Epoch: 613300, elapsed: 1.10e+01, train loss: 2.06531e-06, val loss: 2.73238e-06, min loss: 1.73709e-06\n",
      "Epoch: 613400, elapsed: 1.10e+01, train loss: 1.76574e-06, val loss: 2.53382e-06, min loss: 1.73709e-06\n",
      "Epoch: 613500, elapsed: 1.31e+01, train loss: 1.84711e-06, val loss: 2.64789e-06, min loss: 1.73709e-06\n",
      "Epoch: 613600, elapsed: 1.15e+01, train loss: 1.99806e-06, val loss: 2.87664e-06, min loss: 1.73709e-06\n",
      "Epoch: 613700, elapsed: 1.13e+01, train loss: 2.42714e-06, val loss: 3.08729e-06, min loss: 1.73709e-06\n",
      "Epoch: 613800, elapsed: 1.13e+01, train loss: 2.22224e-06, val loss: 2.77520e-06, min loss: 1.73709e-06\n",
      "Epoch: 613900, elapsed: 1.12e+01, train loss: 1.96221e-06, val loss: 2.61242e-06, min loss: 1.73709e-06\n",
      "Epoch: 614000, elapsed: 1.11e+01, train loss: 1.79653e-06, val loss: 2.50571e-06, min loss: 1.73709e-06\n",
      "Epoch: 614100, elapsed: 1.14e+01, train loss: 3.98298e-06, val loss: 3.96862e-06, min loss: 1.73709e-06\n",
      "Epoch: 614200, elapsed: 1.12e+01, train loss: 3.81892e-06, val loss: 4.14434e-06, min loss: 1.73709e-06\n",
      "Epoch: 614300, elapsed: 1.12e+01, train loss: 1.88966e-06, val loss: 2.61111e-06, min loss: 1.73709e-06\n",
      "Epoch: 614400, elapsed: 1.12e+01, train loss: 1.91942e-06, val loss: 2.70068e-06, min loss: 1.73709e-06\n",
      "Epoch: 614500, elapsed: 1.11e+01, train loss: 1.78594e-06, val loss: 2.50294e-06, min loss: 1.73709e-06\n",
      "Epoch: 614600, elapsed: 1.11e+01, train loss: 1.75850e-06, val loss: 2.54989e-06, min loss: 1.73709e-06\n",
      "Epoch: 614700, elapsed: 1.31e+01, train loss: 1.93907e-06, val loss: 2.75364e-06, min loss: 1.73709e-06\n",
      "Epoch: 614800, elapsed: 1.14e+01, train loss: 1.73455e-06, val loss: 2.50438e-06, min loss: 1.73455e-06\n",
      "Epoch: 614900, elapsed: 1.14e+01, train loss: 1.72934e-06, val loss: 2.46129e-06, min loss: 1.72934e-06\n",
      "Epoch: 615000, elapsed: 1.12e+01, train loss: 1.77606e-06, val loss: 2.48297e-06, min loss: 1.72934e-06\n",
      "Epoch: 615100, elapsed: 1.32e+01, train loss: 7.26335e-06, val loss: 5.88410e-06, min loss: 1.72934e-06\n",
      "Epoch: 615200, elapsed: 1.12e+01, train loss: 1.93453e-06, val loss: 2.73266e-06, min loss: 1.72934e-06\n",
      "Epoch: 615300, elapsed: 1.11e+01, train loss: 8.08931e-06, val loss: 8.38573e-06, min loss: 1.72934e-06\n",
      "Epoch: 615400, elapsed: 1.11e+01, train loss: 1.81420e-06, val loss: 2.66226e-06, min loss: 1.72934e-06\n",
      "Epoch: 615500, elapsed: 1.11e+01, train loss: 1.72403e-06, val loss: 2.47081e-06, min loss: 1.72403e-06\n",
      "Epoch: 615600, elapsed: 1.11e+01, train loss: 1.74115e-06, val loss: 2.51233e-06, min loss: 1.72403e-06\n",
      "Epoch: 615700, elapsed: 1.09e+01, train loss: 1.74315e-06, val loss: 2.48481e-06, min loss: 1.72403e-06\n",
      "Epoch: 615800, elapsed: 1.08e+01, train loss: 2.71431e-06, val loss: 3.02801e-06, min loss: 1.72403e-06\n",
      "Epoch: 615900, elapsed: 1.29e+01, train loss: 1.77154e-06, val loss: 2.50848e-06, min loss: 1.72403e-06\n",
      "Epoch: 616000, elapsed: 1.13e+01, train loss: 1.75229e-06, val loss: 2.47518e-06, min loss: 1.72403e-06\n",
      "Epoch: 616100, elapsed: 1.14e+01, train loss: 1.77595e-06, val loss: 2.47930e-06, min loss: 1.72403e-06\n",
      "Epoch: 616200, elapsed: 1.13e+01, train loss: 1.80756e-06, val loss: 2.60264e-06, min loss: 1.72403e-06\n",
      "Epoch: 616300, elapsed: 1.12e+01, train loss: 1.79406e-06, val loss: 2.47351e-06, min loss: 1.72403e-06\n",
      "Epoch: 616400, elapsed: 1.13e+01, train loss: 1.89143e-06, val loss: 2.69142e-06, min loss: 1.72403e-06\n",
      "Epoch: 616500, elapsed: 1.13e+01, train loss: 7.21585e-06, val loss: 8.99776e-06, min loss: 1.72403e-06\n",
      "Epoch: 616600, elapsed: 1.13e+01, train loss: 1.73419e-06, val loss: 2.52479e-06, min loss: 1.72403e-06\n",
      "Epoch: 616700, elapsed: 1.14e+01, train loss: 1.73155e-06, val loss: 2.46397e-06, min loss: 1.72403e-06\n",
      "Epoch: 616800, elapsed: 1.14e+01, train loss: 1.72415e-06, val loss: 2.45804e-06, min loss: 1.72403e-06\n",
      "Epoch: 616900, elapsed: 1.10e+01, train loss: 1.88321e-06, val loss: 2.70824e-06, min loss: 1.72403e-06\n",
      "Epoch: 617000, elapsed: 1.10e+01, train loss: 7.11384e-06, val loss: 7.34423e-06, min loss: 1.72403e-06\n",
      "Epoch: 617100, elapsed: 1.33e+01, train loss: 1.78701e-06, val loss: 2.62991e-06, min loss: 1.72403e-06\n",
      "Epoch: 617200, elapsed: 1.13e+01, train loss: 1.72064e-06, val loss: 2.46351e-06, min loss: 1.72064e-06\n",
      "Epoch: 617300, elapsed: 1.13e+01, train loss: 2.43694e-06, val loss: 3.02025e-06, min loss: 1.72064e-06\n",
      "Epoch: 617400, elapsed: 1.13e+01, train loss: 3.52403e-06, val loss: 4.40964e-06, min loss: 1.72064e-06\n",
      "Epoch: 617500, elapsed: 1.11e+01, train loss: 5.86670e-06, val loss: 7.14298e-06, min loss: 1.72064e-06\n",
      "Epoch: 617600, elapsed: 1.15e+01, train loss: 1.73034e-06, val loss: 2.48275e-06, min loss: 1.72064e-06\n",
      "Epoch: 617700, elapsed: 1.14e+01, train loss: 1.72174e-06, val loss: 2.45575e-06, min loss: 1.72064e-06\n",
      "Epoch: 617800, elapsed: 1.12e+01, train loss: 2.24851e-06, val loss: 2.52134e-06, min loss: 1.72064e-06\n",
      "Epoch: 617900, elapsed: 1.11e+01, train loss: 1.72290e-06, val loss: 2.46137e-06, min loss: 1.72064e-06\n",
      "Epoch: 618000, elapsed: 1.11e+01, train loss: 1.72946e-06, val loss: 2.49275e-06, min loss: 1.72064e-06\n",
      "Epoch: 618100, elapsed: 1.11e+01, train loss: 1.88486e-06, val loss: 2.56643e-06, min loss: 1.72064e-06\n",
      "Epoch: 618200, elapsed: 1.10e+01, train loss: 2.85134e-06, val loss: 3.59764e-06, min loss: 1.72064e-06\n",
      "Epoch: 618300, elapsed: 1.28e+01, train loss: 2.65540e-06, val loss: 3.06795e-06, min loss: 1.72064e-06\n",
      "Epoch: 618400, elapsed: 1.14e+01, train loss: 2.02203e-06, val loss: 2.60140e-06, min loss: 1.72064e-06\n",
      "Epoch: 618500, elapsed: 1.13e+01, train loss: 1.78376e-06, val loss: 2.48066e-06, min loss: 1.72064e-06\n",
      "Epoch: 618600, elapsed: 1.13e+01, train loss: 1.87047e-06, val loss: 2.61375e-06, min loss: 1.72064e-06\n",
      "Epoch: 618700, elapsed: 1.14e+01, train loss: 1.82428e-06, val loss: 2.65743e-06, min loss: 1.72064e-06\n",
      "Epoch: 618800, elapsed: 1.13e+01, train loss: 2.21209e-06, val loss: 2.85638e-06, min loss: 1.72064e-06\n",
      "Epoch: 618900, elapsed: 1.11e+01, train loss: 1.72525e-06, val loss: 2.47566e-06, min loss: 1.72064e-06\n",
      "Epoch: 619000, elapsed: 1.11e+01, train loss: 1.73627e-06, val loss: 2.47838e-06, min loss: 1.72064e-06\n",
      "Epoch: 619100, elapsed: 1.11e+01, train loss: 2.05822e-06, val loss: 2.56315e-06, min loss: 1.72064e-06\n",
      "Epoch: 619200, elapsed: 1.12e+01, train loss: 1.87170e-06, val loss: 2.84858e-06, min loss: 1.72064e-06\n",
      "Epoch: 619300, elapsed: 1.13e+01, train loss: 5.86888e-06, val loss: 6.60058e-06, min loss: 1.72064e-06\n",
      "Epoch: 619400, elapsed: 1.11e+01, train loss: 2.55426e-06, val loss: 3.38877e-06, min loss: 1.72064e-06\n",
      "Epoch: 619500, elapsed: 1.08e+01, train loss: 1.92630e-06, val loss: 2.52178e-06, min loss: 1.72064e-06\n",
      "Epoch: 619600, elapsed: 1.29e+01, train loss: 1.73412e-06, val loss: 2.45021e-06, min loss: 1.72064e-06\n",
      "Epoch: 619700, elapsed: 1.13e+01, train loss: 1.83541e-06, val loss: 2.64551e-06, min loss: 1.72064e-06\n",
      "Epoch: 619800, elapsed: 1.13e+01, train loss: 5.87827e-06, val loss: 6.34757e-06, min loss: 1.72064e-06\n",
      "Epoch: 619900, elapsed: 1.12e+01, train loss: 2.45711e-06, val loss: 3.31590e-06, min loss: 1.72064e-06\n",
      "Epoch: 620000, elapsed: 1.12e+01, train loss: 2.35543e-06, val loss: 2.81914e-06, min loss: 1.72064e-06\n",
      "Epoch: 620100, elapsed: 1.32e+01, train loss: 4.25562e-06, val loss: 5.62753e-06, min loss: 1.72064e-06\n",
      "Epoch: 620200, elapsed: 1.11e+01, train loss: 1.76344e-06, val loss: 2.53762e-06, min loss: 1.72064e-06\n",
      "Epoch: 620300, elapsed: 1.12e+01, train loss: 1.76208e-06, val loss: 2.52961e-06, min loss: 1.72064e-06\n",
      "Epoch: 620400, elapsed: 1.13e+01, train loss: 1.74549e-06, val loss: 2.55837e-06, min loss: 1.72064e-06\n",
      "Epoch: 620500, elapsed: 1.12e+01, train loss: 1.71749e-06, val loss: 2.47290e-06, min loss: 1.71749e-06\n",
      "Epoch: 620600, elapsed: 1.11e+01, train loss: 1.75713e-06, val loss: 2.57522e-06, min loss: 1.71749e-06\n",
      "Epoch: 620700, elapsed: 1.28e+01, train loss: 2.62215e-06, val loss: 3.15948e-06, min loss: 1.71749e-06\n",
      "Epoch: 620800, elapsed: 1.13e+01, train loss: 2.42284e-06, val loss: 2.87206e-06, min loss: 1.71749e-06\n",
      "Epoch: 620900, elapsed: 1.15e+01, train loss: 8.42070e-06, val loss: 9.30075e-06, min loss: 1.71749e-06\n",
      "Epoch: 621000, elapsed: 1.14e+01, train loss: 1.79333e-06, val loss: 2.64717e-06, min loss: 1.71749e-06\n",
      "Epoch: 621100, elapsed: 1.12e+01, train loss: 1.72410e-06, val loss: 2.49217e-06, min loss: 1.71749e-06\n",
      "Epoch: 621200, elapsed: 1.13e+01, train loss: 1.73075e-06, val loss: 2.46233e-06, min loss: 1.71749e-06\n",
      "Epoch: 621300, elapsed: 1.12e+01, train loss: 1.73230e-06, val loss: 2.45642e-06, min loss: 1.71749e-06\n",
      "Epoch: 621400, elapsed: 1.11e+01, train loss: 1.72960e-06, val loss: 2.49296e-06, min loss: 1.71749e-06\n",
      "Epoch: 621500, elapsed: 1.12e+01, train loss: 1.73088e-06, val loss: 2.46820e-06, min loss: 1.71749e-06\n",
      "Epoch: 621600, elapsed: 1.13e+01, train loss: 2.24120e-06, val loss: 2.82860e-06, min loss: 1.71749e-06\n",
      "Epoch: 621700, elapsed: 1.12e+01, train loss: 9.07522e-06, val loss: 9.48005e-06, min loss: 1.71749e-06\n",
      "Epoch: 621800, elapsed: 1.13e+01, train loss: 1.77470e-06, val loss: 2.57353e-06, min loss: 1.71749e-06\n",
      "Epoch: 621900, elapsed: 1.10e+01, train loss: 2.13380e-06, val loss: 2.93042e-06, min loss: 1.71749e-06\n",
      "Epoch: 622000, elapsed: 1.32e+01, train loss: 2.06479e-06, val loss: 2.99754e-06, min loss: 1.71749e-06\n",
      "Epoch: 622100, elapsed: 1.14e+01, train loss: 1.92613e-06, val loss: 2.67246e-06, min loss: 1.71749e-06\n",
      "Epoch: 622200, elapsed: 1.14e+01, train loss: 1.92476e-06, val loss: 2.72898e-06, min loss: 1.71749e-06\n",
      "Epoch: 622300, elapsed: 1.13e+01, train loss: 1.71712e-06, val loss: 2.44593e-06, min loss: 1.71712e-06\n",
      "Epoch: 622400, elapsed: 1.16e+01, train loss: 1.80126e-06, val loss: 2.60585e-06, min loss: 1.71712e-06\n",
      "Epoch: 622500, elapsed: 1.13e+01, train loss: 1.72247e-06, val loss: 2.46601e-06, min loss: 1.71712e-06\n",
      "Epoch: 622600, elapsed: 1.13e+01, train loss: 1.78147e-06, val loss: 2.48414e-06, min loss: 1.71712e-06\n",
      "Epoch: 622700, elapsed: 1.13e+01, train loss: 2.63564e-06, val loss: 3.68668e-06, min loss: 1.71712e-06\n",
      "Epoch: 622800, elapsed: 1.12e+01, train loss: 2.08147e-06, val loss: 2.71800e-06, min loss: 1.71712e-06\n",
      "Epoch: 622900, elapsed: 1.11e+01, train loss: 1.83573e-06, val loss: 2.55904e-06, min loss: 1.71712e-06\n",
      "Epoch: 623000, elapsed: 1.13e+01, train loss: 1.89686e-06, val loss: 2.81805e-06, min loss: 1.71712e-06\n",
      "Epoch: 623100, elapsed: 1.10e+01, train loss: 1.72768e-06, val loss: 2.52391e-06, min loss: 1.71712e-06\n",
      "Epoch: 623200, elapsed: 1.29e+01, train loss: 1.77697e-06, val loss: 2.57970e-06, min loss: 1.71712e-06\n",
      "Epoch: 623300, elapsed: 1.13e+01, train loss: 1.74467e-06, val loss: 2.55413e-06, min loss: 1.71712e-06\n",
      "Epoch: 623400, elapsed: 1.13e+01, train loss: 1.71227e-06, val loss: 2.47566e-06, min loss: 1.71227e-06\n",
      "Epoch: 623500, elapsed: 1.14e+01, train loss: 1.71524e-06, val loss: 2.45198e-06, min loss: 1.71227e-06\n",
      "Epoch: 623600, elapsed: 1.14e+01, train loss: 1.79081e-06, val loss: 2.46714e-06, min loss: 1.71227e-06\n",
      "Epoch: 623700, elapsed: 1.13e+01, train loss: 1.80203e-06, val loss: 2.59223e-06, min loss: 1.71227e-06\n",
      "Epoch: 623800, elapsed: 1.13e+01, train loss: 1.78450e-06, val loss: 2.52820e-06, min loss: 1.71227e-06\n",
      "Epoch: 623900, elapsed: 1.12e+01, train loss: 1.77494e-06, val loss: 2.48863e-06, min loss: 1.71227e-06\n",
      "Epoch: 624000, elapsed: 1.11e+01, train loss: 1.72571e-06, val loss: 2.43372e-06, min loss: 1.71227e-06\n",
      "Epoch: 624100, elapsed: 1.13e+01, train loss: 1.71677e-06, val loss: 2.46138e-06, min loss: 1.71227e-06\n",
      "Epoch: 624200, elapsed: 1.14e+01, train loss: 1.76957e-06, val loss: 2.56856e-06, min loss: 1.71227e-06\n",
      "Epoch: 624300, elapsed: 1.11e+01, train loss: 2.06174e-06, val loss: 2.89852e-06, min loss: 1.71227e-06\n",
      "Epoch: 624400, elapsed: 1.09e+01, train loss: 1.71017e-06, val loss: 2.45022e-06, min loss: 1.71017e-06\n",
      "Epoch: 624500, elapsed: 1.30e+01, train loss: 1.71369e-06, val loss: 2.46301e-06, min loss: 1.71017e-06\n",
      "Epoch: 624600, elapsed: 1.14e+01, train loss: 1.73708e-06, val loss: 2.52485e-06, min loss: 1.71017e-06\n",
      "Epoch: 624700, elapsed: 1.12e+01, train loss: 1.73170e-06, val loss: 2.52807e-06, min loss: 1.71017e-06\n",
      "Epoch: 624800, elapsed: 1.13e+01, train loss: 1.72280e-06, val loss: 2.48511e-06, min loss: 1.71017e-06\n",
      "Epoch: 624900, elapsed: 1.13e+01, train loss: 1.80351e-06, val loss: 2.47528e-06, min loss: 1.71017e-06\n",
      "Epoch: 625000, elapsed: 1.14e+01, train loss: 2.16459e-06, val loss: 2.55276e-06, min loss: 1.71017e-06\n",
      "Epoch: 625100, elapsed: 1.32e+01, train loss: 3.50490e-06, val loss: 3.92784e-06, min loss: 1.71017e-06\n",
      "Epoch: 625200, elapsed: 1.12e+01, train loss: 6.76308e-06, val loss: 6.34893e-06, min loss: 1.71017e-06\n",
      "Epoch: 625300, elapsed: 1.12e+01, train loss: 2.23000e-06, val loss: 2.87365e-06, min loss: 1.71017e-06\n",
      "Epoch: 625400, elapsed: 1.11e+01, train loss: 1.70572e-06, val loss: 2.45264e-06, min loss: 1.70572e-06\n",
      "Epoch: 625500, elapsed: 1.12e+01, train loss: 2.18365e-06, val loss: 3.94856e-06, min loss: 1.70572e-06\n",
      "Epoch: 625600, elapsed: 1.11e+01, train loss: 1.78855e-06, val loss: 2.48253e-06, min loss: 1.70572e-06\n",
      "Epoch: 625700, elapsed: 1.29e+01, train loss: 1.70840e-06, val loss: 2.44248e-06, min loss: 1.70572e-06\n",
      "Epoch: 625800, elapsed: 1.13e+01, train loss: 1.75589e-06, val loss: 2.48389e-06, min loss: 1.70572e-06\n",
      "Epoch: 625900, elapsed: 1.11e+01, train loss: 2.33811e-06, val loss: 2.93421e-06, min loss: 1.70572e-06\n",
      "Epoch: 626000, elapsed: 1.10e+01, train loss: 1.97035e-06, val loss: 2.77644e-06, min loss: 1.70572e-06\n",
      "Epoch: 626100, elapsed: 1.12e+01, train loss: 2.37679e-06, val loss: 2.70538e-06, min loss: 1.70572e-06\n",
      "Epoch: 626200, elapsed: 1.12e+01, train loss: 1.70394e-06, val loss: 2.44619e-06, min loss: 1.70394e-06\n",
      "Epoch: 626300, elapsed: 1.10e+01, train loss: 1.72785e-06, val loss: 2.49518e-06, min loss: 1.70394e-06\n",
      "Epoch: 626400, elapsed: 1.13e+01, train loss: 2.78471e-06, val loss: 3.17305e-06, min loss: 1.70394e-06\n",
      "Epoch: 626500, elapsed: 1.13e+01, train loss: 1.90846e-06, val loss: 2.66035e-06, min loss: 1.70394e-06\n",
      "Epoch: 626600, elapsed: 1.10e+01, train loss: 1.78786e-06, val loss: 2.60861e-06, min loss: 1.70394e-06\n",
      "Epoch: 626700, elapsed: 1.11e+01, train loss: 1.71536e-06, val loss: 2.45194e-06, min loss: 1.70394e-06\n",
      "Epoch: 626800, elapsed: 1.09e+01, train loss: 1.71347e-06, val loss: 2.48488e-06, min loss: 1.70394e-06\n",
      "Epoch: 626900, elapsed: 1.31e+01, train loss: 1.75054e-06, val loss: 2.54258e-06, min loss: 1.70394e-06\n",
      "Epoch: 627000, elapsed: 1.14e+01, train loss: 2.00615e-06, val loss: 2.84644e-06, min loss: 1.70394e-06\n",
      "Epoch: 627100, elapsed: 1.15e+01, train loss: 2.08685e-06, val loss: 2.64598e-06, min loss: 1.70394e-06\n",
      "Epoch: 627200, elapsed: 1.13e+01, train loss: 1.71471e-06, val loss: 2.48026e-06, min loss: 1.70394e-06\n",
      "Epoch: 627300, elapsed: 1.14e+01, train loss: 2.74403e-06, val loss: 2.84227e-06, min loss: 1.70394e-06\n",
      "Epoch: 627400, elapsed: 1.12e+01, train loss: 2.32926e-06, val loss: 3.08841e-06, min loss: 1.70394e-06\n",
      "Epoch: 627500, elapsed: 1.12e+01, train loss: 1.70345e-06, val loss: 2.43663e-06, min loss: 1.70345e-06\n",
      "Epoch: 627600, elapsed: 1.14e+01, train loss: 1.70978e-06, val loss: 2.47110e-06, min loss: 1.70345e-06\n",
      "Epoch: 627700, elapsed: 1.12e+01, train loss: 1.77130e-06, val loss: 2.48749e-06, min loss: 1.70345e-06\n",
      "Epoch: 627800, elapsed: 1.11e+01, train loss: 1.73086e-06, val loss: 2.47142e-06, min loss: 1.70345e-06\n",
      "Epoch: 627900, elapsed: 1.14e+01, train loss: 1.76767e-06, val loss: 2.56587e-06, min loss: 1.70345e-06\n",
      "Epoch: 628000, elapsed: 1.10e+01, train loss: 1.79171e-06, val loss: 2.46172e-06, min loss: 1.70345e-06\n",
      "Epoch: 628100, elapsed: 1.09e+01, train loss: 1.92394e-06, val loss: 2.75997e-06, min loss: 1.70345e-06\n",
      "Epoch: 628200, elapsed: 1.32e+01, train loss: 1.73604e-06, val loss: 2.47035e-06, min loss: 1.70345e-06\n",
      "Epoch: 628300, elapsed: 1.15e+01, train loss: 1.82689e-06, val loss: 2.69095e-06, min loss: 1.70345e-06\n",
      "Epoch: 628400, elapsed: 1.14e+01, train loss: 2.78850e-06, val loss: 3.93694e-06, min loss: 1.70345e-06\n",
      "Epoch: 628500, elapsed: 1.14e+01, train loss: 1.70674e-06, val loss: 2.43296e-06, min loss: 1.70345e-06\n",
      "Epoch: 628600, elapsed: 1.14e+01, train loss: 1.89292e-06, val loss: 2.81299e-06, min loss: 1.70345e-06\n",
      "Epoch: 628700, elapsed: 1.14e+01, train loss: 2.34035e-06, val loss: 2.51224e-06, min loss: 1.70345e-06\n",
      "Epoch: 628800, elapsed: 1.15e+01, train loss: 2.44134e-06, val loss: 3.08301e-06, min loss: 1.70345e-06\n",
      "Epoch: 628900, elapsed: 1.15e+01, train loss: 1.83582e-06, val loss: 2.54646e-06, min loss: 1.70345e-06\n",
      "Epoch: 629000, elapsed: 1.13e+01, train loss: 3.82833e-06, val loss: 4.28948e-06, min loss: 1.70345e-06\n",
      "Epoch: 629100, elapsed: 1.13e+01, train loss: 7.07634e-06, val loss: 8.31028e-06, min loss: 1.70345e-06\n",
      "Epoch: 629200, elapsed: 1.12e+01, train loss: 2.20771e-06, val loss: 2.83647e-06, min loss: 1.70345e-06\n",
      "Epoch: 629300, elapsed: 1.12e+01, train loss: 1.70618e-06, val loss: 2.44087e-06, min loss: 1.70345e-06\n",
      "Epoch: 629400, elapsed: 1.29e+01, train loss: 1.70503e-06, val loss: 2.53356e-06, min loss: 1.70345e-06\n",
      "Epoch: 629500, elapsed: 1.15e+01, train loss: 1.72355e-06, val loss: 2.52400e-06, min loss: 1.70345e-06\n",
      "Epoch: 629600, elapsed: 1.11e+01, train loss: 1.70838e-06, val loss: 2.43591e-06, min loss: 1.70345e-06\n",
      "Epoch: 629700, elapsed: 1.11e+01, train loss: 1.74531e-06, val loss: 2.52530e-06, min loss: 1.70345e-06\n",
      "Epoch: 629800, elapsed: 1.13e+01, train loss: 2.54650e-06, val loss: 2.91951e-06, min loss: 1.70345e-06\n",
      "Epoch: 629900, elapsed: 1.14e+01, train loss: 6.68930e-06, val loss: 7.85289e-06, min loss: 1.70345e-06\n",
      "Epoch: 630000, elapsed: 1.12e+01, train loss: 3.50714e-06, val loss: 4.49427e-06, min loss: 1.70345e-06\n",
      "Epoch: 630100, elapsed: 1.33e+01, train loss: 3.40122e-06, val loss: 4.43924e-06, min loss: 1.70345e-06\n",
      "Epoch: 630200, elapsed: 1.14e+01, train loss: 2.65044e-06, val loss: 2.90616e-06, min loss: 1.70345e-06\n",
      "Epoch: 630300, elapsed: 1.12e+01, train loss: 2.46705e-06, val loss: 3.19327e-06, min loss: 1.70345e-06\n",
      "Epoch: 630400, elapsed: 1.11e+01, train loss: 2.40677e-06, val loss: 3.21212e-06, min loss: 1.70345e-06\n",
      "Epoch: 630500, elapsed: 1.12e+01, train loss: 1.75843e-06, val loss: 2.52064e-06, min loss: 1.70345e-06\n",
      "Epoch: 630600, elapsed: 1.30e+01, train loss: 2.34398e-06, val loss: 2.92587e-06, min loss: 1.70345e-06\n",
      "Epoch: 630700, elapsed: 1.14e+01, train loss: 1.69510e-06, val loss: 2.44176e-06, min loss: 1.69510e-06\n",
      "Epoch: 630800, elapsed: 1.13e+01, train loss: 1.70151e-06, val loss: 2.43818e-06, min loss: 1.69510e-06\n",
      "Epoch: 630900, elapsed: 1.13e+01, train loss: 1.86302e-06, val loss: 2.53757e-06, min loss: 1.69510e-06\n",
      "Epoch: 631000, elapsed: 1.13e+01, train loss: 2.72762e-06, val loss: 3.72170e-06, min loss: 1.69510e-06\n",
      "Epoch: 631100, elapsed: 1.11e+01, train loss: 1.70788e-06, val loss: 2.42804e-06, min loss: 1.69510e-06\n",
      "Epoch: 631200, elapsed: 1.12e+01, train loss: 1.69852e-06, val loss: 2.43052e-06, min loss: 1.69510e-06\n",
      "Epoch: 631300, elapsed: 1.12e+01, train loss: 1.73818e-06, val loss: 2.52489e-06, min loss: 1.69510e-06\n",
      "Epoch: 631400, elapsed: 1.14e+01, train loss: 2.96220e-06, val loss: 3.70790e-06, min loss: 1.69510e-06\n",
      "Epoch: 631500, elapsed: 1.11e+01, train loss: 2.87445e-06, val loss: 3.56942e-06, min loss: 1.69510e-06\n",
      "Epoch: 631600, elapsed: 1.12e+01, train loss: 2.63292e-06, val loss: 3.45921e-06, min loss: 1.69510e-06\n",
      "Epoch: 631700, elapsed: 1.12e+01, train loss: 2.82267e-06, val loss: 2.89281e-06, min loss: 1.69510e-06\n",
      "Epoch: 631800, elapsed: 1.11e+01, train loss: 1.97892e-06, val loss: 2.72582e-06, min loss: 1.69510e-06\n",
      "Epoch: 631900, elapsed: 1.32e+01, train loss: 2.01989e-06, val loss: 2.60529e-06, min loss: 1.69510e-06\n",
      "Epoch: 632000, elapsed: 1.13e+01, train loss: 3.31857e-06, val loss: 4.94498e-06, min loss: 1.69510e-06\n",
      "Epoch: 632100, elapsed: 1.13e+01, train loss: 3.03094e-06, val loss: 3.79187e-06, min loss: 1.69510e-06\n",
      "Epoch: 632200, elapsed: 1.12e+01, train loss: 1.75346e-06, val loss: 2.59509e-06, min loss: 1.69510e-06\n",
      "Epoch: 632300, elapsed: 1.13e+01, train loss: 2.00563e-06, val loss: 2.64611e-06, min loss: 1.69510e-06\n",
      "Epoch: 632400, elapsed: 1.13e+01, train loss: 9.38721e-06, val loss: 9.65066e-06, min loss: 1.69510e-06\n",
      "Epoch: 632500, elapsed: 1.13e+01, train loss: 1.69350e-06, val loss: 2.42800e-06, min loss: 1.69350e-06\n",
      "Epoch: 632600, elapsed: 1.13e+01, train loss: 1.70125e-06, val loss: 2.46083e-06, min loss: 1.69350e-06\n",
      "Epoch: 632700, elapsed: 1.12e+01, train loss: 1.71093e-06, val loss: 2.48607e-06, min loss: 1.69350e-06\n",
      "Epoch: 632800, elapsed: 1.11e+01, train loss: 1.75699e-06, val loss: 2.45673e-06, min loss: 1.69350e-06\n",
      "Epoch: 632900, elapsed: 1.11e+01, train loss: 2.38927e-06, val loss: 3.12609e-06, min loss: 1.69350e-06\n",
      "Epoch: 633000, elapsed: 1.10e+01, train loss: 2.44143e-06, val loss: 3.18405e-06, min loss: 1.69350e-06\n",
      "Epoch: 633100, elapsed: 1.31e+01, train loss: 1.69235e-06, val loss: 2.42938e-06, min loss: 1.69235e-06\n",
      "Epoch: 633200, elapsed: 1.13e+01, train loss: 1.69222e-06, val loss: 2.42627e-06, min loss: 1.69222e-06\n",
      "Epoch: 633300, elapsed: 1.14e+01, train loss: 2.39547e-06, val loss: 3.63200e-06, min loss: 1.69222e-06\n",
      "Epoch: 633400, elapsed: 1.14e+01, train loss: 4.01304e-06, val loss: 5.06076e-06, min loss: 1.69222e-06\n",
      "Epoch: 633500, elapsed: 1.13e+01, train loss: 1.77757e-06, val loss: 2.47300e-06, min loss: 1.69222e-06\n",
      "Epoch: 633600, elapsed: 1.12e+01, train loss: 1.80472e-06, val loss: 2.52572e-06, min loss: 1.69222e-06\n",
      "Epoch: 633700, elapsed: 1.11e+01, train loss: 1.72448e-06, val loss: 2.52634e-06, min loss: 1.69222e-06\n",
      "Epoch: 633800, elapsed: 1.11e+01, train loss: 2.14777e-06, val loss: 2.89740e-06, min loss: 1.69222e-06\n",
      "Epoch: 633900, elapsed: 1.11e+01, train loss: 1.74539e-06, val loss: 2.56688e-06, min loss: 1.69222e-06\n",
      "Epoch: 634000, elapsed: 1.11e+01, train loss: 1.71789e-06, val loss: 2.47109e-06, min loss: 1.69222e-06\n",
      "Epoch: 634100, elapsed: 1.11e+01, train loss: 2.52209e-06, val loss: 3.33407e-06, min loss: 1.69222e-06\n",
      "Epoch: 634200, elapsed: 1.11e+01, train loss: 2.30667e-06, val loss: 3.29884e-06, min loss: 1.69222e-06\n",
      "Epoch: 634300, elapsed: 1.10e+01, train loss: 6.63900e-06, val loss: 8.48178e-06, min loss: 1.69222e-06\n",
      "Epoch: 634400, elapsed: 1.31e+01, train loss: 1.84566e-06, val loss: 2.78375e-06, min loss: 1.69222e-06\n",
      "Epoch: 634500, elapsed: 1.15e+01, train loss: 1.69774e-06, val loss: 2.42718e-06, min loss: 1.69222e-06\n",
      "Epoch: 634600, elapsed: 1.15e+01, train loss: 1.79436e-06, val loss: 2.48896e-06, min loss: 1.69222e-06\n",
      "Epoch: 634700, elapsed: 1.13e+01, train loss: 3.75705e-06, val loss: 4.46552e-06, min loss: 1.69222e-06\n",
      "Epoch: 634800, elapsed: 1.14e+01, train loss: 3.94506e-06, val loss: 4.46621e-06, min loss: 1.69222e-06\n",
      "Epoch: 634900, elapsed: 1.15e+01, train loss: 6.20598e-06, val loss: 8.21666e-06, min loss: 1.69222e-06\n",
      "Epoch: 635000, elapsed: 1.15e+01, train loss: 1.70064e-06, val loss: 2.43881e-06, min loss: 1.69222e-06\n",
      "Epoch: 635100, elapsed: 1.32e+01, train loss: 1.69213e-06, val loss: 2.45244e-06, min loss: 1.69213e-06\n",
      "Epoch: 635200, elapsed: 1.13e+01, train loss: 1.69091e-06, val loss: 2.42259e-06, min loss: 1.69091e-06\n",
      "Epoch: 635300, elapsed: 1.12e+01, train loss: 1.70539e-06, val loss: 2.46446e-06, min loss: 1.69091e-06\n",
      "Epoch: 635400, elapsed: 1.14e+01, train loss: 2.38259e-06, val loss: 2.80634e-06, min loss: 1.69091e-06\n",
      "Epoch: 635500, elapsed: 1.12e+01, train loss: 2.02242e-06, val loss: 2.61870e-06, min loss: 1.69091e-06\n",
      "Epoch: 635600, elapsed: 1.30e+01, train loss: 2.33954e-06, val loss: 2.89162e-06, min loss: 1.69091e-06\n",
      "Epoch: 635700, elapsed: 1.13e+01, train loss: 1.68613e-06, val loss: 2.42903e-06, min loss: 1.68613e-06\n",
      "Epoch: 635800, elapsed: 1.11e+01, train loss: 1.69585e-06, val loss: 2.41887e-06, min loss: 1.68613e-06\n",
      "Epoch: 635900, elapsed: 1.10e+01, train loss: 1.82414e-06, val loss: 2.41568e-06, min loss: 1.68613e-06\n",
      "Epoch: 636000, elapsed: 1.12e+01, train loss: 1.72931e-06, val loss: 2.46309e-06, min loss: 1.68613e-06\n",
      "Epoch: 636100, elapsed: 1.10e+01, train loss: 1.69009e-06, val loss: 2.42906e-06, min loss: 1.68613e-06\n",
      "Epoch: 636200, elapsed: 1.11e+01, train loss: 3.63082e-06, val loss: 5.32844e-06, min loss: 1.68613e-06\n",
      "Epoch: 636300, elapsed: 1.10e+01, train loss: 1.68512e-06, val loss: 2.43458e-06, min loss: 1.68512e-06\n",
      "Epoch: 636400, elapsed: 1.12e+01, train loss: 1.84719e-06, val loss: 2.54050e-06, min loss: 1.68512e-06\n",
      "Epoch: 636500, elapsed: 1.10e+01, train loss: 2.97756e-06, val loss: 4.19990e-06, min loss: 1.68512e-06\n",
      "Epoch: 636600, elapsed: 1.10e+01, train loss: 2.31655e-06, val loss: 3.13375e-06, min loss: 1.68512e-06\n",
      "Epoch: 636700, elapsed: 1.09e+01, train loss: 2.24598e-06, val loss: 3.33163e-06, min loss: 1.68512e-06\n",
      "Epoch: 636800, elapsed: 1.08e+01, train loss: 7.01012e-06, val loss: 4.74322e-06, min loss: 1.68512e-06\n",
      "Epoch: 636900, elapsed: 1.31e+01, train loss: 2.55610e-06, val loss: 3.15030e-06, min loss: 1.68512e-06\n",
      "Epoch: 637000, elapsed: 1.14e+01, train loss: 1.69517e-06, val loss: 2.43687e-06, min loss: 1.68512e-06\n",
      "Epoch: 637100, elapsed: 1.11e+01, train loss: 1.72066e-06, val loss: 2.42162e-06, min loss: 1.68512e-06\n",
      "Epoch: 637200, elapsed: 1.12e+01, train loss: 2.09642e-06, val loss: 2.72040e-06, min loss: 1.68512e-06\n",
      "Epoch: 637300, elapsed: 1.12e+01, train loss: 1.73189e-06, val loss: 2.42829e-06, min loss: 1.68512e-06\n",
      "Epoch: 637400, elapsed: 1.12e+01, train loss: 2.81768e-06, val loss: 3.19716e-06, min loss: 1.68512e-06\n",
      "Epoch: 637500, elapsed: 1.11e+01, train loss: 2.38927e-06, val loss: 3.09890e-06, min loss: 1.68512e-06\n",
      "Epoch: 637600, elapsed: 1.12e+01, train loss: 2.61858e-06, val loss: 3.00410e-06, min loss: 1.68512e-06\n",
      "Epoch: 637700, elapsed: 1.11e+01, train loss: 1.84529e-06, val loss: 2.43947e-06, min loss: 1.68512e-06\n",
      "Epoch: 637800, elapsed: 1.11e+01, train loss: 1.85286e-06, val loss: 2.67649e-06, min loss: 1.68512e-06\n",
      "Epoch: 637900, elapsed: 1.11e+01, train loss: 1.71898e-06, val loss: 2.41458e-06, min loss: 1.68512e-06\n",
      "Epoch: 638000, elapsed: 1.09e+01, train loss: 1.69612e-06, val loss: 2.45269e-06, min loss: 1.68512e-06\n",
      "Epoch: 638100, elapsed: 1.29e+01, train loss: 3.31121e-06, val loss: 4.47287e-06, min loss: 1.68512e-06\n",
      "Epoch: 638200, elapsed: 1.13e+01, train loss: 1.88003e-06, val loss: 2.70167e-06, min loss: 1.68512e-06\n",
      "Epoch: 638300, elapsed: 1.11e+01, train loss: 1.72440e-06, val loss: 2.45185e-06, min loss: 1.68512e-06\n",
      "Epoch: 638400, elapsed: 1.12e+01, train loss: 1.74006e-06, val loss: 2.45201e-06, min loss: 1.68512e-06\n",
      "Epoch: 638500, elapsed: 1.10e+01, train loss: 1.73512e-06, val loss: 2.40864e-06, min loss: 1.68512e-06\n",
      "Epoch: 638600, elapsed: 1.12e+01, train loss: 1.79790e-06, val loss: 2.61234e-06, min loss: 1.68512e-06\n",
      "Epoch: 638700, elapsed: 1.12e+01, train loss: 3.06840e-06, val loss: 2.89376e-06, min loss: 1.68512e-06\n",
      "Epoch: 638800, elapsed: 1.12e+01, train loss: 2.89603e-06, val loss: 3.61130e-06, min loss: 1.68512e-06\n",
      "Epoch: 638900, elapsed: 1.11e+01, train loss: 2.32847e-06, val loss: 2.87229e-06, min loss: 1.68512e-06\n",
      "Epoch: 639000, elapsed: 1.11e+01, train loss: 9.50423e-06, val loss: 9.92002e-06, min loss: 1.68512e-06\n",
      "Epoch: 639100, elapsed: 1.09e+01, train loss: 1.67979e-06, val loss: 2.42481e-06, min loss: 1.67979e-06\n",
      "Epoch: 639200, elapsed: 1.10e+01, train loss: 1.68312e-06, val loss: 2.43172e-06, min loss: 1.67979e-06\n",
      "Epoch: 639300, elapsed: 1.10e+01, train loss: 1.72957e-06, val loss: 2.41262e-06, min loss: 1.67979e-06\n",
      "Epoch: 639400, elapsed: 1.29e+01, train loss: 2.61483e-06, val loss: 2.72879e-06, min loss: 1.67979e-06\n",
      "Epoch: 639500, elapsed: 1.12e+01, train loss: 1.95110e-06, val loss: 2.74787e-06, min loss: 1.67979e-06\n",
      "Epoch: 639600, elapsed: 1.12e+01, train loss: 1.79715e-06, val loss: 2.43514e-06, min loss: 1.67979e-06\n",
      "Epoch: 639700, elapsed: 1.12e+01, train loss: 2.25989e-06, val loss: 3.15047e-06, min loss: 1.67979e-06\n",
      "Epoch: 639800, elapsed: 1.12e+01, train loss: 1.85777e-06, val loss: 2.57940e-06, min loss: 1.67979e-06\n",
      "Epoch: 639900, elapsed: 1.13e+01, train loss: 1.70887e-06, val loss: 2.40671e-06, min loss: 1.67979e-06\n",
      "Epoch: 640000, elapsed: 1.11e+01, train loss: 1.78332e-06, val loss: 2.49350e-06, min loss: 1.67979e-06\n",
      "Epoch: 640100, elapsed: 1.32e+01, train loss: 1.69589e-06, val loss: 2.42143e-06, min loss: 1.67979e-06\n",
      "Epoch: 640200, elapsed: 1.11e+01, train loss: 1.68566e-06, val loss: 2.44995e-06, min loss: 1.67979e-06\n",
      "Epoch: 640300, elapsed: 1.11e+01, train loss: 8.99030e-06, val loss: 1.01187e-05, min loss: 1.67979e-06\n",
      "Epoch: 640400, elapsed: 1.11e+01, train loss: 1.67777e-06, val loss: 2.42908e-06, min loss: 1.67777e-06\n",
      "Epoch: 640500, elapsed: 1.09e+01, train loss: 1.79052e-06, val loss: 2.54875e-06, min loss: 1.67777e-06\n",
      "Epoch: 640600, elapsed: 1.30e+01, train loss: 6.09291e-06, val loss: 7.60289e-06, min loss: 1.67777e-06\n",
      "Epoch: 640700, elapsed: 1.12e+01, train loss: 2.62934e-06, val loss: 3.48705e-06, min loss: 1.67777e-06\n",
      "Epoch: 640800, elapsed: 1.12e+01, train loss: 1.69487e-06, val loss: 2.42283e-06, min loss: 1.67777e-06\n",
      "Epoch: 640900, elapsed: 1.13e+01, train loss: 1.74278e-06, val loss: 2.50145e-06, min loss: 1.67777e-06\n",
      "Epoch: 641000, elapsed: 1.12e+01, train loss: 1.70097e-06, val loss: 2.48436e-06, min loss: 1.67777e-06\n",
      "Epoch: 641100, elapsed: 1.10e+01, train loss: 2.00359e-06, val loss: 2.61406e-06, min loss: 1.67777e-06\n",
      "Epoch: 641200, elapsed: 1.11e+01, train loss: 1.95463e-06, val loss: 2.55993e-06, min loss: 1.67777e-06\n",
      "Epoch: 641300, elapsed: 1.12e+01, train loss: 1.69341e-06, val loss: 2.42190e-06, min loss: 1.67777e-06\n",
      "Epoch: 641400, elapsed: 1.11e+01, train loss: 1.68516e-06, val loss: 2.45112e-06, min loss: 1.67777e-06\n",
      "Epoch: 641500, elapsed: 1.10e+01, train loss: 1.79978e-06, val loss: 2.58368e-06, min loss: 1.67777e-06\n",
      "Epoch: 641600, elapsed: 1.10e+01, train loss: 1.87863e-06, val loss: 2.55185e-06, min loss: 1.67777e-06\n",
      "Epoch: 641700, elapsed: 1.09e+01, train loss: 1.68270e-06, val loss: 2.41708e-06, min loss: 1.67777e-06\n",
      "Epoch: 641800, elapsed: 1.11e+01, train loss: 1.67944e-06, val loss: 2.42078e-06, min loss: 1.67777e-06\n",
      "Epoch: 641900, elapsed: 1.29e+01, train loss: 1.67515e-06, val loss: 2.42293e-06, min loss: 1.67515e-06\n",
      "Epoch: 642000, elapsed: 1.12e+01, train loss: 1.68783e-06, val loss: 2.46890e-06, min loss: 1.67515e-06\n",
      "Epoch: 642100, elapsed: 1.14e+01, train loss: 1.69904e-06, val loss: 2.47961e-06, min loss: 1.67515e-06\n",
      "Epoch: 642200, elapsed: 1.11e+01, train loss: 1.68817e-06, val loss: 2.43978e-06, min loss: 1.67515e-06\n",
      "Epoch: 642300, elapsed: 1.12e+01, train loss: 1.67995e-06, val loss: 2.43548e-06, min loss: 1.67515e-06\n",
      "Epoch: 642400, elapsed: 1.12e+01, train loss: 1.67683e-06, val loss: 2.42681e-06, min loss: 1.67515e-06\n",
      "Epoch: 642500, elapsed: 1.13e+01, train loss: 1.69669e-06, val loss: 2.44829e-06, min loss: 1.67515e-06\n",
      "Epoch: 642600, elapsed: 1.13e+01, train loss: 1.70247e-06, val loss: 2.42671e-06, min loss: 1.67515e-06\n",
      "Epoch: 642700, elapsed: 1.12e+01, train loss: 1.85249e-06, val loss: 2.51513e-06, min loss: 1.67515e-06\n",
      "Epoch: 642800, elapsed: 1.11e+01, train loss: 1.67532e-06, val loss: 2.43246e-06, min loss: 1.67515e-06\n",
      "Epoch: 642900, elapsed: 1.11e+01, train loss: 1.67336e-06, val loss: 2.41596e-06, min loss: 1.67336e-06\n",
      "Epoch: 643000, elapsed: 1.09e+01, train loss: 6.89346e-06, val loss: 7.90631e-06, min loss: 1.67336e-06\n",
      "Epoch: 643100, elapsed: 1.10e+01, train loss: 3.13272e-06, val loss: 4.06578e-06, min loss: 1.67336e-06\n",
      "Epoch: 643200, elapsed: 1.32e+01, train loss: 2.17622e-06, val loss: 2.86555e-06, min loss: 1.67336e-06\n",
      "Epoch: 643300, elapsed: 1.12e+01, train loss: 1.68879e-06, val loss: 2.46660e-06, min loss: 1.67336e-06\n",
      "Epoch: 643400, elapsed: 1.13e+01, train loss: 1.67916e-06, val loss: 2.43362e-06, min loss: 1.67336e-06\n",
      "Epoch: 643500, elapsed: 1.12e+01, train loss: 1.68581e-06, val loss: 2.41483e-06, min loss: 1.67336e-06\n",
      "Epoch: 643600, elapsed: 1.11e+01, train loss: 2.87930e-06, val loss: 4.09565e-06, min loss: 1.67336e-06\n",
      "Epoch: 643700, elapsed: 1.12e+01, train loss: 1.67181e-06, val loss: 2.41894e-06, min loss: 1.67181e-06\n",
      "Epoch: 643800, elapsed: 1.11e+01, train loss: 1.68365e-06, val loss: 2.42143e-06, min loss: 1.67181e-06\n",
      "Epoch: 643900, elapsed: 1.12e+01, train loss: 1.69421e-06, val loss: 2.47596e-06, min loss: 1.67181e-06\n",
      "Epoch: 644000, elapsed: 1.11e+01, train loss: 1.78593e-06, val loss: 2.50904e-06, min loss: 1.67181e-06\n",
      "Epoch: 644100, elapsed: 1.12e+01, train loss: 1.98602e-06, val loss: 2.68382e-06, min loss: 1.67181e-06\n",
      "Epoch: 644200, elapsed: 1.11e+01, train loss: 1.76910e-06, val loss: 2.63672e-06, min loss: 1.67181e-06\n",
      "Epoch: 644300, elapsed: 1.11e+01, train loss: 4.52375e-06, val loss: 5.56813e-06, min loss: 1.67181e-06\n",
      "Epoch: 644400, elapsed: 1.31e+01, train loss: 1.71162e-06, val loss: 2.41940e-06, min loss: 1.67181e-06\n",
      "Epoch: 644500, elapsed: 1.14e+01, train loss: 1.67390e-06, val loss: 2.42106e-06, min loss: 1.67181e-06\n",
      "Epoch: 644600, elapsed: 1.12e+01, train loss: 1.68057e-06, val loss: 2.40506e-06, min loss: 1.67181e-06\n",
      "Epoch: 644700, elapsed: 1.12e+01, train loss: 2.42363e-06, val loss: 3.45316e-06, min loss: 1.67181e-06\n",
      "Epoch: 644800, elapsed: 1.13e+01, train loss: 2.67996e-06, val loss: 3.41802e-06, min loss: 1.67181e-06\n",
      "Epoch: 644900, elapsed: 1.12e+01, train loss: 2.00759e-06, val loss: 2.97419e-06, min loss: 1.67181e-06\n",
      "Epoch: 645000, elapsed: 1.13e+01, train loss: 1.80161e-06, val loss: 2.71811e-06, min loss: 1.67181e-06\n",
      "Epoch: 645100, elapsed: 1.32e+01, train loss: 3.93544e-06, val loss: 4.77637e-06, min loss: 1.67181e-06\n",
      "Epoch: 645200, elapsed: 1.12e+01, train loss: 1.80644e-06, val loss: 2.54203e-06, min loss: 1.67181e-06\n",
      "Epoch: 645300, elapsed: 1.12e+01, train loss: 1.70227e-06, val loss: 2.46561e-06, min loss: 1.67181e-06\n",
      "Epoch: 645400, elapsed: 1.12e+01, train loss: 3.69733e-06, val loss: 5.18684e-06, min loss: 1.67181e-06\n",
      "Epoch: 645500, elapsed: 1.12e+01, train loss: 2.49918e-06, val loss: 2.90408e-06, min loss: 1.67181e-06\n",
      "Epoch: 645600, elapsed: 1.11e+01, train loss: 5.24995e-06, val loss: 5.95872e-06, min loss: 1.67181e-06\n",
      "Epoch: 645700, elapsed: 1.30e+01, train loss: 7.05764e-06, val loss: 7.53519e-06, min loss: 1.67181e-06\n",
      "Epoch: 645800, elapsed: 1.14e+01, train loss: 7.13185e-06, val loss: 8.45960e-06, min loss: 1.67181e-06\n",
      "Epoch: 645900, elapsed: 1.13e+01, train loss: 1.92981e-06, val loss: 2.71182e-06, min loss: 1.67181e-06\n",
      "Epoch: 646000, elapsed: 1.13e+01, train loss: 1.67744e-06, val loss: 2.39231e-06, min loss: 1.67181e-06\n",
      "Epoch: 646100, elapsed: 1.12e+01, train loss: 1.68533e-06, val loss: 2.45613e-06, min loss: 1.67181e-06\n",
      "Epoch: 646200, elapsed: 1.13e+01, train loss: 1.68002e-06, val loss: 2.42147e-06, min loss: 1.67181e-06\n",
      "Epoch: 646300, elapsed: 1.12e+01, train loss: 1.67096e-06, val loss: 2.41389e-06, min loss: 1.67096e-06\n",
      "Epoch: 646400, elapsed: 1.13e+01, train loss: 1.67502e-06, val loss: 2.42945e-06, min loss: 1.67096e-06\n",
      "Epoch: 646500, elapsed: 1.13e+01, train loss: 1.81253e-06, val loss: 2.51428e-06, min loss: 1.67096e-06\n",
      "Epoch: 646600, elapsed: 1.12e+01, train loss: 1.79291e-06, val loss: 2.70523e-06, min loss: 1.67096e-06\n",
      "Epoch: 646700, elapsed: 1.12e+01, train loss: 2.38770e-06, val loss: 2.68712e-06, min loss: 1.67096e-06\n",
      "Epoch: 646800, elapsed: 1.12e+01, train loss: 4.15107e-06, val loss: 5.08821e-06, min loss: 1.67096e-06\n",
      "Epoch: 646900, elapsed: 1.10e+01, train loss: 3.63522e-06, val loss: 5.01000e-06, min loss: 1.67096e-06\n",
      "Epoch: 647000, elapsed: 1.33e+01, train loss: 2.12765e-06, val loss: 2.95354e-06, min loss: 1.67096e-06\n",
      "Epoch: 647100, elapsed: 1.13e+01, train loss: 1.67173e-06, val loss: 2.41227e-06, min loss: 1.67096e-06\n",
      "Epoch: 647200, elapsed: 1.13e+01, train loss: 1.68077e-06, val loss: 2.44450e-06, min loss: 1.67096e-06\n",
      "Epoch: 647300, elapsed: 1.12e+01, train loss: 1.67962e-06, val loss: 2.42712e-06, min loss: 1.67096e-06\n",
      "Epoch: 647400, elapsed: 1.11e+01, train loss: 1.68044e-06, val loss: 2.45424e-06, min loss: 1.67096e-06\n",
      "Epoch: 647500, elapsed: 1.13e+01, train loss: 1.84872e-06, val loss: 2.65279e-06, min loss: 1.67096e-06\n",
      "Epoch: 647600, elapsed: 1.12e+01, train loss: 3.44055e-06, val loss: 3.95343e-06, min loss: 1.67096e-06\n",
      "Epoch: 647700, elapsed: 1.11e+01, train loss: 1.70728e-06, val loss: 2.41228e-06, min loss: 1.67096e-06\n",
      "Epoch: 647800, elapsed: 1.12e+01, train loss: 2.37605e-06, val loss: 3.14937e-06, min loss: 1.67096e-06\n",
      "Epoch: 647900, elapsed: 1.11e+01, train loss: 5.07397e-06, val loss: 3.82852e-06, min loss: 1.67096e-06\n",
      "Epoch: 648000, elapsed: 1.09e+01, train loss: 1.67991e-06, val loss: 2.39884e-06, min loss: 1.67096e-06\n",
      "Epoch: 648100, elapsed: 1.12e+01, train loss: 1.66444e-06, val loss: 2.41044e-06, min loss: 1.66444e-06\n",
      "Epoch: 648200, elapsed: 1.30e+01, train loss: 1.69856e-06, val loss: 2.42882e-06, min loss: 1.66444e-06\n",
      "Epoch: 648300, elapsed: 1.14e+01, train loss: 1.67962e-06, val loss: 2.40956e-06, min loss: 1.66444e-06\n",
      "Epoch: 648400, elapsed: 1.13e+01, train loss: 1.66957e-06, val loss: 2.43474e-06, min loss: 1.66444e-06\n",
      "Epoch: 648500, elapsed: 1.14e+01, train loss: 1.66354e-06, val loss: 2.40886e-06, min loss: 1.66354e-06\n",
      "Epoch: 648600, elapsed: 1.12e+01, train loss: 1.68544e-06, val loss: 2.45688e-06, min loss: 1.66354e-06\n",
      "Epoch: 648700, elapsed: 1.12e+01, train loss: 1.72018e-06, val loss: 2.42550e-06, min loss: 1.66354e-06\n",
      "Epoch: 648800, elapsed: 1.12e+01, train loss: 2.00972e-06, val loss: 2.93502e-06, min loss: 1.66354e-06\n",
      "Epoch: 648900, elapsed: 1.11e+01, train loss: 4.49423e-06, val loss: 4.59942e-06, min loss: 1.66354e-06\n",
      "Epoch: 649000, elapsed: 1.12e+01, train loss: 1.67765e-06, val loss: 2.40958e-06, min loss: 1.66354e-06\n",
      "Epoch: 649100, elapsed: 1.11e+01, train loss: 1.75335e-06, val loss: 2.41214e-06, min loss: 1.66354e-06\n",
      "Epoch: 649200, elapsed: 1.12e+01, train loss: 1.92496e-06, val loss: 2.77905e-06, min loss: 1.66354e-06\n",
      "Epoch: 649300, elapsed: 1.11e+01, train loss: 2.44309e-06, val loss: 3.47397e-06, min loss: 1.66354e-06\n",
      "Epoch: 649400, elapsed: 1.11e+01, train loss: 6.06065e-06, val loss: 4.89577e-06, min loss: 1.66354e-06\n",
      "Epoch: 649500, elapsed: 1.31e+01, train loss: 2.25340e-06, val loss: 2.96669e-06, min loss: 1.66354e-06\n",
      "Epoch: 649600, elapsed: 1.13e+01, train loss: 1.66328e-06, val loss: 2.41512e-06, min loss: 1.66328e-06\n",
      "Epoch: 649700, elapsed: 1.11e+01, train loss: 1.66674e-06, val loss: 2.39917e-06, min loss: 1.66328e-06\n",
      "Epoch: 649800, elapsed: 1.14e+01, train loss: 1.82336e-06, val loss: 2.54446e-06, min loss: 1.66328e-06\n",
      "Epoch: 649900, elapsed: 1.12e+01, train loss: 2.28633e-06, val loss: 3.34460e-06, min loss: 1.66328e-06\n",
      "Epoch: 650000, elapsed: 1.11e+01, train loss: 1.66245e-06, val loss: 2.41557e-06, min loss: 1.66245e-06\n",
      "Epoch: 650100, elapsed: 1.33e+01, train loss: 1.69515e-06, val loss: 2.48539e-06, min loss: 1.66245e-06\n",
      "Epoch: 650200, elapsed: 1.12e+01, train loss: 1.77322e-06, val loss: 2.53562e-06, min loss: 1.66245e-06\n",
      "Epoch: 650300, elapsed: 1.12e+01, train loss: 1.68420e-06, val loss: 2.41273e-06, min loss: 1.66245e-06\n",
      "Epoch: 650400, elapsed: 1.11e+01, train loss: 1.67970e-06, val loss: 2.45400e-06, min loss: 1.66245e-06\n",
      "Epoch: 650500, elapsed: 1.12e+01, train loss: 1.66725e-06, val loss: 2.39805e-06, min loss: 1.66245e-06\n",
      "Epoch: 650600, elapsed: 1.11e+01, train loss: 1.67135e-06, val loss: 2.43135e-06, min loss: 1.66245e-06\n",
      "Epoch: 650700, elapsed: 1.12e+01, train loss: 1.65897e-06, val loss: 2.40723e-06, min loss: 1.65897e-06\n",
      "Epoch: 650800, elapsed: 1.33e+01, train loss: 1.66693e-06, val loss: 2.40432e-06, min loss: 1.65897e-06\n",
      "Epoch: 650900, elapsed: 1.13e+01, train loss: 5.87571e-06, val loss: 6.54345e-06, min loss: 1.65897e-06\n",
      "Epoch: 651000, elapsed: 1.13e+01, train loss: 1.65861e-06, val loss: 2.40825e-06, min loss: 1.65861e-06\n",
      "Epoch: 651100, elapsed: 1.12e+01, train loss: 1.80824e-06, val loss: 2.60600e-06, min loss: 1.65861e-06\n",
      "Epoch: 651200, elapsed: 1.12e+01, train loss: 7.14936e-06, val loss: 8.04986e-06, min loss: 1.65861e-06\n",
      "Epoch: 651300, elapsed: 1.13e+01, train loss: 1.72873e-06, val loss: 2.45446e-06, min loss: 1.65861e-06\n",
      "Epoch: 651400, elapsed: 1.12e+01, train loss: 1.69946e-06, val loss: 2.42092e-06, min loss: 1.65861e-06\n",
      "Epoch: 651500, elapsed: 1.13e+01, train loss: 1.67474e-06, val loss: 2.43213e-06, min loss: 1.65861e-06\n",
      "Epoch: 651600, elapsed: 1.13e+01, train loss: 1.72735e-06, val loss: 2.44201e-06, min loss: 1.65861e-06\n",
      "Epoch: 651700, elapsed: 1.12e+01, train loss: 1.72589e-06, val loss: 2.40236e-06, min loss: 1.65861e-06\n",
      "Epoch: 651800, elapsed: 1.12e+01, train loss: 1.73146e-06, val loss: 2.55091e-06, min loss: 1.65861e-06\n",
      "Epoch: 651900, elapsed: 1.12e+01, train loss: 4.54233e-06, val loss: 3.15372e-06, min loss: 1.65861e-06\n",
      "Epoch: 652000, elapsed: 1.31e+01, train loss: 1.65699e-06, val loss: 2.40242e-06, min loss: 1.65699e-06\n",
      "Epoch: 652100, elapsed: 1.15e+01, train loss: 1.66036e-06, val loss: 2.41359e-06, min loss: 1.65699e-06\n",
      "Epoch: 652200, elapsed: 1.12e+01, train loss: 1.78995e-06, val loss: 2.42596e-06, min loss: 1.65699e-06\n",
      "Epoch: 652300, elapsed: 1.12e+01, train loss: 1.70725e-06, val loss: 2.66611e-06, min loss: 1.65699e-06\n",
      "Epoch: 652400, elapsed: 1.12e+01, train loss: 3.63223e-06, val loss: 4.58381e-06, min loss: 1.65699e-06\n",
      "Epoch: 652500, elapsed: 1.13e+01, train loss: 3.01258e-06, val loss: 4.05227e-06, min loss: 1.65699e-06\n",
      "Epoch: 652600, elapsed: 1.13e+01, train loss: 2.06990e-06, val loss: 2.82550e-06, min loss: 1.65699e-06\n",
      "Epoch: 652700, elapsed: 1.11e+01, train loss: 2.68263e-06, val loss: 2.93961e-06, min loss: 1.65699e-06\n",
      "Epoch: 652800, elapsed: 1.12e+01, train loss: 1.66951e-06, val loss: 2.39468e-06, min loss: 1.65699e-06\n",
      "Epoch: 652900, elapsed: 1.11e+01, train loss: 1.71884e-06, val loss: 2.49792e-06, min loss: 1.65699e-06\n",
      "Epoch: 653000, elapsed: 1.12e+01, train loss: 1.72510e-06, val loss: 2.55859e-06, min loss: 1.65699e-06\n",
      "Epoch: 653100, elapsed: 1.12e+01, train loss: 1.65963e-06, val loss: 2.41338e-06, min loss: 1.65699e-06\n",
      "Epoch: 653200, elapsed: 1.12e+01, train loss: 1.67760e-06, val loss: 2.45290e-06, min loss: 1.65699e-06\n",
      "Epoch: 653300, elapsed: 1.30e+01, train loss: 1.66892e-06, val loss: 2.42252e-06, min loss: 1.65699e-06\n",
      "Epoch: 653400, elapsed: 1.14e+01, train loss: 1.66492e-06, val loss: 2.38779e-06, min loss: 1.65699e-06\n",
      "Epoch: 653500, elapsed: 1.13e+01, train loss: 1.78818e-06, val loss: 2.68808e-06, min loss: 1.65699e-06\n",
      "Epoch: 653600, elapsed: 1.14e+01, train loss: 3.12244e-06, val loss: 4.18887e-06, min loss: 1.65699e-06\n",
      "Epoch: 653700, elapsed: 1.13e+01, train loss: 1.73179e-06, val loss: 2.59559e-06, min loss: 1.65699e-06\n",
      "Epoch: 653800, elapsed: 1.13e+01, train loss: 1.65991e-06, val loss: 2.42274e-06, min loss: 1.65699e-06\n",
      "Epoch: 653900, elapsed: 1.13e+01, train loss: 1.70351e-06, val loss: 2.49245e-06, min loss: 1.65699e-06\n",
      "Epoch: 654000, elapsed: 1.14e+01, train loss: 1.68786e-06, val loss: 2.41458e-06, min loss: 1.65699e-06\n",
      "Epoch: 654100, elapsed: 1.12e+01, train loss: 1.76454e-06, val loss: 2.56422e-06, min loss: 1.65699e-06\n",
      "Epoch: 654200, elapsed: 1.14e+01, train loss: 1.67529e-06, val loss: 2.44379e-06, min loss: 1.65699e-06\n",
      "Epoch: 654300, elapsed: 1.12e+01, train loss: 1.68895e-06, val loss: 2.47039e-06, min loss: 1.65699e-06\n",
      "Epoch: 654400, elapsed: 1.13e+01, train loss: 1.68796e-06, val loss: 2.45590e-06, min loss: 1.65699e-06\n",
      "Epoch: 654500, elapsed: 1.12e+01, train loss: 1.79027e-06, val loss: 2.40656e-06, min loss: 1.65699e-06\n",
      "Epoch: 654600, elapsed: 1.29e+01, train loss: 1.68131e-06, val loss: 2.40649e-06, min loss: 1.65699e-06\n",
      "Epoch: 654700, elapsed: 1.12e+01, train loss: 1.79234e-06, val loss: 2.62900e-06, min loss: 1.65699e-06\n",
      "Epoch: 654800, elapsed: 1.13e+01, train loss: 2.30227e-06, val loss: 3.41121e-06, min loss: 1.65699e-06\n",
      "Epoch: 654900, elapsed: 1.14e+01, train loss: 1.67104e-06, val loss: 2.44494e-06, min loss: 1.65699e-06\n",
      "Epoch: 655000, elapsed: 1.15e+01, train loss: 1.96461e-06, val loss: 2.53481e-06, min loss: 1.65699e-06\n",
      "Epoch: 655100, elapsed: 1.34e+01, train loss: 1.77179e-06, val loss: 2.48509e-06, min loss: 1.65699e-06\n",
      "Epoch: 655200, elapsed: 1.15e+01, train loss: 1.80350e-06, val loss: 2.51736e-06, min loss: 1.65699e-06\n",
      "Epoch: 655300, elapsed: 1.13e+01, train loss: 1.74402e-06, val loss: 2.56649e-06, min loss: 1.65699e-06\n",
      "Epoch: 655400, elapsed: 1.14e+01, train loss: 1.65999e-06, val loss: 2.43758e-06, min loss: 1.65699e-06\n",
      "Epoch: 655500, elapsed: 1.11e+01, train loss: 1.69897e-06, val loss: 2.43874e-06, min loss: 1.65699e-06\n",
      "Epoch: 655600, elapsed: 1.12e+01, train loss: 3.41990e-06, val loss: 3.20981e-06, min loss: 1.65699e-06\n",
      "Epoch: 655700, elapsed: 1.13e+01, train loss: 1.70026e-06, val loss: 2.58132e-06, min loss: 1.65699e-06\n",
      "Epoch: 655800, elapsed: 1.13e+01, train loss: 1.65440e-06, val loss: 2.40350e-06, min loss: 1.65440e-06\n",
      "Epoch: 655900, elapsed: 1.32e+01, train loss: 1.65319e-06, val loss: 2.39756e-06, min loss: 1.65319e-06\n",
      "Epoch: 656000, elapsed: 1.14e+01, train loss: 1.67489e-06, val loss: 2.42164e-06, min loss: 1.65319e-06\n",
      "Epoch: 656100, elapsed: 1.14e+01, train loss: 9.41642e-06, val loss: 7.58349e-06, min loss: 1.65319e-06\n",
      "Epoch: 656200, elapsed: 1.13e+01, train loss: 1.64927e-06, val loss: 2.39968e-06, min loss: 1.64927e-06\n",
      "Epoch: 656300, elapsed: 1.11e+01, train loss: 1.65824e-06, val loss: 2.40934e-06, min loss: 1.64927e-06\n",
      "Epoch: 656400, elapsed: 1.14e+01, train loss: 4.00268e-06, val loss: 4.03576e-06, min loss: 1.64927e-06\n",
      "Epoch: 656500, elapsed: 1.13e+01, train loss: 1.79821e-06, val loss: 2.50636e-06, min loss: 1.64927e-06\n",
      "Epoch: 656600, elapsed: 1.14e+01, train loss: 1.65382e-06, val loss: 2.39697e-06, min loss: 1.64927e-06\n",
      "Epoch: 656700, elapsed: 1.12e+01, train loss: 1.68492e-06, val loss: 2.46460e-06, min loss: 1.64927e-06\n",
      "Epoch: 656800, elapsed: 1.12e+01, train loss: 2.34948e-06, val loss: 2.75104e-06, min loss: 1.64927e-06\n",
      "Epoch: 656900, elapsed: 1.13e+01, train loss: 3.66771e-06, val loss: 4.66607e-06, min loss: 1.64927e-06\n",
      "Epoch: 657000, elapsed: 1.13e+01, train loss: 1.66343e-06, val loss: 2.39887e-06, min loss: 1.64927e-06\n",
      "Epoch: 657100, elapsed: 1.10e+01, train loss: 1.86091e-06, val loss: 2.77982e-06, min loss: 1.64927e-06\n",
      "Epoch: 657200, elapsed: 1.32e+01, train loss: 1.74055e-06, val loss: 2.55244e-06, min loss: 1.64927e-06\n",
      "Epoch: 657300, elapsed: 1.12e+01, train loss: 4.88272e-06, val loss: 5.05197e-06, min loss: 1.64927e-06\n",
      "Epoch: 657400, elapsed: 1.12e+01, train loss: 2.21532e-06, val loss: 3.22853e-06, min loss: 1.64927e-06\n",
      "Epoch: 657500, elapsed: 1.12e+01, train loss: 1.69121e-06, val loss: 2.49350e-06, min loss: 1.64927e-06\n",
      "Epoch: 657600, elapsed: 1.13e+01, train loss: 1.74930e-06, val loss: 2.59810e-06, min loss: 1.64927e-06\n",
      "Epoch: 657700, elapsed: 1.12e+01, train loss: 1.81606e-06, val loss: 2.54656e-06, min loss: 1.64927e-06\n",
      "Epoch: 657800, elapsed: 1.12e+01, train loss: 1.65693e-06, val loss: 2.44968e-06, min loss: 1.64927e-06\n",
      "Epoch: 657900, elapsed: 1.13e+01, train loss: 2.93978e-06, val loss: 3.98891e-06, min loss: 1.64927e-06\n",
      "Epoch: 658000, elapsed: 1.12e+01, train loss: 1.67184e-06, val loss: 2.43754e-06, min loss: 1.64927e-06\n",
      "Epoch: 658100, elapsed: 1.12e+01, train loss: 1.71357e-06, val loss: 2.40033e-06, min loss: 1.64927e-06\n",
      "Epoch: 658200, elapsed: 1.13e+01, train loss: 1.71456e-06, val loss: 2.50220e-06, min loss: 1.64927e-06\n",
      "Epoch: 658300, elapsed: 1.13e+01, train loss: 1.66265e-06, val loss: 2.44529e-06, min loss: 1.64927e-06\n",
      "Epoch: 658400, elapsed: 1.12e+01, train loss: 2.01357e-06, val loss: 2.57129e-06, min loss: 1.64927e-06\n",
      "Epoch: 658500, elapsed: 1.32e+01, train loss: 2.33449e-06, val loss: 2.79202e-06, min loss: 1.64927e-06\n",
      "Epoch: 658600, elapsed: 1.14e+01, train loss: 1.98401e-06, val loss: 3.28627e-06, min loss: 1.64927e-06\n",
      "Epoch: 658700, elapsed: 1.13e+01, train loss: 4.23620e-06, val loss: 4.82209e-06, min loss: 1.64927e-06\n",
      "Epoch: 658800, elapsed: 1.12e+01, train loss: 1.84307e-06, val loss: 2.54376e-06, min loss: 1.64927e-06\n",
      "Epoch: 658900, elapsed: 1.11e+01, train loss: 1.72007e-06, val loss: 2.51638e-06, min loss: 1.64927e-06\n",
      "Epoch: 659000, elapsed: 1.10e+01, train loss: 2.25038e-06, val loss: 3.17469e-06, min loss: 1.64927e-06\n",
      "Epoch: 659100, elapsed: 1.11e+01, train loss: 1.74210e-06, val loss: 2.54629e-06, min loss: 1.64927e-06\n",
      "Epoch: 659200, elapsed: 1.13e+01, train loss: 2.01974e-06, val loss: 3.08494e-06, min loss: 1.64927e-06\n",
      "Epoch: 659300, elapsed: 1.12e+01, train loss: 1.65530e-06, val loss: 2.42200e-06, min loss: 1.64927e-06\n",
      "Epoch: 659400, elapsed: 1.12e+01, train loss: 1.65067e-06, val loss: 2.38608e-06, min loss: 1.64927e-06\n",
      "Epoch: 659500, elapsed: 1.11e+01, train loss: 2.02392e-06, val loss: 3.01032e-06, min loss: 1.64927e-06\n",
      "Epoch: 659600, elapsed: 1.12e+01, train loss: 2.23195e-06, val loss: 3.08946e-06, min loss: 1.64927e-06\n",
      "Epoch: 659700, elapsed: 1.10e+01, train loss: 2.23678e-06, val loss: 2.66584e-06, min loss: 1.64927e-06\n",
      "Epoch: 659800, elapsed: 1.31e+01, train loss: 2.08268e-06, val loss: 2.61057e-06, min loss: 1.64927e-06\n",
      "Epoch: 659900, elapsed: 1.13e+01, train loss: 1.69320e-06, val loss: 2.37174e-06, min loss: 1.64927e-06\n",
      "Epoch: 660000, elapsed: 1.14e+01, train loss: 4.76755e-06, val loss: 5.99981e-06, min loss: 1.64927e-06\n",
      "Epoch: 660100, elapsed: 1.32e+01, train loss: 1.88986e-06, val loss: 2.56809e-06, min loss: 1.64927e-06\n",
      "Epoch: 660200, elapsed: 1.11e+01, train loss: 1.92165e-06, val loss: 2.48648e-06, min loss: 1.64927e-06\n",
      "Epoch: 660300, elapsed: 1.12e+01, train loss: 2.30725e-06, val loss: 2.87639e-06, min loss: 1.64927e-06\n",
      "Epoch: 660400, elapsed: 1.12e+01, train loss: 1.70510e-06, val loss: 2.40797e-06, min loss: 1.64927e-06\n",
      "Epoch: 660500, elapsed: 1.13e+01, train loss: 1.67834e-06, val loss: 2.46008e-06, min loss: 1.64927e-06\n",
      "Epoch: 660600, elapsed: 1.12e+01, train loss: 1.69421e-06, val loss: 2.40303e-06, min loss: 1.64927e-06\n",
      "Epoch: 660700, elapsed: 1.13e+01, train loss: 3.58858e-06, val loss: 3.87042e-06, min loss: 1.64927e-06\n",
      "Epoch: 660800, elapsed: 1.10e+01, train loss: 1.79352e-06, val loss: 2.51568e-06, min loss: 1.64927e-06\n",
      "Epoch: 660900, elapsed: 1.11e+01, train loss: 1.64274e-06, val loss: 2.39850e-06, min loss: 1.64274e-06\n",
      "Epoch: 661000, elapsed: 1.32e+01, train loss: 1.71181e-06, val loss: 2.41417e-06, min loss: 1.64274e-06\n",
      "Epoch: 661100, elapsed: 1.15e+01, train loss: 2.62098e-06, val loss: 3.12173e-06, min loss: 1.64274e-06\n",
      "Epoch: 661200, elapsed: 1.12e+01, train loss: 1.93251e-06, val loss: 2.78555e-06, min loss: 1.64274e-06\n",
      "Epoch: 661300, elapsed: 1.12e+01, train loss: 1.64284e-06, val loss: 2.39856e-06, min loss: 1.64274e-06\n",
      "Epoch: 661400, elapsed: 1.12e+01, train loss: 1.65535e-06, val loss: 2.40220e-06, min loss: 1.64274e-06\n",
      "Epoch: 661500, elapsed: 1.14e+01, train loss: 1.75590e-06, val loss: 2.48409e-06, min loss: 1.64274e-06\n",
      "Epoch: 661600, elapsed: 1.11e+01, train loss: 2.30643e-06, val loss: 3.31769e-06, min loss: 1.64274e-06\n",
      "Epoch: 661700, elapsed: 1.13e+01, train loss: 5.77274e-06, val loss: 7.73362e-06, min loss: 1.64274e-06\n",
      "Epoch: 661800, elapsed: 1.13e+01, train loss: 1.71468e-06, val loss: 2.48927e-06, min loss: 1.64274e-06\n",
      "Epoch: 661900, elapsed: 1.13e+01, train loss: 1.69054e-06, val loss: 2.49170e-06, min loss: 1.64274e-06\n",
      "Epoch: 662000, elapsed: 1.13e+01, train loss: 1.81736e-06, val loss: 2.58789e-06, min loss: 1.64274e-06\n",
      "Epoch: 662100, elapsed: 1.12e+01, train loss: 1.65139e-06, val loss: 2.42317e-06, min loss: 1.64274e-06\n",
      "Epoch: 662200, elapsed: 1.12e+01, train loss: 1.64391e-06, val loss: 2.38308e-06, min loss: 1.64274e-06\n",
      "Epoch: 662300, elapsed: 1.11e+01, train loss: 2.96693e-06, val loss: 4.03804e-06, min loss: 1.64274e-06\n",
      "Epoch: 662400, elapsed: 1.33e+01, train loss: 1.81964e-06, val loss: 2.50426e-06, min loss: 1.64274e-06\n",
      "Epoch: 662500, elapsed: 1.13e+01, train loss: 2.74447e-06, val loss: 3.63647e-06, min loss: 1.64274e-06\n",
      "Epoch: 662600, elapsed: 1.13e+01, train loss: 2.56603e-06, val loss: 3.54429e-06, min loss: 1.64274e-06\n",
      "Epoch: 662700, elapsed: 1.12e+01, train loss: 1.65271e-06, val loss: 2.42460e-06, min loss: 1.64274e-06\n",
      "Epoch: 662800, elapsed: 1.12e+01, train loss: 1.64417e-06, val loss: 2.39916e-06, min loss: 1.64274e-06\n",
      "Epoch: 662900, elapsed: 1.13e+01, train loss: 1.64947e-06, val loss: 2.42524e-06, min loss: 1.64274e-06\n",
      "Epoch: 663000, elapsed: 1.14e+01, train loss: 1.64061e-06, val loss: 2.38520e-06, min loss: 1.64061e-06\n",
      "Epoch: 663100, elapsed: 1.14e+01, train loss: 1.64848e-06, val loss: 2.38164e-06, min loss: 1.64061e-06\n",
      "Epoch: 663200, elapsed: 1.12e+01, train loss: 1.69373e-06, val loss: 2.48573e-06, min loss: 1.64061e-06\n",
      "Epoch: 663300, elapsed: 1.12e+01, train loss: 1.75929e-06, val loss: 2.55343e-06, min loss: 1.64061e-06\n",
      "Epoch: 663400, elapsed: 1.12e+01, train loss: 2.56098e-06, val loss: 3.39830e-06, min loss: 1.64061e-06\n",
      "Epoch: 663500, elapsed: 1.14e+01, train loss: 1.89936e-06, val loss: 2.63053e-06, min loss: 1.64061e-06\n",
      "Epoch: 663600, elapsed: 1.11e+01, train loss: 1.64098e-06, val loss: 2.40745e-06, min loss: 1.64061e-06\n",
      "Epoch: 663700, elapsed: 1.34e+01, train loss: 2.11833e-06, val loss: 2.80804e-06, min loss: 1.64061e-06\n",
      "Epoch: 663800, elapsed: 1.14e+01, train loss: 1.79098e-06, val loss: 2.50793e-06, min loss: 1.64061e-06\n",
      "Epoch: 663900, elapsed: 1.12e+01, train loss: 1.64174e-06, val loss: 2.38538e-06, min loss: 1.64061e-06\n",
      "Epoch: 664000, elapsed: 1.13e+01, train loss: 2.57623e-06, val loss: 3.32786e-06, min loss: 1.64061e-06\n",
      "Epoch: 664100, elapsed: 1.12e+01, train loss: 2.49972e-06, val loss: 2.93671e-06, min loss: 1.64061e-06\n",
      "Epoch: 664200, elapsed: 1.13e+01, train loss: 1.72968e-06, val loss: 2.43887e-06, min loss: 1.64061e-06\n",
      "Epoch: 664300, elapsed: 1.12e+01, train loss: 3.19474e-06, val loss: 3.89747e-06, min loss: 1.64061e-06\n",
      "Epoch: 664400, elapsed: 1.11e+01, train loss: 2.15796e-06, val loss: 2.86355e-06, min loss: 1.64061e-06\n",
      "Epoch: 664500, elapsed: 1.12e+01, train loss: 2.05840e-06, val loss: 2.82628e-06, min loss: 1.64061e-06\n",
      "Epoch: 664600, elapsed: 1.11e+01, train loss: 1.79552e-06, val loss: 2.49806e-06, min loss: 1.64061e-06\n",
      "Epoch: 664700, elapsed: 1.13e+01, train loss: 2.62395e-06, val loss: 3.55825e-06, min loss: 1.64061e-06\n",
      "Epoch: 664800, elapsed: 1.13e+01, train loss: 1.44375e-05, val loss: 1.06462e-05, min loss: 1.64061e-06\n",
      "Epoch: 664900, elapsed: 1.11e+01, train loss: 1.63423e-06, val loss: 2.39224e-06, min loss: 1.63423e-06\n",
      "Epoch: 665000, elapsed: 1.33e+01, train loss: 2.07023e-06, val loss: 2.65260e-06, min loss: 1.63423e-06\n",
      "Epoch: 665100, elapsed: 1.33e+01, train loss: 1.63705e-06, val loss: 2.39262e-06, min loss: 1.63423e-06\n",
      "Epoch: 665200, elapsed: 1.11e+01, train loss: 1.70175e-06, val loss: 2.45323e-06, min loss: 1.63423e-06\n",
      "Epoch: 665300, elapsed: 1.11e+01, train loss: 1.66323e-06, val loss: 2.39894e-06, min loss: 1.63423e-06\n",
      "Epoch: 665400, elapsed: 1.13e+01, train loss: 1.67519e-06, val loss: 2.48368e-06, min loss: 1.63423e-06\n",
      "Epoch: 665500, elapsed: 1.12e+01, train loss: 1.78626e-06, val loss: 2.47769e-06, min loss: 1.63423e-06\n",
      "Epoch: 665600, elapsed: 1.13e+01, train loss: 2.01409e-06, val loss: 2.65809e-06, min loss: 1.63423e-06\n",
      "Epoch: 665700, elapsed: 1.13e+01, train loss: 1.69386e-06, val loss: 2.40302e-06, min loss: 1.63423e-06\n",
      "Epoch: 665800, elapsed: 1.10e+01, train loss: 2.02926e-06, val loss: 2.78669e-06, min loss: 1.63423e-06\n",
      "Epoch: 665900, elapsed: 1.14e+01, train loss: 1.68332e-06, val loss: 2.51573e-06, min loss: 1.63423e-06\n",
      "Epoch: 666000, elapsed: 1.11e+01, train loss: 1.63476e-06, val loss: 2.38151e-06, min loss: 1.63423e-06\n",
      "Epoch: 666100, elapsed: 1.13e+01, train loss: 1.63911e-06, val loss: 2.39160e-06, min loss: 1.63423e-06\n",
      "Epoch: 666200, elapsed: 1.31e+01, train loss: 1.65910e-06, val loss: 2.42220e-06, min loss: 1.63423e-06\n",
      "Epoch: 666300, elapsed: 1.13e+01, train loss: 1.66824e-06, val loss: 2.41069e-06, min loss: 1.63423e-06\n",
      "Epoch: 666400, elapsed: 1.14e+01, train loss: 1.63690e-06, val loss: 2.39043e-06, min loss: 1.63423e-06\n",
      "Epoch: 666500, elapsed: 1.12e+01, train loss: 1.84735e-06, val loss: 2.64534e-06, min loss: 1.63423e-06\n",
      "Epoch: 666600, elapsed: 1.14e+01, train loss: 2.28432e-06, val loss: 2.96198e-06, min loss: 1.63423e-06\n",
      "Epoch: 666700, elapsed: 1.13e+01, train loss: 1.67351e-06, val loss: 2.48735e-06, min loss: 1.63423e-06\n",
      "Epoch: 666800, elapsed: 1.11e+01, train loss: 1.63089e-06, val loss: 2.38735e-06, min loss: 1.63089e-06\n",
      "Epoch: 666900, elapsed: 1.13e+01, train loss: 2.87833e-06, val loss: 3.98595e-06, min loss: 1.63089e-06\n",
      "Epoch: 667000, elapsed: 1.11e+01, train loss: 2.28753e-06, val loss: 2.93881e-06, min loss: 1.63089e-06\n",
      "Epoch: 667100, elapsed: 1.12e+01, train loss: 1.72393e-06, val loss: 2.48713e-06, min loss: 1.63089e-06\n",
      "Epoch: 667200, elapsed: 1.11e+01, train loss: 1.64665e-06, val loss: 2.43952e-06, min loss: 1.63089e-06\n",
      "Epoch: 667300, elapsed: 1.12e+01, train loss: 1.85625e-06, val loss: 2.57973e-06, min loss: 1.63089e-06\n",
      "Epoch: 667400, elapsed: 1.14e+01, train loss: 3.39345e-06, val loss: 4.76430e-06, min loss: 1.63089e-06\n",
      "Epoch: 667500, elapsed: 1.32e+01, train loss: 6.14711e-06, val loss: 6.89986e-06, min loss: 1.63089e-06\n",
      "Epoch: 667600, elapsed: 1.14e+01, train loss: 2.05146e-06, val loss: 2.49986e-06, min loss: 1.63089e-06\n",
      "Epoch: 667700, elapsed: 1.14e+01, train loss: 4.91422e-06, val loss: 3.68227e-06, min loss: 1.63089e-06\n",
      "Epoch: 667800, elapsed: 1.14e+01, train loss: 2.78473e-06, val loss: 3.16120e-06, min loss: 1.63089e-06\n",
      "Epoch: 667900, elapsed: 1.13e+01, train loss: 2.12781e-06, val loss: 3.16905e-06, min loss: 1.63089e-06\n",
      "Epoch: 668000, elapsed: 1.12e+01, train loss: 1.82781e-06, val loss: 2.62838e-06, min loss: 1.63089e-06\n",
      "Epoch: 668100, elapsed: 1.12e+01, train loss: 4.42747e-06, val loss: 6.24505e-06, min loss: 1.63089e-06\n",
      "Epoch: 668200, elapsed: 1.12e+01, train loss: 2.12548e-06, val loss: 2.82023e-06, min loss: 1.63089e-06\n",
      "Epoch: 668300, elapsed: 1.12e+01, train loss: 4.77861e-06, val loss: 5.24866e-06, min loss: 1.63089e-06\n",
      "Epoch: 668400, elapsed: 1.12e+01, train loss: 1.75507e-06, val loss: 2.61128e-06, min loss: 1.63089e-06\n",
      "Epoch: 668500, elapsed: 1.12e+01, train loss: 2.21615e-06, val loss: 3.00877e-06, min loss: 1.63089e-06\n",
      "Epoch: 668600, elapsed: 1.13e+01, train loss: 7.34603e-06, val loss: 5.60072e-06, min loss: 1.63089e-06\n",
      "Epoch: 668700, elapsed: 1.13e+01, train loss: 1.62769e-06, val loss: 2.38620e-06, min loss: 1.62769e-06\n",
      "Epoch: 668800, elapsed: 1.12e+01, train loss: 1.62846e-06, val loss: 2.38284e-06, min loss: 1.62769e-06\n",
      "Epoch: 668900, elapsed: 1.33e+01, train loss: 1.71505e-06, val loss: 2.50836e-06, min loss: 1.62769e-06\n",
      "Epoch: 669000, elapsed: 1.12e+01, train loss: 1.76836e-06, val loss: 2.43199e-06, min loss: 1.62769e-06\n",
      "Epoch: 669100, elapsed: 1.11e+01, train loss: 2.56486e-06, val loss: 3.31969e-06, min loss: 1.62769e-06\n",
      "Epoch: 669200, elapsed: 1.12e+01, train loss: 1.95088e-06, val loss: 2.89042e-06, min loss: 1.62769e-06\n",
      "Epoch: 669300, elapsed: 1.12e+01, train loss: 2.12276e-06, val loss: 2.68438e-06, min loss: 1.62769e-06\n",
      "Epoch: 669400, elapsed: 1.12e+01, train loss: 2.19604e-06, val loss: 2.87120e-06, min loss: 1.62769e-06\n",
      "Epoch: 669500, elapsed: 1.12e+01, train loss: 2.01403e-06, val loss: 2.59564e-06, min loss: 1.62769e-06\n",
      "Epoch: 669600, elapsed: 1.10e+01, train loss: 1.75844e-06, val loss: 2.56102e-06, min loss: 1.62769e-06\n",
      "Epoch: 669700, elapsed: 1.13e+01, train loss: 3.29982e-06, val loss: 3.34638e-06, min loss: 1.62769e-06\n",
      "Epoch: 669800, elapsed: 1.12e+01, train loss: 1.67831e-06, val loss: 2.47585e-06, min loss: 1.62769e-06\n",
      "Epoch: 669900, elapsed: 1.13e+01, train loss: 2.29750e-06, val loss: 2.83607e-06, min loss: 1.62769e-06\n",
      "Epoch: 670000, elapsed: 1.12e+01, train loss: 1.74439e-06, val loss: 2.50983e-06, min loss: 1.62769e-06\n",
      "Epoch: 670100, elapsed: 1.51e+01, train loss: 1.75327e-06, val loss: 2.60601e-06, min loss: 1.62769e-06\n",
      "Epoch: 670200, elapsed: 1.15e+01, train loss: 1.62636e-06, val loss: 2.38980e-06, min loss: 1.62636e-06\n",
      "Epoch: 670300, elapsed: 1.13e+01, train loss: 1.63449e-06, val loss: 2.39982e-06, min loss: 1.62636e-06\n",
      "Epoch: 670400, elapsed: 1.14e+01, train loss: 1.95359e-06, val loss: 3.02911e-06, min loss: 1.62636e-06\n",
      "Epoch: 670500, elapsed: 1.13e+01, train loss: 1.62860e-06, val loss: 2.38664e-06, min loss: 1.62636e-06\n",
      "Epoch: 670600, elapsed: 1.12e+01, train loss: 1.67825e-06, val loss: 2.42026e-06, min loss: 1.62636e-06\n",
      "Epoch: 670700, elapsed: 1.13e+01, train loss: 1.70594e-06, val loss: 2.53793e-06, min loss: 1.62636e-06\n",
      "Epoch: 670800, elapsed: 1.12e+01, train loss: 2.04007e-06, val loss: 2.68880e-06, min loss: 1.62636e-06\n",
      "Epoch: 670900, elapsed: 1.14e+01, train loss: 2.28278e-06, val loss: 3.52209e-06, min loss: 1.62636e-06\n",
      "Epoch: 671000, elapsed: 1.12e+01, train loss: 1.63247e-06, val loss: 2.41652e-06, min loss: 1.62636e-06\n",
      "Epoch: 671100, elapsed: 1.13e+01, train loss: 1.63206e-06, val loss: 2.41798e-06, min loss: 1.62636e-06\n",
      "Epoch: 671200, elapsed: 1.14e+01, train loss: 1.64122e-06, val loss: 2.38759e-06, min loss: 1.62636e-06\n",
      "Epoch: 671300, elapsed: 1.11e+01, train loss: 2.64128e-06, val loss: 2.78943e-06, min loss: 1.62636e-06\n",
      "Epoch: 671400, elapsed: 1.11e+01, train loss: 2.16614e-06, val loss: 2.97612e-06, min loss: 1.62636e-06\n",
      "Epoch: 671500, elapsed: 1.34e+01, train loss: 1.73671e-06, val loss: 2.65177e-06, min loss: 1.62636e-06\n",
      "Epoch: 671600, elapsed: 1.15e+01, train loss: 1.65655e-06, val loss: 2.38896e-06, min loss: 1.62636e-06\n",
      "Epoch: 671700, elapsed: 1.15e+01, train loss: 1.66103e-06, val loss: 2.43860e-06, min loss: 1.62636e-06\n",
      "Epoch: 671800, elapsed: 1.15e+01, train loss: 1.84738e-06, val loss: 2.70814e-06, min loss: 1.62636e-06\n",
      "Epoch: 671900, elapsed: 1.15e+01, train loss: 5.25132e-06, val loss: 5.62541e-06, min loss: 1.62636e-06\n",
      "Epoch: 672000, elapsed: 1.15e+01, train loss: 1.95410e-06, val loss: 3.22870e-06, min loss: 1.62636e-06\n",
      "Epoch: 672100, elapsed: 1.12e+01, train loss: 3.74531e-06, val loss: 4.95961e-06, min loss: 1.62636e-06\n",
      "Epoch: 672200, elapsed: 1.13e+01, train loss: 5.90928e-06, val loss: 6.39101e-06, min loss: 1.62636e-06\n",
      "Epoch: 672300, elapsed: 1.13e+01, train loss: 1.71580e-06, val loss: 2.62442e-06, min loss: 1.62636e-06\n",
      "Epoch: 672400, elapsed: 1.13e+01, train loss: 2.65095e-06, val loss: 3.83838e-06, min loss: 1.62636e-06\n",
      "Epoch: 672500, elapsed: 1.14e+01, train loss: 4.39407e-06, val loss: 6.37745e-06, min loss: 1.62636e-06\n",
      "Epoch: 672600, elapsed: 1.13e+01, train loss: 1.68617e-06, val loss: 2.47968e-06, min loss: 1.62636e-06\n",
      "Epoch: 672700, elapsed: 1.13e+01, train loss: 1.68728e-06, val loss: 2.41818e-06, min loss: 1.62636e-06\n",
      "Epoch: 672800, elapsed: 1.33e+01, train loss: 1.63136e-06, val loss: 2.37369e-06, min loss: 1.62636e-06\n",
      "Epoch: 672900, elapsed: 1.13e+01, train loss: 1.63609e-06, val loss: 2.41970e-06, min loss: 1.62636e-06\n",
      "Epoch: 673000, elapsed: 1.13e+01, train loss: 2.13406e-06, val loss: 2.66863e-06, min loss: 1.62636e-06\n",
      "Epoch: 673100, elapsed: 1.13e+01, train loss: 1.63761e-06, val loss: 2.39597e-06, min loss: 1.62636e-06\n",
      "Epoch: 673200, elapsed: 1.13e+01, train loss: 1.72064e-06, val loss: 2.45770e-06, min loss: 1.62636e-06\n",
      "Epoch: 673300, elapsed: 1.13e+01, train loss: 1.62323e-06, val loss: 2.40226e-06, min loss: 1.62323e-06\n",
      "Epoch: 673400, elapsed: 1.13e+01, train loss: 1.62625e-06, val loss: 2.38562e-06, min loss: 1.62323e-06\n",
      "Epoch: 673500, elapsed: 1.14e+01, train loss: 1.64355e-06, val loss: 2.37145e-06, min loss: 1.62323e-06\n",
      "Epoch: 673600, elapsed: 1.13e+01, train loss: 1.62977e-06, val loss: 2.39971e-06, min loss: 1.62323e-06\n",
      "Epoch: 673700, elapsed: 1.14e+01, train loss: 1.66410e-06, val loss: 2.41336e-06, min loss: 1.62323e-06\n",
      "Epoch: 673800, elapsed: 1.13e+01, train loss: 1.66474e-06, val loss: 2.39570e-06, min loss: 1.62323e-06\n",
      "Epoch: 673900, elapsed: 1.13e+01, train loss: 1.62316e-06, val loss: 2.39620e-06, min loss: 1.62316e-06\n",
      "Epoch: 674000, elapsed: 1.11e+01, train loss: 1.64322e-06, val loss: 2.43285e-06, min loss: 1.62316e-06\n",
      "Epoch: 674100, elapsed: 1.33e+01, train loss: 1.65248e-06, val loss: 2.46994e-06, min loss: 1.62316e-06\n",
      "Epoch: 674200, elapsed: 1.13e+01, train loss: 1.62271e-06, val loss: 2.39442e-06, min loss: 1.62271e-06\n",
      "Epoch: 674300, elapsed: 1.15e+01, train loss: 1.61947e-06, val loss: 2.39639e-06, min loss: 1.61947e-06\n",
      "Epoch: 674400, elapsed: 1.14e+01, train loss: 3.79962e-06, val loss: 4.88712e-06, min loss: 1.61947e-06\n",
      "Epoch: 674500, elapsed: 1.13e+01, train loss: 1.61813e-06, val loss: 2.38596e-06, min loss: 1.61813e-06\n",
      "Epoch: 674600, elapsed: 1.12e+01, train loss: 1.62609e-06, val loss: 2.37972e-06, min loss: 1.61813e-06\n",
      "Epoch: 674700, elapsed: 1.12e+01, train loss: 1.64803e-06, val loss: 2.40746e-06, min loss: 1.61813e-06\n",
      "Epoch: 674800, elapsed: 1.12e+01, train loss: 1.68620e-06, val loss: 2.43025e-06, min loss: 1.61813e-06\n",
      "Epoch: 674900, elapsed: 1.12e+01, train loss: 1.65440e-06, val loss: 2.45552e-06, min loss: 1.61813e-06\n",
      "Epoch: 675000, elapsed: 1.11e+01, train loss: 4.43397e-06, val loss: 6.62422e-06, min loss: 1.61813e-06\n",
      "Epoch: 675100, elapsed: 1.35e+01, train loss: 1.61842e-06, val loss: 2.38830e-06, min loss: 1.61813e-06\n",
      "Epoch: 675200, elapsed: 1.13e+01, train loss: 1.63499e-06, val loss: 2.38479e-06, min loss: 1.61813e-06\n",
      "Epoch: 675300, elapsed: 1.12e+01, train loss: 1.62573e-06, val loss: 2.36693e-06, min loss: 1.61813e-06\n",
      "Epoch: 675400, elapsed: 1.33e+01, train loss: 6.94491e-06, val loss: 8.01871e-06, min loss: 1.61813e-06\n",
      "Epoch: 675500, elapsed: 1.13e+01, train loss: 1.61660e-06, val loss: 2.37790e-06, min loss: 1.61660e-06\n",
      "Epoch: 675600, elapsed: 1.12e+01, train loss: 1.63043e-06, val loss: 2.39998e-06, min loss: 1.61660e-06\n",
      "Epoch: 675700, elapsed: 1.13e+01, train loss: 1.81847e-06, val loss: 2.58990e-06, min loss: 1.61660e-06\n",
      "Epoch: 675800, elapsed: 1.12e+01, train loss: 1.83100e-06, val loss: 2.66015e-06, min loss: 1.61660e-06\n",
      "Epoch: 675900, elapsed: 1.12e+01, train loss: 2.05620e-06, val loss: 2.55000e-06, min loss: 1.61660e-06\n",
      "Epoch: 676000, elapsed: 1.12e+01, train loss: 2.83249e-06, val loss: 2.98552e-06, min loss: 1.61660e-06\n",
      "Epoch: 676100, elapsed: 1.12e+01, train loss: 1.61501e-06, val loss: 2.38208e-06, min loss: 1.61501e-06\n",
      "Epoch: 676200, elapsed: 1.10e+01, train loss: 1.64860e-06, val loss: 2.42329e-06, min loss: 1.61501e-06\n",
      "Epoch: 676300, elapsed: 1.14e+01, train loss: 2.70268e-06, val loss: 3.66664e-06, min loss: 1.61501e-06\n",
      "Epoch: 676400, elapsed: 1.12e+01, train loss: 1.85738e-06, val loss: 2.50078e-06, min loss: 1.61501e-06\n",
      "Epoch: 676500, elapsed: 1.12e+01, train loss: 1.75881e-06, val loss: 2.57501e-06, min loss: 1.61501e-06\n",
      "Epoch: 676600, elapsed: 1.13e+01, train loss: 3.76367e-06, val loss: 4.86915e-06, min loss: 1.61501e-06\n",
      "Epoch: 676700, elapsed: 1.30e+01, train loss: 1.82513e-06, val loss: 2.43380e-06, min loss: 1.61501e-06\n",
      "Epoch: 676800, elapsed: 1.14e+01, train loss: 1.66553e-06, val loss: 2.46797e-06, min loss: 1.61501e-06\n",
      "Epoch: 676900, elapsed: 1.14e+01, train loss: 1.63577e-06, val loss: 2.47217e-06, min loss: 1.61501e-06\n",
      "Epoch: 677000, elapsed: 1.14e+01, train loss: 1.61601e-06, val loss: 2.38167e-06, min loss: 1.61501e-06\n",
      "Epoch: 677100, elapsed: 1.14e+01, train loss: 1.94016e-06, val loss: 2.53806e-06, min loss: 1.61501e-06\n",
      "Epoch: 677200, elapsed: 1.15e+01, train loss: 1.91531e-06, val loss: 2.62720e-06, min loss: 1.61501e-06\n",
      "Epoch: 677300, elapsed: 1.14e+01, train loss: 1.62040e-06, val loss: 2.40734e-06, min loss: 1.61501e-06\n",
      "Epoch: 677400, elapsed: 1.13e+01, train loss: 2.67631e-06, val loss: 3.63651e-06, min loss: 1.61501e-06\n",
      "Epoch: 677500, elapsed: 1.13e+01, train loss: 1.63953e-06, val loss: 2.42220e-06, min loss: 1.61501e-06\n",
      "Epoch: 677600, elapsed: 1.12e+01, train loss: 1.75137e-06, val loss: 2.52670e-06, min loss: 1.61501e-06\n",
      "Epoch: 677700, elapsed: 1.13e+01, train loss: 1.73913e-06, val loss: 2.45033e-06, min loss: 1.61501e-06\n",
      "Epoch: 677800, elapsed: 1.13e+01, train loss: 1.62205e-06, val loss: 2.38001e-06, min loss: 1.61501e-06\n",
      "Epoch: 677900, elapsed: 1.13e+01, train loss: 1.61731e-06, val loss: 2.37744e-06, min loss: 1.61501e-06\n",
      "Epoch: 678000, elapsed: 1.12e+01, train loss: 1.99706e-06, val loss: 2.65968e-06, min loss: 1.61501e-06\n",
      "Epoch: 678100, elapsed: 1.32e+01, train loss: 1.63904e-06, val loss: 2.37211e-06, min loss: 1.61501e-06\n",
      "Epoch: 678200, elapsed: 1.15e+01, train loss: 2.04797e-06, val loss: 2.47921e-06, min loss: 1.61501e-06\n",
      "Epoch: 678300, elapsed: 1.14e+01, train loss: 1.61119e-06, val loss: 2.38298e-06, min loss: 1.61119e-06\n",
      "Epoch: 678400, elapsed: 1.13e+01, train loss: 1.81729e-06, val loss: 2.55942e-06, min loss: 1.61119e-06\n",
      "Epoch: 678500, elapsed: 1.13e+01, train loss: 2.24991e-06, val loss: 2.99957e-06, min loss: 1.61119e-06\n",
      "Epoch: 678600, elapsed: 1.13e+01, train loss: 1.62502e-06, val loss: 2.39956e-06, min loss: 1.61119e-06\n",
      "Epoch: 678700, elapsed: 1.13e+01, train loss: 1.98644e-06, val loss: 2.67631e-06, min loss: 1.61119e-06\n",
      "Epoch: 678800, elapsed: 1.12e+01, train loss: 1.74309e-06, val loss: 2.55660e-06, min loss: 1.61119e-06\n",
      "Epoch: 678900, elapsed: 1.12e+01, train loss: 1.71627e-06, val loss: 2.40693e-06, min loss: 1.61119e-06\n",
      "Epoch: 679000, elapsed: 1.12e+01, train loss: 1.61268e-06, val loss: 2.37033e-06, min loss: 1.61119e-06\n",
      "Epoch: 679100, elapsed: 1.14e+01, train loss: 1.88951e-06, val loss: 2.87385e-06, min loss: 1.61119e-06\n",
      "Epoch: 679200, elapsed: 1.13e+01, train loss: 1.98334e-06, val loss: 2.66362e-06, min loss: 1.61119e-06\n",
      "Epoch: 679300, elapsed: 1.11e+01, train loss: 1.63488e-06, val loss: 2.40662e-06, min loss: 1.61119e-06\n",
      "Epoch: 679400, elapsed: 1.32e+01, train loss: 2.51008e-06, val loss: 2.83090e-06, min loss: 1.61119e-06\n",
      "Epoch: 679500, elapsed: 1.13e+01, train loss: 5.22990e-06, val loss: 5.63301e-06, min loss: 1.61119e-06\n",
      "Epoch: 679600, elapsed: 1.11e+01, train loss: 8.55611e-06, val loss: 7.08202e-06, min loss: 1.61119e-06\n",
      "Epoch: 679700, elapsed: 1.13e+01, train loss: 1.62330e-06, val loss: 2.36706e-06, min loss: 1.61119e-06\n",
      "Epoch: 679800, elapsed: 1.13e+01, train loss: 1.62741e-06, val loss: 2.43089e-06, min loss: 1.61119e-06\n",
      "Epoch: 679900, elapsed: 1.13e+01, train loss: 1.63080e-06, val loss: 2.42112e-06, min loss: 1.61119e-06\n",
      "Epoch: 680000, elapsed: 1.13e+01, train loss: 1.94328e-06, val loss: 2.61086e-06, min loss: 1.61119e-06\n",
      "Epoch: 680100, elapsed: 1.34e+01, train loss: 2.56215e-06, val loss: 3.62691e-06, min loss: 1.61119e-06\n",
      "Epoch: 680200, elapsed: 1.13e+01, train loss: 2.62835e-06, val loss: 2.89982e-06, min loss: 1.61119e-06\n",
      "Epoch: 680300, elapsed: 1.12e+01, train loss: 2.52943e-06, val loss: 3.59799e-06, min loss: 1.61119e-06\n",
      "Epoch: 680400, elapsed: 1.14e+01, train loss: 1.60767e-06, val loss: 2.37812e-06, min loss: 1.60767e-06\n",
      "Epoch: 680500, elapsed: 1.12e+01, train loss: 1.61454e-06, val loss: 2.39404e-06, min loss: 1.60767e-06\n",
      "Epoch: 680600, elapsed: 1.13e+01, train loss: 2.05104e-06, val loss: 2.80291e-06, min loss: 1.60767e-06\n",
      "Epoch: 680700, elapsed: 1.34e+01, train loss: 1.78827e-06, val loss: 2.40329e-06, min loss: 1.60767e-06\n",
      "Epoch: 680800, elapsed: 1.14e+01, train loss: 1.88201e-06, val loss: 2.64230e-06, min loss: 1.60767e-06\n",
      "Epoch: 680900, elapsed: 1.12e+01, train loss: 1.62689e-06, val loss: 2.42834e-06, min loss: 1.60767e-06\n",
      "Epoch: 681000, elapsed: 1.13e+01, train loss: 1.76800e-06, val loss: 2.68176e-06, min loss: 1.60767e-06\n",
      "Epoch: 681100, elapsed: 1.13e+01, train loss: 5.60954e-06, val loss: 6.88477e-06, min loss: 1.60767e-06\n",
      "Epoch: 681200, elapsed: 1.10e+01, train loss: 2.17402e-06, val loss: 2.88210e-06, min loss: 1.60767e-06\n",
      "Epoch: 681300, elapsed: 1.13e+01, train loss: 1.60614e-06, val loss: 2.37900e-06, min loss: 1.60614e-06\n",
      "Epoch: 681400, elapsed: 1.12e+01, train loss: 1.65327e-06, val loss: 2.39395e-06, min loss: 1.60614e-06\n",
      "Epoch: 681500, elapsed: 1.14e+01, train loss: 1.67008e-06, val loss: 2.40356e-06, min loss: 1.60614e-06\n",
      "Epoch: 681600, elapsed: 1.11e+01, train loss: 2.47567e-06, val loss: 3.61075e-06, min loss: 1.60614e-06\n",
      "Epoch: 681700, elapsed: 1.11e+01, train loss: 5.74651e-06, val loss: 5.94244e-06, min loss: 1.60614e-06\n",
      "Epoch: 681800, elapsed: 1.13e+01, train loss: 4.62531e-06, val loss: 4.99105e-06, min loss: 1.60614e-06\n",
      "Epoch: 681900, elapsed: 1.12e+01, train loss: 1.66882e-06, val loss: 2.69722e-06, min loss: 1.60614e-06\n",
      "Epoch: 682000, elapsed: 1.32e+01, train loss: 2.79851e-06, val loss: 4.23784e-06, min loss: 1.60614e-06\n",
      "Epoch: 682100, elapsed: 1.13e+01, train loss: 1.62171e-06, val loss: 2.37020e-06, min loss: 1.60614e-06\n",
      "Epoch: 682200, elapsed: 1.12e+01, train loss: 1.62426e-06, val loss: 2.41291e-06, min loss: 1.60614e-06\n",
      "Epoch: 682300, elapsed: 1.14e+01, train loss: 1.76348e-06, val loss: 2.46645e-06, min loss: 1.60614e-06\n",
      "Epoch: 682400, elapsed: 1.12e+01, train loss: 1.64500e-06, val loss: 2.43642e-06, min loss: 1.60614e-06\n",
      "Epoch: 682500, elapsed: 1.12e+01, train loss: 1.64806e-06, val loss: 2.47718e-06, min loss: 1.60614e-06\n",
      "Epoch: 682600, elapsed: 1.12e+01, train loss: 1.64463e-06, val loss: 2.38866e-06, min loss: 1.60614e-06\n",
      "Epoch: 682700, elapsed: 1.12e+01, train loss: 1.70657e-06, val loss: 2.47230e-06, min loss: 1.60614e-06\n",
      "Epoch: 682800, elapsed: 1.12e+01, train loss: 1.60683e-06, val loss: 2.38424e-06, min loss: 1.60614e-06\n",
      "Epoch: 682900, elapsed: 1.13e+01, train loss: 1.60988e-06, val loss: 2.37846e-06, min loss: 1.60614e-06\n",
      "Epoch: 683000, elapsed: 1.15e+01, train loss: 1.62662e-06, val loss: 2.48042e-06, min loss: 1.60614e-06\n",
      "Epoch: 683100, elapsed: 1.13e+01, train loss: 1.78006e-06, val loss: 2.61207e-06, min loss: 1.60614e-06\n",
      "Epoch: 683200, elapsed: 1.11e+01, train loss: 1.72855e-06, val loss: 2.46216e-06, min loss: 1.60614e-06\n",
      "Epoch: 683300, elapsed: 1.11e+01, train loss: 1.64637e-06, val loss: 2.44125e-06, min loss: 1.60614e-06\n",
      "Epoch: 683400, elapsed: 1.34e+01, train loss: 2.40705e-06, val loss: 3.73649e-06, min loss: 1.60614e-06\n",
      "Epoch: 683500, elapsed: 1.14e+01, train loss: 2.64042e-06, val loss: 3.55719e-06, min loss: 1.60614e-06\n",
      "Epoch: 683600, elapsed: 1.14e+01, train loss: 1.90100e-06, val loss: 2.73220e-06, min loss: 1.60614e-06\n",
      "Epoch: 683700, elapsed: 1.12e+01, train loss: 2.56603e-06, val loss: 2.82256e-06, min loss: 1.60614e-06\n",
      "Epoch: 683800, elapsed: 1.13e+01, train loss: 1.91646e-06, val loss: 2.49281e-06, min loss: 1.60614e-06\n",
      "Epoch: 683900, elapsed: 1.11e+01, train loss: 2.06808e-06, val loss: 3.64889e-06, min loss: 1.60614e-06\n",
      "Epoch: 684000, elapsed: 1.11e+01, train loss: 1.71178e-06, val loss: 2.39847e-06, min loss: 1.60614e-06\n",
      "Epoch: 684100, elapsed: 1.10e+01, train loss: 1.64402e-06, val loss: 2.39307e-06, min loss: 1.60614e-06\n",
      "Epoch: 684200, elapsed: 1.12e+01, train loss: 1.71026e-06, val loss: 2.52818e-06, min loss: 1.60614e-06\n",
      "Epoch: 684300, elapsed: 1.11e+01, train loss: 1.88445e-06, val loss: 2.55090e-06, min loss: 1.60614e-06\n",
      "Epoch: 684400, elapsed: 1.11e+01, train loss: 4.21774e-06, val loss: 5.35923e-06, min loss: 1.60614e-06\n",
      "Epoch: 684500, elapsed: 1.11e+01, train loss: 1.69449e-06, val loss: 2.36589e-06, min loss: 1.60614e-06\n",
      "Epoch: 684600, elapsed: 1.12e+01, train loss: 1.61066e-06, val loss: 2.37115e-06, min loss: 1.60614e-06\n",
      "Epoch: 684700, elapsed: 1.30e+01, train loss: 1.69255e-06, val loss: 2.48278e-06, min loss: 1.60614e-06\n",
      "Epoch: 684800, elapsed: 1.13e+01, train loss: 1.92716e-06, val loss: 2.57634e-06, min loss: 1.60614e-06\n",
      "Epoch: 684900, elapsed: 1.13e+01, train loss: 2.08851e-06, val loss: 3.11512e-06, min loss: 1.60614e-06\n",
      "Epoch: 685000, elapsed: 1.12e+01, train loss: 1.60044e-06, val loss: 2.37414e-06, min loss: 1.60044e-06\n",
      "Epoch: 685100, elapsed: 1.32e+01, train loss: 1.61178e-06, val loss: 2.39054e-06, min loss: 1.60044e-06\n",
      "Epoch: 685200, elapsed: 1.13e+01, train loss: 1.72685e-06, val loss: 2.46362e-06, min loss: 1.60044e-06\n",
      "Epoch: 685300, elapsed: 1.11e+01, train loss: 1.72439e-06, val loss: 2.84991e-06, min loss: 1.60044e-06\n",
      "Epoch: 685400, elapsed: 1.13e+01, train loss: 4.37642e-06, val loss: 5.48294e-06, min loss: 1.60044e-06\n",
      "Epoch: 685500, elapsed: 1.11e+01, train loss: 1.84301e-06, val loss: 2.64014e-06, min loss: 1.60044e-06\n",
      "Epoch: 685600, elapsed: 1.11e+01, train loss: 1.60514e-06, val loss: 2.40225e-06, min loss: 1.60044e-06\n",
      "Epoch: 685700, elapsed: 1.12e+01, train loss: 1.60872e-06, val loss: 2.40088e-06, min loss: 1.60044e-06\n",
      "Epoch: 685800, elapsed: 1.14e+01, train loss: 1.65778e-06, val loss: 2.41322e-06, min loss: 1.60044e-06\n",
      "Epoch: 685900, elapsed: 1.11e+01, train loss: 1.69704e-06, val loss: 2.52317e-06, min loss: 1.60044e-06\n",
      "Epoch: 686000, elapsed: 1.32e+01, train loss: 1.85538e-06, val loss: 2.86560e-06, min loss: 1.60044e-06\n",
      "Epoch: 686100, elapsed: 1.14e+01, train loss: 1.62158e-06, val loss: 2.39724e-06, min loss: 1.60044e-06\n",
      "Epoch: 686200, elapsed: 1.13e+01, train loss: 7.36237e-06, val loss: 6.82656e-06, min loss: 1.60044e-06\n",
      "Epoch: 686300, elapsed: 1.13e+01, train loss: 1.62022e-06, val loss: 2.38078e-06, min loss: 1.60044e-06\n",
      "Epoch: 686400, elapsed: 1.12e+01, train loss: 1.71124e-06, val loss: 2.65944e-06, min loss: 1.60044e-06\n",
      "Epoch: 686500, elapsed: 1.13e+01, train loss: 1.62717e-06, val loss: 2.43287e-06, min loss: 1.60044e-06\n",
      "Epoch: 686600, elapsed: 1.13e+01, train loss: 1.62063e-06, val loss: 2.36712e-06, min loss: 1.60044e-06\n",
      "Epoch: 686700, elapsed: 1.11e+01, train loss: 2.51719e-06, val loss: 3.07244e-06, min loss: 1.60044e-06\n",
      "Epoch: 686800, elapsed: 1.12e+01, train loss: 1.60545e-06, val loss: 2.43022e-06, min loss: 1.60044e-06\n",
      "Epoch: 686900, elapsed: 1.12e+01, train loss: 1.79336e-06, val loss: 2.56102e-06, min loss: 1.60044e-06\n",
      "Epoch: 687000, elapsed: 1.13e+01, train loss: 1.59731e-06, val loss: 2.37457e-06, min loss: 1.59731e-06\n",
      "Epoch: 687100, elapsed: 1.13e+01, train loss: 1.61253e-06, val loss: 2.42483e-06, min loss: 1.59731e-06\n",
      "Epoch: 687200, elapsed: 1.12e+01, train loss: 1.61745e-06, val loss: 2.38810e-06, min loss: 1.59731e-06\n",
      "Epoch: 687300, elapsed: 1.11e+01, train loss: 1.63384e-06, val loss: 2.38250e-06, min loss: 1.59731e-06\n",
      "Epoch: 687400, elapsed: 1.31e+01, train loss: 3.04713e-06, val loss: 4.07938e-06, min loss: 1.59731e-06\n",
      "Epoch: 687500, elapsed: 1.13e+01, train loss: 7.29223e-06, val loss: 6.86119e-06, min loss: 1.59731e-06\n",
      "Epoch: 687600, elapsed: 1.14e+01, train loss: 1.60450e-06, val loss: 2.37374e-06, min loss: 1.59731e-06\n",
      "Epoch: 687700, elapsed: 1.14e+01, train loss: 1.59611e-06, val loss: 2.38508e-06, min loss: 1.59611e-06\n",
      "Epoch: 687800, elapsed: 1.12e+01, train loss: 1.65994e-06, val loss: 2.47700e-06, min loss: 1.59611e-06\n",
      "Epoch: 687900, elapsed: 1.12e+01, train loss: 1.85623e-06, val loss: 2.45571e-06, min loss: 1.59611e-06\n",
      "Epoch: 688000, elapsed: 1.13e+01, train loss: 1.79024e-06, val loss: 2.63724e-06, min loss: 1.59611e-06\n",
      "Epoch: 688100, elapsed: 1.11e+01, train loss: 1.59676e-06, val loss: 2.38396e-06, min loss: 1.59611e-06\n",
      "Epoch: 688200, elapsed: 1.12e+01, train loss: 1.60410e-06, val loss: 2.38723e-06, min loss: 1.59611e-06\n",
      "Epoch: 688300, elapsed: 1.12e+01, train loss: 2.25614e-06, val loss: 3.23568e-06, min loss: 1.59611e-06\n",
      "Epoch: 688400, elapsed: 1.12e+01, train loss: 2.15337e-06, val loss: 3.12096e-06, min loss: 1.59611e-06\n",
      "Epoch: 688500, elapsed: 1.11e+01, train loss: 1.79970e-06, val loss: 2.51649e-06, min loss: 1.59611e-06\n",
      "Epoch: 688600, elapsed: 1.12e+01, train loss: 1.63420e-06, val loss: 2.49601e-06, min loss: 1.59611e-06\n",
      "Epoch: 688700, elapsed: 1.33e+01, train loss: 1.63576e-06, val loss: 2.47837e-06, min loss: 1.59611e-06\n",
      "Epoch: 688800, elapsed: 1.14e+01, train loss: 1.67188e-06, val loss: 2.42724e-06, min loss: 1.59611e-06\n",
      "Epoch: 688900, elapsed: 1.12e+01, train loss: 1.67288e-06, val loss: 2.40666e-06, min loss: 1.59611e-06\n",
      "Epoch: 689000, elapsed: 1.11e+01, train loss: 1.60962e-06, val loss: 2.40777e-06, min loss: 1.59611e-06\n",
      "Epoch: 689100, elapsed: 1.12e+01, train loss: 1.66059e-06, val loss: 2.41080e-06, min loss: 1.59611e-06\n",
      "Epoch: 689200, elapsed: 1.14e+01, train loss: 1.96655e-06, val loss: 3.08389e-06, min loss: 1.59611e-06\n",
      "Epoch: 689300, elapsed: 1.13e+01, train loss: 1.60201e-06, val loss: 2.37316e-06, min loss: 1.59611e-06\n",
      "Epoch: 689400, elapsed: 1.13e+01, train loss: 1.60468e-06, val loss: 2.38404e-06, min loss: 1.59611e-06\n",
      "Epoch: 689500, elapsed: 1.11e+01, train loss: 1.60066e-06, val loss: 2.41366e-06, min loss: 1.59611e-06\n",
      "Epoch: 689600, elapsed: 1.11e+01, train loss: 1.62824e-06, val loss: 2.46299e-06, min loss: 1.59611e-06\n",
      "Epoch: 689700, elapsed: 1.12e+01, train loss: 1.75536e-06, val loss: 2.63471e-06, min loss: 1.59611e-06\n",
      "Epoch: 689800, elapsed: 1.13e+01, train loss: 6.53734e-06, val loss: 6.73148e-06, min loss: 1.59611e-06\n",
      "Epoch: 689900, elapsed: 1.11e+01, train loss: 6.19048e-06, val loss: 7.46904e-06, min loss: 1.59611e-06\n",
      "Epoch: 690000, elapsed: 1.11e+01, train loss: 1.96413e-06, val loss: 2.97532e-06, min loss: 1.59611e-06\n",
      "Epoch: 690100, elapsed: 1.53e+01, train loss: 1.59393e-06, val loss: 2.37621e-06, min loss: 1.59393e-06\n",
      "Epoch: 690200, elapsed: 1.12e+01, train loss: 1.65710e-06, val loss: 2.40192e-06, min loss: 1.59393e-06\n",
      "Epoch: 690300, elapsed: 1.12e+01, train loss: 2.14382e-06, val loss: 2.54828e-06, min loss: 1.59393e-06\n",
      "Epoch: 690400, elapsed: 1.11e+01, train loss: 1.61692e-06, val loss: 2.36868e-06, min loss: 1.59393e-06\n",
      "Epoch: 690500, elapsed: 1.12e+01, train loss: 1.59618e-06, val loss: 2.37490e-06, min loss: 1.59393e-06\n",
      "Epoch: 690600, elapsed: 1.11e+01, train loss: 1.61600e-06, val loss: 2.40558e-06, min loss: 1.59393e-06\n",
      "Epoch: 690700, elapsed: 1.11e+01, train loss: 1.67239e-06, val loss: 2.46908e-06, min loss: 1.59393e-06\n",
      "Epoch: 690800, elapsed: 1.12e+01, train loss: 1.61935e-06, val loss: 2.43866e-06, min loss: 1.59393e-06\n",
      "Epoch: 690900, elapsed: 1.11e+01, train loss: 1.85118e-06, val loss: 2.76939e-06, min loss: 1.59393e-06\n",
      "Epoch: 691000, elapsed: 1.12e+01, train loss: 1.75058e-06, val loss: 2.51374e-06, min loss: 1.59393e-06\n",
      "Epoch: 691100, elapsed: 1.11e+01, train loss: 2.00261e-06, val loss: 2.96254e-06, min loss: 1.59393e-06\n",
      "Epoch: 691200, elapsed: 1.12e+01, train loss: 2.66980e-06, val loss: 3.26035e-06, min loss: 1.59393e-06\n",
      "Epoch: 691300, elapsed: 1.14e+01, train loss: 3.05591e-06, val loss: 3.35872e-06, min loss: 1.59393e-06\n",
      "Epoch: 691400, elapsed: 1.33e+01, train loss: 3.00280e-06, val loss: 4.09244e-06, min loss: 1.59393e-06\n",
      "Epoch: 691500, elapsed: 1.13e+01, train loss: 1.81685e-06, val loss: 2.54955e-06, min loss: 1.59393e-06\n",
      "Epoch: 691600, elapsed: 1.14e+01, train loss: 1.59568e-06, val loss: 2.45325e-06, min loss: 1.59393e-06\n",
      "Epoch: 691700, elapsed: 1.15e+01, train loss: 1.63396e-06, val loss: 2.42237e-06, min loss: 1.59393e-06\n",
      "Epoch: 691800, elapsed: 1.13e+01, train loss: 1.63254e-06, val loss: 2.46514e-06, min loss: 1.59393e-06\n",
      "Epoch: 691900, elapsed: 1.15e+01, train loss: 3.51365e-06, val loss: 4.21518e-06, min loss: 1.59393e-06\n",
      "Epoch: 692000, elapsed: 1.13e+01, train loss: 2.23757e-06, val loss: 2.92073e-06, min loss: 1.59393e-06\n",
      "Epoch: 692100, elapsed: 1.11e+01, train loss: 1.64886e-06, val loss: 2.44811e-06, min loss: 1.59393e-06\n",
      "Epoch: 692200, elapsed: 1.14e+01, train loss: 1.62534e-06, val loss: 2.41766e-06, min loss: 1.59393e-06\n",
      "Epoch: 692300, elapsed: 1.13e+01, train loss: 1.60323e-06, val loss: 2.36945e-06, min loss: 1.59393e-06\n",
      "Epoch: 692400, elapsed: 1.13e+01, train loss: 2.16314e-06, val loss: 2.62897e-06, min loss: 1.59393e-06\n",
      "Epoch: 692500, elapsed: 1.14e+01, train loss: 6.87676e-06, val loss: 8.55114e-06, min loss: 1.59393e-06\n",
      "Epoch: 692600, elapsed: 1.13e+01, train loss: 2.07397e-06, val loss: 2.68517e-06, min loss: 1.59393e-06\n",
      "Epoch: 692700, elapsed: 1.33e+01, train loss: 1.72287e-06, val loss: 2.49941e-06, min loss: 1.59393e-06\n",
      "Epoch: 692800, elapsed: 1.12e+01, train loss: 4.62530e-06, val loss: 5.04245e-06, min loss: 1.59393e-06\n",
      "Epoch: 692900, elapsed: 1.15e+01, train loss: 1.62059e-06, val loss: 2.39942e-06, min loss: 1.59393e-06\n",
      "Epoch: 693000, elapsed: 1.14e+01, train loss: 3.80539e-06, val loss: 3.68868e-06, min loss: 1.59393e-06\n",
      "Epoch: 693100, elapsed: 1.13e+01, train loss: 2.11069e-06, val loss: 2.89162e-06, min loss: 1.59393e-06\n",
      "Epoch: 693200, elapsed: 1.12e+01, train loss: 1.63438e-06, val loss: 2.40162e-06, min loss: 1.59393e-06\n",
      "Epoch: 693300, elapsed: 1.13e+01, train loss: 1.75453e-06, val loss: 2.54587e-06, min loss: 1.59393e-06\n",
      "Epoch: 693400, elapsed: 1.13e+01, train loss: 1.58783e-06, val loss: 2.41491e-06, min loss: 1.58783e-06\n",
      "Epoch: 693500, elapsed: 1.12e+01, train loss: 2.02015e-06, val loss: 2.45570e-06, min loss: 1.58783e-06\n",
      "Epoch: 693600, elapsed: 1.13e+01, train loss: 1.70274e-06, val loss: 2.40854e-06, min loss: 1.58783e-06\n",
      "Epoch: 693700, elapsed: 1.12e+01, train loss: 1.63112e-06, val loss: 2.44544e-06, min loss: 1.58783e-06\n",
      "Epoch: 693800, elapsed: 1.12e+01, train loss: 2.39296e-06, val loss: 3.55643e-06, min loss: 1.58783e-06\n",
      "Epoch: 693900, elapsed: 1.11e+01, train loss: 4.07942e-06, val loss: 4.87207e-06, min loss: 1.58783e-06\n",
      "Epoch: 694000, elapsed: 1.11e+01, train loss: 1.83411e-06, val loss: 2.59417e-06, min loss: 1.58783e-06\n",
      "Epoch: 694100, elapsed: 1.32e+01, train loss: 2.55533e-06, val loss: 2.95822e-06, min loss: 1.58783e-06\n",
      "Epoch: 694200, elapsed: 1.16e+01, train loss: 2.12165e-06, val loss: 3.18963e-06, min loss: 1.58783e-06\n",
      "Epoch: 694300, elapsed: 1.12e+01, train loss: 1.85600e-06, val loss: 2.81875e-06, min loss: 1.58783e-06\n",
      "Epoch: 694400, elapsed: 1.11e+01, train loss: 1.58787e-06, val loss: 2.39919e-06, min loss: 1.58783e-06\n",
      "Epoch: 694500, elapsed: 1.09e+01, train loss: 1.58636e-06, val loss: 2.37362e-06, min loss: 1.58636e-06\n",
      "Epoch: 694600, elapsed: 1.09e+01, train loss: 1.62760e-06, val loss: 2.42632e-06, min loss: 1.58636e-06\n",
      "Epoch: 694700, elapsed: 1.11e+01, train loss: 1.89350e-06, val loss: 2.76191e-06, min loss: 1.58636e-06\n",
      "Epoch: 694800, elapsed: 1.10e+01, train loss: 1.77301e-06, val loss: 2.79161e-06, min loss: 1.58636e-06\n",
      "Epoch: 694900, elapsed: 1.11e+01, train loss: 2.94500e-06, val loss: 4.00730e-06, min loss: 1.58636e-06\n",
      "Epoch: 695000, elapsed: 1.10e+01, train loss: 1.63600e-06, val loss: 2.37671e-06, min loss: 1.58636e-06\n",
      "Epoch: 695100, elapsed: 1.30e+01, train loss: 1.60126e-06, val loss: 2.40377e-06, min loss: 1.58636e-06\n",
      "Epoch: 695200, elapsed: 1.12e+01, train loss: 1.75584e-06, val loss: 2.52981e-06, min loss: 1.58636e-06\n",
      "Epoch: 695300, elapsed: 1.12e+01, train loss: 3.84263e-06, val loss: 4.08341e-06, min loss: 1.58636e-06\n",
      "Epoch: 695400, elapsed: 1.33e+01, train loss: 1.91329e-06, val loss: 2.87760e-06, min loss: 1.58636e-06\n",
      "Epoch: 695500, elapsed: 1.14e+01, train loss: 4.40948e-06, val loss: 6.10425e-06, min loss: 1.58636e-06\n",
      "Epoch: 695600, elapsed: 1.14e+01, train loss: 1.71348e-06, val loss: 2.50712e-06, min loss: 1.58636e-06\n",
      "Epoch: 695700, elapsed: 1.12e+01, train loss: 1.79234e-06, val loss: 2.68605e-06, min loss: 1.58636e-06\n",
      "Epoch: 695800, elapsed: 1.14e+01, train loss: 1.67885e-06, val loss: 2.60567e-06, min loss: 1.58636e-06\n",
      "Epoch: 695900, elapsed: 1.12e+01, train loss: 1.58291e-06, val loss: 2.38848e-06, min loss: 1.58291e-06\n",
      "Epoch: 696000, elapsed: 1.11e+01, train loss: 1.71800e-06, val loss: 2.55380e-06, min loss: 1.58291e-06\n",
      "Epoch: 696100, elapsed: 1.09e+01, train loss: 9.36326e-06, val loss: 7.39816e-06, min loss: 1.58291e-06\n",
      "Epoch: 696200, elapsed: 1.11e+01, train loss: 1.58095e-06, val loss: 2.37625e-06, min loss: 1.58095e-06\n",
      "Epoch: 696300, elapsed: 1.13e+01, train loss: 1.67859e-06, val loss: 2.47452e-06, min loss: 1.58095e-06\n",
      "Epoch: 696400, elapsed: 1.12e+01, train loss: 1.73770e-06, val loss: 2.52164e-06, min loss: 1.58095e-06\n",
      "Epoch: 696500, elapsed: 1.14e+01, train loss: 1.58137e-06, val loss: 2.37688e-06, min loss: 1.58095e-06\n",
      "Epoch: 696600, elapsed: 1.13e+01, train loss: 1.58605e-06, val loss: 2.37288e-06, min loss: 1.58095e-06\n",
      "Epoch: 696700, elapsed: 1.14e+01, train loss: 1.71847e-06, val loss: 2.62346e-06, min loss: 1.58095e-06\n",
      "Epoch: 696800, elapsed: 1.32e+01, train loss: 3.33596e-06, val loss: 4.46875e-06, min loss: 1.58095e-06\n",
      "Epoch: 696900, elapsed: 1.17e+01, train loss: 1.84684e-06, val loss: 2.57273e-06, min loss: 1.58095e-06\n",
      "Epoch: 697000, elapsed: 1.16e+01, train loss: 1.58240e-06, val loss: 2.37805e-06, min loss: 1.58095e-06\n",
      "Epoch: 697100, elapsed: 1.14e+01, train loss: 1.58425e-06, val loss: 2.38028e-06, min loss: 1.58095e-06\n",
      "Epoch: 697200, elapsed: 1.15e+01, train loss: 1.57992e-06, val loss: 2.37991e-06, min loss: 1.57992e-06\n",
      "Epoch: 697300, elapsed: 1.14e+01, train loss: 1.58605e-06, val loss: 2.39307e-06, min loss: 1.57992e-06\n",
      "Epoch: 697400, elapsed: 1.14e+01, train loss: 3.90747e-06, val loss: 3.13092e-06, min loss: 1.57992e-06\n",
      "Epoch: 697500, elapsed: 1.12e+01, train loss: 2.77092e-06, val loss: 3.25101e-06, min loss: 1.57992e-06\n",
      "Epoch: 697600, elapsed: 1.12e+01, train loss: 1.61482e-06, val loss: 2.37893e-06, min loss: 1.57992e-06\n",
      "Epoch: 697700, elapsed: 1.12e+01, train loss: 1.77461e-06, val loss: 2.46119e-06, min loss: 1.57992e-06\n",
      "Epoch: 697800, elapsed: 1.13e+01, train loss: 1.63491e-06, val loss: 2.38297e-06, min loss: 1.57992e-06\n",
      "Epoch: 697900, elapsed: 1.14e+01, train loss: 4.73852e-06, val loss: 4.92000e-06, min loss: 1.57992e-06\n",
      "Epoch: 698000, elapsed: 1.12e+01, train loss: 1.91656e-06, val loss: 2.51424e-06, min loss: 1.57992e-06\n",
      "Epoch: 698100, elapsed: 1.31e+01, train loss: 1.57991e-06, val loss: 2.37950e-06, min loss: 1.57991e-06\n",
      "Epoch: 698200, elapsed: 1.13e+01, train loss: 1.58603e-06, val loss: 2.36799e-06, min loss: 1.57991e-06\n",
      "Epoch: 698300, elapsed: 1.14e+01, train loss: 1.61082e-06, val loss: 2.37869e-06, min loss: 1.57991e-06\n",
      "Epoch: 698400, elapsed: 1.14e+01, train loss: 1.59782e-06, val loss: 2.43576e-06, min loss: 1.57991e-06\n",
      "Epoch: 698500, elapsed: 1.13e+01, train loss: 1.57896e-06, val loss: 2.38834e-06, min loss: 1.57896e-06\n",
      "Epoch: 698600, elapsed: 1.12e+01, train loss: 1.67557e-06, val loss: 2.42253e-06, min loss: 1.57896e-06\n",
      "Epoch: 698700, elapsed: 1.13e+01, train loss: 1.69629e-06, val loss: 2.52744e-06, min loss: 1.57896e-06\n",
      "Epoch: 698800, elapsed: 1.13e+01, train loss: 1.66391e-06, val loss: 2.49250e-06, min loss: 1.57896e-06\n",
      "Epoch: 698900, elapsed: 1.12e+01, train loss: 1.59446e-06, val loss: 2.37898e-06, min loss: 1.57896e-06\n",
      "Epoch: 699000, elapsed: 1.12e+01, train loss: 1.59437e-06, val loss: 2.43391e-06, min loss: 1.57896e-06\n",
      "Epoch: 699100, elapsed: 1.12e+01, train loss: 1.57668e-06, val loss: 2.38759e-06, min loss: 1.57668e-06\n",
      "Epoch: 699200, elapsed: 1.10e+01, train loss: 1.91126e-06, val loss: 2.43730e-06, min loss: 1.57668e-06\n",
      "Epoch: 699300, elapsed: 1.12e+01, train loss: 1.59236e-06, val loss: 2.37799e-06, min loss: 1.57668e-06\n",
      "Epoch: 699400, elapsed: 1.13e+01, train loss: 1.57891e-06, val loss: 2.38366e-06, min loss: 1.57668e-06\n",
      "Epoch: 699500, elapsed: 1.32e+01, train loss: 2.43382e-06, val loss: 3.64218e-06, min loss: 1.57668e-06\n",
      "Epoch: 699600, elapsed: 1.13e+01, train loss: 1.74682e-06, val loss: 2.53553e-06, min loss: 1.57668e-06\n",
      "Epoch: 699700, elapsed: 1.13e+01, train loss: 2.00127e-06, val loss: 2.46764e-06, min loss: 1.57668e-06\n",
      "Epoch: 699800, elapsed: 1.14e+01, train loss: 1.75208e-06, val loss: 2.50785e-06, min loss: 1.57668e-06\n",
      "Epoch: 699900, elapsed: 1.12e+01, train loss: 2.38705e-06, val loss: 3.28705e-06, min loss: 1.57668e-06\n",
      "Epoch: 700000, elapsed: 1.12e+01, train loss: 5.36938e-06, val loss: 5.49088e-06, min loss: 1.57668e-06\n",
      "Epoch: 700100, elapsed: 1.32e+01, train loss: 5.15670e-06, val loss: 4.17712e-06, min loss: 1.57668e-06\n",
      "Epoch: 700200, elapsed: 1.11e+01, train loss: 2.51591e-06, val loss: 3.42341e-06, min loss: 1.57668e-06\n",
      "Epoch: 700300, elapsed: 1.11e+01, train loss: 1.57630e-06, val loss: 2.37462e-06, min loss: 1.57630e-06\n",
      "Epoch: 700400, elapsed: 1.12e+01, train loss: 1.57690e-06, val loss: 2.39738e-06, min loss: 1.57630e-06\n",
      "Epoch: 700500, elapsed: 1.11e+01, train loss: 1.83484e-06, val loss: 2.61685e-06, min loss: 1.57630e-06\n",
      "Epoch: 700600, elapsed: 1.11e+01, train loss: 2.88428e-06, val loss: 3.91647e-06, min loss: 1.57630e-06\n",
      "Epoch: 700700, elapsed: 1.11e+01, train loss: 3.80198e-06, val loss: 4.99335e-06, min loss: 1.57630e-06\n",
      "Epoch: 700800, elapsed: 1.32e+01, train loss: 6.50425e-06, val loss: 7.65271e-06, min loss: 1.57630e-06\n",
      "Epoch: 700900, elapsed: 1.14e+01, train loss: 2.35657e-06, val loss: 3.28443e-06, min loss: 1.57630e-06\n",
      "Epoch: 701000, elapsed: 1.14e+01, train loss: 1.72020e-06, val loss: 2.43364e-06, min loss: 1.57630e-06\n",
      "Epoch: 701100, elapsed: 1.13e+01, train loss: 1.57474e-06, val loss: 2.39245e-06, min loss: 1.57474e-06\n",
      "Epoch: 701200, elapsed: 1.12e+01, train loss: 1.61209e-06, val loss: 2.43905e-06, min loss: 1.57474e-06\n",
      "Epoch: 701300, elapsed: 1.12e+01, train loss: 2.07127e-06, val loss: 2.98153e-06, min loss: 1.57474e-06\n",
      "Epoch: 701400, elapsed: 1.13e+01, train loss: 1.92340e-06, val loss: 2.73134e-06, min loss: 1.57474e-06\n",
      "Epoch: 701500, elapsed: 1.13e+01, train loss: 1.71332e-06, val loss: 2.52505e-06, min loss: 1.57474e-06\n",
      "Epoch: 701600, elapsed: 1.12e+01, train loss: 2.15012e-06, val loss: 2.51538e-06, min loss: 1.57474e-06\n",
      "Epoch: 701700, elapsed: 1.12e+01, train loss: 2.28446e-06, val loss: 3.02068e-06, min loss: 1.57474e-06\n",
      "Epoch: 701800, elapsed: 1.13e+01, train loss: 2.05292e-06, val loss: 2.61963e-06, min loss: 1.57474e-06\n",
      "Epoch: 701900, elapsed: 1.12e+01, train loss: 1.65354e-06, val loss: 2.42906e-06, min loss: 1.57474e-06\n",
      "Epoch: 702000, elapsed: 1.11e+01, train loss: 1.75664e-06, val loss: 2.46696e-06, min loss: 1.57474e-06\n",
      "Epoch: 702100, elapsed: 1.12e+01, train loss: 4.35470e-06, val loss: 5.35686e-06, min loss: 1.57474e-06\n",
      "Epoch: 702200, elapsed: 1.34e+01, train loss: 1.57077e-06, val loss: 2.38012e-06, min loss: 1.57077e-06\n",
      "Epoch: 702300, elapsed: 1.14e+01, train loss: 1.57138e-06, val loss: 2.38901e-06, min loss: 1.57077e-06\n",
      "Epoch: 702400, elapsed: 1.14e+01, train loss: 1.98257e-06, val loss: 2.52639e-06, min loss: 1.57077e-06\n",
      "Epoch: 702500, elapsed: 1.13e+01, train loss: 1.69033e-06, val loss: 2.52490e-06, min loss: 1.57077e-06\n",
      "Epoch: 702600, elapsed: 1.12e+01, train loss: 1.98328e-06, val loss: 2.49158e-06, min loss: 1.57077e-06\n",
      "Epoch: 702700, elapsed: 1.13e+01, train loss: 1.58622e-06, val loss: 2.40699e-06, min loss: 1.57077e-06\n",
      "Epoch: 702800, elapsed: 1.13e+01, train loss: 1.58180e-06, val loss: 2.36327e-06, min loss: 1.57077e-06\n",
      "Epoch: 702900, elapsed: 1.13e+01, train loss: 1.81138e-06, val loss: 2.45425e-06, min loss: 1.57077e-06\n",
      "Epoch: 703000, elapsed: 1.13e+01, train loss: 1.68591e-06, val loss: 2.58998e-06, min loss: 1.57077e-06\n",
      "Epoch: 703100, elapsed: 1.13e+01, train loss: 1.57124e-06, val loss: 2.38528e-06, min loss: 1.57077e-06\n",
      "Epoch: 703200, elapsed: 1.12e+01, train loss: 1.57008e-06, val loss: 2.39061e-06, min loss: 1.57008e-06\n",
      "Epoch: 703300, elapsed: 1.12e+01, train loss: 1.62725e-06, val loss: 2.54542e-06, min loss: 1.57008e-06\n",
      "Epoch: 703400, elapsed: 1.12e+01, train loss: 1.64490e-06, val loss: 2.38359e-06, min loss: 1.57008e-06\n",
      "Epoch: 703500, elapsed: 1.12e+01, train loss: 1.57079e-06, val loss: 2.37595e-06, min loss: 1.57008e-06\n",
      "Epoch: 703600, elapsed: 1.33e+01, train loss: 1.57786e-06, val loss: 2.38217e-06, min loss: 1.57008e-06\n",
      "Epoch: 703700, elapsed: 1.12e+01, train loss: 1.66869e-06, val loss: 2.42082e-06, min loss: 1.57008e-06\n",
      "Epoch: 703800, elapsed: 1.14e+01, train loss: 2.13507e-06, val loss: 2.77247e-06, min loss: 1.57008e-06\n",
      "Epoch: 703900, elapsed: 1.13e+01, train loss: 3.10419e-06, val loss: 3.09803e-06, min loss: 1.57008e-06\n",
      "Epoch: 704000, elapsed: 1.12e+01, train loss: 6.64772e-06, val loss: 5.38717e-06, min loss: 1.57008e-06\n",
      "Epoch: 704100, elapsed: 1.12e+01, train loss: 2.47252e-06, val loss: 3.45515e-06, min loss: 1.57008e-06\n",
      "Epoch: 704200, elapsed: 1.13e+01, train loss: 1.61182e-06, val loss: 2.37359e-06, min loss: 1.57008e-06\n",
      "Epoch: 704300, elapsed: 1.15e+01, train loss: 1.64814e-06, val loss: 2.48150e-06, min loss: 1.57008e-06\n",
      "Epoch: 704400, elapsed: 1.12e+01, train loss: 1.58208e-06, val loss: 2.41777e-06, min loss: 1.57008e-06\n",
      "Epoch: 704500, elapsed: 1.13e+01, train loss: 1.60461e-06, val loss: 2.43163e-06, min loss: 1.57008e-06\n",
      "Epoch: 704600, elapsed: 1.11e+01, train loss: 1.57519e-06, val loss: 2.38588e-06, min loss: 1.57008e-06\n",
      "Epoch: 704700, elapsed: 1.14e+01, train loss: 1.75333e-06, val loss: 2.75649e-06, min loss: 1.57008e-06\n",
      "Epoch: 704800, elapsed: 1.12e+01, train loss: 1.57653e-06, val loss: 2.43305e-06, min loss: 1.57008e-06\n",
      "Epoch: 704900, elapsed: 1.12e+01, train loss: 1.64231e-06, val loss: 2.46958e-06, min loss: 1.57008e-06\n",
      "Epoch: 705000, elapsed: 1.35e+01, train loss: 3.45318e-06, val loss: 3.82935e-06, min loss: 1.57008e-06\n",
      "Epoch: 705100, elapsed: 1.35e+01, train loss: 1.58076e-06, val loss: 2.36094e-06, min loss: 1.57008e-06\n",
      "Epoch: 705200, elapsed: 1.13e+01, train loss: 1.84461e-06, val loss: 2.98991e-06, min loss: 1.57008e-06\n",
      "Epoch: 705300, elapsed: 1.14e+01, train loss: 1.56831e-06, val loss: 2.38577e-06, min loss: 1.56831e-06\n",
      "Epoch: 705400, elapsed: 1.11e+01, train loss: 3.41367e-06, val loss: 3.03653e-06, min loss: 1.56831e-06\n",
      "Epoch: 705500, elapsed: 1.13e+01, train loss: 1.56749e-06, val loss: 2.36640e-06, min loss: 1.56749e-06\n",
      "Epoch: 705600, elapsed: 1.13e+01, train loss: 1.56677e-06, val loss: 2.39550e-06, min loss: 1.56677e-06\n",
      "Epoch: 705700, elapsed: 1.12e+01, train loss: 1.58202e-06, val loss: 2.36756e-06, min loss: 1.56677e-06\n",
      "Epoch: 705800, elapsed: 1.14e+01, train loss: 1.58439e-06, val loss: 2.40755e-06, min loss: 1.56677e-06\n",
      "Epoch: 705900, elapsed: 1.13e+01, train loss: 1.58964e-06, val loss: 2.38781e-06, min loss: 1.56677e-06\n",
      "Epoch: 706000, elapsed: 1.12e+01, train loss: 1.66609e-06, val loss: 2.43970e-06, min loss: 1.56677e-06\n",
      "Epoch: 706100, elapsed: 1.15e+01, train loss: 2.40612e-06, val loss: 2.67757e-06, min loss: 1.56677e-06\n",
      "Epoch: 706200, elapsed: 1.15e+01, train loss: 1.68090e-06, val loss: 2.68653e-06, min loss: 1.56677e-06\n",
      "Epoch: 706300, elapsed: 1.37e+01, train loss: 1.88191e-06, val loss: 2.88622e-06, min loss: 1.56677e-06\n",
      "Epoch: 706400, elapsed: 1.15e+01, train loss: 1.87150e-06, val loss: 2.71549e-06, min loss: 1.56677e-06\n",
      "Epoch: 706500, elapsed: 1.15e+01, train loss: 2.29161e-06, val loss: 3.07675e-06, min loss: 1.56677e-06\n",
      "Epoch: 706600, elapsed: 1.15e+01, train loss: 1.56630e-06, val loss: 2.38313e-06, min loss: 1.56630e-06\n",
      "Epoch: 706700, elapsed: 1.13e+01, train loss: 1.56841e-06, val loss: 2.38941e-06, min loss: 1.56630e-06\n",
      "Epoch: 706800, elapsed: 1.12e+01, train loss: 1.61323e-06, val loss: 2.43283e-06, min loss: 1.56630e-06\n",
      "Epoch: 706900, elapsed: 1.14e+01, train loss: 1.02470e-05, val loss: 1.00902e-05, min loss: 1.56630e-06\n",
      "Epoch: 707000, elapsed: 1.16e+01, train loss: 1.61049e-06, val loss: 2.52035e-06, min loss: 1.56630e-06\n",
      "Epoch: 707100, elapsed: 1.13e+01, train loss: 1.63680e-06, val loss: 2.39545e-06, min loss: 1.56630e-06\n",
      "Epoch: 707200, elapsed: 1.13e+01, train loss: 1.56421e-06, val loss: 2.38498e-06, min loss: 1.56421e-06\n",
      "Epoch: 707300, elapsed: 1.12e+01, train loss: 1.63620e-06, val loss: 2.47788e-06, min loss: 1.56421e-06\n",
      "Epoch: 707400, elapsed: 1.12e+01, train loss: 1.59633e-06, val loss: 2.44059e-06, min loss: 1.56421e-06\n",
      "Epoch: 707500, elapsed: 1.12e+01, train loss: 1.56500e-06, val loss: 2.37743e-06, min loss: 1.56421e-06\n",
      "Epoch: 707600, elapsed: 1.13e+01, train loss: 1.94370e-06, val loss: 2.44636e-06, min loss: 1.56421e-06\n",
      "Epoch: 707700, elapsed: 1.32e+01, train loss: 5.56339e-06, val loss: 5.62637e-06, min loss: 1.56421e-06\n",
      "Epoch: 707800, elapsed: 1.15e+01, train loss: 2.54873e-06, val loss: 3.56299e-06, min loss: 1.56421e-06\n",
      "Epoch: 707900, elapsed: 1.13e+01, train loss: 1.56193e-06, val loss: 2.38857e-06, min loss: 1.56193e-06\n",
      "Epoch: 708000, elapsed: 1.14e+01, train loss: 1.61014e-06, val loss: 2.44165e-06, min loss: 1.56193e-06\n",
      "Epoch: 708100, elapsed: 1.16e+01, train loss: 1.66289e-06, val loss: 2.59492e-06, min loss: 1.56193e-06\n",
      "Epoch: 708200, elapsed: 1.14e+01, train loss: 4.63832e-06, val loss: 5.72981e-06, min loss: 1.56193e-06\n",
      "Epoch: 708300, elapsed: 1.13e+01, train loss: 1.57209e-06, val loss: 2.37502e-06, min loss: 1.56193e-06\n",
      "Epoch: 708400, elapsed: 1.14e+01, train loss: 1.65537e-06, val loss: 2.53612e-06, min loss: 1.56193e-06\n",
      "Epoch: 708500, elapsed: 1.13e+01, train loss: 1.56552e-06, val loss: 2.38996e-06, min loss: 1.56193e-06\n",
      "Epoch: 708600, elapsed: 1.13e+01, train loss: 1.58554e-06, val loss: 2.39680e-06, min loss: 1.56193e-06\n",
      "Epoch: 708700, elapsed: 1.12e+01, train loss: 1.57757e-06, val loss: 2.38023e-06, min loss: 1.56193e-06\n",
      "Epoch: 708800, elapsed: 1.14e+01, train loss: 1.66866e-06, val loss: 2.42517e-06, min loss: 1.56193e-06\n",
      "Epoch: 708900, elapsed: 1.12e+01, train loss: 2.74177e-06, val loss: 4.16387e-06, min loss: 1.56193e-06\n",
      "Epoch: 709000, elapsed: 1.12e+01, train loss: 1.55960e-06, val loss: 2.38391e-06, min loss: 1.55960e-06\n",
      "Epoch: 709100, elapsed: 1.33e+01, train loss: 1.56076e-06, val loss: 2.37487e-06, min loss: 1.55960e-06\n",
      "Epoch: 709200, elapsed: 1.15e+01, train loss: 1.56390e-06, val loss: 2.37563e-06, min loss: 1.55960e-06\n",
      "Epoch: 709300, elapsed: 1.14e+01, train loss: 1.59169e-06, val loss: 2.42661e-06, min loss: 1.55960e-06\n",
      "Epoch: 709400, elapsed: 1.13e+01, train loss: 1.72826e-06, val loss: 2.47394e-06, min loss: 1.55960e-06\n",
      "Epoch: 709500, elapsed: 1.12e+01, train loss: 1.63964e-06, val loss: 2.44546e-06, min loss: 1.55960e-06\n",
      "Epoch: 709600, elapsed: 1.11e+01, train loss: 1.56022e-06, val loss: 2.38514e-06, min loss: 1.55960e-06\n",
      "Epoch: 709700, elapsed: 1.13e+01, train loss: 2.82630e-06, val loss: 3.48938e-06, min loss: 1.55960e-06\n",
      "Epoch: 709800, elapsed: 1.13e+01, train loss: 3.19287e-06, val loss: 3.33864e-06, min loss: 1.55960e-06\n",
      "Epoch: 709900, elapsed: 1.13e+01, train loss: 4.76603e-06, val loss: 5.58735e-06, min loss: 1.55960e-06\n",
      "Epoch: 710000, elapsed: 1.14e+01, train loss: 1.80322e-06, val loss: 2.72343e-06, min loss: 1.55960e-06\n",
      "Epoch: 710100, elapsed: 1.33e+01, train loss: 1.57740e-06, val loss: 2.43206e-06, min loss: 1.55960e-06\n",
      "Epoch: 710200, elapsed: 1.11e+01, train loss: 1.59701e-06, val loss: 2.39351e-06, min loss: 1.55960e-06\n",
      "Epoch: 710300, elapsed: 1.13e+01, train loss: 1.61122e-06, val loss: 2.36934e-06, min loss: 1.55960e-06\n",
      "Epoch: 710400, elapsed: 1.32e+01, train loss: 1.65959e-06, val loss: 2.56842e-06, min loss: 1.55960e-06\n",
      "Epoch: 710500, elapsed: 1.15e+01, train loss: 1.57757e-06, val loss: 2.42510e-06, min loss: 1.55960e-06\n",
      "Epoch: 710600, elapsed: 1.13e+01, train loss: 1.83625e-06, val loss: 2.89757e-06, min loss: 1.55960e-06\n",
      "Epoch: 710700, elapsed: 1.14e+01, train loss: 1.61158e-06, val loss: 2.49426e-06, min loss: 1.55960e-06\n",
      "Epoch: 710800, elapsed: 1.13e+01, train loss: 1.59921e-06, val loss: 2.52135e-06, min loss: 1.55960e-06\n",
      "Epoch: 710900, elapsed: 1.12e+01, train loss: 1.63510e-06, val loss: 2.51361e-06, min loss: 1.55960e-06\n",
      "Epoch: 711000, elapsed: 1.13e+01, train loss: 2.98397e-06, val loss: 4.14257e-06, min loss: 1.55960e-06\n",
      "Epoch: 711100, elapsed: 1.13e+01, train loss: 1.57313e-06, val loss: 2.39392e-06, min loss: 1.55960e-06\n",
      "Epoch: 711200, elapsed: 1.13e+01, train loss: 1.69398e-06, val loss: 2.51304e-06, min loss: 1.55960e-06\n",
      "Epoch: 711300, elapsed: 1.14e+01, train loss: 1.55842e-06, val loss: 2.38774e-06, min loss: 1.55842e-06\n",
      "Epoch: 711400, elapsed: 1.13e+01, train loss: 4.05997e-06, val loss: 5.96457e-06, min loss: 1.55842e-06\n",
      "Epoch: 711500, elapsed: 1.13e+01, train loss: 1.61436e-06, val loss: 2.47197e-06, min loss: 1.55842e-06\n",
      "Epoch: 711600, elapsed: 1.14e+01, train loss: 1.56063e-06, val loss: 2.39830e-06, min loss: 1.55842e-06\n",
      "Epoch: 711700, elapsed: 1.14e+01, train loss: 1.57663e-06, val loss: 2.39392e-06, min loss: 1.55842e-06\n",
      "Epoch: 711800, elapsed: 1.34e+01, train loss: 4.03429e-06, val loss: 5.71390e-06, min loss: 1.55842e-06\n",
      "Epoch: 711900, elapsed: 1.16e+01, train loss: 1.55454e-06, val loss: 2.38176e-06, min loss: 1.55454e-06\n",
      "Epoch: 712000, elapsed: 1.14e+01, train loss: 1.56449e-06, val loss: 2.38414e-06, min loss: 1.55454e-06\n",
      "Epoch: 712100, elapsed: 1.16e+01, train loss: 1.59835e-06, val loss: 2.38626e-06, min loss: 1.55454e-06\n",
      "Epoch: 712200, elapsed: 1.13e+01, train loss: 1.76765e-06, val loss: 2.60369e-06, min loss: 1.55454e-06\n",
      "Epoch: 712300, elapsed: 1.14e+01, train loss: 1.68872e-06, val loss: 2.36422e-06, min loss: 1.55454e-06\n",
      "Epoch: 712400, elapsed: 1.13e+01, train loss: 1.63211e-06, val loss: 2.42498e-06, min loss: 1.55454e-06\n",
      "Epoch: 712500, elapsed: 1.15e+01, train loss: 2.24326e-06, val loss: 3.16177e-06, min loss: 1.55454e-06\n",
      "Epoch: 712600, elapsed: 1.14e+01, train loss: 2.30754e-06, val loss: 3.45465e-06, min loss: 1.55454e-06\n",
      "Epoch: 712700, elapsed: 1.13e+01, train loss: 3.03118e-06, val loss: 3.11547e-06, min loss: 1.55454e-06\n",
      "Epoch: 712800, elapsed: 1.14e+01, train loss: 1.63088e-06, val loss: 2.52076e-06, min loss: 1.55454e-06\n",
      "Epoch: 712900, elapsed: 1.14e+01, train loss: 1.75286e-06, val loss: 2.42581e-06, min loss: 1.55454e-06\n",
      "Epoch: 713000, elapsed: 1.13e+01, train loss: 1.65348e-06, val loss: 2.48652e-06, min loss: 1.55454e-06\n",
      "Epoch: 713100, elapsed: 1.13e+01, train loss: 1.55591e-06, val loss: 2.38307e-06, min loss: 1.55454e-06\n",
      "Epoch: 713200, elapsed: 1.35e+01, train loss: 4.00254e-06, val loss: 6.67671e-06, min loss: 1.55454e-06\n",
      "Epoch: 713300, elapsed: 1.14e+01, train loss: 1.55244e-06, val loss: 2.38077e-06, min loss: 1.55244e-06\n",
      "Epoch: 713400, elapsed: 1.16e+01, train loss: 2.53424e-06, val loss: 3.57505e-06, min loss: 1.55244e-06\n",
      "Epoch: 713500, elapsed: 1.14e+01, train loss: 1.55203e-06, val loss: 2.38219e-06, min loss: 1.55203e-06\n",
      "Epoch: 713600, elapsed: 1.14e+01, train loss: 1.55802e-06, val loss: 2.38143e-06, min loss: 1.55203e-06\n",
      "Epoch: 713700, elapsed: 1.12e+01, train loss: 1.56149e-06, val loss: 2.41840e-06, min loss: 1.55203e-06\n",
      "Epoch: 713800, elapsed: 1.12e+01, train loss: 1.58178e-06, val loss: 2.43010e-06, min loss: 1.55203e-06\n",
      "Epoch: 713900, elapsed: 1.12e+01, train loss: 1.56285e-06, val loss: 2.41169e-06, min loss: 1.55203e-06\n",
      "Epoch: 714000, elapsed: 1.13e+01, train loss: 1.55527e-06, val loss: 2.37482e-06, min loss: 1.55203e-06\n",
      "Epoch: 714100, elapsed: 1.13e+01, train loss: 1.59653e-06, val loss: 2.43845e-06, min loss: 1.55203e-06\n",
      "Epoch: 714200, elapsed: 1.12e+01, train loss: 1.76814e-06, val loss: 2.49360e-06, min loss: 1.55203e-06\n",
      "Epoch: 714300, elapsed: 1.12e+01, train loss: 1.59271e-06, val loss: 2.39747e-06, min loss: 1.55203e-06\n",
      "Epoch: 714400, elapsed: 1.13e+01, train loss: 2.58992e-06, val loss: 3.86964e-06, min loss: 1.55203e-06\n",
      "Epoch: 714500, elapsed: 1.12e+01, train loss: 2.60707e-06, val loss: 3.54475e-06, min loss: 1.55203e-06\n",
      "Epoch: 714600, elapsed: 1.32e+01, train loss: 1.62486e-06, val loss: 2.37949e-06, min loss: 1.55203e-06\n",
      "Epoch: 714700, elapsed: 1.14e+01, train loss: 1.60753e-06, val loss: 2.37056e-06, min loss: 1.55203e-06\n",
      "Epoch: 714800, elapsed: 1.14e+01, train loss: 1.58439e-06, val loss: 2.40847e-06, min loss: 1.55203e-06\n",
      "Epoch: 714900, elapsed: 1.13e+01, train loss: 1.58541e-06, val loss: 2.42598e-06, min loss: 1.55203e-06\n",
      "Epoch: 715000, elapsed: 1.12e+01, train loss: 1.59076e-06, val loss: 2.42675e-06, min loss: 1.55203e-06\n",
      "Epoch: 715100, elapsed: 1.32e+01, train loss: 1.65036e-06, val loss: 2.53920e-06, min loss: 1.55203e-06\n",
      "Epoch: 715200, elapsed: 1.12e+01, train loss: 1.60263e-06, val loss: 2.42014e-06, min loss: 1.55203e-06\n",
      "Epoch: 715300, elapsed: 1.14e+01, train loss: 5.42963e-06, val loss: 5.66860e-06, min loss: 1.55203e-06\n",
      "Epoch: 715400, elapsed: 1.13e+01, train loss: 4.79604e-06, val loss: 6.63544e-06, min loss: 1.55203e-06\n",
      "Epoch: 715500, elapsed: 1.12e+01, train loss: 1.55560e-06, val loss: 2.37381e-06, min loss: 1.55203e-06\n",
      "Epoch: 715600, elapsed: 1.12e+01, train loss: 1.59903e-06, val loss: 2.37618e-06, min loss: 1.55203e-06\n",
      "Epoch: 715700, elapsed: 1.12e+01, train loss: 2.48142e-06, val loss: 3.55633e-06, min loss: 1.55203e-06\n",
      "Epoch: 715800, elapsed: 1.12e+01, train loss: 1.77845e-06, val loss: 2.72330e-06, min loss: 1.55203e-06\n",
      "Epoch: 715900, elapsed: 1.35e+01, train loss: 1.66154e-06, val loss: 2.45297e-06, min loss: 1.55203e-06\n",
      "Epoch: 716000, elapsed: 1.17e+01, train loss: 1.57742e-06, val loss: 2.41120e-06, min loss: 1.55203e-06\n",
      "Epoch: 716100, elapsed: 1.14e+01, train loss: 1.62123e-06, val loss: 2.46365e-06, min loss: 1.55203e-06\n",
      "Epoch: 716200, elapsed: 1.16e+01, train loss: 1.61264e-06, val loss: 2.46785e-06, min loss: 1.55203e-06\n",
      "Epoch: 716300, elapsed: 1.14e+01, train loss: 1.76254e-06, val loss: 2.61151e-06, min loss: 1.55203e-06\n",
      "Epoch: 716400, elapsed: 1.14e+01, train loss: 2.01681e-06, val loss: 3.15528e-06, min loss: 1.55203e-06\n",
      "Epoch: 716500, elapsed: 1.14e+01, train loss: 1.54685e-06, val loss: 2.38431e-06, min loss: 1.54685e-06\n",
      "Epoch: 716600, elapsed: 1.13e+01, train loss: 1.55091e-06, val loss: 2.37890e-06, min loss: 1.54685e-06\n",
      "Epoch: 716700, elapsed: 1.13e+01, train loss: 1.94036e-06, val loss: 2.63582e-06, min loss: 1.54685e-06\n",
      "Epoch: 716800, elapsed: 1.14e+01, train loss: 1.56098e-06, val loss: 2.37804e-06, min loss: 1.54685e-06\n",
      "Epoch: 716900, elapsed: 1.12e+01, train loss: 1.58753e-06, val loss: 2.37784e-06, min loss: 1.54685e-06\n",
      "Epoch: 717000, elapsed: 1.13e+01, train loss: 1.62989e-06, val loss: 2.58868e-06, min loss: 1.54685e-06\n",
      "Epoch: 717100, elapsed: 1.13e+01, train loss: 2.02222e-06, val loss: 2.82037e-06, min loss: 1.54685e-06\n",
      "Epoch: 717200, elapsed: 1.13e+01, train loss: 2.76765e-06, val loss: 3.70862e-06, min loss: 1.54685e-06\n",
      "Epoch: 717300, elapsed: 1.34e+01, train loss: 1.66462e-06, val loss: 2.44106e-06, min loss: 1.54685e-06\n",
      "Epoch: 717400, elapsed: 1.15e+01, train loss: 1.56856e-06, val loss: 2.39170e-06, min loss: 1.54685e-06\n",
      "Epoch: 717500, elapsed: 1.15e+01, train loss: 1.55803e-06, val loss: 2.39549e-06, min loss: 1.54685e-06\n",
      "Epoch: 717600, elapsed: 1.12e+01, train loss: 1.70387e-06, val loss: 2.43528e-06, min loss: 1.54685e-06\n",
      "Epoch: 717700, elapsed: 1.13e+01, train loss: 1.69233e-06, val loss: 2.45706e-06, min loss: 1.54685e-06\n",
      "Epoch: 717800, elapsed: 1.13e+01, train loss: 2.49009e-06, val loss: 2.80492e-06, min loss: 1.54685e-06\n",
      "Epoch: 717900, elapsed: 1.12e+01, train loss: 1.56362e-06, val loss: 2.42477e-06, min loss: 1.54685e-06\n",
      "Epoch: 718000, elapsed: 1.12e+01, train loss: 1.59222e-06, val loss: 2.46764e-06, min loss: 1.54685e-06\n",
      "Epoch: 718100, elapsed: 1.14e+01, train loss: 1.60999e-06, val loss: 2.47672e-06, min loss: 1.54685e-06\n",
      "Epoch: 718200, elapsed: 1.12e+01, train loss: 1.54863e-06, val loss: 2.38135e-06, min loss: 1.54685e-06\n",
      "Epoch: 718300, elapsed: 1.14e+01, train loss: 1.54813e-06, val loss: 2.40578e-06, min loss: 1.54685e-06\n",
      "Epoch: 718400, elapsed: 1.13e+01, train loss: 1.54722e-06, val loss: 2.39015e-06, min loss: 1.54685e-06\n",
      "Epoch: 718500, elapsed: 1.12e+01, train loss: 1.57321e-06, val loss: 2.38247e-06, min loss: 1.54685e-06\n",
      "Epoch: 718600, elapsed: 1.14e+01, train loss: 1.61048e-06, val loss: 2.47421e-06, min loss: 1.54685e-06\n",
      "Epoch: 718700, elapsed: 1.32e+01, train loss: 1.58186e-06, val loss: 2.37327e-06, min loss: 1.54685e-06\n",
      "Epoch: 718800, elapsed: 1.14e+01, train loss: 1.55428e-06, val loss: 2.38841e-06, min loss: 1.54685e-06\n",
      "Epoch: 718900, elapsed: 1.14e+01, train loss: 1.80358e-06, val loss: 2.71103e-06, min loss: 1.54685e-06\n",
      "Epoch: 719000, elapsed: 1.13e+01, train loss: 2.62442e-06, val loss: 3.55256e-06, min loss: 1.54685e-06\n",
      "Epoch: 719100, elapsed: 1.12e+01, train loss: 1.56826e-06, val loss: 2.45034e-06, min loss: 1.54685e-06\n",
      "Epoch: 719200, elapsed: 1.13e+01, train loss: 1.57332e-06, val loss: 2.36763e-06, min loss: 1.54685e-06\n",
      "Epoch: 719300, elapsed: 1.13e+01, train loss: 1.71269e-06, val loss: 2.50709e-06, min loss: 1.54685e-06\n",
      "Epoch: 719400, elapsed: 1.13e+01, train loss: 1.88312e-06, val loss: 2.67797e-06, min loss: 1.54685e-06\n",
      "Epoch: 719500, elapsed: 1.13e+01, train loss: 1.11185e-05, val loss: 1.26630e-05, min loss: 1.54685e-06\n",
      "Epoch: 719600, elapsed: 1.13e+01, train loss: 1.54218e-06, val loss: 2.39158e-06, min loss: 1.54218e-06\n",
      "Epoch: 719700, elapsed: 1.13e+01, train loss: 1.70646e-06, val loss: 2.44925e-06, min loss: 1.54218e-06\n",
      "Epoch: 719800, elapsed: 1.11e+01, train loss: 1.55114e-06, val loss: 2.39585e-06, min loss: 1.54218e-06\n",
      "Epoch: 719900, elapsed: 1.13e+01, train loss: 1.59476e-06, val loss: 2.47005e-06, min loss: 1.54218e-06\n",
      "Epoch: 720000, elapsed: 1.13e+01, train loss: 3.46152e-06, val loss: 4.29958e-06, min loss: 1.54218e-06\n",
      "Epoch: 720100, elapsed: 1.55e+01, train loss: 1.64699e-06, val loss: 2.61696e-06, min loss: 1.54218e-06\n",
      "Epoch: 720200, elapsed: 1.14e+01, train loss: 1.83240e-06, val loss: 2.88418e-06, min loss: 1.54218e-06\n",
      "Epoch: 720300, elapsed: 1.13e+01, train loss: 4.97368e-06, val loss: 5.91636e-06, min loss: 1.54218e-06\n",
      "Epoch: 720400, elapsed: 1.13e+01, train loss: 2.93890e-06, val loss: 3.60193e-06, min loss: 1.54218e-06\n",
      "Epoch: 720500, elapsed: 1.13e+01, train loss: 3.14234e-06, val loss: 3.96163e-06, min loss: 1.54218e-06\n",
      "Epoch: 720600, elapsed: 1.12e+01, train loss: 5.30320e-06, val loss: 5.59620e-06, min loss: 1.54218e-06\n",
      "Epoch: 720700, elapsed: 1.11e+01, train loss: 2.86182e-06, val loss: 3.28877e-06, min loss: 1.54218e-06\n",
      "Epoch: 720800, elapsed: 1.14e+01, train loss: 3.05060e-06, val loss: 3.35383e-06, min loss: 1.54218e-06\n",
      "Epoch: 720900, elapsed: 1.13e+01, train loss: 2.15447e-06, val loss: 3.15490e-06, min loss: 1.54218e-06\n",
      "Epoch: 721000, elapsed: 1.12e+01, train loss: 1.98032e-06, val loss: 2.68819e-06, min loss: 1.54218e-06\n",
      "Epoch: 721100, elapsed: 1.12e+01, train loss: 1.55272e-06, val loss: 2.39281e-06, min loss: 1.54218e-06\n",
      "Epoch: 721200, elapsed: 1.13e+01, train loss: 1.79196e-06, val loss: 2.53097e-06, min loss: 1.54218e-06\n",
      "Epoch: 721300, elapsed: 1.11e+01, train loss: 1.55802e-06, val loss: 2.44707e-06, min loss: 1.54218e-06\n",
      "Epoch: 721400, elapsed: 1.12e+01, train loss: 3.00629e-06, val loss: 4.37210e-06, min loss: 1.54218e-06\n",
      "Epoch: 721500, elapsed: 1.32e+01, train loss: 2.15998e-06, val loss: 2.95440e-06, min loss: 1.54218e-06\n",
      "Epoch: 721600, elapsed: 1.13e+01, train loss: 1.56660e-06, val loss: 2.39497e-06, min loss: 1.54218e-06\n",
      "Epoch: 721700, elapsed: 1.13e+01, train loss: 1.89767e-06, val loss: 3.05447e-06, min loss: 1.54218e-06\n",
      "Epoch: 721800, elapsed: 1.13e+01, train loss: 1.54182e-06, val loss: 2.39145e-06, min loss: 1.54182e-06\n",
      "Epoch: 721900, elapsed: 1.13e+01, train loss: 1.54538e-06, val loss: 2.41164e-06, min loss: 1.54182e-06\n",
      "Epoch: 722000, elapsed: 1.13e+01, train loss: 1.60143e-06, val loss: 2.40306e-06, min loss: 1.54182e-06\n",
      "Epoch: 722100, elapsed: 1.12e+01, train loss: 1.73409e-06, val loss: 2.51890e-06, min loss: 1.54182e-06\n",
      "Epoch: 722200, elapsed: 1.13e+01, train loss: 1.56996e-06, val loss: 2.43790e-06, min loss: 1.54182e-06\n",
      "Epoch: 722300, elapsed: 1.12e+01, train loss: 1.54551e-06, val loss: 2.37907e-06, min loss: 1.54182e-06\n",
      "Epoch: 722400, elapsed: 1.14e+01, train loss: 1.65147e-06, val loss: 2.39318e-06, min loss: 1.54182e-06\n",
      "Epoch: 722500, elapsed: 1.13e+01, train loss: 2.01597e-06, val loss: 2.83731e-06, min loss: 1.54182e-06\n",
      "Epoch: 722600, elapsed: 1.11e+01, train loss: 1.54151e-06, val loss: 2.40851e-06, min loss: 1.54151e-06\n",
      "Epoch: 722700, elapsed: 1.11e+01, train loss: 1.56796e-06, val loss: 2.42198e-06, min loss: 1.54151e-06\n",
      "Epoch: 722800, elapsed: 1.13e+01, train loss: 1.61871e-06, val loss: 2.44585e-06, min loss: 1.54151e-06\n",
      "Epoch: 722900, elapsed: 1.34e+01, train loss: 6.97975e-06, val loss: 6.47026e-06, min loss: 1.54151e-06\n",
      "Epoch: 723000, elapsed: 1.13e+01, train loss: 1.55414e-06, val loss: 2.38166e-06, min loss: 1.54151e-06\n",
      "Epoch: 723100, elapsed: 1.15e+01, train loss: 1.56556e-06, val loss: 2.46090e-06, min loss: 1.54151e-06\n",
      "Epoch: 723200, elapsed: 1.16e+01, train loss: 1.53723e-06, val loss: 2.39847e-06, min loss: 1.53723e-06\n",
      "Epoch: 723300, elapsed: 1.13e+01, train loss: 1.65268e-06, val loss: 2.67260e-06, min loss: 1.53723e-06\n",
      "Epoch: 723400, elapsed: 1.13e+01, train loss: 1.62285e-06, val loss: 2.37844e-06, min loss: 1.53723e-06\n",
      "Epoch: 723500, elapsed: 1.15e+01, train loss: 4.56682e-06, val loss: 6.33315e-06, min loss: 1.53723e-06\n",
      "Epoch: 723600, elapsed: 1.14e+01, train loss: 1.53557e-06, val loss: 2.39264e-06, min loss: 1.53557e-06\n",
      "Epoch: 723700, elapsed: 1.13e+01, train loss: 1.54270e-06, val loss: 2.38774e-06, min loss: 1.53557e-06\n",
      "Epoch: 723800, elapsed: 1.13e+01, train loss: 1.54497e-06, val loss: 2.42257e-06, min loss: 1.53557e-06\n",
      "Epoch: 723900, elapsed: 1.14e+01, train loss: 1.53476e-06, val loss: 2.39231e-06, min loss: 1.53476e-06\n",
      "Epoch: 724000, elapsed: 1.14e+01, train loss: 2.15534e-06, val loss: 3.03528e-06, min loss: 1.53476e-06\n",
      "Epoch: 724100, elapsed: 1.13e+01, train loss: 1.62843e-06, val loss: 2.51654e-06, min loss: 1.53476e-06\n",
      "Epoch: 724200, elapsed: 1.12e+01, train loss: 1.58564e-06, val loss: 2.40823e-06, min loss: 1.53476e-06\n",
      "Epoch: 724300, elapsed: 1.34e+01, train loss: 1.92100e-06, val loss: 2.95643e-06, min loss: 1.53476e-06\n",
      "Epoch: 724400, elapsed: 1.16e+01, train loss: 1.54177e-06, val loss: 2.39702e-06, min loss: 1.53476e-06\n",
      "Epoch: 724500, elapsed: 1.14e+01, train loss: 1.55460e-06, val loss: 2.42240e-06, min loss: 1.53476e-06\n",
      "Epoch: 724600, elapsed: 1.16e+01, train loss: 1.54448e-06, val loss: 2.37476e-06, min loss: 1.53476e-06\n",
      "Epoch: 724700, elapsed: 1.15e+01, train loss: 1.61006e-06, val loss: 2.42244e-06, min loss: 1.53476e-06\n",
      "Epoch: 724800, elapsed: 1.14e+01, train loss: 1.84912e-06, val loss: 2.67010e-06, min loss: 1.53476e-06\n",
      "Epoch: 724900, elapsed: 1.12e+01, train loss: 1.54021e-06, val loss: 2.39359e-06, min loss: 1.53476e-06\n",
      "Epoch: 725000, elapsed: 1.13e+01, train loss: 1.53740e-06, val loss: 2.42012e-06, min loss: 1.53476e-06\n",
      "Epoch: 725100, elapsed: 1.34e+01, train loss: 1.54296e-06, val loss: 2.45392e-06, min loss: 1.53476e-06\n",
      "Epoch: 725200, elapsed: 1.13e+01, train loss: 2.40972e-06, val loss: 3.28622e-06, min loss: 1.53476e-06\n",
      "Epoch: 725300, elapsed: 1.14e+01, train loss: 2.80394e-06, val loss: 3.79701e-06, min loss: 1.53476e-06\n",
      "Epoch: 725400, elapsed: 1.13e+01, train loss: 1.58007e-06, val loss: 2.43010e-06, min loss: 1.53476e-06\n",
      "Epoch: 725500, elapsed: 1.14e+01, train loss: 1.53487e-06, val loss: 2.38850e-06, min loss: 1.53476e-06\n",
      "Epoch: 725600, elapsed: 1.12e+01, train loss: 1.67560e-06, val loss: 2.37734e-06, min loss: 1.53476e-06\n",
      "Epoch: 725700, elapsed: 1.34e+01, train loss: 4.51124e-06, val loss: 6.51499e-06, min loss: 1.53476e-06\n",
      "Epoch: 725800, elapsed: 1.13e+01, train loss: 1.53146e-06, val loss: 2.39361e-06, min loss: 1.53146e-06\n",
      "Epoch: 725900, elapsed: 1.14e+01, train loss: 1.56928e-06, val loss: 2.40593e-06, min loss: 1.53146e-06\n",
      "Epoch: 726000, elapsed: 1.14e+01, train loss: 1.58906e-06, val loss: 2.45691e-06, min loss: 1.53146e-06\n",
      "Epoch: 726100, elapsed: 1.14e+01, train loss: 1.60208e-06, val loss: 2.36135e-06, min loss: 1.53146e-06\n",
      "Epoch: 726200, elapsed: 1.13e+01, train loss: 1.95067e-06, val loss: 3.14003e-06, min loss: 1.53146e-06\n",
      "Epoch: 726300, elapsed: 1.13e+01, train loss: 2.86677e-06, val loss: 4.22443e-06, min loss: 1.53146e-06\n",
      "Epoch: 726400, elapsed: 1.13e+01, train loss: 4.68065e-06, val loss: 5.32588e-06, min loss: 1.53146e-06\n",
      "Epoch: 726500, elapsed: 1.14e+01, train loss: 1.65734e-06, val loss: 2.62309e-06, min loss: 1.53146e-06\n",
      "Epoch: 726600, elapsed: 1.14e+01, train loss: 1.55911e-06, val loss: 2.46169e-06, min loss: 1.53146e-06\n",
      "Epoch: 726700, elapsed: 1.12e+01, train loss: 1.67330e-06, val loss: 2.79729e-06, min loss: 1.53146e-06\n",
      "Epoch: 726800, elapsed: 1.13e+01, train loss: 7.65019e-06, val loss: 7.58114e-06, min loss: 1.53146e-06\n",
      "Epoch: 726900, elapsed: 1.15e+01, train loss: 1.53407e-06, val loss: 2.37973e-06, min loss: 1.53146e-06\n",
      "Epoch: 727000, elapsed: 1.12e+01, train loss: 1.73258e-06, val loss: 2.48065e-06, min loss: 1.53146e-06\n",
      "Epoch: 727100, elapsed: 1.32e+01, train loss: 1.55193e-06, val loss: 2.37531e-06, min loss: 1.53146e-06\n",
      "Epoch: 727200, elapsed: 1.13e+01, train loss: 1.79047e-06, val loss: 2.54752e-06, min loss: 1.53146e-06\n",
      "Epoch: 727300, elapsed: 1.17e+01, train loss: 1.52937e-06, val loss: 2.38553e-06, min loss: 1.52937e-06\n",
      "Epoch: 727400, elapsed: 1.14e+01, train loss: 1.63758e-06, val loss: 2.44665e-06, min loss: 1.52937e-06\n",
      "Epoch: 727500, elapsed: 1.14e+01, train loss: 2.13003e-06, val loss: 3.53557e-06, min loss: 1.52937e-06\n",
      "Epoch: 727600, elapsed: 1.13e+01, train loss: 1.77614e-06, val loss: 2.65869e-06, min loss: 1.52937e-06\n",
      "Epoch: 727700, elapsed: 1.13e+01, train loss: 1.53093e-06, val loss: 2.40826e-06, min loss: 1.52937e-06\n",
      "Epoch: 727800, elapsed: 1.14e+01, train loss: 1.69148e-06, val loss: 2.42903e-06, min loss: 1.52937e-06\n",
      "Epoch: 727900, elapsed: 1.13e+01, train loss: 1.98089e-06, val loss: 3.25958e-06, min loss: 1.52937e-06\n",
      "Epoch: 728000, elapsed: 1.13e+01, train loss: 1.53027e-06, val loss: 2.38415e-06, min loss: 1.52937e-06\n",
      "Epoch: 728100, elapsed: 1.13e+01, train loss: 1.52755e-06, val loss: 2.38737e-06, min loss: 1.52755e-06\n",
      "Epoch: 728200, elapsed: 1.12e+01, train loss: 1.95634e-06, val loss: 2.56400e-06, min loss: 1.52755e-06\n",
      "Epoch: 728300, elapsed: 1.12e+01, train loss: 1.61684e-06, val loss: 2.44309e-06, min loss: 1.52755e-06\n",
      "Epoch: 728400, elapsed: 1.12e+01, train loss: 1.52753e-06, val loss: 2.39056e-06, min loss: 1.52753e-06\n",
      "Epoch: 728500, elapsed: 1.35e+01, train loss: 1.55600e-06, val loss: 2.38323e-06, min loss: 1.52753e-06\n",
      "Epoch: 728600, elapsed: 1.15e+01, train loss: 1.53329e-06, val loss: 2.39188e-06, min loss: 1.52753e-06\n",
      "Epoch: 728700, elapsed: 1.15e+01, train loss: 2.49308e-06, val loss: 3.72383e-06, min loss: 1.52753e-06\n",
      "Epoch: 728800, elapsed: 1.15e+01, train loss: 1.56329e-06, val loss: 2.42090e-06, min loss: 1.52753e-06\n",
      "Epoch: 728900, elapsed: 1.13e+01, train loss: 1.52635e-06, val loss: 2.39242e-06, min loss: 1.52635e-06\n",
      "Epoch: 729000, elapsed: 1.14e+01, train loss: 1.88430e-06, val loss: 2.74075e-06, min loss: 1.52635e-06\n",
      "Epoch: 729100, elapsed: 1.15e+01, train loss: 1.53475e-06, val loss: 2.38523e-06, min loss: 1.52635e-06\n",
      "Epoch: 729200, elapsed: 1.12e+01, train loss: 1.53225e-06, val loss: 2.39961e-06, min loss: 1.52635e-06\n",
      "Epoch: 729300, elapsed: 1.13e+01, train loss: 1.53814e-06, val loss: 2.37740e-06, min loss: 1.52635e-06\n",
      "Epoch: 729400, elapsed: 1.13e+01, train loss: 1.53276e-06, val loss: 2.38889e-06, min loss: 1.52635e-06\n",
      "Epoch: 729500, elapsed: 1.12e+01, train loss: 1.54850e-06, val loss: 2.40080e-06, min loss: 1.52635e-06\n",
      "Epoch: 729600, elapsed: 1.12e+01, train loss: 1.55660e-06, val loss: 2.39476e-06, min loss: 1.52635e-06\n",
      "Epoch: 729700, elapsed: 1.11e+01, train loss: 1.52586e-06, val loss: 2.39196e-06, min loss: 1.52586e-06\n",
      "Epoch: 729800, elapsed: 1.12e+01, train loss: 1.53497e-06, val loss: 2.36091e-06, min loss: 1.52586e-06\n",
      "Epoch: 729900, elapsed: 1.13e+01, train loss: 4.14452e-06, val loss: 3.44122e-06, min loss: 1.52586e-06\n",
      "Epoch: 730000, elapsed: 1.35e+01, train loss: 1.52448e-06, val loss: 2.38851e-06, min loss: 1.52448e-06\n",
      "Epoch: 730100, elapsed: 1.35e+01, train loss: 1.53113e-06, val loss: 2.42780e-06, min loss: 1.52448e-06\n",
      "Epoch: 730200, elapsed: 1.13e+01, train loss: 1.58443e-06, val loss: 2.56135e-06, min loss: 1.52448e-06\n",
      "Epoch: 730300, elapsed: 1.13e+01, train loss: 1.56281e-06, val loss: 2.49635e-06, min loss: 1.52448e-06\n",
      "Epoch: 730400, elapsed: 1.13e+01, train loss: 1.53994e-06, val loss: 2.39233e-06, min loss: 1.52448e-06\n",
      "Epoch: 730500, elapsed: 1.13e+01, train loss: 1.52904e-06, val loss: 2.39861e-06, min loss: 1.52448e-06\n",
      "Epoch: 730600, elapsed: 1.14e+01, train loss: 1.52761e-06, val loss: 2.37749e-06, min loss: 1.52448e-06\n",
      "Epoch: 730700, elapsed: 1.13e+01, train loss: 1.54973e-06, val loss: 2.41780e-06, min loss: 1.52448e-06\n",
      "Epoch: 730800, elapsed: 1.11e+01, train loss: 1.55289e-06, val loss: 2.51330e-06, min loss: 1.52448e-06\n",
      "Epoch: 730900, elapsed: 1.14e+01, train loss: 1.54099e-06, val loss: 2.40708e-06, min loss: 1.52448e-06\n",
      "Epoch: 731000, elapsed: 1.13e+01, train loss: 1.52877e-06, val loss: 2.39486e-06, min loss: 1.52448e-06\n",
      "Epoch: 731100, elapsed: 1.14e+01, train loss: 5.17841e-06, val loss: 4.19220e-06, min loss: 1.52448e-06\n",
      "Epoch: 731200, elapsed: 1.13e+01, train loss: 1.79735e-06, val loss: 2.71881e-06, min loss: 1.52448e-06\n",
      "Epoch: 731300, elapsed: 1.33e+01, train loss: 5.04750e-06, val loss: 6.14305e-06, min loss: 1.52448e-06\n",
      "Epoch: 731400, elapsed: 1.14e+01, train loss: 1.64268e-06, val loss: 2.46945e-06, min loss: 1.52448e-06\n",
      "Epoch: 731500, elapsed: 1.15e+01, train loss: 1.53766e-06, val loss: 2.40331e-06, min loss: 1.52448e-06\n",
      "Epoch: 731600, elapsed: 1.13e+01, train loss: 1.53035e-06, val loss: 2.39694e-06, min loss: 1.52448e-06\n",
      "Epoch: 731700, elapsed: 1.15e+01, train loss: 1.53956e-06, val loss: 2.36936e-06, min loss: 1.52448e-06\n",
      "Epoch: 731800, elapsed: 1.12e+01, train loss: 2.05902e-06, val loss: 3.13243e-06, min loss: 1.52448e-06\n",
      "Epoch: 731900, elapsed: 1.12e+01, train loss: 1.74965e-06, val loss: 2.58789e-06, min loss: 1.52448e-06\n",
      "Epoch: 732000, elapsed: 1.15e+01, train loss: 1.54195e-06, val loss: 2.36544e-06, min loss: 1.52448e-06\n",
      "Epoch: 732100, elapsed: 1.14e+01, train loss: 1.56741e-06, val loss: 2.44508e-06, min loss: 1.52448e-06\n",
      "Epoch: 732200, elapsed: 1.14e+01, train loss: 1.52672e-06, val loss: 2.38830e-06, min loss: 1.52448e-06\n",
      "Epoch: 732300, elapsed: 1.13e+01, train loss: 1.65041e-06, val loss: 2.72442e-06, min loss: 1.52448e-06\n",
      "Epoch: 732400, elapsed: 1.14e+01, train loss: 2.33730e-06, val loss: 3.23352e-06, min loss: 1.52448e-06\n",
      "Epoch: 732500, elapsed: 1.11e+01, train loss: 2.57712e-06, val loss: 2.81970e-06, min loss: 1.52448e-06\n",
      "Epoch: 732600, elapsed: 1.14e+01, train loss: 3.16000e-06, val loss: 2.98169e-06, min loss: 1.52448e-06\n",
      "Epoch: 732700, elapsed: 1.14e+01, train loss: 3.64308e-06, val loss: 4.51781e-06, min loss: 1.52448e-06\n",
      "Epoch: 732800, elapsed: 1.36e+01, train loss: 3.49086e-06, val loss: 3.88502e-06, min loss: 1.52448e-06\n",
      "Epoch: 732900, elapsed: 1.15e+01, train loss: 1.81810e-06, val loss: 2.78276e-06, min loss: 1.52448e-06\n",
      "Epoch: 733000, elapsed: 1.16e+01, train loss: 1.52257e-06, val loss: 2.39179e-06, min loss: 1.52257e-06\n",
      "Epoch: 733100, elapsed: 1.13e+01, train loss: 1.80099e-06, val loss: 2.53926e-06, min loss: 1.52257e-06\n",
      "Epoch: 733200, elapsed: 1.14e+01, train loss: 1.51869e-06, val loss: 2.39026e-06, min loss: 1.51869e-06\n",
      "Epoch: 733300, elapsed: 1.15e+01, train loss: 1.53037e-06, val loss: 2.37050e-06, min loss: 1.51869e-06\n",
      "Epoch: 733400, elapsed: 1.14e+01, train loss: 1.51898e-06, val loss: 2.38519e-06, min loss: 1.51869e-06\n",
      "Epoch: 733500, elapsed: 1.14e+01, train loss: 1.64720e-06, val loss: 2.62163e-06, min loss: 1.51869e-06\n",
      "Epoch: 733600, elapsed: 1.14e+01, train loss: 1.51966e-06, val loss: 2.40163e-06, min loss: 1.51869e-06\n",
      "Epoch: 733700, elapsed: 1.12e+01, train loss: 1.52963e-06, val loss: 2.41978e-06, min loss: 1.51869e-06\n",
      "Epoch: 733800, elapsed: 1.13e+01, train loss: 1.60168e-06, val loss: 2.43687e-06, min loss: 1.51869e-06\n",
      "Epoch: 733900, elapsed: 1.11e+01, train loss: 1.57284e-06, val loss: 2.38990e-06, min loss: 1.51869e-06\n",
      "Epoch: 734000, elapsed: 1.14e+01, train loss: 1.59306e-06, val loss: 2.48679e-06, min loss: 1.51869e-06\n",
      "Epoch: 734100, elapsed: 1.12e+01, train loss: 5.21519e-06, val loss: 4.02615e-06, min loss: 1.51869e-06\n",
      "Epoch: 734200, elapsed: 1.34e+01, train loss: 1.51899e-06, val loss: 2.40009e-06, min loss: 1.51869e-06\n",
      "Epoch: 734300, elapsed: 1.15e+01, train loss: 1.52089e-06, val loss: 2.38958e-06, min loss: 1.51869e-06\n",
      "Epoch: 734400, elapsed: 1.14e+01, train loss: 1.54057e-06, val loss: 2.37820e-06, min loss: 1.51869e-06\n",
      "Epoch: 734500, elapsed: 1.14e+01, train loss: 1.58528e-06, val loss: 2.46045e-06, min loss: 1.51869e-06\n",
      "Epoch: 734600, elapsed: 1.14e+01, train loss: 1.66920e-06, val loss: 2.50383e-06, min loss: 1.51869e-06\n",
      "Epoch: 734700, elapsed: 1.14e+01, train loss: 2.04056e-06, val loss: 2.75661e-06, min loss: 1.51869e-06\n",
      "Epoch: 734800, elapsed: 1.14e+01, train loss: 1.81501e-06, val loss: 2.76634e-06, min loss: 1.51869e-06\n",
      "Epoch: 734900, elapsed: 1.13e+01, train loss: 1.94134e-06, val loss: 2.99502e-06, min loss: 1.51869e-06\n",
      "Epoch: 735000, elapsed: 1.14e+01, train loss: 1.52043e-06, val loss: 2.41470e-06, min loss: 1.51869e-06\n",
      "Epoch: 735100, elapsed: 1.32e+01, train loss: 1.51995e-06, val loss: 2.40547e-06, min loss: 1.51869e-06\n",
      "Epoch: 735200, elapsed: 1.11e+01, train loss: 1.51945e-06, val loss: 2.39655e-06, min loss: 1.51869e-06\n",
      "Epoch: 735300, elapsed: 1.15e+01, train loss: 1.58633e-06, val loss: 2.37500e-06, min loss: 1.51869e-06\n",
      "Epoch: 735400, elapsed: 1.12e+01, train loss: 1.65157e-06, val loss: 2.62741e-06, min loss: 1.51869e-06\n",
      "Epoch: 735500, elapsed: 1.12e+01, train loss: 1.51552e-06, val loss: 2.39130e-06, min loss: 1.51552e-06\n",
      "Epoch: 735600, elapsed: 1.35e+01, train loss: 1.84798e-06, val loss: 2.80632e-06, min loss: 1.51552e-06\n",
      "Epoch: 735700, elapsed: 1.14e+01, train loss: 1.53930e-06, val loss: 2.39150e-06, min loss: 1.51552e-06\n",
      "Epoch: 735800, elapsed: 1.13e+01, train loss: 1.53001e-06, val loss: 2.42207e-06, min loss: 1.51552e-06\n",
      "Epoch: 735900, elapsed: 1.14e+01, train loss: 1.52967e-06, val loss: 2.38540e-06, min loss: 1.51552e-06\n",
      "Epoch: 736000, elapsed: 1.14e+01, train loss: 1.54468e-06, val loss: 2.39828e-06, min loss: 1.51552e-06\n",
      "Epoch: 736100, elapsed: 1.14e+01, train loss: 1.52011e-06, val loss: 2.40714e-06, min loss: 1.51552e-06\n",
      "Epoch: 736200, elapsed: 1.14e+01, train loss: 1.52051e-06, val loss: 2.40663e-06, min loss: 1.51552e-06\n",
      "Epoch: 736300, elapsed: 1.13e+01, train loss: 1.64054e-06, val loss: 2.43033e-06, min loss: 1.51552e-06\n",
      "Epoch: 736400, elapsed: 1.14e+01, train loss: 1.52857e-06, val loss: 2.40750e-06, min loss: 1.51552e-06\n",
      "Epoch: 736500, elapsed: 1.14e+01, train loss: 1.99768e-06, val loss: 3.01409e-06, min loss: 1.51552e-06\n",
      "Epoch: 736600, elapsed: 1.13e+01, train loss: 5.83477e-06, val loss: 7.00349e-06, min loss: 1.51552e-06\n",
      "Epoch: 736700, elapsed: 1.13e+01, train loss: 3.87660e-06, val loss: 3.81864e-06, min loss: 1.51552e-06\n",
      "Epoch: 736800, elapsed: 1.13e+01, train loss: 1.57023e-06, val loss: 2.38566e-06, min loss: 1.51552e-06\n",
      "Epoch: 736900, elapsed: 1.13e+01, train loss: 2.34286e-06, val loss: 3.37116e-06, min loss: 1.51552e-06\n",
      "Epoch: 737000, elapsed: 1.34e+01, train loss: 1.51859e-06, val loss: 2.38486e-06, min loss: 1.51552e-06\n",
      "Epoch: 737100, elapsed: 1.16e+01, train loss: 1.51320e-06, val loss: 2.39964e-06, min loss: 1.51320e-06\n",
      "Epoch: 737200, elapsed: 1.15e+01, train loss: 1.51712e-06, val loss: 2.38893e-06, min loss: 1.51320e-06\n",
      "Epoch: 737300, elapsed: 1.15e+01, train loss: 1.58804e-06, val loss: 2.48850e-06, min loss: 1.51320e-06\n",
      "Epoch: 737400, elapsed: 1.13e+01, train loss: 1.54655e-06, val loss: 2.40791e-06, min loss: 1.51320e-06\n",
      "Epoch: 737500, elapsed: 1.13e+01, train loss: 1.53626e-06, val loss: 2.43809e-06, min loss: 1.51320e-06\n",
      "Epoch: 737600, elapsed: 1.12e+01, train loss: 1.58524e-06, val loss: 2.40956e-06, min loss: 1.51320e-06\n",
      "Epoch: 737700, elapsed: 1.12e+01, train loss: 3.56378e-06, val loss: 4.77486e-06, min loss: 1.51320e-06\n",
      "Epoch: 737800, elapsed: 1.13e+01, train loss: 2.95398e-06, val loss: 4.75716e-06, min loss: 1.51320e-06\n",
      "Epoch: 737900, elapsed: 1.13e+01, train loss: 1.51145e-06, val loss: 2.39217e-06, min loss: 1.51145e-06\n",
      "Epoch: 738000, elapsed: 1.13e+01, train loss: 1.51956e-06, val loss: 2.38081e-06, min loss: 1.51145e-06\n",
      "Epoch: 738100, elapsed: 1.13e+01, train loss: 4.70848e-06, val loss: 6.57871e-06, min loss: 1.51145e-06\n",
      "Epoch: 738200, elapsed: 1.12e+01, train loss: 1.51051e-06, val loss: 2.39062e-06, min loss: 1.51051e-06\n",
      "Epoch: 738300, elapsed: 1.12e+01, train loss: 3.46454e-06, val loss: 4.70114e-06, min loss: 1.51051e-06\n",
      "Epoch: 738400, elapsed: 1.33e+01, train loss: 1.51007e-06, val loss: 2.39229e-06, min loss: 1.51007e-06\n",
      "Epoch: 738500, elapsed: 1.13e+01, train loss: 3.33394e-06, val loss: 4.60519e-06, min loss: 1.51007e-06\n",
      "Epoch: 738600, elapsed: 1.14e+01, train loss: 1.61988e-06, val loss: 2.67876e-06, min loss: 1.51007e-06\n",
      "Epoch: 738700, elapsed: 1.13e+01, train loss: 1.64728e-06, val loss: 2.50183e-06, min loss: 1.51007e-06\n",
      "Epoch: 738800, elapsed: 1.13e+01, train loss: 1.52252e-06, val loss: 2.37422e-06, min loss: 1.51007e-06\n",
      "Epoch: 738900, elapsed: 1.13e+01, train loss: 1.69181e-06, val loss: 2.39645e-06, min loss: 1.51007e-06\n",
      "Epoch: 739000, elapsed: 1.12e+01, train loss: 2.54811e-06, val loss: 3.08325e-06, min loss: 1.51007e-06\n",
      "Epoch: 739100, elapsed: 1.13e+01, train loss: 1.93076e-06, val loss: 2.65036e-06, min loss: 1.51007e-06\n",
      "Epoch: 739200, elapsed: 1.13e+01, train loss: 1.59644e-06, val loss: 2.68006e-06, min loss: 1.51007e-06\n",
      "Epoch: 739300, elapsed: 1.13e+01, train loss: 3.25118e-06, val loss: 3.88263e-06, min loss: 1.51007e-06\n",
      "Epoch: 739400, elapsed: 1.11e+01, train loss: 5.99426e-06, val loss: 6.74412e-06, min loss: 1.51007e-06\n",
      "Epoch: 739500, elapsed: 1.13e+01, train loss: 1.57165e-06, val loss: 2.50407e-06, min loss: 1.51007e-06\n",
      "Epoch: 739600, elapsed: 1.14e+01, train loss: 1.50918e-06, val loss: 2.40093e-06, min loss: 1.50918e-06\n",
      "Epoch: 739700, elapsed: 1.14e+01, train loss: 1.53811e-06, val loss: 2.41485e-06, min loss: 1.50918e-06\n",
      "Epoch: 739800, elapsed: 1.11e+01, train loss: 1.96396e-06, val loss: 2.57432e-06, min loss: 1.50918e-06\n",
      "Epoch: 739900, elapsed: 1.35e+01, train loss: 2.65086e-06, val loss: 2.87763e-06, min loss: 1.50918e-06\n",
      "Epoch: 740000, elapsed: 1.15e+01, train loss: 1.51117e-06, val loss: 2.39340e-06, min loss: 1.50918e-06\n",
      "Epoch: 740100, elapsed: 1.34e+01, train loss: 1.52032e-06, val loss: 2.40083e-06, min loss: 1.50918e-06\n",
      "Epoch: 740200, elapsed: 1.14e+01, train loss: 1.56381e-06, val loss: 2.39349e-06, min loss: 1.50918e-06\n",
      "Epoch: 740300, elapsed: 1.12e+01, train loss: 1.93576e-06, val loss: 2.74574e-06, min loss: 1.50918e-06\n",
      "Epoch: 740400, elapsed: 1.14e+01, train loss: 1.65071e-06, val loss: 2.59218e-06, min loss: 1.50918e-06\n",
      "Epoch: 740500, elapsed: 1.13e+01, train loss: 1.51274e-06, val loss: 2.38202e-06, min loss: 1.50918e-06\n",
      "Epoch: 740600, elapsed: 1.13e+01, train loss: 1.52196e-06, val loss: 2.38498e-06, min loss: 1.50918e-06\n",
      "Epoch: 740700, elapsed: 1.13e+01, train loss: 1.53878e-06, val loss: 2.39595e-06, min loss: 1.50918e-06\n",
      "Epoch: 740800, elapsed: 1.13e+01, train loss: 1.71476e-06, val loss: 2.59185e-06, min loss: 1.50918e-06\n",
      "Epoch: 740900, elapsed: 1.12e+01, train loss: 7.11066e-06, val loss: 7.49363e-06, min loss: 1.50918e-06\n",
      "Epoch: 741000, elapsed: 1.13e+01, train loss: 1.53233e-06, val loss: 2.45526e-06, min loss: 1.50918e-06\n",
      "Epoch: 741100, elapsed: 1.13e+01, train loss: 1.51572e-06, val loss: 2.40564e-06, min loss: 1.50918e-06\n",
      "Epoch: 741200, elapsed: 1.13e+01, train loss: 1.54739e-06, val loss: 2.47314e-06, min loss: 1.50918e-06\n",
      "Epoch: 741300, elapsed: 1.35e+01, train loss: 1.52177e-06, val loss: 2.42782e-06, min loss: 1.50918e-06\n",
      "Epoch: 741400, elapsed: 1.13e+01, train loss: 1.56451e-06, val loss: 2.53656e-06, min loss: 1.50918e-06\n",
      "Epoch: 741500, elapsed: 1.13e+01, train loss: 1.52067e-06, val loss: 2.53399e-06, min loss: 1.50918e-06\n",
      "Epoch: 741600, elapsed: 1.13e+01, train loss: 1.64153e-06, val loss: 2.56262e-06, min loss: 1.50918e-06\n",
      "Epoch: 741700, elapsed: 1.11e+01, train loss: 1.60154e-06, val loss: 2.49291e-06, min loss: 1.50918e-06\n",
      "Epoch: 741800, elapsed: 1.13e+01, train loss: 1.62553e-06, val loss: 2.54549e-06, min loss: 1.50918e-06\n",
      "Epoch: 741900, elapsed: 1.13e+01, train loss: 1.61471e-06, val loss: 2.49026e-06, min loss: 1.50918e-06\n",
      "Epoch: 742000, elapsed: 1.13e+01, train loss: 1.59875e-06, val loss: 2.44533e-06, min loss: 1.50918e-06\n",
      "Epoch: 742100, elapsed: 1.14e+01, train loss: 2.19516e-06, val loss: 3.19278e-06, min loss: 1.50918e-06\n",
      "Epoch: 742200, elapsed: 1.14e+01, train loss: 4.44341e-06, val loss: 6.02672e-06, min loss: 1.50918e-06\n",
      "Epoch: 742300, elapsed: 1.12e+01, train loss: 2.93199e-06, val loss: 3.76091e-06, min loss: 1.50918e-06\n",
      "Epoch: 742400, elapsed: 1.11e+01, train loss: 1.76656e-06, val loss: 2.55121e-06, min loss: 1.50918e-06\n",
      "Epoch: 742500, elapsed: 1.12e+01, train loss: 1.52267e-06, val loss: 2.43985e-06, min loss: 1.50918e-06\n",
      "Epoch: 742600, elapsed: 1.14e+01, train loss: 1.58634e-06, val loss: 2.53281e-06, min loss: 1.50918e-06\n",
      "Epoch: 742700, elapsed: 1.34e+01, train loss: 1.57023e-06, val loss: 2.49624e-06, min loss: 1.50918e-06\n",
      "Epoch: 742800, elapsed: 1.14e+01, train loss: 1.79739e-06, val loss: 2.51273e-06, min loss: 1.50918e-06\n",
      "Epoch: 742900, elapsed: 1.14e+01, train loss: 6.02655e-06, val loss: 5.69079e-06, min loss: 1.50918e-06\n",
      "Epoch: 743000, elapsed: 1.13e+01, train loss: 4.59892e-06, val loss: 6.26420e-06, min loss: 1.50918e-06\n",
      "Epoch: 743100, elapsed: 1.15e+01, train loss: 2.06504e-06, val loss: 3.20218e-06, min loss: 1.50918e-06\n",
      "Epoch: 743200, elapsed: 1.13e+01, train loss: 5.43736e-06, val loss: 6.04214e-06, min loss: 1.50918e-06\n",
      "Epoch: 743300, elapsed: 1.12e+01, train loss: 4.15313e-06, val loss: 5.23592e-06, min loss: 1.50918e-06\n",
      "Epoch: 743400, elapsed: 1.13e+01, train loss: 1.71514e-06, val loss: 2.75161e-06, min loss: 1.50918e-06\n",
      "Epoch: 743500, elapsed: 1.12e+01, train loss: 1.63280e-06, val loss: 2.56092e-06, min loss: 1.50918e-06\n",
      "Epoch: 743600, elapsed: 1.14e+01, train loss: 1.50348e-06, val loss: 2.39218e-06, min loss: 1.50348e-06\n",
      "Epoch: 743700, elapsed: 1.13e+01, train loss: 1.55481e-06, val loss: 2.44066e-06, min loss: 1.50348e-06\n",
      "Epoch: 743800, elapsed: 1.13e+01, train loss: 2.06763e-06, val loss: 3.23383e-06, min loss: 1.50348e-06\n",
      "Epoch: 743900, elapsed: 1.14e+01, train loss: 1.58139e-06, val loss: 2.55229e-06, min loss: 1.50348e-06\n",
      "Epoch: 744000, elapsed: 1.13e+01, train loss: 1.50092e-06, val loss: 2.39240e-06, min loss: 1.50092e-06\n",
      "Epoch: 744100, elapsed: 1.12e+01, train loss: 1.51106e-06, val loss: 2.41286e-06, min loss: 1.50092e-06\n",
      "Epoch: 744200, elapsed: 1.39e+01, train loss: 1.53012e-06, val loss: 2.42344e-06, min loss: 1.50092e-06\n",
      "Epoch: 744300, elapsed: 1.12e+01, train loss: 1.51575e-06, val loss: 2.36593e-06, min loss: 1.50092e-06\n",
      "Epoch: 744400, elapsed: 1.12e+01, train loss: 1.56031e-06, val loss: 2.38332e-06, min loss: 1.50092e-06\n",
      "Epoch: 744500, elapsed: 1.13e+01, train loss: 1.80955e-06, val loss: 2.37999e-06, min loss: 1.50092e-06\n",
      "Epoch: 744600, elapsed: 1.12e+01, train loss: 2.06563e-06, val loss: 3.16391e-06, min loss: 1.50092e-06\n",
      "Epoch: 744700, elapsed: 1.12e+01, train loss: 2.24548e-06, val loss: 2.89697e-06, min loss: 1.50092e-06\n",
      "Epoch: 744800, elapsed: 1.13e+01, train loss: 1.55416e-06, val loss: 2.41057e-06, min loss: 1.50092e-06\n",
      "Epoch: 744900, elapsed: 1.14e+01, train loss: 1.60597e-06, val loss: 2.46138e-06, min loss: 1.50092e-06\n",
      "Epoch: 745000, elapsed: 1.11e+01, train loss: 2.61483e-06, val loss: 2.96529e-06, min loss: 1.50092e-06\n",
      "Epoch: 745100, elapsed: 1.32e+01, train loss: 1.55731e-06, val loss: 2.40923e-06, min loss: 1.50092e-06\n",
      "Epoch: 745200, elapsed: 1.12e+01, train loss: 2.10762e-06, val loss: 3.43508e-06, min loss: 1.50092e-06\n",
      "Epoch: 745300, elapsed: 1.12e+01, train loss: 1.52741e-06, val loss: 2.45532e-06, min loss: 1.50092e-06\n",
      "Epoch: 745400, elapsed: 1.11e+01, train loss: 1.50777e-06, val loss: 2.38263e-06, min loss: 1.50092e-06\n",
      "Epoch: 745500, elapsed: 1.12e+01, train loss: 1.51932e-06, val loss: 2.42922e-06, min loss: 1.50092e-06\n",
      "Epoch: 745600, elapsed: 1.36e+01, train loss: 1.53745e-06, val loss: 2.45645e-06, min loss: 1.50092e-06\n",
      "Epoch: 745700, elapsed: 1.13e+01, train loss: 1.75167e-06, val loss: 2.64958e-06, min loss: 1.50092e-06\n",
      "Epoch: 745800, elapsed: 1.13e+01, train loss: 1.63462e-06, val loss: 2.45688e-06, min loss: 1.50092e-06\n",
      "Epoch: 745900, elapsed: 1.13e+01, train loss: 1.59953e-06, val loss: 2.43604e-06, min loss: 1.50092e-06\n",
      "Epoch: 746000, elapsed: 1.13e+01, train loss: 1.52170e-06, val loss: 2.39427e-06, min loss: 1.50092e-06\n",
      "Epoch: 746100, elapsed: 1.13e+01, train loss: 2.40629e-06, val loss: 3.53438e-06, min loss: 1.50092e-06\n",
      "Epoch: 746200, elapsed: 1.13e+01, train loss: 1.49712e-06, val loss: 2.39242e-06, min loss: 1.49712e-06\n",
      "Epoch: 746300, elapsed: 1.14e+01, train loss: 1.50827e-06, val loss: 2.44661e-06, min loss: 1.49712e-06\n",
      "Epoch: 746400, elapsed: 1.13e+01, train loss: 2.99084e-06, val loss: 3.23526e-06, min loss: 1.49712e-06\n",
      "Epoch: 746500, elapsed: 1.13e+01, train loss: 1.68985e-06, val loss: 2.66113e-06, min loss: 1.49712e-06\n",
      "Epoch: 746600, elapsed: 1.14e+01, train loss: 1.65878e-06, val loss: 2.45263e-06, min loss: 1.49712e-06\n",
      "Epoch: 746700, elapsed: 1.11e+01, train loss: 1.64572e-06, val loss: 2.48992e-06, min loss: 1.49712e-06\n",
      "Epoch: 746800, elapsed: 1.12e+01, train loss: 6.09030e-06, val loss: 5.68513e-06, min loss: 1.49712e-06\n",
      "Epoch: 746900, elapsed: 1.12e+01, train loss: 2.95756e-06, val loss: 4.01645e-06, min loss: 1.49712e-06\n",
      "Epoch: 747000, elapsed: 1.34e+01, train loss: 1.83699e-06, val loss: 2.94607e-06, min loss: 1.49712e-06\n",
      "Epoch: 747100, elapsed: 1.16e+01, train loss: 1.51106e-06, val loss: 2.39257e-06, min loss: 1.49712e-06\n",
      "Epoch: 747200, elapsed: 1.16e+01, train loss: 1.50401e-06, val loss: 2.42572e-06, min loss: 1.49712e-06\n",
      "Epoch: 747300, elapsed: 1.13e+01, train loss: 1.50316e-06, val loss: 2.42016e-06, min loss: 1.49712e-06\n",
      "Epoch: 747400, elapsed: 1.14e+01, train loss: 1.50383e-06, val loss: 2.38318e-06, min loss: 1.49712e-06\n",
      "Epoch: 747500, elapsed: 1.14e+01, train loss: 1.53266e-06, val loss: 2.40369e-06, min loss: 1.49712e-06\n",
      "Epoch: 747600, elapsed: 1.15e+01, train loss: 3.52118e-06, val loss: 4.76600e-06, min loss: 1.49712e-06\n",
      "Epoch: 747700, elapsed: 1.13e+01, train loss: 4.13795e-06, val loss: 5.81845e-06, min loss: 1.49712e-06\n",
      "Epoch: 747800, elapsed: 1.15e+01, train loss: 1.49540e-06, val loss: 2.38608e-06, min loss: 1.49540e-06\n",
      "Epoch: 747900, elapsed: 1.13e+01, train loss: 1.49416e-06, val loss: 2.38981e-06, min loss: 1.49416e-06\n",
      "Epoch: 748000, elapsed: 1.14e+01, train loss: 9.13530e-06, val loss: 8.20405e-06, min loss: 1.49416e-06\n",
      "Epoch: 748100, elapsed: 1.13e+01, train loss: 1.49390e-06, val loss: 2.39804e-06, min loss: 1.49390e-06\n",
      "Epoch: 748200, elapsed: 1.14e+01, train loss: 1.49708e-06, val loss: 2.39195e-06, min loss: 1.49390e-06\n",
      "Epoch: 748300, elapsed: 1.13e+01, train loss: 2.24421e-06, val loss: 2.65993e-06, min loss: 1.49390e-06\n",
      "Epoch: 748400, elapsed: 1.14e+01, train loss: 2.46935e-06, val loss: 3.54502e-06, min loss: 1.49390e-06\n",
      "Epoch: 748500, elapsed: 1.39e+01, train loss: 1.49631e-06, val loss: 2.37613e-06, min loss: 1.49390e-06\n",
      "Epoch: 748600, elapsed: 1.16e+01, train loss: 1.50694e-06, val loss: 2.39030e-06, min loss: 1.49390e-06\n",
      "Epoch: 748700, elapsed: 1.16e+01, train loss: 2.28557e-06, val loss: 3.54162e-06, min loss: 1.49390e-06\n",
      "Epoch: 748800, elapsed: 1.14e+01, train loss: 1.49254e-06, val loss: 2.39140e-06, min loss: 1.49254e-06\n",
      "Epoch: 748900, elapsed: 1.14e+01, train loss: 1.66191e-06, val loss: 2.63239e-06, min loss: 1.49254e-06\n",
      "Epoch: 749000, elapsed: 1.12e+01, train loss: 1.73878e-06, val loss: 2.58716e-06, min loss: 1.49254e-06\n",
      "Epoch: 749100, elapsed: 1.13e+01, train loss: 1.54455e-06, val loss: 2.43527e-06, min loss: 1.49254e-06\n",
      "Epoch: 749200, elapsed: 1.14e+01, train loss: 1.59483e-06, val loss: 2.44378e-06, min loss: 1.49254e-06\n",
      "Epoch: 749300, elapsed: 1.14e+01, train loss: 3.78582e-06, val loss: 5.05011e-06, min loss: 1.49254e-06\n",
      "Epoch: 749400, elapsed: 1.13e+01, train loss: 3.85612e-06, val loss: 3.84764e-06, min loss: 1.49254e-06\n",
      "Epoch: 749500, elapsed: 1.12e+01, train loss: 1.50145e-06, val loss: 2.37118e-06, min loss: 1.49254e-06\n",
      "Epoch: 749600, elapsed: 1.12e+01, train loss: 1.51271e-06, val loss: 2.41949e-06, min loss: 1.49254e-06\n",
      "Epoch: 749700, elapsed: 1.14e+01, train loss: 2.79675e-06, val loss: 3.12531e-06, min loss: 1.49254e-06\n",
      "Epoch: 749800, elapsed: 1.13e+01, train loss: 1.64144e-06, val loss: 2.48180e-06, min loss: 1.49254e-06\n",
      "Epoch: 749900, elapsed: 1.34e+01, train loss: 1.49562e-06, val loss: 2.39664e-06, min loss: 1.49254e-06\n",
      "Epoch: 750000, elapsed: 1.15e+01, train loss: 1.49573e-06, val loss: 2.42668e-06, min loss: 1.49254e-06\n",
      "Epoch: 750100, elapsed: 1.36e+01, train loss: 1.75919e-06, val loss: 2.80451e-06, min loss: 1.49254e-06\n",
      "Epoch: 750200, elapsed: 1.13e+01, train loss: 1.50605e-06, val loss: 2.38629e-06, min loss: 1.49254e-06\n",
      "Epoch: 750300, elapsed: 1.12e+01, train loss: 1.49049e-06, val loss: 2.39200e-06, min loss: 1.49049e-06\n",
      "Epoch: 750400, elapsed: 1.13e+01, train loss: 1.50923e-06, val loss: 2.39430e-06, min loss: 1.49049e-06\n",
      "Epoch: 750500, elapsed: 1.14e+01, train loss: 1.50561e-06, val loss: 2.39019e-06, min loss: 1.49049e-06\n",
      "Epoch: 750600, elapsed: 1.13e+01, train loss: 1.49132e-06, val loss: 2.39264e-06, min loss: 1.49049e-06\n",
      "Epoch: 750700, elapsed: 1.12e+01, train loss: 1.49784e-06, val loss: 2.40667e-06, min loss: 1.49049e-06\n",
      "Epoch: 750800, elapsed: 1.13e+01, train loss: 1.51128e-06, val loss: 2.44178e-06, min loss: 1.49049e-06\n",
      "Epoch: 750900, elapsed: 1.14e+01, train loss: 1.80791e-06, val loss: 2.59071e-06, min loss: 1.49049e-06\n",
      "Epoch: 751000, elapsed: 1.12e+01, train loss: 1.99690e-06, val loss: 2.62280e-06, min loss: 1.49049e-06\n",
      "Epoch: 751100, elapsed: 1.13e+01, train loss: 2.56517e-06, val loss: 2.97080e-06, min loss: 1.49049e-06\n",
      "Epoch: 751200, elapsed: 1.14e+01, train loss: 1.59144e-06, val loss: 2.54352e-06, min loss: 1.49049e-06\n",
      "Epoch: 751300, elapsed: 1.34e+01, train loss: 1.83918e-06, val loss: 2.89311e-06, min loss: 1.49049e-06\n",
      "Epoch: 751400, elapsed: 1.16e+01, train loss: 1.79366e-06, val loss: 2.55588e-06, min loss: 1.49049e-06\n",
      "Epoch: 751500, elapsed: 1.16e+01, train loss: 1.52126e-06, val loss: 2.40838e-06, min loss: 1.49049e-06\n",
      "Epoch: 751600, elapsed: 1.15e+01, train loss: 1.53247e-06, val loss: 2.49206e-06, min loss: 1.49049e-06\n",
      "Epoch: 751700, elapsed: 1.15e+01, train loss: 1.51599e-06, val loss: 2.42446e-06, min loss: 1.49049e-06\n",
      "Epoch: 751800, elapsed: 1.13e+01, train loss: 1.48921e-06, val loss: 2.39506e-06, min loss: 1.48921e-06\n",
      "Epoch: 751900, elapsed: 1.14e+01, train loss: 1.48997e-06, val loss: 2.40106e-06, min loss: 1.48921e-06\n",
      "Epoch: 752000, elapsed: 1.14e+01, train loss: 1.55593e-06, val loss: 2.53183e-06, min loss: 1.48921e-06\n",
      "Epoch: 752100, elapsed: 1.15e+01, train loss: 1.48948e-06, val loss: 2.39205e-06, min loss: 1.48921e-06\n",
      "Epoch: 752200, elapsed: 1.13e+01, train loss: 1.56998e-06, val loss: 2.39234e-06, min loss: 1.48921e-06\n",
      "Epoch: 752300, elapsed: 1.13e+01, train loss: 1.67567e-06, val loss: 2.53799e-06, min loss: 1.48921e-06\n",
      "Epoch: 752400, elapsed: 1.14e+01, train loss: 1.49776e-06, val loss: 2.37882e-06, min loss: 1.48921e-06\n",
      "Epoch: 752500, elapsed: 1.13e+01, train loss: 1.48707e-06, val loss: 2.38690e-06, min loss: 1.48707e-06\n",
      "Epoch: 752600, elapsed: 1.13e+01, train loss: 1.50191e-06, val loss: 2.39013e-06, min loss: 1.48707e-06\n",
      "Epoch: 752700, elapsed: 1.14e+01, train loss: 1.51601e-06, val loss: 2.50523e-06, min loss: 1.48707e-06\n",
      "Epoch: 752800, elapsed: 1.37e+01, train loss: 1.72830e-06, val loss: 2.71673e-06, min loss: 1.48707e-06\n",
      "Epoch: 752900, elapsed: 1.15e+01, train loss: 1.48663e-06, val loss: 2.39455e-06, min loss: 1.48663e-06\n",
      "Epoch: 753000, elapsed: 1.14e+01, train loss: 1.50212e-06, val loss: 2.41447e-06, min loss: 1.48663e-06\n",
      "Epoch: 753100, elapsed: 1.15e+01, train loss: 1.95050e-06, val loss: 3.10892e-06, min loss: 1.48663e-06\n",
      "Epoch: 753200, elapsed: 1.15e+01, train loss: 4.25962e-06, val loss: 5.70488e-06, min loss: 1.48663e-06\n",
      "Epoch: 753300, elapsed: 1.12e+01, train loss: 1.48630e-06, val loss: 2.39208e-06, min loss: 1.48630e-06\n",
      "Epoch: 753400, elapsed: 1.15e+01, train loss: 1.49286e-06, val loss: 2.42091e-06, min loss: 1.48630e-06\n",
      "Epoch: 753500, elapsed: 1.14e+01, train loss: 1.48716e-06, val loss: 2.41218e-06, min loss: 1.48630e-06\n",
      "Epoch: 753600, elapsed: 1.14e+01, train loss: 1.89401e-06, val loss: 2.93629e-06, min loss: 1.48630e-06\n",
      "Epoch: 753700, elapsed: 1.14e+01, train loss: 1.58722e-06, val loss: 2.57694e-06, min loss: 1.48630e-06\n",
      "Epoch: 753800, elapsed: 1.13e+01, train loss: 1.49002e-06, val loss: 2.42665e-06, min loss: 1.48630e-06\n",
      "Epoch: 753900, elapsed: 1.12e+01, train loss: 5.09293e-06, val loss: 5.16538e-06, min loss: 1.48630e-06\n",
      "Epoch: 754000, elapsed: 1.13e+01, train loss: 5.25292e-06, val loss: 5.48927e-06, min loss: 1.48630e-06\n",
      "Epoch: 754100, elapsed: 1.13e+01, train loss: 2.46292e-06, val loss: 3.52000e-06, min loss: 1.48630e-06\n",
      "Epoch: 754200, elapsed: 1.13e+01, train loss: 1.94015e-06, val loss: 2.57835e-06, min loss: 1.48630e-06\n",
      "Epoch: 754300, elapsed: 1.36e+01, train loss: 2.63670e-06, val loss: 3.53870e-06, min loss: 1.48630e-06\n",
      "Epoch: 754400, elapsed: 1.14e+01, train loss: 2.16210e-06, val loss: 3.73266e-06, min loss: 1.48630e-06\n",
      "Epoch: 754500, elapsed: 1.14e+01, train loss: 1.86971e-06, val loss: 2.64919e-06, min loss: 1.48630e-06\n",
      "Epoch: 754600, elapsed: 1.14e+01, train loss: 1.53826e-06, val loss: 2.49565e-06, min loss: 1.48630e-06\n",
      "Epoch: 754700, elapsed: 1.13e+01, train loss: 1.49421e-06, val loss: 2.43898e-06, min loss: 1.48630e-06\n",
      "Epoch: 754800, elapsed: 1.13e+01, train loss: 1.48930e-06, val loss: 2.40673e-06, min loss: 1.48630e-06\n",
      "Epoch: 754900, elapsed: 1.13e+01, train loss: 1.52801e-06, val loss: 2.49429e-06, min loss: 1.48630e-06\n",
      "Epoch: 755000, elapsed: 1.14e+01, train loss: 1.56726e-06, val loss: 2.48960e-06, min loss: 1.48630e-06\n",
      "Epoch: 755100, elapsed: 1.35e+01, train loss: 3.14447e-06, val loss: 4.65019e-06, min loss: 1.48630e-06\n",
      "Epoch: 755200, elapsed: 1.14e+01, train loss: 2.20079e-06, val loss: 2.86748e-06, min loss: 1.48630e-06\n",
      "Epoch: 755300, elapsed: 1.14e+01, train loss: 1.56094e-06, val loss: 2.43472e-06, min loss: 1.48630e-06\n",
      "Epoch: 755400, elapsed: 1.13e+01, train loss: 1.74530e-06, val loss: 2.57228e-06, min loss: 1.48630e-06\n",
      "Epoch: 755500, elapsed: 1.14e+01, train loss: 2.43118e-06, val loss: 3.32432e-06, min loss: 1.48630e-06\n",
      "Epoch: 755600, elapsed: 1.12e+01, train loss: 1.58286e-06, val loss: 2.40034e-06, min loss: 1.48630e-06\n",
      "Epoch: 755700, elapsed: 1.37e+01, train loss: 2.38564e-06, val loss: 3.67365e-06, min loss: 1.48630e-06\n",
      "Epoch: 755800, elapsed: 1.14e+01, train loss: 2.09323e-06, val loss: 2.99814e-06, min loss: 1.48630e-06\n",
      "Epoch: 755900, elapsed: 1.14e+01, train loss: 1.55674e-06, val loss: 2.48952e-06, min loss: 1.48630e-06\n",
      "Epoch: 756000, elapsed: 1.14e+01, train loss: 5.01129e-06, val loss: 5.96874e-06, min loss: 1.48630e-06\n",
      "Epoch: 756100, elapsed: 1.15e+01, train loss: 1.71569e-06, val loss: 2.55574e-06, min loss: 1.48630e-06\n",
      "Epoch: 756200, elapsed: 1.16e+01, train loss: 1.62160e-06, val loss: 2.63035e-06, min loss: 1.48630e-06\n",
      "Epoch: 756300, elapsed: 1.13e+01, train loss: 1.48598e-06, val loss: 2.42031e-06, min loss: 1.48598e-06\n",
      "Epoch: 756400, elapsed: 1.13e+01, train loss: 1.48543e-06, val loss: 2.41034e-06, min loss: 1.48543e-06\n",
      "Epoch: 756500, elapsed: 1.15e+01, train loss: 1.49704e-06, val loss: 2.37613e-06, min loss: 1.48543e-06\n",
      "Epoch: 756600, elapsed: 1.13e+01, train loss: 1.49849e-06, val loss: 2.40992e-06, min loss: 1.48543e-06\n",
      "Epoch: 756700, elapsed: 1.13e+01, train loss: 1.77400e-06, val loss: 2.67780e-06, min loss: 1.48543e-06\n",
      "Epoch: 756800, elapsed: 1.14e+01, train loss: 1.54646e-06, val loss: 2.53379e-06, min loss: 1.48543e-06\n",
      "Epoch: 756900, elapsed: 1.12e+01, train loss: 1.49530e-06, val loss: 2.40327e-06, min loss: 1.48543e-06\n",
      "Epoch: 757000, elapsed: 1.13e+01, train loss: 1.76324e-06, val loss: 2.57904e-06, min loss: 1.48543e-06\n",
      "Epoch: 757100, elapsed: 1.13e+01, train loss: 1.57360e-06, val loss: 2.59528e-06, min loss: 1.48543e-06\n",
      "Epoch: 757200, elapsed: 1.38e+01, train loss: 1.73122e-06, val loss: 2.56954e-06, min loss: 1.48543e-06\n",
      "Epoch: 757300, elapsed: 1.15e+01, train loss: 1.62576e-06, val loss: 2.43319e-06, min loss: 1.48543e-06\n",
      "Epoch: 757400, elapsed: 1.16e+01, train loss: 1.86060e-06, val loss: 2.55714e-06, min loss: 1.48543e-06\n",
      "Epoch: 757500, elapsed: 1.15e+01, train loss: 2.00187e-06, val loss: 2.91988e-06, min loss: 1.48543e-06\n",
      "Epoch: 757600, elapsed: 1.13e+01, train loss: 1.61719e-06, val loss: 3.23967e-06, min loss: 1.48543e-06\n",
      "Epoch: 757700, elapsed: 1.14e+01, train loss: 1.48104e-06, val loss: 2.41452e-06, min loss: 1.48104e-06\n",
      "Epoch: 757800, elapsed: 1.14e+01, train loss: 1.48021e-06, val loss: 2.39636e-06, min loss: 1.48021e-06\n",
      "Epoch: 757900, elapsed: 1.14e+01, train loss: 1.52037e-06, val loss: 2.41167e-06, min loss: 1.48021e-06\n",
      "Epoch: 758000, elapsed: 1.13e+01, train loss: 1.82582e-06, val loss: 2.62156e-06, min loss: 1.48021e-06\n",
      "Epoch: 758100, elapsed: 1.14e+01, train loss: 1.52645e-06, val loss: 2.46834e-06, min loss: 1.48021e-06\n",
      "Epoch: 758200, elapsed: 1.14e+01, train loss: 1.71710e-06, val loss: 2.50003e-06, min loss: 1.48021e-06\n",
      "Epoch: 758300, elapsed: 1.13e+01, train loss: 2.41610e-06, val loss: 2.56982e-06, min loss: 1.48021e-06\n",
      "Epoch: 758400, elapsed: 1.13e+01, train loss: 1.94049e-06, val loss: 2.82581e-06, min loss: 1.48021e-06\n",
      "Epoch: 758500, elapsed: 1.12e+01, train loss: 1.48276e-06, val loss: 2.37203e-06, min loss: 1.48021e-06\n",
      "Epoch: 758600, elapsed: 1.35e+01, train loss: 1.52786e-06, val loss: 2.45898e-06, min loss: 1.48021e-06\n",
      "Epoch: 758700, elapsed: 1.14e+01, train loss: 2.08562e-06, val loss: 3.34178e-06, min loss: 1.48021e-06\n",
      "Epoch: 758800, elapsed: 1.12e+01, train loss: 2.60262e-06, val loss: 3.34226e-06, min loss: 1.48021e-06\n",
      "Epoch: 758900, elapsed: 1.12e+01, train loss: 1.66558e-06, val loss: 2.45780e-06, min loss: 1.48021e-06\n",
      "Epoch: 759000, elapsed: 1.13e+01, train loss: 1.50978e-06, val loss: 2.39095e-06, min loss: 1.48021e-06\n",
      "Epoch: 759100, elapsed: 1.12e+01, train loss: 2.31552e-06, val loss: 2.86709e-06, min loss: 1.48021e-06\n",
      "Epoch: 759200, elapsed: 1.12e+01, train loss: 1.47888e-06, val loss: 2.37528e-06, min loss: 1.47888e-06\n",
      "Epoch: 759300, elapsed: 1.12e+01, train loss: 1.47631e-06, val loss: 2.39890e-06, min loss: 1.47631e-06\n",
      "Epoch: 759400, elapsed: 1.12e+01, train loss: 1.69691e-06, val loss: 2.62161e-06, min loss: 1.47631e-06\n",
      "Epoch: 759500, elapsed: 1.13e+01, train loss: 2.04109e-06, val loss: 2.82271e-06, min loss: 1.47631e-06\n",
      "Epoch: 759600, elapsed: 1.13e+01, train loss: 1.74555e-06, val loss: 2.47800e-06, min loss: 1.47631e-06\n",
      "Epoch: 759700, elapsed: 1.13e+01, train loss: 1.48275e-06, val loss: 2.39500e-06, min loss: 1.47631e-06\n",
      "Epoch: 759800, elapsed: 1.11e+01, train loss: 2.99662e-06, val loss: 4.14029e-06, min loss: 1.47631e-06\n",
      "Epoch: 759900, elapsed: 1.13e+01, train loss: 1.74489e-06, val loss: 2.70638e-06, min loss: 1.47631e-06\n",
      "Epoch: 760000, elapsed: 1.13e+01, train loss: 1.65953e-06, val loss: 2.55335e-06, min loss: 1.47631e-06\n",
      "Epoch: 760100, elapsed: 1.58e+01, train loss: 7.06421e-06, val loss: 8.96907e-06, min loss: 1.47631e-06\n",
      "Epoch: 760200, elapsed: 1.16e+01, train loss: 1.47368e-06, val loss: 2.39371e-06, min loss: 1.47368e-06\n",
      "Epoch: 760300, elapsed: 1.14e+01, train loss: 1.76796e-06, val loss: 2.60324e-06, min loss: 1.47368e-06\n",
      "Epoch: 760400, elapsed: 1.14e+01, train loss: 1.48467e-06, val loss: 2.42981e-06, min loss: 1.47368e-06\n",
      "Epoch: 760500, elapsed: 1.12e+01, train loss: 1.47499e-06, val loss: 2.38426e-06, min loss: 1.47368e-06\n",
      "Epoch: 760600, elapsed: 1.13e+01, train loss: 1.48995e-06, val loss: 2.42696e-06, min loss: 1.47368e-06\n",
      "Epoch: 760700, elapsed: 1.14e+01, train loss: 1.47323e-06, val loss: 2.38510e-06, min loss: 1.47323e-06\n",
      "Epoch: 760800, elapsed: 1.12e+01, train loss: 1.55861e-06, val loss: 2.44230e-06, min loss: 1.47323e-06\n",
      "Epoch: 760900, elapsed: 1.14e+01, train loss: 1.51549e-06, val loss: 2.43811e-06, min loss: 1.47323e-06\n",
      "Epoch: 761000, elapsed: 1.12e+01, train loss: 1.48131e-06, val loss: 2.41355e-06, min loss: 1.47323e-06\n",
      "Epoch: 761100, elapsed: 1.13e+01, train loss: 1.47356e-06, val loss: 2.38887e-06, min loss: 1.47323e-06\n",
      "Epoch: 761200, elapsed: 1.12e+01, train loss: 1.47743e-06, val loss: 2.37757e-06, min loss: 1.47323e-06\n",
      "Epoch: 761300, elapsed: 1.13e+01, train loss: 1.49183e-06, val loss: 2.38634e-06, min loss: 1.47323e-06\n",
      "Epoch: 761400, elapsed: 1.12e+01, train loss: 1.58499e-06, val loss: 2.41959e-06, min loss: 1.47323e-06\n",
      "Epoch: 761500, elapsed: 1.35e+01, train loss: 1.68752e-06, val loss: 2.88341e-06, min loss: 1.47323e-06\n",
      "Epoch: 761600, elapsed: 1.13e+01, train loss: 2.42759e-06, val loss: 2.83002e-06, min loss: 1.47323e-06\n",
      "Epoch: 761700, elapsed: 1.13e+01, train loss: 1.67644e-06, val loss: 2.73455e-06, min loss: 1.47323e-06\n",
      "Epoch: 761800, elapsed: 1.13e+01, train loss: 1.52282e-06, val loss: 2.41381e-06, min loss: 1.47323e-06\n",
      "Epoch: 761900, elapsed: 1.12e+01, train loss: 1.84227e-06, val loss: 2.71873e-06, min loss: 1.47323e-06\n",
      "Epoch: 762000, elapsed: 1.13e+01, train loss: 3.65890e-06, val loss: 4.86972e-06, min loss: 1.47323e-06\n",
      "Epoch: 762100, elapsed: 1.13e+01, train loss: 1.62257e-06, val loss: 2.45086e-06, min loss: 1.47323e-06\n",
      "Epoch: 762200, elapsed: 1.14e+01, train loss: 3.13335e-06, val loss: 4.43156e-06, min loss: 1.47323e-06\n",
      "Epoch: 762300, elapsed: 1.14e+01, train loss: 2.17984e-06, val loss: 3.14484e-06, min loss: 1.47323e-06\n",
      "Epoch: 762400, elapsed: 1.13e+01, train loss: 1.47228e-06, val loss: 2.41097e-06, min loss: 1.47228e-06\n",
      "Epoch: 762500, elapsed: 1.14e+01, train loss: 1.47112e-06, val loss: 2.38959e-06, min loss: 1.47112e-06\n",
      "Epoch: 762600, elapsed: 1.13e+01, train loss: 1.77691e-06, val loss: 2.72981e-06, min loss: 1.47112e-06\n",
      "Epoch: 762700, elapsed: 1.12e+01, train loss: 1.46893e-06, val loss: 2.38351e-06, min loss: 1.46893e-06\n",
      "Epoch: 762800, elapsed: 1.13e+01, train loss: 1.47464e-06, val loss: 2.40552e-06, min loss: 1.46893e-06\n",
      "Epoch: 762900, elapsed: 1.13e+01, train loss: 1.54388e-06, val loss: 2.42787e-06, min loss: 1.46893e-06\n",
      "Epoch: 763000, elapsed: 1.35e+01, train loss: 2.09754e-06, val loss: 3.10951e-06, min loss: 1.46893e-06\n",
      "Epoch: 763100, elapsed: 1.15e+01, train loss: 1.47962e-06, val loss: 2.41424e-06, min loss: 1.46893e-06\n",
      "Epoch: 763200, elapsed: 1.14e+01, train loss: 1.49382e-06, val loss: 2.37675e-06, min loss: 1.46893e-06\n",
      "Epoch: 763300, elapsed: 1.13e+01, train loss: 1.48530e-06, val loss: 2.42088e-06, min loss: 1.46893e-06\n",
      "Epoch: 763400, elapsed: 1.14e+01, train loss: 1.47663e-06, val loss: 2.39674e-06, min loss: 1.46893e-06\n",
      "Epoch: 763500, elapsed: 1.14e+01, train loss: 1.52727e-06, val loss: 2.41402e-06, min loss: 1.46893e-06\n",
      "Epoch: 763600, elapsed: 1.15e+01, train loss: 1.83496e-06, val loss: 2.67582e-06, min loss: 1.46893e-06\n",
      "Epoch: 763700, elapsed: 1.16e+01, train loss: 1.71338e-06, val loss: 2.63277e-06, min loss: 1.46893e-06\n",
      "Epoch: 763800, elapsed: 1.11e+01, train loss: 1.49055e-06, val loss: 2.42293e-06, min loss: 1.46893e-06\n",
      "Epoch: 763900, elapsed: 1.15e+01, train loss: 1.46909e-06, val loss: 2.39360e-06, min loss: 1.46893e-06\n",
      "Epoch: 764000, elapsed: 1.15e+01, train loss: 1.48131e-06, val loss: 2.39406e-06, min loss: 1.46893e-06\n",
      "Epoch: 764100, elapsed: 1.14e+01, train loss: 1.50376e-06, val loss: 2.40043e-06, min loss: 1.46893e-06\n",
      "Epoch: 764200, elapsed: 1.14e+01, train loss: 1.50825e-06, val loss: 2.36733e-06, min loss: 1.46893e-06\n",
      "Epoch: 764300, elapsed: 1.13e+01, train loss: 1.59275e-06, val loss: 2.38586e-06, min loss: 1.46893e-06\n",
      "Epoch: 764400, elapsed: 1.12e+01, train loss: 1.69415e-06, val loss: 2.71971e-06, min loss: 1.46893e-06\n",
      "Epoch: 764500, elapsed: 1.37e+01, train loss: 8.22687e-06, val loss: 9.24050e-06, min loss: 1.46893e-06\n",
      "Epoch: 764600, elapsed: 1.14e+01, train loss: 1.46562e-06, val loss: 2.39006e-06, min loss: 1.46562e-06\n",
      "Epoch: 764700, elapsed: 1.14e+01, train loss: 1.46595e-06, val loss: 2.38118e-06, min loss: 1.46562e-06\n",
      "Epoch: 764800, elapsed: 1.14e+01, train loss: 1.49391e-06, val loss: 2.43630e-06, min loss: 1.46562e-06\n",
      "Epoch: 764900, elapsed: 1.12e+01, train loss: 1.49789e-06, val loss: 2.38455e-06, min loss: 1.46562e-06\n",
      "Epoch: 765000, elapsed: 1.13e+01, train loss: 1.47017e-06, val loss: 2.36528e-06, min loss: 1.46562e-06\n",
      "Epoch: 765100, elapsed: 1.35e+01, train loss: 1.49760e-06, val loss: 2.36456e-06, min loss: 1.46562e-06\n",
      "Epoch: 765200, elapsed: 1.13e+01, train loss: 1.50108e-06, val loss: 2.43321e-06, min loss: 1.46562e-06\n",
      "Epoch: 765300, elapsed: 1.14e+01, train loss: 1.46459e-06, val loss: 2.38508e-06, min loss: 1.46459e-06\n",
      "Epoch: 765400, elapsed: 1.12e+01, train loss: 1.47997e-06, val loss: 2.40976e-06, min loss: 1.46459e-06\n",
      "Epoch: 765500, elapsed: 1.14e+01, train loss: 1.68145e-06, val loss: 2.85530e-06, min loss: 1.46459e-06\n",
      "Epoch: 765600, elapsed: 1.12e+01, train loss: 1.46820e-06, val loss: 2.39467e-06, min loss: 1.46459e-06\n",
      "Epoch: 765700, elapsed: 1.13e+01, train loss: 1.48506e-06, val loss: 2.38754e-06, min loss: 1.46459e-06\n",
      "Epoch: 765800, elapsed: 1.13e+01, train loss: 2.40374e-06, val loss: 3.32848e-06, min loss: 1.46459e-06\n",
      "Epoch: 765900, elapsed: 1.37e+01, train loss: 2.21538e-06, val loss: 3.24856e-06, min loss: 1.46459e-06\n",
      "Epoch: 766000, elapsed: 1.15e+01, train loss: 1.62702e-06, val loss: 2.55422e-06, min loss: 1.46459e-06\n",
      "Epoch: 766100, elapsed: 1.15e+01, train loss: 1.86282e-06, val loss: 2.93241e-06, min loss: 1.46459e-06\n",
      "Epoch: 766200, elapsed: 1.13e+01, train loss: 1.54670e-06, val loss: 2.57651e-06, min loss: 1.46459e-06\n",
      "Epoch: 766300, elapsed: 1.13e+01, train loss: 1.46579e-06, val loss: 2.39069e-06, min loss: 1.46459e-06\n",
      "Epoch: 766400, elapsed: 1.14e+01, train loss: 1.46673e-06, val loss: 2.38311e-06, min loss: 1.46459e-06\n",
      "Epoch: 766500, elapsed: 1.13e+01, train loss: 1.47842e-06, val loss: 2.38345e-06, min loss: 1.46459e-06\n",
      "Epoch: 766600, elapsed: 1.15e+01, train loss: 1.46193e-06, val loss: 2.38114e-06, min loss: 1.46193e-06\n",
      "Epoch: 766700, elapsed: 1.13e+01, train loss: 1.53257e-06, val loss: 2.36278e-06, min loss: 1.46193e-06\n",
      "Epoch: 766800, elapsed: 1.13e+01, train loss: 2.13703e-06, val loss: 2.95847e-06, min loss: 1.46193e-06\n",
      "Epoch: 766900, elapsed: 1.12e+01, train loss: 4.90906e-06, val loss: 6.42977e-06, min loss: 1.46193e-06\n",
      "Epoch: 767000, elapsed: 1.14e+01, train loss: 1.46212e-06, val loss: 2.37689e-06, min loss: 1.46193e-06\n",
      "Epoch: 767100, elapsed: 1.12e+01, train loss: 1.47014e-06, val loss: 2.40232e-06, min loss: 1.46193e-06\n",
      "Epoch: 767200, elapsed: 1.13e+01, train loss: 3.68071e-06, val loss: 3.87152e-06, min loss: 1.46193e-06\n",
      "Epoch: 767300, elapsed: 1.13e+01, train loss: 1.46062e-06, val loss: 2.37947e-06, min loss: 1.46062e-06\n",
      "Epoch: 767400, elapsed: 1.34e+01, train loss: 1.46386e-06, val loss: 2.40234e-06, min loss: 1.46062e-06\n",
      "Epoch: 767500, elapsed: 1.15e+01, train loss: 1.54918e-06, val loss: 2.40300e-06, min loss: 1.46062e-06\n",
      "Epoch: 767600, elapsed: 1.13e+01, train loss: 3.45349e-06, val loss: 4.01194e-06, min loss: 1.46062e-06\n",
      "Epoch: 767700, elapsed: 1.12e+01, train loss: 7.86989e-06, val loss: 7.89802e-06, min loss: 1.46062e-06\n",
      "Epoch: 767800, elapsed: 1.14e+01, train loss: 1.46263e-06, val loss: 2.37532e-06, min loss: 1.46062e-06\n",
      "Epoch: 767900, elapsed: 1.13e+01, train loss: 1.49765e-06, val loss: 2.42139e-06, min loss: 1.46062e-06\n",
      "Epoch: 768000, elapsed: 1.13e+01, train loss: 1.45974e-06, val loss: 2.38255e-06, min loss: 1.45974e-06\n",
      "Epoch: 768100, elapsed: 1.14e+01, train loss: 1.46591e-06, val loss: 2.39526e-06, min loss: 1.45974e-06\n",
      "Epoch: 768200, elapsed: 1.13e+01, train loss: 1.56089e-06, val loss: 2.40372e-06, min loss: 1.45974e-06\n",
      "Epoch: 768300, elapsed: 1.14e+01, train loss: 1.49201e-06, val loss: 2.40583e-06, min loss: 1.45974e-06\n",
      "Epoch: 768400, elapsed: 1.14e+01, train loss: 1.47826e-06, val loss: 2.38003e-06, min loss: 1.45974e-06\n",
      "Epoch: 768500, elapsed: 1.14e+01, train loss: 1.55474e-06, val loss: 2.61541e-06, min loss: 1.45974e-06\n",
      "Epoch: 768600, elapsed: 1.14e+01, train loss: 3.95011e-06, val loss: 5.70770e-06, min loss: 1.45974e-06\n",
      "Epoch: 768700, elapsed: 1.11e+01, train loss: 2.57574e-06, val loss: 3.94059e-06, min loss: 1.45974e-06\n",
      "Epoch: 768800, elapsed: 1.13e+01, train loss: 1.51987e-06, val loss: 2.50552e-06, min loss: 1.45974e-06\n",
      "Epoch: 768900, elapsed: 1.37e+01, train loss: 1.46201e-06, val loss: 2.39754e-06, min loss: 1.45974e-06\n",
      "Epoch: 769000, elapsed: 1.14e+01, train loss: 1.46153e-06, val loss: 2.40206e-06, min loss: 1.45974e-06\n",
      "Epoch: 769100, elapsed: 1.15e+01, train loss: 1.56477e-06, val loss: 2.51476e-06, min loss: 1.45974e-06\n",
      "Epoch: 769200, elapsed: 1.15e+01, train loss: 2.81443e-06, val loss: 4.04888e-06, min loss: 1.45974e-06\n",
      "Epoch: 769300, elapsed: 1.13e+01, train loss: 1.47330e-06, val loss: 2.42318e-06, min loss: 1.45974e-06\n",
      "Epoch: 769400, elapsed: 1.13e+01, train loss: 1.57479e-06, val loss: 2.45419e-06, min loss: 1.45974e-06\n",
      "Epoch: 769500, elapsed: 1.13e+01, train loss: 1.53224e-06, val loss: 2.50976e-06, min loss: 1.45974e-06\n",
      "Epoch: 769600, elapsed: 1.14e+01, train loss: 5.13313e-06, val loss: 5.88438e-06, min loss: 1.45974e-06\n",
      "Epoch: 769700, elapsed: 1.14e+01, train loss: 2.18431e-06, val loss: 2.62203e-06, min loss: 1.45974e-06\n",
      "Epoch: 769800, elapsed: 1.13e+01, train loss: 1.46097e-06, val loss: 2.59863e-06, min loss: 1.45974e-06\n",
      "Epoch: 769900, elapsed: 1.13e+01, train loss: 1.65174e-06, val loss: 2.44592e-06, min loss: 1.45974e-06\n",
      "Epoch: 770000, elapsed: 1.12e+01, train loss: 1.45729e-06, val loss: 2.38484e-06, min loss: 1.45729e-06\n",
      "Epoch: 770100, elapsed: 1.33e+01, train loss: 1.46612e-06, val loss: 2.39930e-06, min loss: 1.45729e-06\n",
      "Epoch: 770200, elapsed: 1.12e+01, train loss: 1.50504e-06, val loss: 2.43234e-06, min loss: 1.45729e-06\n",
      "Epoch: 770300, elapsed: 1.12e+01, train loss: 1.50311e-06, val loss: 2.41411e-06, min loss: 1.45729e-06\n",
      "Epoch: 770400, elapsed: 1.34e+01, train loss: 1.47635e-06, val loss: 2.40863e-06, min loss: 1.45729e-06\n",
      "Epoch: 770500, elapsed: 1.14e+01, train loss: 1.46718e-06, val loss: 2.40132e-06, min loss: 1.45729e-06\n",
      "Epoch: 770600, elapsed: 1.14e+01, train loss: 1.95021e-06, val loss: 2.79882e-06, min loss: 1.45729e-06\n",
      "Epoch: 770700, elapsed: 1.13e+01, train loss: 1.46972e-06, val loss: 2.42797e-06, min loss: 1.45729e-06\n",
      "Epoch: 770800, elapsed: 1.14e+01, train loss: 1.46722e-06, val loss: 2.38215e-06, min loss: 1.45729e-06\n",
      "Epoch: 770900, elapsed: 1.15e+01, train loss: 1.47527e-06, val loss: 2.43409e-06, min loss: 1.45729e-06\n",
      "Epoch: 771000, elapsed: 1.14e+01, train loss: 1.81231e-06, val loss: 2.60802e-06, min loss: 1.45729e-06\n",
      "Epoch: 771100, elapsed: 1.12e+01, train loss: 1.47201e-06, val loss: 2.45317e-06, min loss: 1.45729e-06\n",
      "Epoch: 771200, elapsed: 1.12e+01, train loss: 1.56525e-06, val loss: 2.57298e-06, min loss: 1.45729e-06\n",
      "Epoch: 771300, elapsed: 1.12e+01, train loss: 2.43286e-06, val loss: 3.32920e-06, min loss: 1.45729e-06\n",
      "Epoch: 771400, elapsed: 1.14e+01, train loss: 1.55392e-06, val loss: 2.38098e-06, min loss: 1.45729e-06\n",
      "Epoch: 771500, elapsed: 1.15e+01, train loss: 3.43396e-06, val loss: 3.65122e-06, min loss: 1.45729e-06\n",
      "Epoch: 771600, elapsed: 1.11e+01, train loss: 4.35021e-06, val loss: 5.93081e-06, min loss: 1.45729e-06\n",
      "Epoch: 771700, elapsed: 1.13e+01, train loss: 1.45373e-06, val loss: 2.37395e-06, min loss: 1.45373e-06\n",
      "Epoch: 771800, elapsed: 1.35e+01, train loss: 1.46321e-06, val loss: 2.40456e-06, min loss: 1.45373e-06\n",
      "Epoch: 771900, elapsed: 1.16e+01, train loss: 1.66337e-06, val loss: 2.51153e-06, min loss: 1.45373e-06\n",
      "Epoch: 772000, elapsed: 1.15e+01, train loss: 1.48141e-06, val loss: 2.36194e-06, min loss: 1.45373e-06\n",
      "Epoch: 772100, elapsed: 1.15e+01, train loss: 1.45401e-06, val loss: 2.39357e-06, min loss: 1.45373e-06\n",
      "Epoch: 772200, elapsed: 1.14e+01, train loss: 1.60948e-06, val loss: 2.41937e-06, min loss: 1.45373e-06\n",
      "Epoch: 772300, elapsed: 1.13e+01, train loss: 8.17169e-06, val loss: 9.09929e-06, min loss: 1.45373e-06\n",
      "Epoch: 772400, elapsed: 1.12e+01, train loss: 1.45205e-06, val loss: 2.37399e-06, min loss: 1.45205e-06\n",
      "Epoch: 772500, elapsed: 1.15e+01, train loss: 1.46209e-06, val loss: 2.37207e-06, min loss: 1.45205e-06\n",
      "Epoch: 772600, elapsed: 1.12e+01, train loss: 1.46532e-06, val loss: 2.37047e-06, min loss: 1.45205e-06\n",
      "Epoch: 772700, elapsed: 1.12e+01, train loss: 1.45287e-06, val loss: 2.36695e-06, min loss: 1.45205e-06\n",
      "Epoch: 772800, elapsed: 1.13e+01, train loss: 2.00693e-06, val loss: 3.22734e-06, min loss: 1.45205e-06\n",
      "Epoch: 772900, elapsed: 1.12e+01, train loss: 7.08244e-06, val loss: 7.63866e-06, min loss: 1.45205e-06\n",
      "Epoch: 773000, elapsed: 1.13e+01, train loss: 1.46175e-06, val loss: 2.40069e-06, min loss: 1.45205e-06\n",
      "Epoch: 773100, elapsed: 1.13e+01, train loss: 1.45621e-06, val loss: 2.38058e-06, min loss: 1.45205e-06\n",
      "Epoch: 773200, elapsed: 1.14e+01, train loss: 1.81034e-06, val loss: 2.87739e-06, min loss: 1.45205e-06\n",
      "Epoch: 773300, elapsed: 1.36e+01, train loss: 1.45746e-06, val loss: 2.41211e-06, min loss: 1.45205e-06\n",
      "Epoch: 773400, elapsed: 1.15e+01, train loss: 1.45384e-06, val loss: 2.37264e-06, min loss: 1.45205e-06\n",
      "Epoch: 773500, elapsed: 1.15e+01, train loss: 1.63291e-06, val loss: 2.61645e-06, min loss: 1.45205e-06\n",
      "Epoch: 773600, elapsed: 1.15e+01, train loss: 2.28884e-06, val loss: 3.57844e-06, min loss: 1.45205e-06\n",
      "Epoch: 773700, elapsed: 1.13e+01, train loss: 1.44953e-06, val loss: 2.37766e-06, min loss: 1.44953e-06\n",
      "Epoch: 773800, elapsed: 1.14e+01, train loss: 1.45874e-06, val loss: 2.37558e-06, min loss: 1.44953e-06\n",
      "Epoch: 773900, elapsed: 1.14e+01, train loss: 2.18076e-06, val loss: 2.90120e-06, min loss: 1.44953e-06\n",
      "Epoch: 774000, elapsed: 1.14e+01, train loss: 5.57518e-06, val loss: 7.65470e-06, min loss: 1.44953e-06\n",
      "Epoch: 774100, elapsed: 1.13e+01, train loss: 1.45507e-06, val loss: 2.36057e-06, min loss: 1.44953e-06\n",
      "Epoch: 774200, elapsed: 1.15e+01, train loss: 1.46090e-06, val loss: 2.39995e-06, min loss: 1.44953e-06\n",
      "Epoch: 774300, elapsed: 1.13e+01, train loss: 1.45862e-06, val loss: 2.37920e-06, min loss: 1.44953e-06\n",
      "Epoch: 774400, elapsed: 1.13e+01, train loss: 1.45311e-06, val loss: 2.37040e-06, min loss: 1.44953e-06\n",
      "Epoch: 774500, elapsed: 1.14e+01, train loss: 2.16703e-06, val loss: 3.29599e-06, min loss: 1.44953e-06\n",
      "Epoch: 774600, elapsed: 1.13e+01, train loss: 1.70979e-06, val loss: 2.68529e-06, min loss: 1.44953e-06\n",
      "Epoch: 774700, elapsed: 1.13e+01, train loss: 1.54668e-06, val loss: 2.55119e-06, min loss: 1.44953e-06\n",
      "Epoch: 774800, elapsed: 1.34e+01, train loss: 3.28010e-06, val loss: 4.61256e-06, min loss: 1.44953e-06\n",
      "Epoch: 774900, elapsed: 1.15e+01, train loss: 1.44828e-06, val loss: 2.38083e-06, min loss: 1.44828e-06\n",
      "Epoch: 775000, elapsed: 1.15e+01, train loss: 1.47030e-06, val loss: 2.42005e-06, min loss: 1.44828e-06\n",
      "Epoch: 775100, elapsed: 1.35e+01, train loss: 1.88943e-06, val loss: 2.56760e-06, min loss: 1.44828e-06\n",
      "Epoch: 775200, elapsed: 1.13e+01, train loss: 1.53412e-06, val loss: 2.57882e-06, min loss: 1.44828e-06\n",
      "Epoch: 775300, elapsed: 1.13e+01, train loss: 5.03795e-06, val loss: 5.93835e-06, min loss: 1.44828e-06\n",
      "Epoch: 775400, elapsed: 1.12e+01, train loss: 2.11403e-06, val loss: 3.49330e-06, min loss: 1.44828e-06\n",
      "Epoch: 775500, elapsed: 1.14e+01, train loss: 2.88318e-06, val loss: 3.97903e-06, min loss: 1.44828e-06\n",
      "Epoch: 775600, elapsed: 1.13e+01, train loss: 1.57473e-06, val loss: 2.51635e-06, min loss: 1.44828e-06\n",
      "Epoch: 775700, elapsed: 1.12e+01, train loss: 1.45178e-06, val loss: 2.40094e-06, min loss: 1.44828e-06\n",
      "Epoch: 775800, elapsed: 1.14e+01, train loss: 1.45748e-06, val loss: 2.37829e-06, min loss: 1.44828e-06\n",
      "Epoch: 775900, elapsed: 1.14e+01, train loss: 1.45673e-06, val loss: 2.39567e-06, min loss: 1.44828e-06\n",
      "Epoch: 776000, elapsed: 1.14e+01, train loss: 2.61791e-06, val loss: 3.05909e-06, min loss: 1.44828e-06\n",
      "Epoch: 776100, elapsed: 1.14e+01, train loss: 1.56827e-06, val loss: 2.40170e-06, min loss: 1.44828e-06\n",
      "Epoch: 776200, elapsed: 1.14e+01, train loss: 1.52059e-06, val loss: 2.39163e-06, min loss: 1.44828e-06\n",
      "Epoch: 776300, elapsed: 1.36e+01, train loss: 1.69852e-06, val loss: 2.39802e-06, min loss: 1.44828e-06\n",
      "Epoch: 776400, elapsed: 1.15e+01, train loss: 1.46398e-06, val loss: 2.36354e-06, min loss: 1.44828e-06\n",
      "Epoch: 776500, elapsed: 1.16e+01, train loss: 1.69219e-06, val loss: 2.89199e-06, min loss: 1.44828e-06\n",
      "Epoch: 776600, elapsed: 1.15e+01, train loss: 3.33964e-06, val loss: 4.49861e-06, min loss: 1.44828e-06\n",
      "Epoch: 776700, elapsed: 1.14e+01, train loss: 2.09384e-06, val loss: 3.00941e-06, min loss: 1.44828e-06\n",
      "Epoch: 776800, elapsed: 1.13e+01, train loss: 2.68075e-06, val loss: 4.23733e-06, min loss: 1.44828e-06\n",
      "Epoch: 776900, elapsed: 1.13e+01, train loss: 1.44441e-06, val loss: 2.37894e-06, min loss: 1.44441e-06\n",
      "Epoch: 777000, elapsed: 1.14e+01, train loss: 1.48445e-06, val loss: 2.42743e-06, min loss: 1.44441e-06\n",
      "Epoch: 777100, elapsed: 1.14e+01, train loss: 1.44380e-06, val loss: 2.37471e-06, min loss: 1.44380e-06\n",
      "Epoch: 777200, elapsed: 1.14e+01, train loss: 1.44593e-06, val loss: 2.36785e-06, min loss: 1.44380e-06\n",
      "Epoch: 777300, elapsed: 1.12e+01, train loss: 3.80493e-06, val loss: 5.41166e-06, min loss: 1.44380e-06\n",
      "Epoch: 777400, elapsed: 1.13e+01, train loss: 1.44328e-06, val loss: 2.36823e-06, min loss: 1.44328e-06\n",
      "Epoch: 777500, elapsed: 1.13e+01, train loss: 1.45910e-06, val loss: 2.39411e-06, min loss: 1.44328e-06\n",
      "Epoch: 777600, elapsed: 1.15e+01, train loss: 1.59414e-06, val loss: 2.80590e-06, min loss: 1.44328e-06\n",
      "Epoch: 777700, elapsed: 1.13e+01, train loss: 1.51972e-06, val loss: 2.47319e-06, min loss: 1.44328e-06\n",
      "Epoch: 777800, elapsed: 1.38e+01, train loss: 1.44353e-06, val loss: 2.36175e-06, min loss: 1.44328e-06\n",
      "Epoch: 777900, elapsed: 1.15e+01, train loss: 1.45763e-06, val loss: 2.36692e-06, min loss: 1.44328e-06\n",
      "Epoch: 778000, elapsed: 1.14e+01, train loss: 1.49506e-06, val loss: 2.48751e-06, min loss: 1.44328e-06\n",
      "Epoch: 778100, elapsed: 1.14e+01, train loss: 1.71231e-06, val loss: 2.60121e-06, min loss: 1.44328e-06\n",
      "Epoch: 778200, elapsed: 1.13e+01, train loss: 1.85632e-06, val loss: 2.77812e-06, min loss: 1.44328e-06\n",
      "Epoch: 778300, elapsed: 1.13e+01, train loss: 1.67343e-06, val loss: 2.68251e-06, min loss: 1.44328e-06\n",
      "Epoch: 778400, elapsed: 1.12e+01, train loss: 1.45795e-06, val loss: 2.38798e-06, min loss: 1.44328e-06\n",
      "Epoch: 778500, elapsed: 1.12e+01, train loss: 1.47114e-06, val loss: 2.37627e-06, min loss: 1.44328e-06\n",
      "Epoch: 778600, elapsed: 1.10e+01, train loss: 2.74118e-06, val loss: 3.16178e-06, min loss: 1.44328e-06\n",
      "Epoch: 778700, elapsed: 1.13e+01, train loss: 1.44102e-06, val loss: 2.36535e-06, min loss: 1.44102e-06\n",
      "Epoch: 778800, elapsed: 1.12e+01, train loss: 1.54773e-06, val loss: 2.41963e-06, min loss: 1.44102e-06\n",
      "Epoch: 778900, elapsed: 1.14e+01, train loss: 2.96689e-06, val loss: 4.42571e-06, min loss: 1.44102e-06\n",
      "Epoch: 779000, elapsed: 1.12e+01, train loss: 4.47323e-06, val loss: 6.12453e-06, min loss: 1.44102e-06\n",
      "Epoch: 779100, elapsed: 1.12e+01, train loss: 3.21067e-06, val loss: 4.84234e-06, min loss: 1.44102e-06\n",
      "Epoch: 779200, elapsed: 1.13e+01, train loss: 5.25857e-06, val loss: 6.45877e-06, min loss: 1.44102e-06\n",
      "Epoch: 779300, elapsed: 1.36e+01, train loss: 1.79127e-06, val loss: 2.92668e-06, min loss: 1.44102e-06\n",
      "Epoch: 779400, elapsed: 1.15e+01, train loss: 1.44463e-06, val loss: 2.37708e-06, min loss: 1.44102e-06\n",
      "Epoch: 779500, elapsed: 1.14e+01, train loss: 1.52438e-06, val loss: 2.37866e-06, min loss: 1.44102e-06\n",
      "Epoch: 779600, elapsed: 1.14e+01, train loss: 1.60806e-06, val loss: 2.52191e-06, min loss: 1.44102e-06\n",
      "Epoch: 779700, elapsed: 1.15e+01, train loss: 1.43903e-06, val loss: 2.37290e-06, min loss: 1.43903e-06\n",
      "Epoch: 779800, elapsed: 1.16e+01, train loss: 1.45134e-06, val loss: 2.39922e-06, min loss: 1.43903e-06\n",
      "Epoch: 779900, elapsed: 1.14e+01, train loss: 1.50453e-06, val loss: 2.40740e-06, min loss: 1.43903e-06\n",
      "Epoch: 780000, elapsed: 1.14e+01, train loss: 1.50749e-06, val loss: 2.37831e-06, min loss: 1.43903e-06\n",
      "Epoch: 780100, elapsed: 1.36e+01, train loss: 1.43894e-06, val loss: 2.37265e-06, min loss: 1.43894e-06\n",
      "Epoch: 780200, elapsed: 1.13e+01, train loss: 1.44132e-06, val loss: 2.37631e-06, min loss: 1.43894e-06\n",
      "Epoch: 780300, elapsed: 1.12e+01, train loss: 3.26648e-06, val loss: 2.95823e-06, min loss: 1.43894e-06\n",
      "Epoch: 780400, elapsed: 1.14e+01, train loss: 1.43780e-06, val loss: 2.36974e-06, min loss: 1.43780e-06\n",
      "Epoch: 780500, elapsed: 1.11e+01, train loss: 1.44736e-06, val loss: 2.37807e-06, min loss: 1.43780e-06\n",
      "Epoch: 780600, elapsed: 1.13e+01, train loss: 2.60647e-06, val loss: 3.73005e-06, min loss: 1.43780e-06\n",
      "Epoch: 780700, elapsed: 1.12e+01, train loss: 1.43775e-06, val loss: 2.37202e-06, min loss: 1.43775e-06\n",
      "Epoch: 780800, elapsed: 1.37e+01, train loss: 1.44177e-06, val loss: 2.36453e-06, min loss: 1.43775e-06\n",
      "Epoch: 780900, elapsed: 1.14e+01, train loss: 1.49830e-06, val loss: 2.37569e-06, min loss: 1.43775e-06\n",
      "Epoch: 781000, elapsed: 1.13e+01, train loss: 1.46080e-06, val loss: 2.44015e-06, min loss: 1.43775e-06\n",
      "Epoch: 781100, elapsed: 1.13e+01, train loss: 2.02560e-06, val loss: 2.95373e-06, min loss: 1.43775e-06\n",
      "Epoch: 781200, elapsed: 1.12e+01, train loss: 1.43696e-06, val loss: 2.36081e-06, min loss: 1.43696e-06\n",
      "Epoch: 781300, elapsed: 1.13e+01, train loss: 1.48251e-06, val loss: 2.46010e-06, min loss: 1.43696e-06\n",
      "Epoch: 781400, elapsed: 1.15e+01, train loss: 8.12360e-06, val loss: 8.45425e-06, min loss: 1.43696e-06\n",
      "Epoch: 781500, elapsed: 1.13e+01, train loss: 1.43727e-06, val loss: 2.37430e-06, min loss: 1.43696e-06\n",
      "Epoch: 781600, elapsed: 1.14e+01, train loss: 1.43627e-06, val loss: 2.36975e-06, min loss: 1.43627e-06\n",
      "Epoch: 781700, elapsed: 1.13e+01, train loss: 1.44696e-06, val loss: 2.37920e-06, min loss: 1.43627e-06\n",
      "Epoch: 781800, elapsed: 1.12e+01, train loss: 2.50536e-06, val loss: 3.15096e-06, min loss: 1.43627e-06\n",
      "Epoch: 781900, elapsed: 1.11e+01, train loss: 2.85556e-06, val loss: 3.45897e-06, min loss: 1.43627e-06\n",
      "Epoch: 782000, elapsed: 1.13e+01, train loss: 1.46845e-06, val loss: 2.38702e-06, min loss: 1.43627e-06\n",
      "Epoch: 782100, elapsed: 1.13e+01, train loss: 3.14142e-06, val loss: 4.55405e-06, min loss: 1.43627e-06\n",
      "Epoch: 782200, elapsed: 1.13e+01, train loss: 1.56110e-06, val loss: 2.50920e-06, min loss: 1.43627e-06\n",
      "Epoch: 782300, elapsed: 1.37e+01, train loss: 2.30005e-06, val loss: 3.81929e-06, min loss: 1.43627e-06\n",
      "Epoch: 782400, elapsed: 1.14e+01, train loss: 2.21869e-06, val loss: 3.49374e-06, min loss: 1.43627e-06\n",
      "Epoch: 782500, elapsed: 1.14e+01, train loss: 1.90853e-06, val loss: 3.23363e-06, min loss: 1.43627e-06\n",
      "Epoch: 782600, elapsed: 1.14e+01, train loss: 1.43354e-06, val loss: 2.36270e-06, min loss: 1.43354e-06\n",
      "Epoch: 782700, elapsed: 1.13e+01, train loss: 1.45304e-06, val loss: 2.36954e-06, min loss: 1.43354e-06\n",
      "Epoch: 782800, elapsed: 1.14e+01, train loss: 1.77044e-06, val loss: 2.67725e-06, min loss: 1.43354e-06\n",
      "Epoch: 782900, elapsed: 1.13e+01, train loss: 3.91062e-06, val loss: 4.92313e-06, min loss: 1.43354e-06\n",
      "Epoch: 783000, elapsed: 1.14e+01, train loss: 2.14265e-06, val loss: 2.76025e-06, min loss: 1.43354e-06\n",
      "Epoch: 783100, elapsed: 1.14e+01, train loss: 1.64650e-06, val loss: 2.64974e-06, min loss: 1.43354e-06\n",
      "Epoch: 783200, elapsed: 1.12e+01, train loss: 1.48851e-06, val loss: 2.44198e-06, min loss: 1.43354e-06\n",
      "Epoch: 783300, elapsed: 1.13e+01, train loss: 1.46621e-06, val loss: 2.45607e-06, min loss: 1.43354e-06\n",
      "Epoch: 783400, elapsed: 1.12e+01, train loss: 1.43631e-06, val loss: 2.36827e-06, min loss: 1.43354e-06\n",
      "Epoch: 783500, elapsed: 1.12e+01, train loss: 1.50193e-06, val loss: 2.35851e-06, min loss: 1.43354e-06\n",
      "Epoch: 783600, elapsed: 1.12e+01, train loss: 1.57556e-06, val loss: 2.36566e-06, min loss: 1.43354e-06\n",
      "Epoch: 783700, elapsed: 1.11e+01, train loss: 3.13635e-06, val loss: 3.96469e-06, min loss: 1.43354e-06\n",
      "Epoch: 783800, elapsed: 1.34e+01, train loss: 1.62757e-06, val loss: 2.38343e-06, min loss: 1.43354e-06\n",
      "Epoch: 783900, elapsed: 1.15e+01, train loss: 1.57021e-06, val loss: 2.43583e-06, min loss: 1.43354e-06\n",
      "Epoch: 784000, elapsed: 1.14e+01, train loss: 1.62467e-06, val loss: 2.40813e-06, min loss: 1.43354e-06\n",
      "Epoch: 784100, elapsed: 1.15e+01, train loss: 2.43501e-06, val loss: 2.61741e-06, min loss: 1.43354e-06\n",
      "Epoch: 784200, elapsed: 1.14e+01, train loss: 1.47096e-06, val loss: 2.37812e-06, min loss: 1.43354e-06\n",
      "Epoch: 784300, elapsed: 1.14e+01, train loss: 1.43651e-06, val loss: 2.38479e-06, min loss: 1.43354e-06\n",
      "Epoch: 784400, elapsed: 1.13e+01, train loss: 1.43327e-06, val loss: 2.37358e-06, min loss: 1.43327e-06\n",
      "Epoch: 784500, elapsed: 1.13e+01, train loss: 1.43766e-06, val loss: 2.37090e-06, min loss: 1.43327e-06\n",
      "Epoch: 784600, elapsed: 1.14e+01, train loss: 1.52643e-06, val loss: 2.42412e-06, min loss: 1.43327e-06\n",
      "Epoch: 784700, elapsed: 1.14e+01, train loss: 1.56445e-06, val loss: 2.39932e-06, min loss: 1.43327e-06\n",
      "Epoch: 784800, elapsed: 1.14e+01, train loss: 1.44011e-06, val loss: 2.39865e-06, min loss: 1.43327e-06\n",
      "Epoch: 784900, elapsed: 1.12e+01, train loss: 1.43275e-06, val loss: 2.36658e-06, min loss: 1.43275e-06\n",
      "Epoch: 785000, elapsed: 1.14e+01, train loss: 1.42936e-06, val loss: 2.35734e-06, min loss: 1.42936e-06\n",
      "Epoch: 785100, elapsed: 1.35e+01, train loss: 1.57730e-06, val loss: 2.51274e-06, min loss: 1.42936e-06\n",
      "Epoch: 785200, elapsed: 1.12e+01, train loss: 7.65617e-06, val loss: 5.47942e-06, min loss: 1.42936e-06\n",
      "Epoch: 785300, elapsed: 1.38e+01, train loss: 1.43198e-06, val loss: 2.38606e-06, min loss: 1.42936e-06\n",
      "Epoch: 785400, elapsed: 1.15e+01, train loss: 1.51408e-06, val loss: 2.42129e-06, min loss: 1.42936e-06\n",
      "Epoch: 785500, elapsed: 1.15e+01, train loss: 1.79785e-06, val loss: 2.55604e-06, min loss: 1.42936e-06\n",
      "Epoch: 785600, elapsed: 1.13e+01, train loss: 1.84410e-06, val loss: 2.62695e-06, min loss: 1.42936e-06\n",
      "Epoch: 785700, elapsed: 1.14e+01, train loss: 1.48161e-06, val loss: 2.47733e-06, min loss: 1.42936e-06\n",
      "Epoch: 785800, elapsed: 1.12e+01, train loss: 1.56157e-06, val loss: 2.61909e-06, min loss: 1.42936e-06\n",
      "Epoch: 785900, elapsed: 1.13e+01, train loss: 2.93381e-06, val loss: 3.79088e-06, min loss: 1.42936e-06\n",
      "Epoch: 786000, elapsed: 1.12e+01, train loss: 1.45740e-06, val loss: 2.37924e-06, min loss: 1.42936e-06\n",
      "Epoch: 786100, elapsed: 1.13e+01, train loss: 1.46131e-06, val loss: 2.42528e-06, min loss: 1.42936e-06\n",
      "Epoch: 786200, elapsed: 1.12e+01, train loss: 1.74735e-06, val loss: 2.53278e-06, min loss: 1.42936e-06\n",
      "Epoch: 786300, elapsed: 1.13e+01, train loss: 1.93083e-06, val loss: 3.19954e-06, min loss: 1.42936e-06\n",
      "Epoch: 786400, elapsed: 1.13e+01, train loss: 2.36590e-06, val loss: 2.98195e-06, min loss: 1.42936e-06\n",
      "Epoch: 786500, elapsed: 1.13e+01, train loss: 2.00216e-06, val loss: 2.48949e-06, min loss: 1.42936e-06\n",
      "Epoch: 786600, elapsed: 1.12e+01, train loss: 2.15619e-06, val loss: 2.69198e-06, min loss: 1.42936e-06\n",
      "Epoch: 786700, elapsed: 1.13e+01, train loss: 1.60710e-06, val loss: 2.42224e-06, min loss: 1.42936e-06\n",
      "Epoch: 786800, elapsed: 1.36e+01, train loss: 1.64799e-06, val loss: 2.49417e-06, min loss: 1.42936e-06\n",
      "Epoch: 786900, elapsed: 1.15e+01, train loss: 1.48349e-06, val loss: 2.37298e-06, min loss: 1.42936e-06\n",
      "Epoch: 787000, elapsed: 1.13e+01, train loss: 1.46111e-06, val loss: 2.41411e-06, min loss: 1.42936e-06\n",
      "Epoch: 787100, elapsed: 1.14e+01, train loss: 1.59633e-06, val loss: 2.43005e-06, min loss: 1.42936e-06\n",
      "Epoch: 787200, elapsed: 1.12e+01, train loss: 1.75179e-06, val loss: 2.52183e-06, min loss: 1.42936e-06\n",
      "Epoch: 787300, elapsed: 1.12e+01, train loss: 1.45918e-06, val loss: 2.33367e-06, min loss: 1.42936e-06\n",
      "Epoch: 787400, elapsed: 1.14e+01, train loss: 1.64756e-06, val loss: 2.49590e-06, min loss: 1.42936e-06\n",
      "Epoch: 787500, elapsed: 1.14e+01, train loss: 5.91634e-06, val loss: 4.44356e-06, min loss: 1.42936e-06\n",
      "Epoch: 787600, elapsed: 1.14e+01, train loss: 1.42547e-06, val loss: 2.34275e-06, min loss: 1.42547e-06\n",
      "Epoch: 787700, elapsed: 1.12e+01, train loss: 1.43146e-06, val loss: 2.34694e-06, min loss: 1.42547e-06\n",
      "Epoch: 787800, elapsed: 1.13e+01, train loss: 1.46332e-06, val loss: 2.36750e-06, min loss: 1.42547e-06\n",
      "Epoch: 787900, elapsed: 1.12e+01, train loss: 1.45504e-06, val loss: 2.34505e-06, min loss: 1.42547e-06\n",
      "Epoch: 788000, elapsed: 1.13e+01, train loss: 1.48847e-06, val loss: 2.35783e-06, min loss: 1.42547e-06\n",
      "Epoch: 788100, elapsed: 1.12e+01, train loss: 1.62490e-06, val loss: 2.51576e-06, min loss: 1.42547e-06\n",
      "Epoch: 788200, elapsed: 1.14e+01, train loss: 1.53962e-06, val loss: 2.39866e-06, min loss: 1.42547e-06\n",
      "Epoch: 788300, elapsed: 1.37e+01, train loss: 1.42580e-06, val loss: 2.34534e-06, min loss: 1.42547e-06\n",
      "Epoch: 788400, elapsed: 1.15e+01, train loss: 1.43397e-06, val loss: 2.37211e-06, min loss: 1.42547e-06\n",
      "Epoch: 788500, elapsed: 1.13e+01, train loss: 1.51455e-06, val loss: 2.49568e-06, min loss: 1.42547e-06\n",
      "Epoch: 788600, elapsed: 1.14e+01, train loss: 1.55778e-06, val loss: 2.59666e-06, min loss: 1.42547e-06\n",
      "Epoch: 788700, elapsed: 1.13e+01, train loss: 1.53069e-06, val loss: 2.47869e-06, min loss: 1.42547e-06\n",
      "Epoch: 788800, elapsed: 1.14e+01, train loss: 1.49530e-06, val loss: 2.47243e-06, min loss: 1.42547e-06\n",
      "Epoch: 788900, elapsed: 1.14e+01, train loss: 1.51629e-06, val loss: 2.39181e-06, min loss: 1.42547e-06\n",
      "Epoch: 789000, elapsed: 1.14e+01, train loss: 3.88633e-06, val loss: 4.96073e-06, min loss: 1.42547e-06\n",
      "Epoch: 789100, elapsed: 1.13e+01, train loss: 1.42181e-06, val loss: 2.35063e-06, min loss: 1.42181e-06\n",
      "Epoch: 789200, elapsed: 1.13e+01, train loss: 1.43357e-06, val loss: 2.35473e-06, min loss: 1.42181e-06\n",
      "Epoch: 789300, elapsed: 1.12e+01, train loss: 5.65192e-06, val loss: 6.30138e-06, min loss: 1.42181e-06\n",
      "Epoch: 789400, elapsed: 1.12e+01, train loss: 5.60062e-06, val loss: 5.92130e-06, min loss: 1.42181e-06\n",
      "Epoch: 789500, elapsed: 1.11e+01, train loss: 2.02522e-06, val loss: 3.23348e-06, min loss: 1.42181e-06\n",
      "Epoch: 789600, elapsed: 1.13e+01, train loss: 1.42822e-06, val loss: 2.34297e-06, min loss: 1.42181e-06\n",
      "Epoch: 789700, elapsed: 1.14e+01, train loss: 1.43047e-06, val loss: 2.37623e-06, min loss: 1.42181e-06\n",
      "Epoch: 789800, elapsed: 1.36e+01, train loss: 1.42384e-06, val loss: 2.36726e-06, min loss: 1.42181e-06\n",
      "Epoch: 789900, elapsed: 1.15e+01, train loss: 1.44980e-06, val loss: 2.40931e-06, min loss: 1.42181e-06\n",
      "Epoch: 790000, elapsed: 1.15e+01, train loss: 1.77844e-06, val loss: 2.67561e-06, min loss: 1.42181e-06\n",
      "Epoch: 790100, elapsed: 1.37e+01, train loss: 1.42639e-06, val loss: 2.35013e-06, min loss: 1.42181e-06\n",
      "Epoch: 790200, elapsed: 1.15e+01, train loss: 1.43058e-06, val loss: 2.38927e-06, min loss: 1.42181e-06\n",
      "Epoch: 790300, elapsed: 1.14e+01, train loss: 2.32325e-06, val loss: 2.90591e-06, min loss: 1.42181e-06\n",
      "Epoch: 790400, elapsed: 1.13e+01, train loss: 1.46448e-06, val loss: 2.37904e-06, min loss: 1.42181e-06\n",
      "Epoch: 790500, elapsed: 1.13e+01, train loss: 3.51156e-06, val loss: 3.67229e-06, min loss: 1.42181e-06\n",
      "Epoch: 790600, elapsed: 1.14e+01, train loss: 1.47500e-06, val loss: 2.49979e-06, min loss: 1.42181e-06\n",
      "Epoch: 790700, elapsed: 1.12e+01, train loss: 1.43523e-06, val loss: 2.34317e-06, min loss: 1.42181e-06\n",
      "Epoch: 790800, elapsed: 1.12e+01, train loss: 1.61193e-06, val loss: 2.55307e-06, min loss: 1.42181e-06\n",
      "Epoch: 790900, elapsed: 1.15e+01, train loss: 2.23234e-06, val loss: 2.85738e-06, min loss: 1.42181e-06\n",
      "Epoch: 791000, elapsed: 1.15e+01, train loss: 1.41820e-06, val loss: 2.34081e-06, min loss: 1.41820e-06\n",
      "Epoch: 791100, elapsed: 1.13e+01, train loss: 3.26934e-06, val loss: 3.74909e-06, min loss: 1.41820e-06\n",
      "Epoch: 791200, elapsed: 1.13e+01, train loss: 1.54399e-06, val loss: 2.41497e-06, min loss: 1.41820e-06\n",
      "Epoch: 791300, elapsed: 1.37e+01, train loss: 3.71268e-06, val loss: 3.92833e-06, min loss: 1.41820e-06\n",
      "Epoch: 791400, elapsed: 1.14e+01, train loss: 1.89053e-06, val loss: 2.82696e-06, min loss: 1.41820e-06\n",
      "Epoch: 791500, elapsed: 1.13e+01, train loss: 1.67618e-06, val loss: 2.37797e-06, min loss: 1.41820e-06\n",
      "Epoch: 791600, elapsed: 1.14e+01, train loss: 1.83994e-06, val loss: 2.93961e-06, min loss: 1.41820e-06\n",
      "Epoch: 791700, elapsed: 1.13e+01, train loss: 2.06855e-06, val loss: 3.72314e-06, min loss: 1.41820e-06\n",
      "Epoch: 791800, elapsed: 1.13e+01, train loss: 1.54068e-06, val loss: 2.43592e-06, min loss: 1.41820e-06\n",
      "Epoch: 791900, elapsed: 1.14e+01, train loss: 1.41664e-06, val loss: 2.34099e-06, min loss: 1.41664e-06\n",
      "Epoch: 792000, elapsed: 1.13e+01, train loss: 1.42648e-06, val loss: 2.36662e-06, min loss: 1.41664e-06\n",
      "Epoch: 792100, elapsed: 1.15e+01, train loss: 1.46681e-06, val loss: 2.34088e-06, min loss: 1.41664e-06\n",
      "Epoch: 792200, elapsed: 1.14e+01, train loss: 1.93759e-06, val loss: 2.84054e-06, min loss: 1.41664e-06\n",
      "Epoch: 792300, elapsed: 1.15e+01, train loss: 2.97737e-06, val loss: 3.96522e-06, min loss: 1.41664e-06\n",
      "Epoch: 792400, elapsed: 1.14e+01, train loss: 1.50131e-06, val loss: 2.47757e-06, min loss: 1.41664e-06\n",
      "Epoch: 792500, elapsed: 1.14e+01, train loss: 1.56192e-06, val loss: 2.68789e-06, min loss: 1.41664e-06\n",
      "Epoch: 792600, elapsed: 1.14e+01, train loss: 1.50308e-06, val loss: 2.39722e-06, min loss: 1.41664e-06\n",
      "Epoch: 792700, elapsed: 1.13e+01, train loss: 1.51948e-06, val loss: 2.48523e-06, min loss: 1.41664e-06\n",
      "Epoch: 792800, elapsed: 1.36e+01, train loss: 1.43664e-06, val loss: 2.39993e-06, min loss: 1.41664e-06\n",
      "Epoch: 792900, elapsed: 1.16e+01, train loss: 1.42145e-06, val loss: 2.36396e-06, min loss: 1.41664e-06\n",
      "Epoch: 793000, elapsed: 1.16e+01, train loss: 1.43760e-06, val loss: 2.38372e-06, min loss: 1.41664e-06\n",
      "Epoch: 793100, elapsed: 1.14e+01, train loss: 1.43730e-06, val loss: 2.34584e-06, min loss: 1.41664e-06\n",
      "Epoch: 793200, elapsed: 1.14e+01, train loss: 1.55538e-06, val loss: 2.47747e-06, min loss: 1.41664e-06\n",
      "Epoch: 793300, elapsed: 1.14e+01, train loss: 1.61554e-06, val loss: 2.64768e-06, min loss: 1.41664e-06\n",
      "Epoch: 793400, elapsed: 1.13e+01, train loss: 4.29160e-06, val loss: 5.75580e-06, min loss: 1.41664e-06\n",
      "Epoch: 793500, elapsed: 1.14e+01, train loss: 1.42273e-06, val loss: 2.41895e-06, min loss: 1.41664e-06\n",
      "Epoch: 793600, elapsed: 1.12e+01, train loss: 1.42057e-06, val loss: 2.33902e-06, min loss: 1.41664e-06\n",
      "Epoch: 793700, elapsed: 1.14e+01, train loss: 2.57330e-06, val loss: 3.07038e-06, min loss: 1.41664e-06\n",
      "Epoch: 793800, elapsed: 1.14e+01, train loss: 1.43732e-06, val loss: 2.35042e-06, min loss: 1.41664e-06\n",
      "Epoch: 793900, elapsed: 1.12e+01, train loss: 1.55335e-06, val loss: 2.52745e-06, min loss: 1.41664e-06\n",
      "Epoch: 794000, elapsed: 1.14e+01, train loss: 1.43227e-06, val loss: 2.33666e-06, min loss: 1.41664e-06\n",
      "Epoch: 794100, elapsed: 1.13e+01, train loss: 1.41257e-06, val loss: 2.34173e-06, min loss: 1.41257e-06\n",
      "Epoch: 794200, elapsed: 1.13e+01, train loss: 1.43171e-06, val loss: 2.32906e-06, min loss: 1.41257e-06\n",
      "Epoch: 794300, elapsed: 1.12e+01, train loss: 1.65163e-06, val loss: 2.58935e-06, min loss: 1.41257e-06\n",
      "Epoch: 794400, elapsed: 1.36e+01, train loss: 3.58716e-06, val loss: 4.40805e-06, min loss: 1.41257e-06\n",
      "Epoch: 794500, elapsed: 1.14e+01, train loss: 1.45895e-06, val loss: 2.41955e-06, min loss: 1.41257e-06\n",
      "Epoch: 794600, elapsed: 1.14e+01, train loss: 3.45462e-06, val loss: 4.86831e-06, min loss: 1.41257e-06\n",
      "Epoch: 794700, elapsed: 1.14e+01, train loss: 1.51173e-06, val loss: 2.43418e-06, min loss: 1.41257e-06\n",
      "Epoch: 794800, elapsed: 1.14e+01, train loss: 2.04608e-06, val loss: 2.80449e-06, min loss: 1.41257e-06\n",
      "Epoch: 794900, elapsed: 1.12e+01, train loss: 1.57272e-06, val loss: 2.48713e-06, min loss: 1.41257e-06\n",
      "Epoch: 795000, elapsed: 1.12e+01, train loss: 1.41057e-06, val loss: 2.34263e-06, min loss: 1.41057e-06\n",
      "Epoch: 795100, elapsed: 1.32e+01, train loss: 1.52176e-06, val loss: 2.52146e-06, min loss: 1.41057e-06\n",
      "Epoch: 795200, elapsed: 1.12e+01, train loss: 3.64433e-06, val loss: 4.72170e-06, min loss: 1.41057e-06\n",
      "Epoch: 795300, elapsed: 1.13e+01, train loss: 2.36878e-06, val loss: 3.40922e-06, min loss: 1.41057e-06\n",
      "Epoch: 795400, elapsed: 1.13e+01, train loss: 1.02033e-05, val loss: 1.10487e-05, min loss: 1.41057e-06\n",
      "Epoch: 795500, elapsed: 1.13e+01, train loss: 1.40978e-06, val loss: 2.33644e-06, min loss: 1.40978e-06\n",
      "Epoch: 795600, elapsed: 1.13e+01, train loss: 1.50170e-06, val loss: 2.35088e-06, min loss: 1.40978e-06\n",
      "Epoch: 795700, elapsed: 1.13e+01, train loss: 1.45123e-06, val loss: 2.35475e-06, min loss: 1.40978e-06\n",
      "Epoch: 795800, elapsed: 1.13e+01, train loss: 1.42092e-06, val loss: 2.31797e-06, min loss: 1.40978e-06\n",
      "Epoch: 795900, elapsed: 1.38e+01, train loss: 1.41303e-06, val loss: 2.34767e-06, min loss: 1.40978e-06\n",
      "Epoch: 796000, elapsed: 1.16e+01, train loss: 1.41573e-06, val loss: 2.35418e-06, min loss: 1.40978e-06\n",
      "Epoch: 796100, elapsed: 1.16e+01, train loss: 1.43596e-06, val loss: 2.38325e-06, min loss: 1.40978e-06\n",
      "Epoch: 796200, elapsed: 1.15e+01, train loss: 1.42755e-06, val loss: 2.32492e-06, min loss: 1.40978e-06\n",
      "Epoch: 796300, elapsed: 1.13e+01, train loss: 1.44238e-06, val loss: 2.40556e-06, min loss: 1.40978e-06\n",
      "Epoch: 796400, elapsed: 1.13e+01, train loss: 1.42923e-06, val loss: 2.32862e-06, min loss: 1.40978e-06\n",
      "Epoch: 796500, elapsed: 1.14e+01, train loss: 1.46017e-06, val loss: 2.38684e-06, min loss: 1.40978e-06\n",
      "Epoch: 796600, elapsed: 1.15e+01, train loss: 1.46450e-06, val loss: 2.35223e-06, min loss: 1.40978e-06\n",
      "Epoch: 796700, elapsed: 1.13e+01, train loss: 1.46739e-06, val loss: 2.39574e-06, min loss: 1.40978e-06\n",
      "Epoch: 796800, elapsed: 1.12e+01, train loss: 1.92183e-06, val loss: 2.69750e-06, min loss: 1.40978e-06\n",
      "Epoch: 796900, elapsed: 1.13e+01, train loss: 3.61713e-06, val loss: 3.84115e-06, min loss: 1.40978e-06\n",
      "Epoch: 797000, elapsed: 1.15e+01, train loss: 1.43898e-06, val loss: 2.30830e-06, min loss: 1.40978e-06\n",
      "Epoch: 797100, elapsed: 1.12e+01, train loss: 1.41170e-06, val loss: 2.34884e-06, min loss: 1.40978e-06\n",
      "Epoch: 797200, elapsed: 1.13e+01, train loss: 1.41927e-06, val loss: 2.35363e-06, min loss: 1.40978e-06\n",
      "Epoch: 797300, elapsed: 1.13e+01, train loss: 1.45607e-06, val loss: 2.37342e-06, min loss: 1.40978e-06\n",
      "Epoch: 797400, elapsed: 1.37e+01, train loss: 1.56401e-06, val loss: 2.37519e-06, min loss: 1.40978e-06\n",
      "Epoch: 797500, elapsed: 1.15e+01, train loss: 1.53140e-06, val loss: 2.58025e-06, min loss: 1.40978e-06\n",
      "Epoch: 797600, elapsed: 1.15e+01, train loss: 1.43871e-06, val loss: 2.34974e-06, min loss: 1.40978e-06\n",
      "Epoch: 797700, elapsed: 1.14e+01, train loss: 1.41781e-06, val loss: 2.31962e-06, min loss: 1.40978e-06\n",
      "Epoch: 797800, elapsed: 1.14e+01, train loss: 1.40972e-06, val loss: 2.34532e-06, min loss: 1.40972e-06\n",
      "Epoch: 797900, elapsed: 1.14e+01, train loss: 1.49594e-06, val loss: 2.36943e-06, min loss: 1.40972e-06\n",
      "Epoch: 798000, elapsed: 1.13e+01, train loss: 1.51939e-06, val loss: 2.33666e-06, min loss: 1.40972e-06\n",
      "Epoch: 798100, elapsed: 1.14e+01, train loss: 1.60559e-06, val loss: 2.39590e-06, min loss: 1.40972e-06\n",
      "Epoch: 798200, elapsed: 1.14e+01, train loss: 5.10780e-06, val loss: 5.27448e-06, min loss: 1.40972e-06\n",
      "Epoch: 798300, elapsed: 1.15e+01, train loss: 6.33165e-06, val loss: 6.46821e-06, min loss: 1.40972e-06\n",
      "Epoch: 798400, elapsed: 1.15e+01, train loss: 1.47795e-06, val loss: 2.37096e-06, min loss: 1.40972e-06\n",
      "Epoch: 798500, elapsed: 1.13e+01, train loss: 1.40442e-06, val loss: 2.33122e-06, min loss: 1.40442e-06\n",
      "Epoch: 798600, elapsed: 1.13e+01, train loss: 1.57103e-06, val loss: 2.69353e-06, min loss: 1.40442e-06\n",
      "Epoch: 798700, elapsed: 1.13e+01, train loss: 2.26607e-06, val loss: 3.26570e-06, min loss: 1.40442e-06\n",
      "Epoch: 798800, elapsed: 1.13e+01, train loss: 1.40378e-06, val loss: 2.33050e-06, min loss: 1.40378e-06\n",
      "Epoch: 798900, elapsed: 1.37e+01, train loss: 1.41972e-06, val loss: 2.32486e-06, min loss: 1.40378e-06\n",
      "Epoch: 799000, elapsed: 1.15e+01, train loss: 1.41351e-06, val loss: 2.35311e-06, min loss: 1.40378e-06\n",
      "Epoch: 799100, elapsed: 1.15e+01, train loss: 1.41623e-06, val loss: 2.35122e-06, min loss: 1.40378e-06\n",
      "Epoch: 799200, elapsed: 1.13e+01, train loss: 1.53104e-06, val loss: 2.33573e-06, min loss: 1.40378e-06\n",
      "Epoch: 799300, elapsed: 1.15e+01, train loss: 1.60212e-06, val loss: 2.51440e-06, min loss: 1.40378e-06\n",
      "Epoch: 799400, elapsed: 1.14e+01, train loss: 1.53631e-06, val loss: 2.63403e-06, min loss: 1.40378e-06\n",
      "Epoch: 799500, elapsed: 1.16e+01, train loss: 3.01132e-06, val loss: 3.01047e-06, min loss: 1.40378e-06\n",
      "Epoch: 799600, elapsed: 1.14e+01, train loss: 1.44755e-06, val loss: 2.34644e-06, min loss: 1.40378e-06\n",
      "Epoch: 799700, elapsed: 1.13e+01, train loss: 1.40195e-06, val loss: 2.32244e-06, min loss: 1.40195e-06\n",
      "Epoch: 799800, elapsed: 1.15e+01, train loss: 1.60688e-06, val loss: 2.67807e-06, min loss: 1.40195e-06\n",
      "Epoch: 799900, elapsed: 1.13e+01, train loss: 2.02147e-06, val loss: 3.08246e-06, min loss: 1.40195e-06\n",
      "Epoch: 800000, elapsed: 1.13e+01, train loss: 1.40567e-06, val loss: 2.31844e-06, min loss: 1.40195e-06\n",
      "Epoch: 800100, elapsed: 1.35e+01, train loss: 1.40358e-06, val loss: 2.31678e-06, min loss: 1.40195e-06\n",
      "Epoch: 800200, elapsed: 1.12e+01, train loss: 1.41110e-06, val loss: 2.36081e-06, min loss: 1.40195e-06\n",
      "Epoch: 800300, elapsed: 1.13e+01, train loss: 1.40579e-06, val loss: 2.31680e-06, min loss: 1.40195e-06\n",
      "Epoch: 800400, elapsed: 1.36e+01, train loss: 1.46542e-06, val loss: 2.43312e-06, min loss: 1.40195e-06\n",
      "Epoch: 800500, elapsed: 1.14e+01, train loss: 1.96071e-06, val loss: 2.87323e-06, min loss: 1.40195e-06\n",
      "Epoch: 800600, elapsed: 1.16e+01, train loss: 1.41865e-06, val loss: 2.32214e-06, min loss: 1.40195e-06\n",
      "Epoch: 800700, elapsed: 1.15e+01, train loss: 1.42927e-06, val loss: 2.30887e-06, min loss: 1.40195e-06\n",
      "Epoch: 800800, elapsed: 1.15e+01, train loss: 1.40380e-06, val loss: 2.32397e-06, min loss: 1.40195e-06\n",
      "Epoch: 800900, elapsed: 1.13e+01, train loss: 1.44410e-06, val loss: 2.35908e-06, min loss: 1.40195e-06\n",
      "Epoch: 801000, elapsed: 1.14e+01, train loss: 2.19225e-06, val loss: 2.90805e-06, min loss: 1.40195e-06\n",
      "Epoch: 801100, elapsed: 1.14e+01, train loss: 1.39925e-06, val loss: 2.32980e-06, min loss: 1.39925e-06\n",
      "Epoch: 801200, elapsed: 1.15e+01, train loss: 6.53273e-06, val loss: 9.08898e-06, min loss: 1.39925e-06\n",
      "Epoch: 801300, elapsed: 1.13e+01, train loss: 1.40500e-06, val loss: 2.33942e-06, min loss: 1.39925e-06\n",
      "Epoch: 801400, elapsed: 1.13e+01, train loss: 1.42823e-06, val loss: 2.39447e-06, min loss: 1.39925e-06\n",
      "Epoch: 801500, elapsed: 1.12e+01, train loss: 1.42701e-06, val loss: 2.31019e-06, min loss: 1.39925e-06\n",
      "Epoch: 801600, elapsed: 1.11e+01, train loss: 1.51831e-06, val loss: 2.47427e-06, min loss: 1.39925e-06\n",
      "Epoch: 801700, elapsed: 1.13e+01, train loss: 5.05001e-06, val loss: 4.80520e-06, min loss: 1.39925e-06\n",
      "Epoch: 801800, elapsed: 1.12e+01, train loss: 2.01747e-06, val loss: 3.24142e-06, min loss: 1.39925e-06\n",
      "Epoch: 801900, elapsed: 1.14e+01, train loss: 1.93677e-06, val loss: 2.58834e-06, min loss: 1.39925e-06\n",
      "Epoch: 802000, elapsed: 1.36e+01, train loss: 3.94672e-06, val loss: 4.73390e-06, min loss: 1.39925e-06\n",
      "Epoch: 802100, elapsed: 1.16e+01, train loss: 1.77344e-06, val loss: 2.59393e-06, min loss: 1.39925e-06\n",
      "Epoch: 802200, elapsed: 1.15e+01, train loss: 1.60496e-06, val loss: 2.46753e-06, min loss: 1.39925e-06\n",
      "Epoch: 802300, elapsed: 1.15e+01, train loss: 1.84701e-06, val loss: 2.47812e-06, min loss: 1.39925e-06\n",
      "Epoch: 802400, elapsed: 1.15e+01, train loss: 1.60071e-06, val loss: 2.45561e-06, min loss: 1.39925e-06\n",
      "Epoch: 802500, elapsed: 1.13e+01, train loss: 1.53488e-06, val loss: 2.51113e-06, min loss: 1.39925e-06\n",
      "Epoch: 802600, elapsed: 1.13e+01, train loss: 1.90289e-06, val loss: 2.78029e-06, min loss: 1.39925e-06\n",
      "Epoch: 802700, elapsed: 1.14e+01, train loss: 1.45443e-06, val loss: 2.52305e-06, min loss: 1.39925e-06\n",
      "Epoch: 802800, elapsed: 1.14e+01, train loss: 4.87043e-06, val loss: 5.46781e-06, min loss: 1.39925e-06\n",
      "Epoch: 802900, elapsed: 1.15e+01, train loss: 1.50470e-06, val loss: 2.73986e-06, min loss: 1.39925e-06\n",
      "Epoch: 803000, elapsed: 1.13e+01, train loss: 2.07722e-06, val loss: 2.95555e-06, min loss: 1.39925e-06\n",
      "Epoch: 803100, elapsed: 1.11e+01, train loss: 1.41698e-06, val loss: 2.34009e-06, min loss: 1.39925e-06\n",
      "Epoch: 803200, elapsed: 1.13e+01, train loss: 1.42596e-06, val loss: 2.30862e-06, min loss: 1.39925e-06\n",
      "Epoch: 803300, elapsed: 1.13e+01, train loss: 1.51755e-06, val loss: 2.43079e-06, min loss: 1.39925e-06\n",
      "Epoch: 803400, elapsed: 1.14e+01, train loss: 1.46626e-06, val loss: 2.30915e-06, min loss: 1.39925e-06\n",
      "Epoch: 803500, elapsed: 1.37e+01, train loss: 1.39616e-06, val loss: 2.32481e-06, min loss: 1.39616e-06\n",
      "Epoch: 803600, elapsed: 1.14e+01, train loss: 1.84923e-06, val loss: 2.91295e-06, min loss: 1.39616e-06\n",
      "Epoch: 803700, elapsed: 1.13e+01, train loss: 1.40572e-06, val loss: 2.34854e-06, min loss: 1.39616e-06\n",
      "Epoch: 803800, elapsed: 1.13e+01, train loss: 1.39475e-06, val loss: 2.31204e-06, min loss: 1.39475e-06\n",
      "Epoch: 803900, elapsed: 1.13e+01, train loss: 1.42020e-06, val loss: 2.31280e-06, min loss: 1.39475e-06\n",
      "Epoch: 804000, elapsed: 1.14e+01, train loss: 1.45459e-06, val loss: 2.33387e-06, min loss: 1.39475e-06\n",
      "Epoch: 804100, elapsed: 1.12e+01, train loss: 1.64634e-06, val loss: 2.52387e-06, min loss: 1.39475e-06\n",
      "Epoch: 804200, elapsed: 1.13e+01, train loss: 3.62040e-06, val loss: 3.95610e-06, min loss: 1.39475e-06\n",
      "Epoch: 804300, elapsed: 1.15e+01, train loss: 1.61240e-06, val loss: 2.48530e-06, min loss: 1.39475e-06\n",
      "Epoch: 804400, elapsed: 1.13e+01, train loss: 1.49036e-06, val loss: 2.32395e-06, min loss: 1.39475e-06\n",
      "Epoch: 804500, elapsed: 1.13e+01, train loss: 1.43495e-06, val loss: 2.43992e-06, min loss: 1.39475e-06\n",
      "Epoch: 804600, elapsed: 1.13e+01, train loss: 1.51832e-06, val loss: 2.30837e-06, min loss: 1.39475e-06\n",
      "Epoch: 804700, elapsed: 1.14e+01, train loss: 1.61694e-06, val loss: 2.53523e-06, min loss: 1.39475e-06\n",
      "Epoch: 804800, elapsed: 1.13e+01, train loss: 1.57591e-06, val loss: 2.34693e-06, min loss: 1.39475e-06\n",
      "Epoch: 804900, elapsed: 1.13e+01, train loss: 1.39650e-06, val loss: 2.33230e-06, min loss: 1.39475e-06\n",
      "Epoch: 805000, elapsed: 1.12e+01, train loss: 1.41567e-06, val loss: 2.32304e-06, min loss: 1.39475e-06\n",
      "Epoch: 805100, elapsed: 1.59e+01, train loss: 1.39176e-06, val loss: 2.31048e-06, min loss: 1.39176e-06\n",
      "Epoch: 805200, elapsed: 1.14e+01, train loss: 1.40033e-06, val loss: 2.32827e-06, min loss: 1.39176e-06\n",
      "Epoch: 805300, elapsed: 1.13e+01, train loss: 1.40437e-06, val loss: 2.34046e-06, min loss: 1.39176e-06\n",
      "Epoch: 805400, elapsed: 1.14e+01, train loss: 1.41157e-06, val loss: 2.35814e-06, min loss: 1.39176e-06\n",
      "Epoch: 805500, elapsed: 1.14e+01, train loss: 1.45739e-06, val loss: 2.36715e-06, min loss: 1.39176e-06\n",
      "Epoch: 805600, elapsed: 1.13e+01, train loss: 1.45800e-06, val loss: 2.32780e-06, min loss: 1.39176e-06\n",
      "Epoch: 805700, elapsed: 1.15e+01, train loss: 1.50607e-06, val loss: 2.45935e-06, min loss: 1.39176e-06\n",
      "Epoch: 805800, elapsed: 1.15e+01, train loss: 1.54699e-06, val loss: 2.41659e-06, min loss: 1.39176e-06\n",
      "Epoch: 805900, elapsed: 1.13e+01, train loss: 1.39791e-06, val loss: 2.34242e-06, min loss: 1.39176e-06\n",
      "Epoch: 806000, elapsed: 1.13e+01, train loss: 1.39628e-06, val loss: 2.31988e-06, min loss: 1.39176e-06\n",
      "Epoch: 806100, elapsed: 1.15e+01, train loss: 1.39954e-06, val loss: 2.30319e-06, min loss: 1.39176e-06\n",
      "Epoch: 806200, elapsed: 1.14e+01, train loss: 1.52855e-06, val loss: 2.74184e-06, min loss: 1.39176e-06\n",
      "Epoch: 806300, elapsed: 1.13e+01, train loss: 1.38831e-06, val loss: 2.31156e-06, min loss: 1.38831e-06\n",
      "Epoch: 806400, elapsed: 1.14e+01, train loss: 1.39960e-06, val loss: 2.31318e-06, min loss: 1.38831e-06\n",
      "Epoch: 806500, elapsed: 1.12e+01, train loss: 1.57174e-06, val loss: 2.48320e-06, min loss: 1.38831e-06\n",
      "Epoch: 806600, elapsed: 1.37e+01, train loss: 1.41307e-06, val loss: 2.30301e-06, min loss: 1.38831e-06\n",
      "Epoch: 806700, elapsed: 1.16e+01, train loss: 1.46905e-06, val loss: 2.34734e-06, min loss: 1.38831e-06\n",
      "Epoch: 806800, elapsed: 1.14e+01, train loss: 1.68264e-06, val loss: 2.54519e-06, min loss: 1.38831e-06\n",
      "Epoch: 806900, elapsed: 1.12e+01, train loss: 1.67684e-06, val loss: 2.71042e-06, min loss: 1.38831e-06\n",
      "Epoch: 807000, elapsed: 1.14e+01, train loss: 1.42168e-06, val loss: 2.36877e-06, min loss: 1.38831e-06\n",
      "Epoch: 807100, elapsed: 1.13e+01, train loss: 1.40167e-06, val loss: 2.35279e-06, min loss: 1.38831e-06\n",
      "Epoch: 807200, elapsed: 1.14e+01, train loss: 1.38762e-06, val loss: 2.31460e-06, min loss: 1.38762e-06\n",
      "Epoch: 807300, elapsed: 1.13e+01, train loss: 1.44668e-06, val loss: 2.33501e-06, min loss: 1.38762e-06\n",
      "Epoch: 807400, elapsed: 1.13e+01, train loss: 1.57259e-06, val loss: 2.64198e-06, min loss: 1.38762e-06\n",
      "Epoch: 807500, elapsed: 1.12e+01, train loss: 2.86425e-06, val loss: 3.42125e-06, min loss: 1.38762e-06\n",
      "Epoch: 807600, elapsed: 1.12e+01, train loss: 1.72538e-06, val loss: 2.68157e-06, min loss: 1.38762e-06\n",
      "Epoch: 807700, elapsed: 1.12e+01, train loss: 1.60086e-06, val loss: 2.70451e-06, min loss: 1.38762e-06\n",
      "Epoch: 807800, elapsed: 1.13e+01, train loss: 6.70765e-06, val loss: 7.54936e-06, min loss: 1.38762e-06\n",
      "Epoch: 807900, elapsed: 1.14e+01, train loss: 1.40987e-06, val loss: 2.29814e-06, min loss: 1.38762e-06\n",
      "Epoch: 808000, elapsed: 1.11e+01, train loss: 1.38684e-06, val loss: 2.30328e-06, min loss: 1.38684e-06\n",
      "Epoch: 808100, elapsed: 1.34e+01, train loss: 1.38938e-06, val loss: 2.30456e-06, min loss: 1.38684e-06\n",
      "Epoch: 808200, elapsed: 1.15e+01, train loss: 1.44083e-06, val loss: 2.33317e-06, min loss: 1.38684e-06\n",
      "Epoch: 808300, elapsed: 1.15e+01, train loss: 3.70101e-06, val loss: 4.30783e-06, min loss: 1.38684e-06\n",
      "Epoch: 808400, elapsed: 1.13e+01, train loss: 1.48631e-06, val loss: 2.38548e-06, min loss: 1.38684e-06\n",
      "Epoch: 808500, elapsed: 1.14e+01, train loss: 1.49781e-06, val loss: 2.43140e-06, min loss: 1.38684e-06\n",
      "Epoch: 808600, elapsed: 1.14e+01, train loss: 3.91265e-06, val loss: 5.08366e-06, min loss: 1.38684e-06\n",
      "Epoch: 808700, elapsed: 1.15e+01, train loss: 1.93446e-06, val loss: 2.95825e-06, min loss: 1.38684e-06\n",
      "Epoch: 808800, elapsed: 1.15e+01, train loss: 1.49849e-06, val loss: 2.31681e-06, min loss: 1.38684e-06\n",
      "Epoch: 808900, elapsed: 1.14e+01, train loss: 2.31858e-06, val loss: 2.95878e-06, min loss: 1.38684e-06\n",
      "Epoch: 809000, elapsed: 1.15e+01, train loss: 2.30330e-06, val loss: 3.55829e-06, min loss: 1.38684e-06\n",
      "Epoch: 809100, elapsed: 1.14e+01, train loss: 6.31506e-06, val loss: 7.24149e-06, min loss: 1.38684e-06\n",
      "Epoch: 809200, elapsed: 1.15e+01, train loss: 1.42347e-06, val loss: 2.36554e-06, min loss: 1.38684e-06\n",
      "Epoch: 809300, elapsed: 1.14e+01, train loss: 1.48651e-06, val loss: 2.36711e-06, min loss: 1.38684e-06\n",
      "Epoch: 809400, elapsed: 1.13e+01, train loss: 1.68454e-06, val loss: 2.83936e-06, min loss: 1.38684e-06\n",
      "Epoch: 809500, elapsed: 1.13e+01, train loss: 4.21860e-06, val loss: 5.54323e-06, min loss: 1.38684e-06\n",
      "Epoch: 809600, elapsed: 1.12e+01, train loss: 1.71487e-06, val loss: 2.62553e-06, min loss: 1.38684e-06\n",
      "Epoch: 809700, elapsed: 1.38e+01, train loss: 1.55698e-06, val loss: 2.40258e-06, min loss: 1.38684e-06\n",
      "Epoch: 809800, elapsed: 1.15e+01, train loss: 1.39353e-06, val loss: 2.33875e-06, min loss: 1.38684e-06\n",
      "Epoch: 809900, elapsed: 1.13e+01, train loss: 1.38088e-06, val loss: 2.29925e-06, min loss: 1.38088e-06\n",
      "Epoch: 810000, elapsed: 1.15e+01, train loss: 1.40920e-06, val loss: 2.30849e-06, min loss: 1.38088e-06\n",
      "Epoch: 810100, elapsed: 1.34e+01, train loss: 1.38030e-06, val loss: 2.29622e-06, min loss: 1.38030e-06\n",
      "Epoch: 810200, elapsed: 1.15e+01, train loss: 1.39469e-06, val loss: 2.29530e-06, min loss: 1.38030e-06\n",
      "Epoch: 810300, elapsed: 1.12e+01, train loss: 1.38004e-06, val loss: 2.29699e-06, min loss: 1.38004e-06\n",
      "Epoch: 810400, elapsed: 1.14e+01, train loss: 1.38280e-06, val loss: 2.28893e-06, min loss: 1.38004e-06\n",
      "Epoch: 810500, elapsed: 1.15e+01, train loss: 1.47338e-06, val loss: 2.71231e-06, min loss: 1.38004e-06\n",
      "Epoch: 810600, elapsed: 1.15e+01, train loss: 1.37968e-06, val loss: 2.29765e-06, min loss: 1.37968e-06\n",
      "Epoch: 810700, elapsed: 1.14e+01, train loss: 1.37978e-06, val loss: 2.29979e-06, min loss: 1.37968e-06\n",
      "Epoch: 810800, elapsed: 1.14e+01, train loss: 1.38420e-06, val loss: 2.28860e-06, min loss: 1.37968e-06\n",
      "Epoch: 810900, elapsed: 1.15e+01, train loss: 1.64071e-06, val loss: 2.56987e-06, min loss: 1.37968e-06\n",
      "Epoch: 811000, elapsed: 1.14e+01, train loss: 5.90996e-06, val loss: 4.39202e-06, min loss: 1.37968e-06\n",
      "Epoch: 811100, elapsed: 1.15e+01, train loss: 1.37838e-06, val loss: 2.29480e-06, min loss: 1.37838e-06\n",
      "Epoch: 811200, elapsed: 1.36e+01, train loss: 1.41980e-06, val loss: 2.37344e-06, min loss: 1.37838e-06\n",
      "Epoch: 811300, elapsed: 1.17e+01, train loss: 1.38783e-06, val loss: 2.26411e-06, min loss: 1.37838e-06\n",
      "Epoch: 811400, elapsed: 1.14e+01, train loss: 1.37861e-06, val loss: 2.29870e-06, min loss: 1.37838e-06\n",
      "Epoch: 811500, elapsed: 1.14e+01, train loss: 1.54354e-06, val loss: 2.34833e-06, min loss: 1.37838e-06\n",
      "Epoch: 811600, elapsed: 1.13e+01, train loss: 4.97652e-06, val loss: 5.11644e-06, min loss: 1.37838e-06\n",
      "Epoch: 811700, elapsed: 1.14e+01, train loss: 1.58643e-06, val loss: 2.67427e-06, min loss: 1.37838e-06\n",
      "Epoch: 811800, elapsed: 1.13e+01, train loss: 1.38369e-06, val loss: 2.27032e-06, min loss: 1.37838e-06\n",
      "Epoch: 811900, elapsed: 1.15e+01, train loss: 1.77671e-06, val loss: 2.66302e-06, min loss: 1.37838e-06\n",
      "Epoch: 812000, elapsed: 1.14e+01, train loss: 2.71758e-06, val loss: 3.97615e-06, min loss: 1.37838e-06\n",
      "Epoch: 812100, elapsed: 1.14e+01, train loss: 1.38760e-06, val loss: 2.34181e-06, min loss: 1.37838e-06\n",
      "Epoch: 812200, elapsed: 1.16e+01, train loss: 2.36840e-06, val loss: 3.69681e-06, min loss: 1.37838e-06\n",
      "Epoch: 812300, elapsed: 1.16e+01, train loss: 1.41948e-06, val loss: 2.32871e-06, min loss: 1.37838e-06\n",
      "Epoch: 812400, elapsed: 1.14e+01, train loss: 1.38010e-06, val loss: 2.30224e-06, min loss: 1.37838e-06\n",
      "Epoch: 812500, elapsed: 1.16e+01, train loss: 1.38289e-06, val loss: 2.28902e-06, min loss: 1.37838e-06\n",
      "Epoch: 812600, elapsed: 1.16e+01, train loss: 1.98753e-06, val loss: 2.95233e-06, min loss: 1.37838e-06\n",
      "Epoch: 812700, elapsed: 1.13e+01, train loss: 1.66035e-06, val loss: 2.58298e-06, min loss: 1.37838e-06\n",
      "Epoch: 812800, elapsed: 1.39e+01, train loss: 1.92438e-06, val loss: 2.86444e-06, min loss: 1.37838e-06\n",
      "Epoch: 812900, elapsed: 1.16e+01, train loss: 1.58676e-06, val loss: 2.48943e-06, min loss: 1.37838e-06\n",
      "Epoch: 813000, elapsed: 1.13e+01, train loss: 1.52386e-06, val loss: 2.44759e-06, min loss: 1.37838e-06\n",
      "Epoch: 813100, elapsed: 1.14e+01, train loss: 1.45949e-06, val loss: 2.44613e-06, min loss: 1.37838e-06\n",
      "Epoch: 813200, elapsed: 1.13e+01, train loss: 1.53496e-06, val loss: 2.50778e-06, min loss: 1.37838e-06\n",
      "Epoch: 813300, elapsed: 1.13e+01, train loss: 1.38646e-06, val loss: 2.31241e-06, min loss: 1.37838e-06\n",
      "Epoch: 813400, elapsed: 1.14e+01, train loss: 1.50018e-06, val loss: 2.27203e-06, min loss: 1.37838e-06\n",
      "Epoch: 813500, elapsed: 1.13e+01, train loss: 2.12393e-06, val loss: 3.46610e-06, min loss: 1.37838e-06\n",
      "Epoch: 813600, elapsed: 1.13e+01, train loss: 1.37430e-06, val loss: 2.28059e-06, min loss: 1.37430e-06\n",
      "Epoch: 813700, elapsed: 1.14e+01, train loss: 1.38551e-06, val loss: 2.28005e-06, min loss: 1.37430e-06\n",
      "Epoch: 813800, elapsed: 1.13e+01, train loss: 1.42608e-06, val loss: 2.29226e-06, min loss: 1.37430e-06\n",
      "Epoch: 813900, elapsed: 1.12e+01, train loss: 2.61849e-06, val loss: 3.55591e-06, min loss: 1.37430e-06\n",
      "Epoch: 814000, elapsed: 1.12e+01, train loss: 2.61073e-06, val loss: 4.03285e-06, min loss: 1.37430e-06\n",
      "Epoch: 814100, elapsed: 1.13e+01, train loss: 1.40169e-06, val loss: 2.35154e-06, min loss: 1.37430e-06\n",
      "Epoch: 814200, elapsed: 1.13e+01, train loss: 1.38714e-06, val loss: 2.29547e-06, min loss: 1.37430e-06\n",
      "Epoch: 814300, elapsed: 1.37e+01, train loss: 1.37379e-06, val loss: 2.29752e-06, min loss: 1.37379e-06\n",
      "Epoch: 814400, elapsed: 1.15e+01, train loss: 1.37441e-06, val loss: 2.29136e-06, min loss: 1.37379e-06\n",
      "Epoch: 814500, elapsed: 1.14e+01, train loss: 1.39617e-06, val loss: 2.27652e-06, min loss: 1.37379e-06\n",
      "Epoch: 814600, elapsed: 1.14e+01, train loss: 1.65487e-06, val loss: 2.42275e-06, min loss: 1.37379e-06\n",
      "Epoch: 814700, elapsed: 1.14e+01, train loss: 1.47089e-06, val loss: 2.75434e-06, min loss: 1.37379e-06\n",
      "Epoch: 814800, elapsed: 1.12e+01, train loss: 1.37070e-06, val loss: 2.28284e-06, min loss: 1.37070e-06\n",
      "Epoch: 814900, elapsed: 1.12e+01, train loss: 1.43570e-06, val loss: 2.35303e-06, min loss: 1.37070e-06\n",
      "Epoch: 815000, elapsed: 1.12e+01, train loss: 1.51731e-06, val loss: 2.45526e-06, min loss: 1.37070e-06\n",
      "Epoch: 815100, elapsed: 1.36e+01, train loss: 1.97553e-06, val loss: 2.51158e-06, min loss: 1.37070e-06\n",
      "Epoch: 815200, elapsed: 1.13e+01, train loss: 1.37809e-06, val loss: 2.27428e-06, min loss: 1.37070e-06\n",
      "Epoch: 815300, elapsed: 1.12e+01, train loss: 1.37362e-06, val loss: 2.27876e-06, min loss: 1.37070e-06\n",
      "Epoch: 815400, elapsed: 1.14e+01, train loss: 1.99772e-06, val loss: 2.78933e-06, min loss: 1.37070e-06\n",
      "Epoch: 815500, elapsed: 1.14e+01, train loss: 1.60826e-06, val loss: 2.60255e-06, min loss: 1.37070e-06\n",
      "Epoch: 815600, elapsed: 1.14e+01, train loss: 1.36908e-06, val loss: 2.27885e-06, min loss: 1.36908e-06\n",
      "Epoch: 815700, elapsed: 1.14e+01, train loss: 1.38352e-06, val loss: 2.27103e-06, min loss: 1.36908e-06\n",
      "Epoch: 815800, elapsed: 1.15e+01, train loss: 1.40777e-06, val loss: 2.29053e-06, min loss: 1.36908e-06\n",
      "Epoch: 815900, elapsed: 1.38e+01, train loss: 2.17479e-06, val loss: 2.95589e-06, min loss: 1.36908e-06\n",
      "Epoch: 816000, elapsed: 1.16e+01, train loss: 2.54814e-06, val loss: 3.30826e-06, min loss: 1.36908e-06\n",
      "Epoch: 816100, elapsed: 1.16e+01, train loss: 1.38236e-06, val loss: 2.29466e-06, min loss: 1.36908e-06\n",
      "Epoch: 816200, elapsed: 1.15e+01, train loss: 1.39096e-06, val loss: 2.31792e-06, min loss: 1.36908e-06\n",
      "Epoch: 816300, elapsed: 1.16e+01, train loss: 1.37695e-06, val loss: 2.30114e-06, min loss: 1.36908e-06\n",
      "Epoch: 816400, elapsed: 1.14e+01, train loss: 1.37141e-06, val loss: 2.26877e-06, min loss: 1.36908e-06\n",
      "Epoch: 816500, elapsed: 1.14e+01, train loss: 4.08254e-06, val loss: 4.53381e-06, min loss: 1.36908e-06\n",
      "Epoch: 816600, elapsed: 1.13e+01, train loss: 3.11240e-06, val loss: 3.96424e-06, min loss: 1.36908e-06\n",
      "Epoch: 816700, elapsed: 1.15e+01, train loss: 1.54257e-06, val loss: 2.43159e-06, min loss: 1.36908e-06\n",
      "Epoch: 816800, elapsed: 1.14e+01, train loss: 2.75255e-06, val loss: 3.99281e-06, min loss: 1.36908e-06\n",
      "Epoch: 816900, elapsed: 1.14e+01, train loss: 1.53548e-06, val loss: 2.48553e-06, min loss: 1.36908e-06\n",
      "Epoch: 817000, elapsed: 1.14e+01, train loss: 1.80951e-06, val loss: 2.59980e-06, min loss: 1.36908e-06\n",
      "Epoch: 817100, elapsed: 1.15e+01, train loss: 1.37115e-06, val loss: 2.28310e-06, min loss: 1.36908e-06\n",
      "Epoch: 817200, elapsed: 1.15e+01, train loss: 1.38828e-06, val loss: 2.29733e-06, min loss: 1.36908e-06\n",
      "Epoch: 817300, elapsed: 1.14e+01, train loss: 2.22640e-06, val loss: 2.99699e-06, min loss: 1.36908e-06\n",
      "Epoch: 817400, elapsed: 1.37e+01, train loss: 1.70712e-06, val loss: 2.47309e-06, min loss: 1.36908e-06\n",
      "Epoch: 817500, elapsed: 1.15e+01, train loss: 1.38142e-06, val loss: 2.36723e-06, min loss: 1.36908e-06\n",
      "Epoch: 817600, elapsed: 1.17e+01, train loss: 2.92102e-06, val loss: 3.20819e-06, min loss: 1.36908e-06\n",
      "Epoch: 817700, elapsed: 1.15e+01, train loss: 2.12655e-06, val loss: 3.46910e-06, min loss: 1.36908e-06\n",
      "Epoch: 817800, elapsed: 1.15e+01, train loss: 1.85334e-06, val loss: 2.62882e-06, min loss: 1.36908e-06\n",
      "Epoch: 817900, elapsed: 1.12e+01, train loss: 1.38002e-06, val loss: 2.29400e-06, min loss: 1.36908e-06\n",
      "Epoch: 818000, elapsed: 1.14e+01, train loss: 1.46643e-06, val loss: 2.26939e-06, min loss: 1.36908e-06\n",
      "Epoch: 818100, elapsed: 1.14e+01, train loss: 1.57612e-06, val loss: 2.32723e-06, min loss: 1.36908e-06\n",
      "Epoch: 818200, elapsed: 1.13e+01, train loss: 1.82170e-06, val loss: 2.42366e-06, min loss: 1.36908e-06\n",
      "Epoch: 818300, elapsed: 1.13e+01, train loss: 4.94609e-06, val loss: 6.35117e-06, min loss: 1.36908e-06\n",
      "Epoch: 818400, elapsed: 1.16e+01, train loss: 1.48452e-06, val loss: 2.47895e-06, min loss: 1.36908e-06\n",
      "Epoch: 818500, elapsed: 1.14e+01, train loss: 1.48206e-06, val loss: 2.37339e-06, min loss: 1.36908e-06\n",
      "Epoch: 818600, elapsed: 1.14e+01, train loss: 1.99269e-06, val loss: 3.12030e-06, min loss: 1.36908e-06\n",
      "Epoch: 818700, elapsed: 1.14e+01, train loss: 3.42212e-06, val loss: 4.77893e-06, min loss: 1.36908e-06\n",
      "Epoch: 818800, elapsed: 1.13e+01, train loss: 2.11254e-06, val loss: 3.04106e-06, min loss: 1.36908e-06\n",
      "Epoch: 818900, elapsed: 1.13e+01, train loss: 1.42623e-06, val loss: 2.25435e-06, min loss: 1.36908e-06\n",
      "Epoch: 819000, elapsed: 1.38e+01, train loss: 1.68906e-06, val loss: 2.80034e-06, min loss: 1.36908e-06\n",
      "Epoch: 819100, elapsed: 1.15e+01, train loss: 1.36115e-06, val loss: 2.26415e-06, min loss: 1.36115e-06\n",
      "Epoch: 819200, elapsed: 1.13e+01, train loss: 1.36574e-06, val loss: 2.25945e-06, min loss: 1.36115e-06\n",
      "Epoch: 819300, elapsed: 1.14e+01, train loss: 1.45546e-06, val loss: 2.61611e-06, min loss: 1.36115e-06\n",
      "Epoch: 819400, elapsed: 1.16e+01, train loss: 1.86275e-06, val loss: 2.93295e-06, min loss: 1.36115e-06\n",
      "Epoch: 819500, elapsed: 1.15e+01, train loss: 1.36349e-06, val loss: 2.27064e-06, min loss: 1.36115e-06\n",
      "Epoch: 819600, elapsed: 1.15e+01, train loss: 1.38667e-06, val loss: 2.32870e-06, min loss: 1.36115e-06\n",
      "Epoch: 819700, elapsed: 1.13e+01, train loss: 5.26226e-06, val loss: 4.33210e-06, min loss: 1.36115e-06\n",
      "Epoch: 819800, elapsed: 1.13e+01, train loss: 1.36814e-06, val loss: 2.24963e-06, min loss: 1.36115e-06\n",
      "Epoch: 819900, elapsed: 1.14e+01, train loss: 1.36157e-06, val loss: 2.26098e-06, min loss: 1.36115e-06\n",
      "Epoch: 820000, elapsed: 1.13e+01, train loss: 1.36672e-06, val loss: 2.29710e-06, min loss: 1.36115e-06\n",
      "Epoch: 820100, elapsed: 1.34e+01, train loss: 1.64736e-06, val loss: 2.54050e-06, min loss: 1.36115e-06\n",
      "Epoch: 820200, elapsed: 1.12e+01, train loss: 1.70387e-06, val loss: 2.57899e-06, min loss: 1.36115e-06\n",
      "Epoch: 820300, elapsed: 1.14e+01, train loss: 1.61151e-06, val loss: 2.57519e-06, min loss: 1.36115e-06\n",
      "Epoch: 820400, elapsed: 1.14e+01, train loss: 1.38348e-06, val loss: 2.24917e-06, min loss: 1.36115e-06\n",
      "Epoch: 820500, elapsed: 1.35e+01, train loss: 1.79037e-06, val loss: 2.88043e-06, min loss: 1.36115e-06\n",
      "Epoch: 820600, elapsed: 1.17e+01, train loss: 4.04493e-06, val loss: 5.15028e-06, min loss: 1.36115e-06\n",
      "Epoch: 820700, elapsed: 1.15e+01, train loss: 1.56711e-06, val loss: 2.42482e-06, min loss: 1.36115e-06\n",
      "Epoch: 820800, elapsed: 1.16e+01, train loss: 1.41227e-06, val loss: 2.26288e-06, min loss: 1.36115e-06\n",
      "Epoch: 820900, elapsed: 1.14e+01, train loss: 1.37925e-06, val loss: 2.27453e-06, min loss: 1.36115e-06\n",
      "Epoch: 821000, elapsed: 1.15e+01, train loss: 1.42038e-06, val loss: 2.29555e-06, min loss: 1.36115e-06\n",
      "Epoch: 821100, elapsed: 1.15e+01, train loss: 2.32164e-06, val loss: 2.74238e-06, min loss: 1.36115e-06\n",
      "Epoch: 821200, elapsed: 1.14e+01, train loss: 2.07030e-06, val loss: 2.70095e-06, min loss: 1.36115e-06\n",
      "Epoch: 821300, elapsed: 1.13e+01, train loss: 1.36182e-06, val loss: 2.24341e-06, min loss: 1.36115e-06\n",
      "Epoch: 821400, elapsed: 1.15e+01, train loss: 2.55612e-06, val loss: 3.72509e-06, min loss: 1.36115e-06\n",
      "Epoch: 821500, elapsed: 1.14e+01, train loss: 1.70475e-06, val loss: 2.38551e-06, min loss: 1.36115e-06\n",
      "Epoch: 821600, elapsed: 1.13e+01, train loss: 1.41662e-06, val loss: 2.35691e-06, min loss: 1.36115e-06\n",
      "Epoch: 821700, elapsed: 1.13e+01, train loss: 1.42452e-06, val loss: 2.34944e-06, min loss: 1.36115e-06\n",
      "Epoch: 821800, elapsed: 1.15e+01, train loss: 1.46670e-06, val loss: 2.37766e-06, min loss: 1.36115e-06\n",
      "Epoch: 821900, elapsed: 1.14e+01, train loss: 1.37177e-06, val loss: 2.30715e-06, min loss: 1.36115e-06\n",
      "Epoch: 822000, elapsed: 1.13e+01, train loss: 1.35964e-06, val loss: 2.27564e-06, min loss: 1.35964e-06\n",
      "Epoch: 822100, elapsed: 1.35e+01, train loss: 1.36743e-06, val loss: 2.26121e-06, min loss: 1.35964e-06\n",
      "Epoch: 822200, elapsed: 1.12e+01, train loss: 1.66318e-06, val loss: 2.70104e-06, min loss: 1.35964e-06\n",
      "Epoch: 822300, elapsed: 1.11e+01, train loss: 1.43632e-06, val loss: 2.32878e-06, min loss: 1.35964e-06\n",
      "Epoch: 822400, elapsed: 1.14e+01, train loss: 4.81120e-06, val loss: 4.90864e-06, min loss: 1.35964e-06\n",
      "Epoch: 822500, elapsed: 1.12e+01, train loss: 4.18466e-06, val loss: 5.87697e-06, min loss: 1.35964e-06\n",
      "Epoch: 822600, elapsed: 1.12e+01, train loss: 1.55577e-06, val loss: 2.50411e-06, min loss: 1.35964e-06\n",
      "Epoch: 822700, elapsed: 1.11e+01, train loss: 1.38512e-06, val loss: 2.25187e-06, min loss: 1.35964e-06\n",
      "Epoch: 822800, elapsed: 1.11e+01, train loss: 1.39470e-06, val loss: 2.35688e-06, min loss: 1.35964e-06\n",
      "Epoch: 822900, elapsed: 1.14e+01, train loss: 1.73119e-06, val loss: 2.67312e-06, min loss: 1.35964e-06\n",
      "Epoch: 823000, elapsed: 1.12e+01, train loss: 1.35288e-06, val loss: 2.25065e-06, min loss: 1.35288e-06\n",
      "Epoch: 823100, elapsed: 1.14e+01, train loss: 1.40224e-06, val loss: 2.27664e-06, min loss: 1.35288e-06\n",
      "Epoch: 823200, elapsed: 1.13e+01, train loss: 1.42222e-06, val loss: 2.28310e-06, min loss: 1.35288e-06\n",
      "Epoch: 823300, elapsed: 1.12e+01, train loss: 1.41990e-06, val loss: 2.44671e-06, min loss: 1.35288e-06\n",
      "Epoch: 823400, elapsed: 1.13e+01, train loss: 1.35274e-06, val loss: 2.25097e-06, min loss: 1.35274e-06\n",
      "Epoch: 823500, elapsed: 1.12e+01, train loss: 1.36831e-06, val loss: 2.25557e-06, min loss: 1.35274e-06\n",
      "Epoch: 823600, elapsed: 1.12e+01, train loss: 1.43411e-06, val loss: 2.32061e-06, min loss: 1.35274e-06\n",
      "Epoch: 823700, elapsed: 1.36e+01, train loss: 1.40440e-06, val loss: 2.28358e-06, min loss: 1.35274e-06\n",
      "Epoch: 823800, elapsed: 1.12e+01, train loss: 3.19648e-06, val loss: 3.10552e-06, min loss: 1.35274e-06\n",
      "Epoch: 823900, elapsed: 1.15e+01, train loss: 1.35458e-06, val loss: 2.27467e-06, min loss: 1.35274e-06\n",
      "Epoch: 824000, elapsed: 1.13e+01, train loss: 1.35111e-06, val loss: 2.24276e-06, min loss: 1.35111e-06\n",
      "Epoch: 824100, elapsed: 1.12e+01, train loss: 1.97827e-06, val loss: 2.59165e-06, min loss: 1.35111e-06\n",
      "Epoch: 824200, elapsed: 1.13e+01, train loss: 1.35542e-06, val loss: 2.28420e-06, min loss: 1.35111e-06\n",
      "Epoch: 824300, elapsed: 1.13e+01, train loss: 1.35153e-06, val loss: 2.25658e-06, min loss: 1.35111e-06\n",
      "Epoch: 824400, elapsed: 1.13e+01, train loss: 1.36341e-06, val loss: 2.26176e-06, min loss: 1.35111e-06\n",
      "Epoch: 824500, elapsed: 1.12e+01, train loss: 1.51089e-06, val loss: 2.40609e-06, min loss: 1.35111e-06\n",
      "Epoch: 824600, elapsed: 1.13e+01, train loss: 1.73948e-06, val loss: 2.56527e-06, min loss: 1.35111e-06\n",
      "Epoch: 824700, elapsed: 1.12e+01, train loss: 1.35570e-06, val loss: 2.23849e-06, min loss: 1.35111e-06\n",
      "Epoch: 824800, elapsed: 1.13e+01, train loss: 1.38209e-06, val loss: 2.28743e-06, min loss: 1.35111e-06\n",
      "Epoch: 824900, elapsed: 1.13e+01, train loss: 1.51048e-06, val loss: 2.29492e-06, min loss: 1.35111e-06\n",
      "Epoch: 825000, elapsed: 1.13e+01, train loss: 1.68714e-06, val loss: 2.46234e-06, min loss: 1.35111e-06\n",
      "Epoch: 825100, elapsed: 1.33e+01, train loss: 4.27337e-06, val loss: 6.53419e-06, min loss: 1.35111e-06\n",
      "Epoch: 825200, elapsed: 1.37e+01, train loss: 1.34759e-06, val loss: 2.24479e-06, min loss: 1.34759e-06\n",
      "Epoch: 825300, elapsed: 1.15e+01, train loss: 1.42181e-06, val loss: 2.36097e-06, min loss: 1.34759e-06\n",
      "Epoch: 825400, elapsed: 1.13e+01, train loss: 1.49539e-06, val loss: 2.63872e-06, min loss: 1.34759e-06\n",
      "Epoch: 825500, elapsed: 1.13e+01, train loss: 1.35093e-06, val loss: 2.25046e-06, min loss: 1.34759e-06\n",
      "Epoch: 825600, elapsed: 1.13e+01, train loss: 1.35029e-06, val loss: 2.23630e-06, min loss: 1.34759e-06\n",
      "Epoch: 825700, elapsed: 1.12e+01, train loss: 1.54215e-06, val loss: 2.32852e-06, min loss: 1.34759e-06\n",
      "Epoch: 825800, elapsed: 1.12e+01, train loss: 1.36933e-06, val loss: 2.24639e-06, min loss: 1.34759e-06\n",
      "Epoch: 825900, elapsed: 1.13e+01, train loss: 1.34801e-06, val loss: 2.24125e-06, min loss: 1.34759e-06\n",
      "Epoch: 826000, elapsed: 1.14e+01, train loss: 1.40487e-06, val loss: 2.26207e-06, min loss: 1.34759e-06\n",
      "Epoch: 826100, elapsed: 1.14e+01, train loss: 1.55755e-06, val loss: 2.29063e-06, min loss: 1.34759e-06\n",
      "Epoch: 826200, elapsed: 1.13e+01, train loss: 1.79204e-06, val loss: 3.00178e-06, min loss: 1.34759e-06\n",
      "Epoch: 826300, elapsed: 1.11e+01, train loss: 1.36383e-06, val loss: 2.28417e-06, min loss: 1.34759e-06\n",
      "Epoch: 826400, elapsed: 1.13e+01, train loss: 1.35268e-06, val loss: 2.22422e-06, min loss: 1.34759e-06\n",
      "Epoch: 826500, elapsed: 1.12e+01, train loss: 1.35243e-06, val loss: 2.25114e-06, min loss: 1.34759e-06\n",
      "Epoch: 826600, elapsed: 1.12e+01, train loss: 1.35815e-06, val loss: 2.26737e-06, min loss: 1.34759e-06\n",
      "Epoch: 826700, elapsed: 1.12e+01, train loss: 1.53274e-06, val loss: 2.42447e-06, min loss: 1.34759e-06\n",
      "Epoch: 826800, elapsed: 1.38e+01, train loss: 2.11776e-06, val loss: 2.71936e-06, min loss: 1.34759e-06\n",
      "Epoch: 826900, elapsed: 1.15e+01, train loss: 2.64074e-06, val loss: 3.29659e-06, min loss: 1.34759e-06\n",
      "Epoch: 827000, elapsed: 1.14e+01, train loss: 2.13565e-06, val loss: 2.92750e-06, min loss: 1.34759e-06\n",
      "Epoch: 827100, elapsed: 1.13e+01, train loss: 1.38227e-06, val loss: 2.26531e-06, min loss: 1.34759e-06\n",
      "Epoch: 827200, elapsed: 1.13e+01, train loss: 1.45135e-06, val loss: 2.29051e-06, min loss: 1.34759e-06\n",
      "Epoch: 827300, elapsed: 1.13e+01, train loss: 1.41544e-06, val loss: 2.35275e-06, min loss: 1.34759e-06\n",
      "Epoch: 827400, elapsed: 1.13e+01, train loss: 1.38990e-06, val loss: 2.28789e-06, min loss: 1.34759e-06\n",
      "Epoch: 827500, elapsed: 1.13e+01, train loss: 1.90140e-06, val loss: 3.12292e-06, min loss: 1.34759e-06\n",
      "Epoch: 827600, elapsed: 1.11e+01, train loss: 1.34246e-06, val loss: 2.23796e-06, min loss: 1.34246e-06\n",
      "Epoch: 827700, elapsed: 1.14e+01, train loss: 1.39184e-06, val loss: 2.25554e-06, min loss: 1.34246e-06\n",
      "Epoch: 827800, elapsed: 1.13e+01, train loss: 1.74336e-06, val loss: 2.83753e-06, min loss: 1.34246e-06\n",
      "Epoch: 827900, elapsed: 1.12e+01, train loss: 1.60695e-06, val loss: 2.38862e-06, min loss: 1.34246e-06\n",
      "Epoch: 828000, elapsed: 1.13e+01, train loss: 2.33937e-06, val loss: 3.33984e-06, min loss: 1.34246e-06\n",
      "Epoch: 828100, elapsed: 1.13e+01, train loss: 2.00197e-06, val loss: 2.87046e-06, min loss: 1.34246e-06\n",
      "Epoch: 828200, elapsed: 1.14e+01, train loss: 3.84511e-06, val loss: 4.29498e-06, min loss: 1.34246e-06\n",
      "Epoch: 828300, elapsed: 1.12e+01, train loss: 1.35772e-06, val loss: 2.22329e-06, min loss: 1.34246e-06\n",
      "Epoch: 828400, elapsed: 1.37e+01, train loss: 1.37312e-06, val loss: 2.24498e-06, min loss: 1.34246e-06\n",
      "Epoch: 828500, elapsed: 1.14e+01, train loss: 1.94243e-06, val loss: 2.84216e-06, min loss: 1.34246e-06\n",
      "Epoch: 828600, elapsed: 1.14e+01, train loss: 1.52916e-06, val loss: 2.36842e-06, min loss: 1.34246e-06\n",
      "Epoch: 828700, elapsed: 1.13e+01, train loss: 1.34179e-06, val loss: 2.21890e-06, min loss: 1.34179e-06\n",
      "Epoch: 828800, elapsed: 1.14e+01, train loss: 1.34614e-06, val loss: 2.24275e-06, min loss: 1.34179e-06\n",
      "Epoch: 828900, elapsed: 1.13e+01, train loss: 1.37984e-06, val loss: 2.24072e-06, min loss: 1.34179e-06\n",
      "Epoch: 829000, elapsed: 1.13e+01, train loss: 3.41815e-06, val loss: 4.45465e-06, min loss: 1.34179e-06\n",
      "Epoch: 829100, elapsed: 1.12e+01, train loss: 1.68025e-06, val loss: 2.71943e-06, min loss: 1.34179e-06\n",
      "Epoch: 829200, elapsed: 1.14e+01, train loss: 2.02989e-06, val loss: 2.70732e-06, min loss: 1.34179e-06\n",
      "Epoch: 829300, elapsed: 1.12e+01, train loss: 1.69993e-06, val loss: 2.43428e-06, min loss: 1.34179e-06\n",
      "Epoch: 829400, elapsed: 1.13e+01, train loss: 1.84819e-06, val loss: 2.71003e-06, min loss: 1.34179e-06\n",
      "Epoch: 829500, elapsed: 1.13e+01, train loss: 1.43332e-06, val loss: 2.28297e-06, min loss: 1.34179e-06\n",
      "Epoch: 829600, elapsed: 1.13e+01, train loss: 1.33892e-06, val loss: 2.22126e-06, min loss: 1.33892e-06\n",
      "Epoch: 829700, elapsed: 1.15e+01, train loss: 1.48898e-06, val loss: 2.48332e-06, min loss: 1.33892e-06\n",
      "Epoch: 829800, elapsed: 1.13e+01, train loss: 2.07883e-06, val loss: 2.99832e-06, min loss: 1.33892e-06\n",
      "Epoch: 829900, elapsed: 1.14e+01, train loss: 3.73309e-06, val loss: 3.61170e-06, min loss: 1.33892e-06\n",
      "Epoch: 830000, elapsed: 1.35e+01, train loss: 1.70507e-06, val loss: 2.71854e-06, min loss: 1.33892e-06\n",
      "Epoch: 830100, elapsed: 1.36e+01, train loss: 1.43065e-06, val loss: 2.26005e-06, min loss: 1.33892e-06\n",
      "Epoch: 830200, elapsed: 1.15e+01, train loss: 1.48662e-06, val loss: 2.28681e-06, min loss: 1.33892e-06\n",
      "Epoch: 830300, elapsed: 1.14e+01, train loss: 1.35315e-06, val loss: 2.24697e-06, min loss: 1.33892e-06\n",
      "Epoch: 830400, elapsed: 1.15e+01, train loss: 1.34002e-06, val loss: 2.22786e-06, min loss: 1.33892e-06\n",
      "Epoch: 830500, elapsed: 1.15e+01, train loss: 1.34710e-06, val loss: 2.23649e-06, min loss: 1.33892e-06\n",
      "Epoch: 830600, elapsed: 1.15e+01, train loss: 1.35120e-06, val loss: 2.27431e-06, min loss: 1.33892e-06\n",
      "Epoch: 830700, elapsed: 1.13e+01, train loss: 1.35656e-06, val loss: 2.23449e-06, min loss: 1.33892e-06\n",
      "Epoch: 830800, elapsed: 1.13e+01, train loss: 2.10630e-06, val loss: 2.43674e-06, min loss: 1.33892e-06\n",
      "Epoch: 830900, elapsed: 1.13e+01, train loss: 2.47175e-06, val loss: 3.48073e-06, min loss: 1.33892e-06\n",
      "Epoch: 831000, elapsed: 1.11e+01, train loss: 1.52068e-06, val loss: 2.39586e-06, min loss: 1.33892e-06\n",
      "Epoch: 831100, elapsed: 1.12e+01, train loss: 1.34349e-06, val loss: 2.22260e-06, min loss: 1.33892e-06\n",
      "Epoch: 831200, elapsed: 1.11e+01, train loss: 1.33691e-06, val loss: 2.21904e-06, min loss: 1.33691e-06\n",
      "Epoch: 831300, elapsed: 1.13e+01, train loss: 3.09165e-06, val loss: 3.11717e-06, min loss: 1.33691e-06\n",
      "Epoch: 831400, elapsed: 1.11e+01, train loss: 1.33294e-06, val loss: 2.22059e-06, min loss: 1.33294e-06\n",
      "Epoch: 831500, elapsed: 1.12e+01, train loss: 1.33973e-06, val loss: 2.21531e-06, min loss: 1.33294e-06\n",
      "Epoch: 831600, elapsed: 1.40e+01, train loss: 1.35316e-06, val loss: 2.24544e-06, min loss: 1.33294e-06\n",
      "Epoch: 831700, elapsed: 1.16e+01, train loss: 1.90986e-06, val loss: 2.44725e-06, min loss: 1.33294e-06\n",
      "Epoch: 831800, elapsed: 1.14e+01, train loss: 2.39406e-06, val loss: 2.64823e-06, min loss: 1.33294e-06\n",
      "Epoch: 831900, elapsed: 1.13e+01, train loss: 3.97007e-06, val loss: 3.64298e-06, min loss: 1.33294e-06\n",
      "Epoch: 832000, elapsed: 1.14e+01, train loss: 1.48921e-06, val loss: 2.39134e-06, min loss: 1.33294e-06\n",
      "Epoch: 832100, elapsed: 1.13e+01, train loss: 1.40177e-06, val loss: 2.29233e-06, min loss: 1.33294e-06\n",
      "Epoch: 832200, elapsed: 1.12e+01, train loss: 1.72941e-06, val loss: 2.36671e-06, min loss: 1.33294e-06\n",
      "Epoch: 832300, elapsed: 1.14e+01, train loss: 1.33087e-06, val loss: 2.21141e-06, min loss: 1.33087e-06\n",
      "Epoch: 832400, elapsed: 1.12e+01, train loss: 1.33829e-06, val loss: 2.22604e-06, min loss: 1.33087e-06\n",
      "Epoch: 832500, elapsed: 1.14e+01, train loss: 2.30574e-06, val loss: 3.22132e-06, min loss: 1.33087e-06\n",
      "Epoch: 832600, elapsed: 1.14e+01, train loss: 1.70099e-06, val loss: 2.61307e-06, min loss: 1.33087e-06\n",
      "Epoch: 832700, elapsed: 1.12e+01, train loss: 1.42023e-06, val loss: 2.30563e-06, min loss: 1.33087e-06\n",
      "Epoch: 832800, elapsed: 1.12e+01, train loss: 1.61592e-06, val loss: 2.44490e-06, min loss: 1.33087e-06\n",
      "Epoch: 832900, elapsed: 1.13e+01, train loss: 1.77381e-06, val loss: 2.94285e-06, min loss: 1.33087e-06\n",
      "Epoch: 833000, elapsed: 1.15e+01, train loss: 1.75388e-06, val loss: 3.07864e-06, min loss: 1.33087e-06\n",
      "Epoch: 833100, elapsed: 1.13e+01, train loss: 1.72833e-06, val loss: 2.74809e-06, min loss: 1.33087e-06\n",
      "Epoch: 833200, elapsed: 1.39e+01, train loss: 1.37198e-06, val loss: 2.23263e-06, min loss: 1.33087e-06\n",
      "Epoch: 833300, elapsed: 1.14e+01, train loss: 1.33345e-06, val loss: 2.20088e-06, min loss: 1.33087e-06\n",
      "Epoch: 833400, elapsed: 1.14e+01, train loss: 1.33734e-06, val loss: 2.25450e-06, min loss: 1.33087e-06\n",
      "Epoch: 833500, elapsed: 1.14e+01, train loss: 1.90590e-06, val loss: 2.78279e-06, min loss: 1.33087e-06\n",
      "Epoch: 833600, elapsed: 1.15e+01, train loss: 1.99338e-06, val loss: 2.72955e-06, min loss: 1.33087e-06\n",
      "Epoch: 833700, elapsed: 1.16e+01, train loss: 1.55163e-06, val loss: 2.40540e-06, min loss: 1.33087e-06\n",
      "Epoch: 833800, elapsed: 1.15e+01, train loss: 1.94028e-06, val loss: 2.75365e-06, min loss: 1.33087e-06\n",
      "Epoch: 833900, elapsed: 1.14e+01, train loss: 1.36459e-06, val loss: 2.25662e-06, min loss: 1.33087e-06\n",
      "Epoch: 834000, elapsed: 1.15e+01, train loss: 1.48828e-06, val loss: 2.46571e-06, min loss: 1.33087e-06\n",
      "Epoch: 834100, elapsed: 1.14e+01, train loss: 2.32444e-06, val loss: 3.63762e-06, min loss: 1.33087e-06\n",
      "Epoch: 834200, elapsed: 1.15e+01, train loss: 1.58124e-06, val loss: 2.54208e-06, min loss: 1.33087e-06\n",
      "Epoch: 834300, elapsed: 1.14e+01, train loss: 3.82317e-06, val loss: 4.24748e-06, min loss: 1.33087e-06\n",
      "Epoch: 834400, elapsed: 1.14e+01, train loss: 2.16348e-06, val loss: 2.95910e-06, min loss: 1.33087e-06\n",
      "Epoch: 834500, elapsed: 1.12e+01, train loss: 1.79109e-06, val loss: 2.38221e-06, min loss: 1.33087e-06\n",
      "Epoch: 834600, elapsed: 1.12e+01, train loss: 1.88140e-06, val loss: 2.82782e-06, min loss: 1.33087e-06\n",
      "Epoch: 834700, elapsed: 1.12e+01, train loss: 1.37538e-06, val loss: 2.28631e-06, min loss: 1.33087e-06\n",
      "Epoch: 834800, elapsed: 1.39e+01, train loss: 1.33287e-06, val loss: 2.20643e-06, min loss: 1.33087e-06\n",
      "Epoch: 834900, elapsed: 1.14e+01, train loss: 1.36584e-06, val loss: 2.26215e-06, min loss: 1.33087e-06\n",
      "Epoch: 835000, elapsed: 1.14e+01, train loss: 1.35840e-06, val loss: 2.23989e-06, min loss: 1.33087e-06\n",
      "Epoch: 835100, elapsed: 1.35e+01, train loss: 1.33466e-06, val loss: 2.23269e-06, min loss: 1.33087e-06\n",
      "Epoch: 835200, elapsed: 1.14e+01, train loss: 1.42073e-06, val loss: 2.38070e-06, min loss: 1.33087e-06\n",
      "Epoch: 835300, elapsed: 1.13e+01, train loss: 1.40986e-06, val loss: 2.30220e-06, min loss: 1.33087e-06\n",
      "Epoch: 835400, elapsed: 1.14e+01, train loss: 1.37171e-06, val loss: 2.29576e-06, min loss: 1.33087e-06\n",
      "Epoch: 835500, elapsed: 1.16e+01, train loss: 1.32534e-06, val loss: 2.21088e-06, min loss: 1.32534e-06\n",
      "Epoch: 835600, elapsed: 1.12e+01, train loss: 1.32722e-06, val loss: 2.20432e-06, min loss: 1.32534e-06\n",
      "Epoch: 835700, elapsed: 1.13e+01, train loss: 1.33283e-06, val loss: 2.20061e-06, min loss: 1.32534e-06\n",
      "Epoch: 835800, elapsed: 1.16e+01, train loss: 1.44488e-06, val loss: 2.19109e-06, min loss: 1.32534e-06\n",
      "Epoch: 835900, elapsed: 1.14e+01, train loss: 1.32189e-06, val loss: 2.20079e-06, min loss: 1.32189e-06\n",
      "Epoch: 836000, elapsed: 1.14e+01, train loss: 1.33559e-06, val loss: 2.20491e-06, min loss: 1.32189e-06\n",
      "Epoch: 836100, elapsed: 1.15e+01, train loss: 4.47420e-06, val loss: 6.51664e-06, min loss: 1.32189e-06\n",
      "Epoch: 836200, elapsed: 1.13e+01, train loss: 1.32134e-06, val loss: 2.19833e-06, min loss: 1.32134e-06\n",
      "Epoch: 836300, elapsed: 1.36e+01, train loss: 4.26195e-06, val loss: 3.60057e-06, min loss: 1.32134e-06\n",
      "Epoch: 836400, elapsed: 1.17e+01, train loss: 1.37874e-06, val loss: 2.22542e-06, min loss: 1.32134e-06\n",
      "Epoch: 836500, elapsed: 1.14e+01, train loss: 1.40803e-06, val loss: 2.23949e-06, min loss: 1.32134e-06\n",
      "Epoch: 836600, elapsed: 1.15e+01, train loss: 4.67855e-06, val loss: 5.29566e-06, min loss: 1.32134e-06\n",
      "Epoch: 836700, elapsed: 1.14e+01, train loss: 1.66231e-06, val loss: 2.69294e-06, min loss: 1.32134e-06\n",
      "Epoch: 836800, elapsed: 1.15e+01, train loss: 2.26608e-06, val loss: 2.87997e-06, min loss: 1.32134e-06\n",
      "Epoch: 836900, elapsed: 1.14e+01, train loss: 1.33014e-06, val loss: 2.25746e-06, min loss: 1.32134e-06\n",
      "Epoch: 837000, elapsed: 1.14e+01, train loss: 1.40471e-06, val loss: 2.37609e-06, min loss: 1.32134e-06\n",
      "Epoch: 837100, elapsed: 1.14e+01, train loss: 4.60666e-06, val loss: 5.37049e-06, min loss: 1.32134e-06\n",
      "Epoch: 837200, elapsed: 1.15e+01, train loss: 4.11917e-06, val loss: 5.82515e-06, min loss: 1.32134e-06\n",
      "Epoch: 837300, elapsed: 1.15e+01, train loss: 1.31881e-06, val loss: 2.19247e-06, min loss: 1.31881e-06\n",
      "Epoch: 837400, elapsed: 1.16e+01, train loss: 1.32986e-06, val loss: 2.18585e-06, min loss: 1.31881e-06\n",
      "Epoch: 837500, elapsed: 1.14e+01, train loss: 1.56387e-06, val loss: 2.39472e-06, min loss: 1.31881e-06\n",
      "Epoch: 837600, elapsed: 1.14e+01, train loss: 1.33262e-06, val loss: 2.19666e-06, min loss: 1.31881e-06\n",
      "Epoch: 837700, elapsed: 1.14e+01, train loss: 1.39149e-06, val loss: 2.32342e-06, min loss: 1.31881e-06\n",
      "Epoch: 837800, elapsed: 1.14e+01, train loss: 1.41904e-06, val loss: 2.31301e-06, min loss: 1.31881e-06\n",
      "Epoch: 837900, elapsed: 1.37e+01, train loss: 1.32950e-06, val loss: 2.20387e-06, min loss: 1.31881e-06\n",
      "Epoch: 838000, elapsed: 1.14e+01, train loss: 1.32880e-06, val loss: 2.19906e-06, min loss: 1.31881e-06\n",
      "Epoch: 838100, elapsed: 1.14e+01, train loss: 1.35523e-06, val loss: 2.19723e-06, min loss: 1.31881e-06\n",
      "Epoch: 838200, elapsed: 1.19e+01, train loss: 1.32107e-06, val loss: 2.19437e-06, min loss: 1.31881e-06\n",
      "Epoch: 838300, elapsed: 1.13e+01, train loss: 1.42842e-06, val loss: 2.28299e-06, min loss: 1.31881e-06\n",
      "Epoch: 838400, elapsed: 1.13e+01, train loss: 2.21553e-06, val loss: 3.01942e-06, min loss: 1.31881e-06\n",
      "Epoch: 838500, elapsed: 1.14e+01, train loss: 1.33689e-06, val loss: 2.22222e-06, min loss: 1.31881e-06\n",
      "Epoch: 838600, elapsed: 1.14e+01, train loss: 1.37736e-06, val loss: 2.23765e-06, min loss: 1.31881e-06\n",
      "Epoch: 838700, elapsed: 1.14e+01, train loss: 1.72471e-06, val loss: 2.48531e-06, min loss: 1.31881e-06\n",
      "Epoch: 838800, elapsed: 1.13e+01, train loss: 3.04892e-06, val loss: 4.25498e-06, min loss: 1.31881e-06\n",
      "Epoch: 838900, elapsed: 1.12e+01, train loss: 1.48721e-06, val loss: 2.48996e-06, min loss: 1.31881e-06\n",
      "Epoch: 839000, elapsed: 1.15e+01, train loss: 1.31538e-06, val loss: 2.18769e-06, min loss: 1.31538e-06\n",
      "Epoch: 839100, elapsed: 1.13e+01, train loss: 1.39078e-06, val loss: 2.24763e-06, min loss: 1.31538e-06\n",
      "Epoch: 839200, elapsed: 1.13e+01, train loss: 1.40680e-06, val loss: 2.25110e-06, min loss: 1.31538e-06\n",
      "Epoch: 839300, elapsed: 1.13e+01, train loss: 6.50791e-06, val loss: 5.47864e-06, min loss: 1.31538e-06\n",
      "Epoch: 839400, elapsed: 1.13e+01, train loss: 1.38199e-06, val loss: 2.31969e-06, min loss: 1.31538e-06\n",
      "Epoch: 839500, elapsed: 1.36e+01, train loss: 1.32673e-06, val loss: 2.18425e-06, min loss: 1.31538e-06\n",
      "Epoch: 839600, elapsed: 1.15e+01, train loss: 3.18448e-06, val loss: 4.02950e-06, min loss: 1.31538e-06\n",
      "Epoch: 839700, elapsed: 1.13e+01, train loss: 1.31610e-06, val loss: 2.19773e-06, min loss: 1.31538e-06\n",
      "Epoch: 839800, elapsed: 1.16e+01, train loss: 1.31253e-06, val loss: 2.18738e-06, min loss: 1.31253e-06\n",
      "Epoch: 839900, elapsed: 1.15e+01, train loss: 1.33925e-06, val loss: 2.18163e-06, min loss: 1.31253e-06\n",
      "Epoch: 840000, elapsed: 1.14e+01, train loss: 1.74241e-06, val loss: 2.36028e-06, min loss: 1.31253e-06\n",
      "Epoch: 840100, elapsed: 1.34e+01, train loss: 6.17631e-06, val loss: 6.24119e-06, min loss: 1.31253e-06\n",
      "Epoch: 840200, elapsed: 1.14e+01, train loss: 1.59795e-06, val loss: 2.64782e-06, min loss: 1.31253e-06\n",
      "Epoch: 840300, elapsed: 1.14e+01, train loss: 1.31339e-06, val loss: 2.19679e-06, min loss: 1.31253e-06\n",
      "Epoch: 840400, elapsed: 1.14e+01, train loss: 1.31502e-06, val loss: 2.18650e-06, min loss: 1.31253e-06\n",
      "Epoch: 840500, elapsed: 1.14e+01, train loss: 1.31705e-06, val loss: 2.19538e-06, min loss: 1.31253e-06\n",
      "Epoch: 840600, elapsed: 1.14e+01, train loss: 1.32811e-06, val loss: 2.20043e-06, min loss: 1.31253e-06\n",
      "Epoch: 840700, elapsed: 1.13e+01, train loss: 2.65923e-06, val loss: 2.95590e-06, min loss: 1.31253e-06\n",
      "Epoch: 840800, elapsed: 1.13e+01, train loss: 1.82420e-06, val loss: 2.74327e-06, min loss: 1.31253e-06\n",
      "Epoch: 840900, elapsed: 1.14e+01, train loss: 1.43880e-06, val loss: 2.30261e-06, min loss: 1.31253e-06\n",
      "Epoch: 841000, elapsed: 1.13e+01, train loss: 1.33395e-06, val loss: 2.18994e-06, min loss: 1.31253e-06\n",
      "Epoch: 841100, elapsed: 1.38e+01, train loss: 1.35780e-06, val loss: 2.27339e-06, min loss: 1.31253e-06\n",
      "Epoch: 841200, elapsed: 1.15e+01, train loss: 1.31830e-06, val loss: 2.17604e-06, min loss: 1.31253e-06\n",
      "Epoch: 841300, elapsed: 1.14e+01, train loss: 1.44434e-06, val loss: 2.25488e-06, min loss: 1.31253e-06\n",
      "Epoch: 841400, elapsed: 1.15e+01, train loss: 1.37392e-06, val loss: 2.28006e-06, min loss: 1.31253e-06\n",
      "Epoch: 841500, elapsed: 1.17e+01, train loss: 1.33980e-06, val loss: 2.24722e-06, min loss: 1.31253e-06\n",
      "Epoch: 841600, elapsed: 1.13e+01, train loss: 1.30932e-06, val loss: 2.17749e-06, min loss: 1.30932e-06\n",
      "Epoch: 841700, elapsed: 1.13e+01, train loss: 1.38182e-06, val loss: 2.31686e-06, min loss: 1.30932e-06\n",
      "Epoch: 841800, elapsed: 1.14e+01, train loss: 2.76301e-06, val loss: 3.44624e-06, min loss: 1.30932e-06\n",
      "Epoch: 841900, elapsed: 1.14e+01, train loss: 6.07107e-06, val loss: 7.18054e-06, min loss: 1.30932e-06\n",
      "Epoch: 842000, elapsed: 1.12e+01, train loss: 3.68324e-06, val loss: 4.93967e-06, min loss: 1.30932e-06\n",
      "Epoch: 842100, elapsed: 1.13e+01, train loss: 3.51290e-06, val loss: 4.41608e-06, min loss: 1.30932e-06\n",
      "Epoch: 842200, elapsed: 1.14e+01, train loss: 1.31175e-06, val loss: 2.19495e-06, min loss: 1.30932e-06\n",
      "Epoch: 842300, elapsed: 1.13e+01, train loss: 1.30758e-06, val loss: 2.17306e-06, min loss: 1.30758e-06\n",
      "Epoch: 842400, elapsed: 1.14e+01, train loss: 1.67567e-06, val loss: 2.21380e-06, min loss: 1.30758e-06\n",
      "Epoch: 842500, elapsed: 1.12e+01, train loss: 5.82666e-06, val loss: 6.04175e-06, min loss: 1.30758e-06\n",
      "Epoch: 842600, elapsed: 1.13e+01, train loss: 1.44270e-06, val loss: 2.28793e-06, min loss: 1.30758e-06\n",
      "Epoch: 842700, elapsed: 1.37e+01, train loss: 1.30988e-06, val loss: 2.18667e-06, min loss: 1.30758e-06\n",
      "Epoch: 842800, elapsed: 1.14e+01, train loss: 1.30895e-06, val loss: 2.18809e-06, min loss: 1.30758e-06\n",
      "Epoch: 842900, elapsed: 1.14e+01, train loss: 1.35113e-06, val loss: 2.18378e-06, min loss: 1.30758e-06\n",
      "Epoch: 843000, elapsed: 1.14e+01, train loss: 1.32995e-06, val loss: 2.20330e-06, min loss: 1.30758e-06\n",
      "Epoch: 843100, elapsed: 1.15e+01, train loss: 1.70552e-06, val loss: 2.52180e-06, min loss: 1.30758e-06\n",
      "Epoch: 843200, elapsed: 1.14e+01, train loss: 1.46204e-06, val loss: 2.35259e-06, min loss: 1.30758e-06\n",
      "Epoch: 843300, elapsed: 1.13e+01, train loss: 2.04122e-06, val loss: 2.89102e-06, min loss: 1.30758e-06\n",
      "Epoch: 843400, elapsed: 1.14e+01, train loss: 1.41922e-06, val loss: 2.35581e-06, min loss: 1.30758e-06\n",
      "Epoch: 843500, elapsed: 1.13e+01, train loss: 5.18531e-06, val loss: 5.84718e-06, min loss: 1.30758e-06\n",
      "Epoch: 843600, elapsed: 1.14e+01, train loss: 4.47159e-06, val loss: 5.77734e-06, min loss: 1.30758e-06\n",
      "Epoch: 843700, elapsed: 1.14e+01, train loss: 5.63108e-06, val loss: 6.12360e-06, min loss: 1.30758e-06\n",
      "Epoch: 843800, elapsed: 1.13e+01, train loss: 4.62964e-06, val loss: 4.04216e-06, min loss: 1.30758e-06\n",
      "Epoch: 843900, elapsed: 1.14e+01, train loss: 1.30497e-06, val loss: 2.16318e-06, min loss: 1.30497e-06\n",
      "Epoch: 844000, elapsed: 1.13e+01, train loss: 1.30303e-06, val loss: 2.16909e-06, min loss: 1.30303e-06\n",
      "Epoch: 844100, elapsed: 1.13e+01, train loss: 1.34409e-06, val loss: 2.16413e-06, min loss: 1.30303e-06\n",
      "Epoch: 844200, elapsed: 1.13e+01, train loss: 2.11603e-06, val loss: 2.86333e-06, min loss: 1.30303e-06\n",
      "Epoch: 844300, elapsed: 1.37e+01, train loss: 2.33546e-06, val loss: 3.14792e-06, min loss: 1.30303e-06\n",
      "Epoch: 844400, elapsed: 1.13e+01, train loss: 1.31681e-06, val loss: 2.20039e-06, min loss: 1.30303e-06\n",
      "Epoch: 844500, elapsed: 1.14e+01, train loss: 1.72273e-06, val loss: 3.00505e-06, min loss: 1.30303e-06\n",
      "Epoch: 844600, elapsed: 1.14e+01, train loss: 1.38485e-06, val loss: 2.28688e-06, min loss: 1.30303e-06\n",
      "Epoch: 844700, elapsed: 1.13e+01, train loss: 4.31840e-06, val loss: 5.76412e-06, min loss: 1.30303e-06\n",
      "Epoch: 844800, elapsed: 1.14e+01, train loss: 2.52187e-06, val loss: 4.01407e-06, min loss: 1.30303e-06\n",
      "Epoch: 844900, elapsed: 1.14e+01, train loss: 1.29961e-06, val loss: 2.16421e-06, min loss: 1.29961e-06\n",
      "Epoch: 845000, elapsed: 1.17e+01, train loss: 1.33278e-06, val loss: 2.20492e-06, min loss: 1.29961e-06\n",
      "Epoch: 845100, elapsed: 1.36e+01, train loss: 5.60201e-06, val loss: 3.26168e-06, min loss: 1.29961e-06\n",
      "Epoch: 845200, elapsed: 1.13e+01, train loss: 1.29916e-06, val loss: 2.16554e-06, min loss: 1.29916e-06\n",
      "Epoch: 845300, elapsed: 1.12e+01, train loss: 5.30232e-06, val loss: 4.79735e-06, min loss: 1.29916e-06\n",
      "Epoch: 845400, elapsed: 1.12e+01, train loss: 1.29967e-06, val loss: 2.17578e-06, min loss: 1.29916e-06\n",
      "Epoch: 845500, elapsed: 1.13e+01, train loss: 5.05501e-06, val loss: 6.03560e-06, min loss: 1.29916e-06\n",
      "Epoch: 845600, elapsed: 1.12e+01, train loss: 1.40793e-06, val loss: 2.26915e-06, min loss: 1.29916e-06\n",
      "Epoch: 845700, elapsed: 1.12e+01, train loss: 1.30626e-06, val loss: 2.14285e-06, min loss: 1.29916e-06\n",
      "Epoch: 845800, elapsed: 1.12e+01, train loss: 1.33546e-06, val loss: 2.18303e-06, min loss: 1.29916e-06\n",
      "Epoch: 845900, elapsed: 1.38e+01, train loss: 1.49356e-06, val loss: 2.35362e-06, min loss: 1.29916e-06\n",
      "Epoch: 846000, elapsed: 1.14e+01, train loss: 1.69517e-06, val loss: 2.63431e-06, min loss: 1.29916e-06\n",
      "Epoch: 846100, elapsed: 1.14e+01, train loss: 1.87199e-06, val loss: 2.86229e-06, min loss: 1.29916e-06\n",
      "Epoch: 846200, elapsed: 1.15e+01, train loss: 1.39265e-06, val loss: 2.36773e-06, min loss: 1.29916e-06\n",
      "Epoch: 846300, elapsed: 1.14e+01, train loss: 1.34261e-06, val loss: 2.19278e-06, min loss: 1.29916e-06\n",
      "Epoch: 846400, elapsed: 1.13e+01, train loss: 1.31137e-06, val loss: 2.17960e-06, min loss: 1.29916e-06\n",
      "Epoch: 846500, elapsed: 1.14e+01, train loss: 1.32039e-06, val loss: 2.16555e-06, min loss: 1.29916e-06\n",
      "Epoch: 846600, elapsed: 1.14e+01, train loss: 1.34011e-06, val loss: 2.16269e-06, min loss: 1.29916e-06\n",
      "Epoch: 846700, elapsed: 1.13e+01, train loss: 1.32343e-06, val loss: 2.20267e-06, min loss: 1.29916e-06\n",
      "Epoch: 846800, elapsed: 1.15e+01, train loss: 2.28532e-06, val loss: 4.33276e-06, min loss: 1.29916e-06\n",
      "Epoch: 846900, elapsed: 1.14e+01, train loss: 1.29466e-06, val loss: 2.15827e-06, min loss: 1.29466e-06\n",
      "Epoch: 847000, elapsed: 1.13e+01, train loss: 1.31235e-06, val loss: 2.14859e-06, min loss: 1.29466e-06\n",
      "Epoch: 847100, elapsed: 1.14e+01, train loss: 1.60210e-06, val loss: 2.22801e-06, min loss: 1.29466e-06\n",
      "Epoch: 847200, elapsed: 1.14e+01, train loss: 1.29380e-06, val loss: 2.15497e-06, min loss: 1.29380e-06\n",
      "Epoch: 847300, elapsed: 1.14e+01, train loss: 1.74863e-06, val loss: 2.37713e-06, min loss: 1.29380e-06\n",
      "Epoch: 847400, elapsed: 1.14e+01, train loss: 1.39313e-06, val loss: 2.50953e-06, min loss: 1.29380e-06\n",
      "Epoch: 847500, elapsed: 1.38e+01, train loss: 4.10113e-06, val loss: 3.90187e-06, min loss: 1.29380e-06\n",
      "Epoch: 847600, elapsed: 1.14e+01, train loss: 3.48498e-06, val loss: 4.56861e-06, min loss: 1.29380e-06\n",
      "Epoch: 847700, elapsed: 1.15e+01, train loss: 1.35481e-06, val loss: 2.19674e-06, min loss: 1.29380e-06\n",
      "Epoch: 847800, elapsed: 1.15e+01, train loss: 1.32853e-06, val loss: 2.22649e-06, min loss: 1.29380e-06\n",
      "Epoch: 847900, elapsed: 1.13e+01, train loss: 1.30526e-06, val loss: 2.14151e-06, min loss: 1.29380e-06\n",
      "Epoch: 848000, elapsed: 1.15e+01, train loss: 1.30163e-06, val loss: 2.17439e-06, min loss: 1.29380e-06\n",
      "Epoch: 848100, elapsed: 1.13e+01, train loss: 2.28934e-06, val loss: 3.14483e-06, min loss: 1.29380e-06\n",
      "Epoch: 848200, elapsed: 1.13e+01, train loss: 1.34305e-06, val loss: 2.18665e-06, min loss: 1.29380e-06\n",
      "Epoch: 848300, elapsed: 1.13e+01, train loss: 3.09115e-06, val loss: 3.18782e-06, min loss: 1.29380e-06\n",
      "Epoch: 848400, elapsed: 1.15e+01, train loss: 1.29079e-06, val loss: 2.14826e-06, min loss: 1.29079e-06\n",
      "Epoch: 848500, elapsed: 1.15e+01, train loss: 1.40642e-06, val loss: 2.20564e-06, min loss: 1.29079e-06\n",
      "Epoch: 848600, elapsed: 1.14e+01, train loss: 1.29952e-06, val loss: 2.15425e-06, min loss: 1.29079e-06\n",
      "Epoch: 848700, elapsed: 1.15e+01, train loss: 1.63757e-06, val loss: 2.40543e-06, min loss: 1.29079e-06\n",
      "Epoch: 848800, elapsed: 1.14e+01, train loss: 1.40281e-06, val loss: 2.24713e-06, min loss: 1.29079e-06\n",
      "Epoch: 848900, elapsed: 1.14e+01, train loss: 1.76784e-06, val loss: 2.83507e-06, min loss: 1.29079e-06\n",
      "Epoch: 849000, elapsed: 1.13e+01, train loss: 1.50029e-06, val loss: 2.34226e-06, min loss: 1.29079e-06\n",
      "Epoch: 849100, elapsed: 1.14e+01, train loss: 1.35584e-06, val loss: 2.19466e-06, min loss: 1.29079e-06\n",
      "Epoch: 849200, elapsed: 1.39e+01, train loss: 1.36537e-06, val loss: 2.21004e-06, min loss: 1.29079e-06\n",
      "Epoch: 849300, elapsed: 1.14e+01, train loss: 1.28998e-06, val loss: 2.14208e-06, min loss: 1.28998e-06\n",
      "Epoch: 849400, elapsed: 1.15e+01, train loss: 1.30259e-06, val loss: 2.14165e-06, min loss: 1.28998e-06\n",
      "Epoch: 849500, elapsed: 1.14e+01, train loss: 1.53063e-06, val loss: 2.51058e-06, min loss: 1.28998e-06\n",
      "Epoch: 849600, elapsed: 1.13e+01, train loss: 3.24560e-06, val loss: 4.48386e-06, min loss: 1.28998e-06\n",
      "Epoch: 849700, elapsed: 1.14e+01, train loss: 1.41323e-06, val loss: 2.22722e-06, min loss: 1.28998e-06\n",
      "Epoch: 849800, elapsed: 1.14e+01, train loss: 2.64504e-06, val loss: 3.78320e-06, min loss: 1.28998e-06\n",
      "Epoch: 849900, elapsed: 1.12e+01, train loss: 1.28736e-06, val loss: 2.14215e-06, min loss: 1.28736e-06\n",
      "Epoch: 850000, elapsed: 1.13e+01, train loss: 1.29107e-06, val loss: 2.15970e-06, min loss: 1.28736e-06\n",
      "Epoch: 850100, elapsed: 1.33e+01, train loss: 1.41234e-06, val loss: 2.34989e-06, min loss: 1.28736e-06\n",
      "Epoch: 850200, elapsed: 1.14e+01, train loss: 1.29591e-06, val loss: 2.17239e-06, min loss: 1.28736e-06\n",
      "Epoch: 850300, elapsed: 1.13e+01, train loss: 2.97445e-06, val loss: 3.65047e-06, min loss: 1.28736e-06\n",
      "Epoch: 850400, elapsed: 1.14e+01, train loss: 1.33070e-06, val loss: 2.22501e-06, min loss: 1.28736e-06\n",
      "Epoch: 850500, elapsed: 1.14e+01, train loss: 2.03617e-06, val loss: 2.54176e-06, min loss: 1.28736e-06\n",
      "Epoch: 850600, elapsed: 1.14e+01, train loss: 1.32425e-06, val loss: 2.15733e-06, min loss: 1.28736e-06\n",
      "Epoch: 850700, elapsed: 1.38e+01, train loss: 1.46729e-06, val loss: 2.37522e-06, min loss: 1.28736e-06\n",
      "Epoch: 850800, elapsed: 1.15e+01, train loss: 2.80028e-06, val loss: 3.75305e-06, min loss: 1.28736e-06\n",
      "Epoch: 850900, elapsed: 1.13e+01, train loss: 1.53818e-06, val loss: 2.53313e-06, min loss: 1.28736e-06\n",
      "Epoch: 851000, elapsed: 1.15e+01, train loss: 1.29735e-06, val loss: 2.17528e-06, min loss: 1.28736e-06\n",
      "Epoch: 851100, elapsed: 1.14e+01, train loss: 3.69721e-06, val loss: 4.41156e-06, min loss: 1.28736e-06\n",
      "Epoch: 851200, elapsed: 1.12e+01, train loss: 1.65765e-06, val loss: 2.31840e-06, min loss: 1.28736e-06\n",
      "Epoch: 851300, elapsed: 1.14e+01, train loss: 1.35580e-06, val loss: 2.18928e-06, min loss: 1.28736e-06\n",
      "Epoch: 851400, elapsed: 1.14e+01, train loss: 3.91340e-06, val loss: 4.97830e-06, min loss: 1.28736e-06\n",
      "Epoch: 851500, elapsed: 1.15e+01, train loss: 1.39007e-06, val loss: 2.33384e-06, min loss: 1.28736e-06\n",
      "Epoch: 851600, elapsed: 1.13e+01, train loss: 2.23971e-06, val loss: 2.77268e-06, min loss: 1.28736e-06\n",
      "Epoch: 851700, elapsed: 1.13e+01, train loss: 8.10769e-06, val loss: 9.20853e-06, min loss: 1.28736e-06\n",
      "Epoch: 851800, elapsed: 1.13e+01, train loss: 1.39336e-06, val loss: 2.23898e-06, min loss: 1.28736e-06\n",
      "Epoch: 851900, elapsed: 1.14e+01, train loss: 1.30519e-06, val loss: 2.27225e-06, min loss: 1.28736e-06\n",
      "Epoch: 852000, elapsed: 1.13e+01, train loss: 2.31070e-06, val loss: 3.20254e-06, min loss: 1.28736e-06\n",
      "Epoch: 852100, elapsed: 1.13e+01, train loss: 1.28786e-06, val loss: 2.13693e-06, min loss: 1.28736e-06\n",
      "Epoch: 852200, elapsed: 1.13e+01, train loss: 1.48463e-06, val loss: 2.19584e-06, min loss: 1.28736e-06\n",
      "Epoch: 852300, elapsed: 1.12e+01, train loss: 1.81864e-06, val loss: 2.60266e-06, min loss: 1.28736e-06\n",
      "Epoch: 852400, elapsed: 1.38e+01, train loss: 1.37261e-06, val loss: 2.15106e-06, min loss: 1.28736e-06\n",
      "Epoch: 852500, elapsed: 1.13e+01, train loss: 1.28980e-06, val loss: 2.14546e-06, min loss: 1.28736e-06\n",
      "Epoch: 852600, elapsed: 1.15e+01, train loss: 1.28400e-06, val loss: 2.14026e-06, min loss: 1.28400e-06\n",
      "Epoch: 852700, elapsed: 1.14e+01, train loss: 1.38329e-06, val loss: 2.20253e-06, min loss: 1.28400e-06\n",
      "Epoch: 852800, elapsed: 1.13e+01, train loss: 2.87555e-06, val loss: 2.40588e-06, min loss: 1.28400e-06\n",
      "Epoch: 852900, elapsed: 1.14e+01, train loss: 1.27984e-06, val loss: 2.13428e-06, min loss: 1.27984e-06\n",
      "Epoch: 853000, elapsed: 1.13e+01, train loss: 1.28297e-06, val loss: 2.12375e-06, min loss: 1.27984e-06\n",
      "Epoch: 853100, elapsed: 1.14e+01, train loss: 1.48191e-06, val loss: 2.16219e-06, min loss: 1.27984e-06\n",
      "Epoch: 853200, elapsed: 1.13e+01, train loss: 2.41736e-06, val loss: 2.92994e-06, min loss: 1.27984e-06\n",
      "Epoch: 853300, elapsed: 1.12e+01, train loss: 1.59854e-06, val loss: 2.29705e-06, min loss: 1.27984e-06\n",
      "Epoch: 853400, elapsed: 1.12e+01, train loss: 1.32757e-06, val loss: 2.18459e-06, min loss: 1.27984e-06\n",
      "Epoch: 853500, elapsed: 1.13e+01, train loss: 2.05953e-06, val loss: 3.08527e-06, min loss: 1.27984e-06\n",
      "Epoch: 853600, elapsed: 1.12e+01, train loss: 1.27829e-06, val loss: 2.12965e-06, min loss: 1.27829e-06\n",
      "Epoch: 853700, elapsed: 1.13e+01, train loss: 1.28884e-06, val loss: 2.12581e-06, min loss: 1.27829e-06\n",
      "Epoch: 853800, elapsed: 1.13e+01, train loss: 1.32685e-06, val loss: 2.19266e-06, min loss: 1.27829e-06\n",
      "Epoch: 853900, elapsed: 1.12e+01, train loss: 1.39565e-06, val loss: 2.16271e-06, min loss: 1.27829e-06\n",
      "Epoch: 854000, elapsed: 1.39e+01, train loss: 8.00871e-06, val loss: 6.96423e-06, min loss: 1.27829e-06\n",
      "Epoch: 854100, elapsed: 1.15e+01, train loss: 1.27694e-06, val loss: 2.13255e-06, min loss: 1.27694e-06\n",
      "Epoch: 854200, elapsed: 1.13e+01, train loss: 1.28209e-06, val loss: 2.13672e-06, min loss: 1.27694e-06\n",
      "Epoch: 854300, elapsed: 1.16e+01, train loss: 2.26449e-06, val loss: 3.00544e-06, min loss: 1.27694e-06\n",
      "Epoch: 854400, elapsed: 1.14e+01, train loss: 1.29021e-06, val loss: 2.12084e-06, min loss: 1.27694e-06\n",
      "Epoch: 854500, elapsed: 1.15e+01, train loss: 1.28770e-06, val loss: 2.13092e-06, min loss: 1.27694e-06\n",
      "Epoch: 854600, elapsed: 1.15e+01, train loss: 1.27724e-06, val loss: 2.13053e-06, min loss: 1.27694e-06\n",
      "Epoch: 854700, elapsed: 1.12e+01, train loss: 1.28040e-06, val loss: 2.11596e-06, min loss: 1.27694e-06\n",
      "Epoch: 854800, elapsed: 1.12e+01, train loss: 1.35187e-06, val loss: 2.23915e-06, min loss: 1.27694e-06\n",
      "Epoch: 854900, elapsed: 1.13e+01, train loss: 3.25737e-06, val loss: 3.62045e-06, min loss: 1.27694e-06\n",
      "Epoch: 855000, elapsed: 1.14e+01, train loss: 1.27691e-06, val loss: 2.13518e-06, min loss: 1.27691e-06\n",
      "Epoch: 855100, elapsed: 1.36e+01, train loss: 1.27859e-06, val loss: 2.13793e-06, min loss: 1.27691e-06\n",
      "Epoch: 855200, elapsed: 1.16e+01, train loss: 1.31994e-06, val loss: 2.14818e-06, min loss: 1.27691e-06\n",
      "Epoch: 855300, elapsed: 1.14e+01, train loss: 2.03308e-06, val loss: 2.82358e-06, min loss: 1.27691e-06\n",
      "Epoch: 855400, elapsed: 1.15e+01, train loss: 4.70938e-06, val loss: 5.57847e-06, min loss: 1.27691e-06\n",
      "Epoch: 855500, elapsed: 1.13e+01, train loss: 1.97292e-06, val loss: 3.14810e-06, min loss: 1.27691e-06\n",
      "Epoch: 855600, elapsed: 1.40e+01, train loss: 1.27449e-06, val loss: 2.13025e-06, min loss: 1.27449e-06\n",
      "Epoch: 855700, elapsed: 1.16e+01, train loss: 1.28237e-06, val loss: 2.14281e-06, min loss: 1.27449e-06\n",
      "Epoch: 855800, elapsed: 1.15e+01, train loss: 1.27361e-06, val loss: 2.13059e-06, min loss: 1.27361e-06\n",
      "Epoch: 855900, elapsed: 1.13e+01, train loss: 1.28555e-06, val loss: 2.15917e-06, min loss: 1.27361e-06\n",
      "Epoch: 856000, elapsed: 1.14e+01, train loss: 1.32297e-06, val loss: 2.16093e-06, min loss: 1.27361e-06\n",
      "Epoch: 856100, elapsed: 1.14e+01, train loss: 1.32299e-06, val loss: 2.14870e-06, min loss: 1.27361e-06\n",
      "Epoch: 856200, elapsed: 1.15e+01, train loss: 1.31602e-06, val loss: 2.19495e-06, min loss: 1.27361e-06\n",
      "Epoch: 856300, elapsed: 1.13e+01, train loss: 1.35365e-06, val loss: 2.16291e-06, min loss: 1.27361e-06\n",
      "Epoch: 856400, elapsed: 1.14e+01, train loss: 1.32723e-06, val loss: 2.12131e-06, min loss: 1.27361e-06\n",
      "Epoch: 856500, elapsed: 1.14e+01, train loss: 1.27610e-06, val loss: 2.11595e-06, min loss: 1.27361e-06\n",
      "Epoch: 856600, elapsed: 1.14e+01, train loss: 1.27205e-06, val loss: 2.11640e-06, min loss: 1.27205e-06\n",
      "Epoch: 856700, elapsed: 1.12e+01, train loss: 1.80952e-06, val loss: 2.58016e-06, min loss: 1.27205e-06\n",
      "Epoch: 856800, elapsed: 1.14e+01, train loss: 1.27509e-06, val loss: 2.12597e-06, min loss: 1.27205e-06\n",
      "Epoch: 856900, elapsed: 1.14e+01, train loss: 1.73532e-06, val loss: 2.82946e-06, min loss: 1.27205e-06\n",
      "Epoch: 857000, elapsed: 1.13e+01, train loss: 1.26961e-06, val loss: 2.11937e-06, min loss: 1.26961e-06\n",
      "Epoch: 857100, elapsed: 1.14e+01, train loss: 1.28861e-06, val loss: 2.12204e-06, min loss: 1.26961e-06\n",
      "Epoch: 857200, elapsed: 1.38e+01, train loss: 2.15005e-06, val loss: 3.25066e-06, min loss: 1.26961e-06\n",
      "Epoch: 857300, elapsed: 1.15e+01, train loss: 1.26945e-06, val loss: 2.11732e-06, min loss: 1.26945e-06\n",
      "Epoch: 857400, elapsed: 1.14e+01, train loss: 1.46811e-06, val loss: 2.28400e-06, min loss: 1.26945e-06\n",
      "Epoch: 857500, elapsed: 1.14e+01, train loss: 1.30255e-06, val loss: 2.14131e-06, min loss: 1.26945e-06\n",
      "Epoch: 857600, elapsed: 1.14e+01, train loss: 1.31984e-06, val loss: 2.15319e-06, min loss: 1.26945e-06\n",
      "Epoch: 857700, elapsed: 1.14e+01, train loss: 1.31578e-06, val loss: 2.24119e-06, min loss: 1.26945e-06\n",
      "Epoch: 857800, elapsed: 1.14e+01, train loss: 1.36309e-06, val loss: 2.14340e-06, min loss: 1.26945e-06\n",
      "Epoch: 857900, elapsed: 1.13e+01, train loss: 1.29978e-06, val loss: 2.14644e-06, min loss: 1.26945e-06\n",
      "Epoch: 858000, elapsed: 1.14e+01, train loss: 1.26762e-06, val loss: 2.11047e-06, min loss: 1.26762e-06\n",
      "Epoch: 858100, elapsed: 1.15e+01, train loss: 1.26863e-06, val loss: 2.12630e-06, min loss: 1.26762e-06\n",
      "Epoch: 858200, elapsed: 1.12e+01, train loss: 1.32860e-06, val loss: 2.19054e-06, min loss: 1.26762e-06\n",
      "Epoch: 858300, elapsed: 1.13e+01, train loss: 1.28927e-06, val loss: 2.20403e-06, min loss: 1.26762e-06\n",
      "Epoch: 858400, elapsed: 1.13e+01, train loss: 1.31167e-06, val loss: 2.14891e-06, min loss: 1.26762e-06\n",
      "Epoch: 858500, elapsed: 1.14e+01, train loss: 1.27434e-06, val loss: 2.12626e-06, min loss: 1.26762e-06\n",
      "Epoch: 858600, elapsed: 1.14e+01, train loss: 1.41879e-06, val loss: 2.26573e-06, min loss: 1.26762e-06\n",
      "Epoch: 858700, elapsed: 1.12e+01, train loss: 4.67696e-06, val loss: 3.99170e-06, min loss: 1.26762e-06\n",
      "Epoch: 858800, elapsed: 1.12e+01, train loss: 1.29356e-06, val loss: 2.11979e-06, min loss: 1.26762e-06\n",
      "Epoch: 858900, elapsed: 1.39e+01, train loss: 1.27754e-06, val loss: 2.12469e-06, min loss: 1.26762e-06\n",
      "Epoch: 859000, elapsed: 1.15e+01, train loss: 2.95766e-06, val loss: 4.29742e-06, min loss: 1.26762e-06\n",
      "Epoch: 859100, elapsed: 1.14e+01, train loss: 1.26499e-06, val loss: 2.11531e-06, min loss: 1.26499e-06\n",
      "Epoch: 859200, elapsed: 1.16e+01, train loss: 1.29290e-06, val loss: 2.12150e-06, min loss: 1.26499e-06\n",
      "Epoch: 859300, elapsed: 1.14e+01, train loss: 1.26398e-06, val loss: 2.11051e-06, min loss: 1.26398e-06\n",
      "Epoch: 859400, elapsed: 1.14e+01, train loss: 1.27386e-06, val loss: 2.10830e-06, min loss: 1.26398e-06\n",
      "Epoch: 859500, elapsed: 1.13e+01, train loss: 1.26402e-06, val loss: 2.10556e-06, min loss: 1.26398e-06\n",
      "Epoch: 859600, elapsed: 1.13e+01, train loss: 1.26557e-06, val loss: 2.11832e-06, min loss: 1.26398e-06\n",
      "Epoch: 859700, elapsed: 1.13e+01, train loss: 9.96756e-06, val loss: 7.93285e-06, min loss: 1.26398e-06\n",
      "Epoch: 859800, elapsed: 1.12e+01, train loss: 1.26316e-06, val loss: 2.10362e-06, min loss: 1.26316e-06\n",
      "Epoch: 859900, elapsed: 1.13e+01, train loss: 1.32675e-06, val loss: 2.11166e-06, min loss: 1.26316e-06\n",
      "Epoch: 860000, elapsed: 1.14e+01, train loss: 1.44554e-06, val loss: 2.16097e-06, min loss: 1.26316e-06\n",
      "Epoch: 860100, elapsed: 1.33e+01, train loss: 5.40347e-06, val loss: 5.35770e-06, min loss: 1.26316e-06\n",
      "Epoch: 860200, elapsed: 1.13e+01, train loss: 1.39196e-06, val loss: 2.38185e-06, min loss: 1.26316e-06\n",
      "Epoch: 860300, elapsed: 1.13e+01, train loss: 1.26249e-06, val loss: 2.10342e-06, min loss: 1.26249e-06\n",
      "Epoch: 860400, elapsed: 1.15e+01, train loss: 1.30158e-06, val loss: 2.14908e-06, min loss: 1.26249e-06\n",
      "Epoch: 860500, elapsed: 1.39e+01, train loss: 1.27702e-06, val loss: 2.10919e-06, min loss: 1.26249e-06\n",
      "Epoch: 860600, elapsed: 1.14e+01, train loss: 1.26308e-06, val loss: 2.10267e-06, min loss: 1.26249e-06\n",
      "Epoch: 860700, elapsed: 1.15e+01, train loss: 2.02577e-06, val loss: 2.29723e-06, min loss: 1.26249e-06\n",
      "Epoch: 860800, elapsed: 1.15e+01, train loss: 1.27944e-06, val loss: 2.11585e-06, min loss: 1.26249e-06\n",
      "Epoch: 860900, elapsed: 1.15e+01, train loss: 1.31020e-06, val loss: 2.11817e-06, min loss: 1.26249e-06\n",
      "Epoch: 861000, elapsed: 1.14e+01, train loss: 2.92057e-06, val loss: 3.61641e-06, min loss: 1.26249e-06\n",
      "Epoch: 861100, elapsed: 1.14e+01, train loss: 1.37984e-06, val loss: 2.22095e-06, min loss: 1.26249e-06\n",
      "Epoch: 861200, elapsed: 1.14e+01, train loss: 1.40425e-06, val loss: 2.27164e-06, min loss: 1.26249e-06\n",
      "Epoch: 861300, elapsed: 1.14e+01, train loss: 1.26204e-06, val loss: 2.10025e-06, min loss: 1.26204e-06\n",
      "Epoch: 861400, elapsed: 1.14e+01, train loss: 1.28106e-06, val loss: 2.11920e-06, min loss: 1.26204e-06\n",
      "Epoch: 861500, elapsed: 1.14e+01, train loss: 1.27457e-06, val loss: 2.12158e-06, min loss: 1.26204e-06\n",
      "Epoch: 861600, elapsed: 1.14e+01, train loss: 1.45047e-06, val loss: 2.16027e-06, min loss: 1.26204e-06\n",
      "Epoch: 861700, elapsed: 1.12e+01, train loss: 1.44849e-06, val loss: 2.25624e-06, min loss: 1.26204e-06\n",
      "Epoch: 861800, elapsed: 1.13e+01, train loss: 7.52605e-06, val loss: 9.28789e-06, min loss: 1.26204e-06\n",
      "Epoch: 861900, elapsed: 1.13e+01, train loss: 1.25828e-06, val loss: 2.10166e-06, min loss: 1.25828e-06\n",
      "Epoch: 862000, elapsed: 1.12e+01, train loss: 2.59943e-06, val loss: 3.19557e-06, min loss: 1.25828e-06\n",
      "Epoch: 862100, elapsed: 1.37e+01, train loss: 1.25760e-06, val loss: 2.09788e-06, min loss: 1.25760e-06\n",
      "Epoch: 862200, elapsed: 1.14e+01, train loss: 1.30111e-06, val loss: 2.10748e-06, min loss: 1.25760e-06\n",
      "Epoch: 862300, elapsed: 1.15e+01, train loss: 3.09365e-06, val loss: 3.85579e-06, min loss: 1.25760e-06\n",
      "Epoch: 862400, elapsed: 1.16e+01, train loss: 1.27112e-06, val loss: 2.12867e-06, min loss: 1.25760e-06\n",
      "Epoch: 862500, elapsed: 1.13e+01, train loss: 1.28719e-06, val loss: 2.09830e-06, min loss: 1.25760e-06\n",
      "Epoch: 862600, elapsed: 1.16e+01, train loss: 1.40296e-06, val loss: 2.25214e-06, min loss: 1.25760e-06\n",
      "Epoch: 862700, elapsed: 1.15e+01, train loss: 1.26778e-06, val loss: 2.13236e-06, min loss: 1.25760e-06\n",
      "Epoch: 862800, elapsed: 1.16e+01, train loss: 1.27024e-06, val loss: 2.09424e-06, min loss: 1.25760e-06\n",
      "Epoch: 862900, elapsed: 1.15e+01, train loss: 2.67127e-06, val loss: 3.41599e-06, min loss: 1.25760e-06\n",
      "Epoch: 863000, elapsed: 1.16e+01, train loss: 1.29119e-06, val loss: 2.16710e-06, min loss: 1.25760e-06\n",
      "Epoch: 863100, elapsed: 1.14e+01, train loss: 1.26240e-06, val loss: 2.09604e-06, min loss: 1.25760e-06\n",
      "Epoch: 863200, elapsed: 1.14e+01, train loss: 1.28299e-06, val loss: 2.10611e-06, min loss: 1.25760e-06\n",
      "Epoch: 863300, elapsed: 1.12e+01, train loss: 1.28968e-06, val loss: 2.11200e-06, min loss: 1.25760e-06\n",
      "Epoch: 863400, elapsed: 1.14e+01, train loss: 1.35009e-06, val loss: 2.20067e-06, min loss: 1.25760e-06\n",
      "Epoch: 863500, elapsed: 1.11e+01, train loss: 6.89926e-06, val loss: 5.93893e-06, min loss: 1.25760e-06\n",
      "Epoch: 863600, elapsed: 1.13e+01, train loss: 2.03484e-06, val loss: 3.07234e-06, min loss: 1.25760e-06\n",
      "Epoch: 863700, elapsed: 1.13e+01, train loss: 1.40349e-06, val loss: 2.27785e-06, min loss: 1.25760e-06\n",
      "Epoch: 863800, elapsed: 1.39e+01, train loss: 1.64997e-06, val loss: 2.74013e-06, min loss: 1.25760e-06\n",
      "Epoch: 863900, elapsed: 1.16e+01, train loss: 1.84450e-06, val loss: 2.52797e-06, min loss: 1.25760e-06\n",
      "Epoch: 864000, elapsed: 1.15e+01, train loss: 1.42082e-06, val loss: 2.12751e-06, min loss: 1.25760e-06\n",
      "Epoch: 864100, elapsed: 1.14e+01, train loss: 2.39559e-06, val loss: 2.74684e-06, min loss: 1.25760e-06\n",
      "Epoch: 864200, elapsed: 1.13e+01, train loss: 1.26465e-06, val loss: 2.12729e-06, min loss: 1.25760e-06\n",
      "Epoch: 864300, elapsed: 1.14e+01, train loss: 1.39113e-06, val loss: 2.17533e-06, min loss: 1.25760e-06\n",
      "Epoch: 864400, elapsed: 1.13e+01, train loss: 1.40514e-06, val loss: 2.22367e-06, min loss: 1.25760e-06\n",
      "Epoch: 864500, elapsed: 1.14e+01, train loss: 1.25373e-06, val loss: 2.09363e-06, min loss: 1.25373e-06\n",
      "Epoch: 864600, elapsed: 1.14e+01, train loss: 1.58852e-06, val loss: 2.33913e-06, min loss: 1.25373e-06\n",
      "Epoch: 864700, elapsed: 1.13e+01, train loss: 1.54570e-06, val loss: 2.40795e-06, min loss: 1.25373e-06\n",
      "Epoch: 864800, elapsed: 1.13e+01, train loss: 1.32860e-06, val loss: 2.13769e-06, min loss: 1.25373e-06\n",
      "Epoch: 864900, elapsed: 1.14e+01, train loss: 1.44436e-06, val loss: 2.33377e-06, min loss: 1.25373e-06\n",
      "Epoch: 865000, elapsed: 1.16e+01, train loss: 1.39114e-06, val loss: 2.10426e-06, min loss: 1.25373e-06\n",
      "Epoch: 865100, elapsed: 1.34e+01, train loss: 2.73517e-06, val loss: 4.11988e-06, min loss: 1.25373e-06\n",
      "Epoch: 865200, elapsed: 1.14e+01, train loss: 1.25375e-06, val loss: 2.09840e-06, min loss: 1.25373e-06\n",
      "Epoch: 865300, elapsed: 1.13e+01, train loss: 1.25406e-06, val loss: 2.08220e-06, min loss: 1.25373e-06\n",
      "Epoch: 865400, elapsed: 1.39e+01, train loss: 1.25726e-06, val loss: 2.09637e-06, min loss: 1.25373e-06\n",
      "Epoch: 865500, elapsed: 1.14e+01, train loss: 1.54774e-06, val loss: 2.31464e-06, min loss: 1.25373e-06\n",
      "Epoch: 865600, elapsed: 1.14e+01, train loss: 3.43058e-06, val loss: 4.73820e-06, min loss: 1.25373e-06\n",
      "Epoch: 865700, elapsed: 1.15e+01, train loss: 1.25041e-06, val loss: 2.09147e-06, min loss: 1.25041e-06\n",
      "Epoch: 865800, elapsed: 1.13e+01, train loss: 1.25303e-06, val loss: 2.08064e-06, min loss: 1.25041e-06\n",
      "Epoch: 865900, elapsed: 1.13e+01, train loss: 1.27773e-06, val loss: 2.09773e-06, min loss: 1.25041e-06\n",
      "Epoch: 866000, elapsed: 1.14e+01, train loss: 2.12453e-06, val loss: 2.55663e-06, min loss: 1.25041e-06\n",
      "Epoch: 866100, elapsed: 1.12e+01, train loss: 1.68708e-06, val loss: 2.62194e-06, min loss: 1.25041e-06\n",
      "Epoch: 866200, elapsed: 1.13e+01, train loss: 1.29621e-06, val loss: 2.17499e-06, min loss: 1.25041e-06\n",
      "Epoch: 866300, elapsed: 1.14e+01, train loss: 1.24921e-06, val loss: 2.08052e-06, min loss: 1.24921e-06\n",
      "Epoch: 866400, elapsed: 1.13e+01, train loss: 2.51644e-06, val loss: 3.83370e-06, min loss: 1.24921e-06\n",
      "Epoch: 866500, elapsed: 1.14e+01, train loss: 1.24839e-06, val loss: 2.08891e-06, min loss: 1.24839e-06\n",
      "Epoch: 866600, elapsed: 1.14e+01, train loss: 1.26525e-06, val loss: 2.07482e-06, min loss: 1.24839e-06\n",
      "Epoch: 866700, elapsed: 1.16e+01, train loss: 1.26098e-06, val loss: 2.08606e-06, min loss: 1.24839e-06\n",
      "Epoch: 866800, elapsed: 1.13e+01, train loss: 1.26548e-06, val loss: 2.13118e-06, min loss: 1.24839e-06\n",
      "Epoch: 866900, elapsed: 1.13e+01, train loss: 1.26154e-06, val loss: 2.10419e-06, min loss: 1.24839e-06\n",
      "Epoch: 867000, elapsed: 1.35e+01, train loss: 1.25234e-06, val loss: 2.08090e-06, min loss: 1.24839e-06\n",
      "Epoch: 867100, elapsed: 1.15e+01, train loss: 1.24792e-06, val loss: 2.08923e-06, min loss: 1.24792e-06\n",
      "Epoch: 867200, elapsed: 1.15e+01, train loss: 1.24790e-06, val loss: 2.08846e-06, min loss: 1.24790e-06\n",
      "Epoch: 867300, elapsed: 1.14e+01, train loss: 1.25384e-06, val loss: 2.08046e-06, min loss: 1.24790e-06\n",
      "Epoch: 867400, elapsed: 1.14e+01, train loss: 1.25788e-06, val loss: 2.08194e-06, min loss: 1.24790e-06\n",
      "Epoch: 867500, elapsed: 1.14e+01, train loss: 1.30769e-06, val loss: 2.11608e-06, min loss: 1.24790e-06\n",
      "Epoch: 867600, elapsed: 1.14e+01, train loss: 1.78699e-06, val loss: 2.75071e-06, min loss: 1.24790e-06\n",
      "Epoch: 867700, elapsed: 1.13e+01, train loss: 1.25115e-06, val loss: 2.10347e-06, min loss: 1.24790e-06\n",
      "Epoch: 867800, elapsed: 1.13e+01, train loss: 4.04871e-06, val loss: 4.76038e-06, min loss: 1.24790e-06\n",
      "Epoch: 867900, elapsed: 1.15e+01, train loss: 1.24478e-06, val loss: 2.08062e-06, min loss: 1.24478e-06\n",
      "Epoch: 868000, elapsed: 1.14e+01, train loss: 1.25799e-06, val loss: 2.07969e-06, min loss: 1.24478e-06\n",
      "Epoch: 868100, elapsed: 1.13e+01, train loss: 5.83806e-06, val loss: 7.25607e-06, min loss: 1.24478e-06\n",
      "Epoch: 868200, elapsed: 1.14e+01, train loss: 1.24398e-06, val loss: 2.07626e-06, min loss: 1.24398e-06\n",
      "Epoch: 868300, elapsed: 1.13e+01, train loss: 1.26164e-06, val loss: 2.07148e-06, min loss: 1.24398e-06\n",
      "Epoch: 868400, elapsed: 1.13e+01, train loss: 1.62760e-06, val loss: 2.60222e-06, min loss: 1.24398e-06\n",
      "Epoch: 868500, elapsed: 1.13e+01, train loss: 1.24339e-06, val loss: 2.07885e-06, min loss: 1.24339e-06\n",
      "Epoch: 868600, elapsed: 1.13e+01, train loss: 1.24985e-06, val loss: 2.09904e-06, min loss: 1.24339e-06\n",
      "Epoch: 868700, elapsed: 1.41e+01, train loss: 2.17863e-06, val loss: 2.51855e-06, min loss: 1.24339e-06\n",
      "Epoch: 868800, elapsed: 1.16e+01, train loss: 1.24321e-06, val loss: 2.07889e-06, min loss: 1.24321e-06\n",
      "Epoch: 868900, elapsed: 1.13e+01, train loss: 1.24742e-06, val loss: 2.07528e-06, min loss: 1.24321e-06\n",
      "Epoch: 869000, elapsed: 1.15e+01, train loss: 1.30221e-06, val loss: 2.28634e-06, min loss: 1.24321e-06\n",
      "Epoch: 869100, elapsed: 1.13e+01, train loss: 1.89861e-06, val loss: 2.62642e-06, min loss: 1.24321e-06\n",
      "Epoch: 869200, elapsed: 1.15e+01, train loss: 1.36104e-06, val loss: 2.20073e-06, min loss: 1.24321e-06\n",
      "Epoch: 869300, elapsed: 1.14e+01, train loss: 2.25196e-06, val loss: 3.38165e-06, min loss: 1.24321e-06\n",
      "Epoch: 869400, elapsed: 1.13e+01, train loss: 1.86775e-06, val loss: 2.59099e-06, min loss: 1.24321e-06\n",
      "Epoch: 869500, elapsed: 1.12e+01, train loss: 1.24413e-06, val loss: 2.07697e-06, min loss: 1.24321e-06\n",
      "Epoch: 869600, elapsed: 1.14e+01, train loss: 1.36119e-06, val loss: 2.20795e-06, min loss: 1.24321e-06\n",
      "Epoch: 869700, elapsed: 1.14e+01, train loss: 1.24610e-06, val loss: 2.07852e-06, min loss: 1.24321e-06\n",
      "Epoch: 869800, elapsed: 1.13e+01, train loss: 2.74241e-06, val loss: 4.02729e-06, min loss: 1.24321e-06\n",
      "Epoch: 869900, elapsed: 1.13e+01, train loss: 2.68339e-06, val loss: 3.67475e-06, min loss: 1.24321e-06\n",
      "Epoch: 870000, elapsed: 1.13e+01, train loss: 1.85920e-06, val loss: 2.74604e-06, min loss: 1.24321e-06\n",
      "Epoch: 870100, elapsed: 1.34e+01, train loss: 1.38939e-06, val loss: 2.29866e-06, min loss: 1.24321e-06\n",
      "Epoch: 870200, elapsed: 1.13e+01, train loss: 1.24111e-06, val loss: 2.07811e-06, min loss: 1.24111e-06\n",
      "Epoch: 870300, elapsed: 1.38e+01, train loss: 1.24189e-06, val loss: 2.06251e-06, min loss: 1.24111e-06\n",
      "Epoch: 870400, elapsed: 1.15e+01, train loss: 2.42845e-06, val loss: 3.95621e-06, min loss: 1.24111e-06\n",
      "Epoch: 870500, elapsed: 1.16e+01, train loss: 1.24002e-06, val loss: 2.07433e-06, min loss: 1.24002e-06\n",
      "Epoch: 870600, elapsed: 1.13e+01, train loss: 1.24255e-06, val loss: 2.08247e-06, min loss: 1.24002e-06\n",
      "Epoch: 870700, elapsed: 1.15e+01, train loss: 1.25262e-06, val loss: 2.05508e-06, min loss: 1.24002e-06\n",
      "Epoch: 870800, elapsed: 1.14e+01, train loss: 2.09537e-06, val loss: 3.06748e-06, min loss: 1.24002e-06\n",
      "Epoch: 870900, elapsed: 1.13e+01, train loss: 1.68632e-06, val loss: 2.53933e-06, min loss: 1.24002e-06\n",
      "Epoch: 871000, elapsed: 1.14e+01, train loss: 1.36370e-06, val loss: 2.13190e-06, min loss: 1.24002e-06\n",
      "Epoch: 871100, elapsed: 1.15e+01, train loss: 1.32875e-06, val loss: 2.17031e-06, min loss: 1.24002e-06\n",
      "Epoch: 871200, elapsed: 1.12e+01, train loss: 1.32117e-06, val loss: 2.16552e-06, min loss: 1.24002e-06\n",
      "Epoch: 871300, elapsed: 1.15e+01, train loss: 1.26005e-06, val loss: 2.13283e-06, min loss: 1.24002e-06\n",
      "Epoch: 871400, elapsed: 1.14e+01, train loss: 5.05225e-06, val loss: 5.00064e-06, min loss: 1.24002e-06\n",
      "Epoch: 871500, elapsed: 1.14e+01, train loss: 5.12806e-06, val loss: 6.18294e-06, min loss: 1.24002e-06\n",
      "Epoch: 871600, elapsed: 1.14e+01, train loss: 2.32107e-06, val loss: 3.38395e-06, min loss: 1.24002e-06\n",
      "Epoch: 871700, elapsed: 1.14e+01, train loss: 4.32030e-06, val loss: 3.97498e-06, min loss: 1.24002e-06\n",
      "Epoch: 871800, elapsed: 1.15e+01, train loss: 3.18714e-06, val loss: 4.35474e-06, min loss: 1.24002e-06\n",
      "Epoch: 871900, elapsed: 1.13e+01, train loss: 5.16945e-06, val loss: 5.66501e-06, min loss: 1.24002e-06\n",
      "Epoch: 872000, elapsed: 1.40e+01, train loss: 2.95778e-06, val loss: 3.66795e-06, min loss: 1.24002e-06\n",
      "Epoch: 872100, elapsed: 1.15e+01, train loss: 1.25618e-06, val loss: 2.06580e-06, min loss: 1.24002e-06\n",
      "Epoch: 872200, elapsed: 1.15e+01, train loss: 1.23678e-06, val loss: 2.06317e-06, min loss: 1.23678e-06\n",
      "Epoch: 872300, elapsed: 1.16e+01, train loss: 1.23694e-06, val loss: 2.05896e-06, min loss: 1.23678e-06\n",
      "Epoch: 872400, elapsed: 1.15e+01, train loss: 1.47499e-06, val loss: 2.27738e-06, min loss: 1.23678e-06\n",
      "Epoch: 872500, elapsed: 1.15e+01, train loss: 1.23490e-06, val loss: 2.06534e-06, min loss: 1.23490e-06\n",
      "Epoch: 872600, elapsed: 1.14e+01, train loss: 1.23989e-06, val loss: 2.07545e-06, min loss: 1.23490e-06\n",
      "Epoch: 872700, elapsed: 1.15e+01, train loss: 1.36823e-06, val loss: 2.15140e-06, min loss: 1.23490e-06\n",
      "Epoch: 872800, elapsed: 1.13e+01, train loss: 1.27296e-06, val loss: 2.07944e-06, min loss: 1.23490e-06\n",
      "Epoch: 872900, elapsed: 1.14e+01, train loss: 2.16881e-06, val loss: 3.31737e-06, min loss: 1.23490e-06\n",
      "Epoch: 873000, elapsed: 1.14e+01, train loss: 1.23509e-06, val loss: 2.05983e-06, min loss: 1.23490e-06\n",
      "Epoch: 873100, elapsed: 1.15e+01, train loss: 1.23862e-06, val loss: 2.06644e-06, min loss: 1.23490e-06\n",
      "Epoch: 873200, elapsed: 1.15e+01, train loss: 1.41947e-06, val loss: 2.25899e-06, min loss: 1.23490e-06\n",
      "Epoch: 873300, elapsed: 1.13e+01, train loss: 3.06100e-06, val loss: 3.21640e-06, min loss: 1.23490e-06\n",
      "Epoch: 873400, elapsed: 1.14e+01, train loss: 1.27227e-06, val loss: 2.18754e-06, min loss: 1.23490e-06\n",
      "Epoch: 873500, elapsed: 1.13e+01, train loss: 1.23373e-06, val loss: 2.05478e-06, min loss: 1.23373e-06\n",
      "Epoch: 873600, elapsed: 1.15e+01, train loss: 1.24040e-06, val loss: 2.04781e-06, min loss: 1.23373e-06\n",
      "Epoch: 873700, elapsed: 1.40e+01, train loss: 1.25305e-06, val loss: 2.08889e-06, min loss: 1.23373e-06\n",
      "Epoch: 873800, elapsed: 1.15e+01, train loss: 1.45794e-06, val loss: 2.16791e-06, min loss: 1.23373e-06\n",
      "Epoch: 873900, elapsed: 1.14e+01, train loss: 1.80861e-06, val loss: 2.45879e-06, min loss: 1.23373e-06\n",
      "Epoch: 874000, elapsed: 1.16e+01, train loss: 2.31418e-06, val loss: 2.95966e-06, min loss: 1.23373e-06\n",
      "Epoch: 874100, elapsed: 1.14e+01, train loss: 1.23415e-06, val loss: 2.06895e-06, min loss: 1.23373e-06\n",
      "Epoch: 874200, elapsed: 1.14e+01, train loss: 1.99975e-06, val loss: 3.03218e-06, min loss: 1.23373e-06\n",
      "Epoch: 874300, elapsed: 1.16e+01, train loss: 1.60400e-06, val loss: 2.48074e-06, min loss: 1.23373e-06\n",
      "Epoch: 874400, elapsed: 1.15e+01, train loss: 2.56653e-06, val loss: 3.28614e-06, min loss: 1.23373e-06\n",
      "Epoch: 874500, elapsed: 1.16e+01, train loss: 1.24278e-06, val loss: 2.06014e-06, min loss: 1.23373e-06\n",
      "Epoch: 874600, elapsed: 1.13e+01, train loss: 1.30619e-06, val loss: 2.06810e-06, min loss: 1.23373e-06\n",
      "Epoch: 874700, elapsed: 1.13e+01, train loss: 1.23189e-06, val loss: 2.06148e-06, min loss: 1.23189e-06\n",
      "Epoch: 874800, elapsed: 1.13e+01, train loss: 1.28278e-06, val loss: 2.08059e-06, min loss: 1.23189e-06\n",
      "Epoch: 874900, elapsed: 1.13e+01, train loss: 1.24308e-06, val loss: 2.06807e-06, min loss: 1.23189e-06\n",
      "Epoch: 875000, elapsed: 1.13e+01, train loss: 1.31195e-06, val loss: 2.21051e-06, min loss: 1.23189e-06\n",
      "Epoch: 875100, elapsed: 1.34e+01, train loss: 1.23121e-06, val loss: 2.06115e-06, min loss: 1.23121e-06\n",
      "Epoch: 875200, elapsed: 1.13e+01, train loss: 1.23347e-06, val loss: 2.06048e-06, min loss: 1.23121e-06\n",
      "Epoch: 875300, elapsed: 1.38e+01, train loss: 1.25707e-06, val loss: 2.11906e-06, min loss: 1.23121e-06\n",
      "Epoch: 875400, elapsed: 1.15e+01, train loss: 1.39493e-06, val loss: 2.29332e-06, min loss: 1.23121e-06\n",
      "Epoch: 875500, elapsed: 1.14e+01, train loss: 1.31205e-06, val loss: 2.19961e-06, min loss: 1.23121e-06\n",
      "Epoch: 875600, elapsed: 1.15e+01, train loss: 1.37864e-06, val loss: 2.38061e-06, min loss: 1.23121e-06\n",
      "Epoch: 875700, elapsed: 1.15e+01, train loss: 1.30387e-06, val loss: 2.13331e-06, min loss: 1.23121e-06\n",
      "Epoch: 875800, elapsed: 1.13e+01, train loss: 1.25029e-06, val loss: 2.11674e-06, min loss: 1.23121e-06\n",
      "Epoch: 875900, elapsed: 1.15e+01, train loss: 1.89304e-06, val loss: 2.86382e-06, min loss: 1.23121e-06\n",
      "Epoch: 876000, elapsed: 1.14e+01, train loss: 4.97296e-06, val loss: 4.56114e-06, min loss: 1.23121e-06\n",
      "Epoch: 876100, elapsed: 1.14e+01, train loss: 1.35865e-06, val loss: 2.13577e-06, min loss: 1.23121e-06\n",
      "Epoch: 876200, elapsed: 1.14e+01, train loss: 1.49588e-06, val loss: 2.32938e-06, min loss: 1.23121e-06\n",
      "Epoch: 876300, elapsed: 1.13e+01, train loss: 3.49851e-06, val loss: 4.52351e-06, min loss: 1.23121e-06\n",
      "Epoch: 876400, elapsed: 1.12e+01, train loss: 3.66426e-06, val loss: 4.56169e-06, min loss: 1.23121e-06\n",
      "Epoch: 876500, elapsed: 1.13e+01, train loss: 1.49806e-06, val loss: 2.33388e-06, min loss: 1.23121e-06\n",
      "Epoch: 876600, elapsed: 1.14e+01, train loss: 1.29799e-06, val loss: 2.06390e-06, min loss: 1.23121e-06\n",
      "Epoch: 876700, elapsed: 1.13e+01, train loss: 1.76230e-06, val loss: 2.49473e-06, min loss: 1.23121e-06\n",
      "Epoch: 876800, elapsed: 1.14e+01, train loss: 1.22618e-06, val loss: 2.04990e-06, min loss: 1.22618e-06\n",
      "Epoch: 876900, elapsed: 1.14e+01, train loss: 1.24454e-06, val loss: 2.07617e-06, min loss: 1.22618e-06\n",
      "Epoch: 877000, elapsed: 1.41e+01, train loss: 1.61714e-06, val loss: 2.63134e-06, min loss: 1.22618e-06\n",
      "Epoch: 877100, elapsed: 1.15e+01, train loss: 4.04150e-06, val loss: 5.01630e-06, min loss: 1.22618e-06\n",
      "Epoch: 877200, elapsed: 1.16e+01, train loss: 4.60314e-06, val loss: 5.61973e-06, min loss: 1.22618e-06\n",
      "Epoch: 877300, elapsed: 1.15e+01, train loss: 1.32129e-06, val loss: 2.22780e-06, min loss: 1.22618e-06\n",
      "Epoch: 877400, elapsed: 1.15e+01, train loss: 1.27636e-06, val loss: 2.10725e-06, min loss: 1.22618e-06\n",
      "Epoch: 877500, elapsed: 1.15e+01, train loss: 1.23773e-06, val loss: 2.05356e-06, min loss: 1.22618e-06\n",
      "Epoch: 877600, elapsed: 1.13e+01, train loss: 1.28857e-06, val loss: 2.09722e-06, min loss: 1.22618e-06\n",
      "Epoch: 877700, elapsed: 1.13e+01, train loss: 1.23994e-06, val loss: 2.03294e-06, min loss: 1.22618e-06\n",
      "Epoch: 877800, elapsed: 1.14e+01, train loss: 1.25147e-06, val loss: 2.07098e-06, min loss: 1.22618e-06\n",
      "Epoch: 877900, elapsed: 1.14e+01, train loss: 1.38794e-06, val loss: 2.18730e-06, min loss: 1.22618e-06\n",
      "Epoch: 878000, elapsed: 1.13e+01, train loss: 1.31861e-06, val loss: 2.28179e-06, min loss: 1.22618e-06\n",
      "Epoch: 878100, elapsed: 1.13e+01, train loss: 1.32014e-06, val loss: 2.13816e-06, min loss: 1.22618e-06\n",
      "Epoch: 878200, elapsed: 1.13e+01, train loss: 1.29940e-06, val loss: 2.06050e-06, min loss: 1.22618e-06\n",
      "Epoch: 878300, elapsed: 1.14e+01, train loss: 1.74845e-06, val loss: 2.72443e-06, min loss: 1.22618e-06\n",
      "Epoch: 878400, elapsed: 1.13e+01, train loss: 1.24453e-06, val loss: 2.05816e-06, min loss: 1.22618e-06\n",
      "Epoch: 878500, elapsed: 1.14e+01, train loss: 1.22407e-06, val loss: 2.04156e-06, min loss: 1.22407e-06\n",
      "Epoch: 878600, elapsed: 1.39e+01, train loss: 1.25270e-06, val loss: 2.05774e-06, min loss: 1.22407e-06\n",
      "Epoch: 878700, elapsed: 1.15e+01, train loss: 1.24896e-06, val loss: 2.08893e-06, min loss: 1.22407e-06\n",
      "Epoch: 878800, elapsed: 1.15e+01, train loss: 1.33697e-06, val loss: 2.11705e-06, min loss: 1.22407e-06\n",
      "Epoch: 878900, elapsed: 1.14e+01, train loss: 1.24858e-06, val loss: 2.03944e-06, min loss: 1.22407e-06\n",
      "Epoch: 879000, elapsed: 1.14e+01, train loss: 1.22803e-06, val loss: 2.05575e-06, min loss: 1.22407e-06\n",
      "Epoch: 879100, elapsed: 1.16e+01, train loss: 1.93097e-06, val loss: 2.61249e-06, min loss: 1.22407e-06\n",
      "Epoch: 879200, elapsed: 1.15e+01, train loss: 2.13409e-06, val loss: 2.93710e-06, min loss: 1.22407e-06\n",
      "Epoch: 879300, elapsed: 1.13e+01, train loss: 1.68456e-06, val loss: 2.40416e-06, min loss: 1.22407e-06\n",
      "Epoch: 879400, elapsed: 1.13e+01, train loss: 2.29682e-06, val loss: 2.47497e-06, min loss: 1.22407e-06\n",
      "Epoch: 879500, elapsed: 1.13e+01, train loss: 1.30457e-06, val loss: 2.26914e-06, min loss: 1.22407e-06\n",
      "Epoch: 879600, elapsed: 1.14e+01, train loss: 1.25220e-06, val loss: 2.04313e-06, min loss: 1.22407e-06\n",
      "Epoch: 879700, elapsed: 1.13e+01, train loss: 2.53420e-06, val loss: 3.51594e-06, min loss: 1.22407e-06\n",
      "Epoch: 879800, elapsed: 1.12e+01, train loss: 1.89828e-06, val loss: 2.75438e-06, min loss: 1.22407e-06\n",
      "Epoch: 879900, elapsed: 1.14e+01, train loss: 1.22038e-06, val loss: 2.04296e-06, min loss: 1.22038e-06\n",
      "Epoch: 880000, elapsed: 1.15e+01, train loss: 1.22755e-06, val loss: 2.05102e-06, min loss: 1.22038e-06\n",
      "Epoch: 880100, elapsed: 1.33e+01, train loss: 1.78755e-06, val loss: 3.13211e-06, min loss: 1.22038e-06\n",
      "Epoch: 880200, elapsed: 1.14e+01, train loss: 1.47527e-06, val loss: 2.48928e-06, min loss: 1.22038e-06\n",
      "Epoch: 880300, elapsed: 1.43e+01, train loss: 1.56443e-06, val loss: 2.37384e-06, min loss: 1.22038e-06\n",
      "Epoch: 880400, elapsed: 1.15e+01, train loss: 2.00798e-06, val loss: 2.81904e-06, min loss: 1.22038e-06\n",
      "Epoch: 880500, elapsed: 1.15e+01, train loss: 1.31489e-06, val loss: 2.13010e-06, min loss: 1.22038e-06\n",
      "Epoch: 880600, elapsed: 1.15e+01, train loss: 1.23458e-06, val loss: 2.05124e-06, min loss: 1.22038e-06\n",
      "Epoch: 880700, elapsed: 1.15e+01, train loss: 1.24728e-06, val loss: 2.08856e-06, min loss: 1.22038e-06\n",
      "Epoch: 880800, elapsed: 1.16e+01, train loss: 1.22461e-06, val loss: 2.05141e-06, min loss: 1.22038e-06\n",
      "Epoch: 880900, elapsed: 1.16e+01, train loss: 1.22972e-06, val loss: 2.04301e-06, min loss: 1.22038e-06\n",
      "Epoch: 881000, elapsed: 1.15e+01, train loss: 1.22945e-06, val loss: 2.05633e-06, min loss: 1.22038e-06\n",
      "Epoch: 881100, elapsed: 1.13e+01, train loss: 1.22742e-06, val loss: 2.03422e-06, min loss: 1.22038e-06\n",
      "Epoch: 881200, elapsed: 1.13e+01, train loss: 1.34799e-06, val loss: 2.21636e-06, min loss: 1.22038e-06\n",
      "Epoch: 881300, elapsed: 1.12e+01, train loss: 1.57275e-06, val loss: 2.11167e-06, min loss: 1.22038e-06\n",
      "Epoch: 881400, elapsed: 1.14e+01, train loss: 1.52738e-06, val loss: 2.34171e-06, min loss: 1.22038e-06\n",
      "Epoch: 881500, elapsed: 1.13e+01, train loss: 1.55835e-06, val loss: 2.19110e-06, min loss: 1.22038e-06\n",
      "Epoch: 881600, elapsed: 1.13e+01, train loss: 5.52945e-06, val loss: 6.56807e-06, min loss: 1.22038e-06\n",
      "Epoch: 881700, elapsed: 1.13e+01, train loss: 1.77393e-06, val loss: 2.44805e-06, min loss: 1.22038e-06\n",
      "Epoch: 881800, elapsed: 1.15e+01, train loss: 1.83785e-06, val loss: 2.69880e-06, min loss: 1.22038e-06\n",
      "Epoch: 881900, elapsed: 1.13e+01, train loss: 1.44305e-06, val loss: 2.19325e-06, min loss: 1.22038e-06\n",
      "Epoch: 882000, elapsed: 1.42e+01, train loss: 3.07054e-06, val loss: 4.49714e-06, min loss: 1.22038e-06\n",
      "Epoch: 882100, elapsed: 1.14e+01, train loss: 1.21708e-06, val loss: 2.04125e-06, min loss: 1.21708e-06\n",
      "Epoch: 882200, elapsed: 1.13e+01, train loss: 1.22146e-06, val loss: 2.04215e-06, min loss: 1.21708e-06\n",
      "Epoch: 882300, elapsed: 1.13e+01, train loss: 1.22351e-06, val loss: 2.04399e-06, min loss: 1.21708e-06\n",
      "Epoch: 882400, elapsed: 1.15e+01, train loss: 1.49059e-06, val loss: 2.37175e-06, min loss: 1.21708e-06\n",
      "Epoch: 882500, elapsed: 1.16e+01, train loss: 1.25426e-06, val loss: 2.07081e-06, min loss: 1.21708e-06\n",
      "Epoch: 882600, elapsed: 1.15e+01, train loss: 1.22316e-06, val loss: 2.02574e-06, min loss: 1.21708e-06\n",
      "Epoch: 882700, elapsed: 1.13e+01, train loss: 1.25319e-06, val loss: 2.13826e-06, min loss: 1.21708e-06\n",
      "Epoch: 882800, elapsed: 1.14e+01, train loss: 1.21607e-06, val loss: 2.03313e-06, min loss: 1.21607e-06\n",
      "Epoch: 882900, elapsed: 1.14e+01, train loss: 1.29610e-06, val loss: 2.08159e-06, min loss: 1.21607e-06\n",
      "Epoch: 883000, elapsed: 1.12e+01, train loss: 1.34681e-06, val loss: 2.08056e-06, min loss: 1.21607e-06\n",
      "Epoch: 883100, elapsed: 1.14e+01, train loss: 1.44400e-06, val loss: 2.06802e-06, min loss: 1.21607e-06\n",
      "Epoch: 883200, elapsed: 1.12e+01, train loss: 1.32751e-06, val loss: 2.06171e-06, min loss: 1.21607e-06\n",
      "Epoch: 883300, elapsed: 1.12e+01, train loss: 2.79452e-06, val loss: 2.84314e-06, min loss: 1.21607e-06\n",
      "Epoch: 883400, elapsed: 1.13e+01, train loss: 2.31999e-06, val loss: 3.42746e-06, min loss: 1.21607e-06\n",
      "Epoch: 883500, elapsed: 1.14e+01, train loss: 4.46421e-06, val loss: 5.54419e-06, min loss: 1.21607e-06\n",
      "Epoch: 883600, elapsed: 1.38e+01, train loss: 1.93303e-06, val loss: 2.86688e-06, min loss: 1.21607e-06\n",
      "Epoch: 883700, elapsed: 1.14e+01, train loss: 4.79361e-06, val loss: 5.34031e-06, min loss: 1.21607e-06\n",
      "Epoch: 883800, elapsed: 1.15e+01, train loss: 2.24193e-06, val loss: 3.20180e-06, min loss: 1.21607e-06\n",
      "Epoch: 883900, elapsed: 1.14e+01, train loss: 1.21419e-06, val loss: 2.03464e-06, min loss: 1.21419e-06\n",
      "Epoch: 884000, elapsed: 1.15e+01, train loss: 1.21812e-06, val loss: 2.03319e-06, min loss: 1.21419e-06\n",
      "Epoch: 884100, elapsed: 1.14e+01, train loss: 1.28765e-06, val loss: 2.28951e-06, min loss: 1.21419e-06\n",
      "Epoch: 884200, elapsed: 1.14e+01, train loss: 1.21244e-06, val loss: 2.02961e-06, min loss: 1.21244e-06\n",
      "Epoch: 884300, elapsed: 1.14e+01, train loss: 1.21221e-06, val loss: 2.03155e-06, min loss: 1.21221e-06\n",
      "Epoch: 884400, elapsed: 1.13e+01, train loss: 1.48187e-06, val loss: 2.41272e-06, min loss: 1.21221e-06\n",
      "Epoch: 884500, elapsed: 1.14e+01, train loss: 2.06077e-06, val loss: 3.16999e-06, min loss: 1.21221e-06\n",
      "Epoch: 884600, elapsed: 1.16e+01, train loss: 1.31394e-06, val loss: 2.10159e-06, min loss: 1.21221e-06\n",
      "Epoch: 884700, elapsed: 1.13e+01, train loss: 1.21415e-06, val loss: 2.02083e-06, min loss: 1.21221e-06\n",
      "Epoch: 884800, elapsed: 1.13e+01, train loss: 1.21663e-06, val loss: 2.01508e-06, min loss: 1.21221e-06\n",
      "Epoch: 884900, elapsed: 1.14e+01, train loss: 1.26217e-06, val loss: 2.03791e-06, min loss: 1.21221e-06\n",
      "Epoch: 885000, elapsed: 1.15e+01, train loss: 1.26255e-06, val loss: 2.04712e-06, min loss: 1.21221e-06\n",
      "Epoch: 885100, elapsed: 1.39e+01, train loss: 2.84559e-06, val loss: 3.57380e-06, min loss: 1.21221e-06\n",
      "Epoch: 885200, elapsed: 1.14e+01, train loss: 1.24037e-06, val loss: 2.04695e-06, min loss: 1.21221e-06\n",
      "Epoch: 885300, elapsed: 1.40e+01, train loss: 1.23436e-06, val loss: 2.05730e-06, min loss: 1.21221e-06\n",
      "Epoch: 885400, elapsed: 1.15e+01, train loss: 1.21345e-06, val loss: 2.02963e-06, min loss: 1.21221e-06\n",
      "Epoch: 885500, elapsed: 1.13e+01, train loss: 1.23816e-06, val loss: 2.03473e-06, min loss: 1.21221e-06\n",
      "Epoch: 885600, elapsed: 1.15e+01, train loss: 1.23505e-06, val loss: 2.04391e-06, min loss: 1.21221e-06\n",
      "Epoch: 885700, elapsed: 1.13e+01, train loss: 1.22271e-06, val loss: 2.03646e-06, min loss: 1.21221e-06\n",
      "Epoch: 885800, elapsed: 1.14e+01, train loss: 1.50636e-06, val loss: 2.17635e-06, min loss: 1.21221e-06\n",
      "Epoch: 885900, elapsed: 1.13e+01, train loss: 1.59241e-06, val loss: 2.49855e-06, min loss: 1.21221e-06\n",
      "Epoch: 886000, elapsed: 1.13e+01, train loss: 1.23492e-06, val loss: 2.08620e-06, min loss: 1.21221e-06\n",
      "Epoch: 886100, elapsed: 1.14e+01, train loss: 1.26899e-06, val loss: 2.10037e-06, min loss: 1.21221e-06\n",
      "Epoch: 886200, elapsed: 1.13e+01, train loss: 1.22073e-06, val loss: 2.05110e-06, min loss: 1.21221e-06\n",
      "Epoch: 886300, elapsed: 1.13e+01, train loss: 1.29557e-06, val loss: 2.05665e-06, min loss: 1.21221e-06\n",
      "Epoch: 886400, elapsed: 1.14e+01, train loss: 1.56632e-06, val loss: 2.66850e-06, min loss: 1.21221e-06\n",
      "Epoch: 886500, elapsed: 1.15e+01, train loss: 2.06713e-06, val loss: 3.02532e-06, min loss: 1.21221e-06\n",
      "Epoch: 886600, elapsed: 1.13e+01, train loss: 1.51243e-06, val loss: 2.25571e-06, min loss: 1.21221e-06\n",
      "Epoch: 886700, elapsed: 1.14e+01, train loss: 1.23137e-06, val loss: 2.01176e-06, min loss: 1.21221e-06\n",
      "Epoch: 886800, elapsed: 1.14e+01, train loss: 1.22688e-06, val loss: 2.02610e-06, min loss: 1.21221e-06\n",
      "Epoch: 886900, elapsed: 1.13e+01, train loss: 1.24601e-06, val loss: 2.03882e-06, min loss: 1.21221e-06\n",
      "Epoch: 887000, elapsed: 1.40e+01, train loss: 1.49959e-06, val loss: 2.31280e-06, min loss: 1.21221e-06\n",
      "Epoch: 887100, elapsed: 1.16e+01, train loss: 2.50253e-06, val loss: 3.27629e-06, min loss: 1.21221e-06\n",
      "Epoch: 887200, elapsed: 1.15e+01, train loss: 2.02861e-06, val loss: 3.55635e-06, min loss: 1.21221e-06\n",
      "Epoch: 887300, elapsed: 1.14e+01, train loss: 1.20716e-06, val loss: 2.01662e-06, min loss: 1.20716e-06\n",
      "Epoch: 887400, elapsed: 1.13e+01, train loss: 1.20950e-06, val loss: 2.01940e-06, min loss: 1.20716e-06\n",
      "Epoch: 887500, elapsed: 1.14e+01, train loss: 1.21690e-06, val loss: 2.02435e-06, min loss: 1.20716e-06\n",
      "Epoch: 887600, elapsed: 1.12e+01, train loss: 3.84732e-06, val loss: 2.95416e-06, min loss: 1.20716e-06\n",
      "Epoch: 887700, elapsed: 1.12e+01, train loss: 1.20584e-06, val loss: 2.01870e-06, min loss: 1.20584e-06\n",
      "Epoch: 887800, elapsed: 1.15e+01, train loss: 1.25088e-06, val loss: 2.03522e-06, min loss: 1.20584e-06\n",
      "Epoch: 887900, elapsed: 1.13e+01, train loss: 1.23134e-06, val loss: 2.12284e-06, min loss: 1.20584e-06\n",
      "Epoch: 888000, elapsed: 1.15e+01, train loss: 1.28092e-06, val loss: 2.08365e-06, min loss: 1.20584e-06\n",
      "Epoch: 888100, elapsed: 1.15e+01, train loss: 2.06862e-06, val loss: 3.02254e-06, min loss: 1.20584e-06\n",
      "Epoch: 888200, elapsed: 1.16e+01, train loss: 1.75493e-06, val loss: 2.88194e-06, min loss: 1.20584e-06\n",
      "Epoch: 888300, elapsed: 1.13e+01, train loss: 1.43784e-06, val loss: 2.09496e-06, min loss: 1.20584e-06\n",
      "Epoch: 888400, elapsed: 1.13e+01, train loss: 1.35025e-06, val loss: 2.22839e-06, min loss: 1.20584e-06\n",
      "Epoch: 888500, elapsed: 1.13e+01, train loss: 1.60618e-06, val loss: 2.27463e-06, min loss: 1.20584e-06\n",
      "Epoch: 888600, elapsed: 1.14e+01, train loss: 6.68408e-06, val loss: 7.18610e-06, min loss: 1.20584e-06\n",
      "Epoch: 888700, elapsed: 1.40e+01, train loss: 1.80359e-06, val loss: 2.89985e-06, min loss: 1.20584e-06\n",
      "Epoch: 888800, elapsed: 1.15e+01, train loss: 3.14150e-06, val loss: 4.29757e-06, min loss: 1.20584e-06\n",
      "Epoch: 888900, elapsed: 1.13e+01, train loss: 1.36043e-06, val loss: 2.16495e-06, min loss: 1.20584e-06\n",
      "Epoch: 889000, elapsed: 1.17e+01, train loss: 1.64664e-06, val loss: 2.12007e-06, min loss: 1.20584e-06\n",
      "Epoch: 889100, elapsed: 1.13e+01, train loss: 1.74102e-06, val loss: 2.43548e-06, min loss: 1.20584e-06\n",
      "Epoch: 889200, elapsed: 1.16e+01, train loss: 3.08007e-06, val loss: 4.50779e-06, min loss: 1.20584e-06\n",
      "Epoch: 889300, elapsed: 1.14e+01, train loss: 1.27881e-06, val loss: 2.23853e-06, min loss: 1.20584e-06\n",
      "Epoch: 889400, elapsed: 1.14e+01, train loss: 1.26200e-06, val loss: 2.15967e-06, min loss: 1.20584e-06\n",
      "Epoch: 889500, elapsed: 1.14e+01, train loss: 1.20481e-06, val loss: 2.00999e-06, min loss: 1.20481e-06\n",
      "Epoch: 889600, elapsed: 1.15e+01, train loss: 1.25598e-06, val loss: 2.11096e-06, min loss: 1.20481e-06\n",
      "Epoch: 889700, elapsed: 1.14e+01, train loss: 1.26902e-06, val loss: 2.04861e-06, min loss: 1.20481e-06\n",
      "Epoch: 889800, elapsed: 1.14e+01, train loss: 1.23696e-06, val loss: 2.06163e-06, min loss: 1.20481e-06\n",
      "Epoch: 889900, elapsed: 1.14e+01, train loss: 1.23911e-06, val loss: 2.04439e-06, min loss: 1.20481e-06\n",
      "Epoch: 890000, elapsed: 1.13e+01, train loss: 2.17027e-06, val loss: 2.25349e-06, min loss: 1.20481e-06\n",
      "Epoch: 890100, elapsed: 1.34e+01, train loss: 1.36376e-06, val loss: 2.23513e-06, min loss: 1.20481e-06\n",
      "Epoch: 890200, elapsed: 1.12e+01, train loss: 2.83047e-06, val loss: 3.72878e-06, min loss: 1.20481e-06\n",
      "Epoch: 890300, elapsed: 1.40e+01, train loss: 2.20127e-06, val loss: 2.75595e-06, min loss: 1.20481e-06\n",
      "Epoch: 890400, elapsed: 1.14e+01, train loss: 1.35448e-06, val loss: 2.08462e-06, min loss: 1.20481e-06\n",
      "Epoch: 890500, elapsed: 1.14e+01, train loss: 2.60418e-06, val loss: 3.67022e-06, min loss: 1.20481e-06\n",
      "Epoch: 890600, elapsed: 1.15e+01, train loss: 1.20344e-06, val loss: 2.01391e-06, min loss: 1.20344e-06\n",
      "Epoch: 890700, elapsed: 1.12e+01, train loss: 1.20200e-06, val loss: 2.01621e-06, min loss: 1.20200e-06\n",
      "Epoch: 890800, elapsed: 1.14e+01, train loss: 1.22680e-06, val loss: 2.00603e-06, min loss: 1.20200e-06\n",
      "Epoch: 890900, elapsed: 1.14e+01, train loss: 1.42291e-06, val loss: 2.24308e-06, min loss: 1.20200e-06\n",
      "Epoch: 891000, elapsed: 1.14e+01, train loss: 1.45669e-06, val loss: 2.24622e-06, min loss: 1.20200e-06\n",
      "Epoch: 891100, elapsed: 1.14e+01, train loss: 1.20644e-06, val loss: 2.01123e-06, min loss: 1.20200e-06\n",
      "Epoch: 891200, elapsed: 1.14e+01, train loss: 1.56357e-06, val loss: 2.50800e-06, min loss: 1.20200e-06\n",
      "Epoch: 891300, elapsed: 1.15e+01, train loss: 5.09445e-06, val loss: 5.97899e-06, min loss: 1.20200e-06\n",
      "Epoch: 891400, elapsed: 1.14e+01, train loss: 3.07606e-06, val loss: 3.99982e-06, min loss: 1.20200e-06\n",
      "Epoch: 891500, elapsed: 1.13e+01, train loss: 1.85543e-06, val loss: 2.22038e-06, min loss: 1.20200e-06\n",
      "Epoch: 891600, elapsed: 1.15e+01, train loss: 2.95431e-06, val loss: 4.49736e-06, min loss: 1.20200e-06\n",
      "Epoch: 891700, elapsed: 1.15e+01, train loss: 1.20037e-06, val loss: 2.01486e-06, min loss: 1.20037e-06\n",
      "Epoch: 891800, elapsed: 1.12e+01, train loss: 1.20917e-06, val loss: 2.00477e-06, min loss: 1.20037e-06\n",
      "Epoch: 891900, elapsed: 1.13e+01, train loss: 2.79257e-06, val loss: 2.82201e-06, min loss: 1.20037e-06\n",
      "Epoch: 892000, elapsed: 1.38e+01, train loss: 4.30423e-06, val loss: 4.06470e-06, min loss: 1.20037e-06\n",
      "Epoch: 892100, elapsed: 1.15e+01, train loss: 4.52891e-06, val loss: 4.58056e-06, min loss: 1.20037e-06\n",
      "Epoch: 892200, elapsed: 1.14e+01, train loss: 2.99459e-06, val loss: 4.52641e-06, min loss: 1.20037e-06\n",
      "Epoch: 892300, elapsed: 1.15e+01, train loss: 1.19847e-06, val loss: 2.01378e-06, min loss: 1.19847e-06\n",
      "Epoch: 892400, elapsed: 1.14e+01, train loss: 1.23298e-06, val loss: 2.01246e-06, min loss: 1.19847e-06\n",
      "Epoch: 892500, elapsed: 1.15e+01, train loss: 1.45656e-06, val loss: 2.29766e-06, min loss: 1.19847e-06\n",
      "Epoch: 892600, elapsed: 1.15e+01, train loss: 2.75429e-06, val loss: 2.85390e-06, min loss: 1.19847e-06\n",
      "Epoch: 892700, elapsed: 1.13e+01, train loss: 1.23390e-06, val loss: 2.03783e-06, min loss: 1.19847e-06\n",
      "Epoch: 892800, elapsed: 1.13e+01, train loss: 1.39411e-06, val loss: 2.18958e-06, min loss: 1.19847e-06\n",
      "Epoch: 892900, elapsed: 1.14e+01, train loss: 2.86241e-06, val loss: 3.86892e-06, min loss: 1.19847e-06\n",
      "Epoch: 893000, elapsed: 1.13e+01, train loss: 1.67061e-06, val loss: 2.65761e-06, min loss: 1.19847e-06\n",
      "Epoch: 893100, elapsed: 1.14e+01, train loss: 1.22572e-06, val loss: 2.03529e-06, min loss: 1.19847e-06\n",
      "Epoch: 893200, elapsed: 1.13e+01, train loss: 1.19950e-06, val loss: 2.01019e-06, min loss: 1.19847e-06\n",
      "Epoch: 893300, elapsed: 1.14e+01, train loss: 1.19735e-06, val loss: 2.00700e-06, min loss: 1.19735e-06\n",
      "Epoch: 893400, elapsed: 1.13e+01, train loss: 1.28113e-06, val loss: 2.05466e-06, min loss: 1.19735e-06\n",
      "Epoch: 893500, elapsed: 1.13e+01, train loss: 2.06124e-06, val loss: 2.70920e-06, min loss: 1.19735e-06\n",
      "Epoch: 893600, elapsed: 1.13e+01, train loss: 1.24881e-06, val loss: 2.09811e-06, min loss: 1.19735e-06\n",
      "Epoch: 893700, elapsed: 1.37e+01, train loss: 1.19738e-06, val loss: 1.99727e-06, min loss: 1.19735e-06\n",
      "Epoch: 893800, elapsed: 1.15e+01, train loss: 1.21631e-06, val loss: 2.02231e-06, min loss: 1.19735e-06\n",
      "Epoch: 893900, elapsed: 1.15e+01, train loss: 1.45070e-06, val loss: 2.31052e-06, min loss: 1.19735e-06\n",
      "Epoch: 894000, elapsed: 1.13e+01, train loss: 1.46151e-06, val loss: 2.14446e-06, min loss: 1.19735e-06\n",
      "Epoch: 894100, elapsed: 1.14e+01, train loss: 1.25036e-06, val loss: 2.06907e-06, min loss: 1.19735e-06\n",
      "Epoch: 894200, elapsed: 1.14e+01, train loss: 1.59563e-06, val loss: 2.36561e-06, min loss: 1.19735e-06\n",
      "Epoch: 894300, elapsed: 1.14e+01, train loss: 2.78240e-06, val loss: 2.97308e-06, min loss: 1.19735e-06\n",
      "Epoch: 894400, elapsed: 1.13e+01, train loss: 2.12211e-06, val loss: 2.77905e-06, min loss: 1.19735e-06\n",
      "Epoch: 894500, elapsed: 1.14e+01, train loss: 1.76241e-06, val loss: 2.51697e-06, min loss: 1.19735e-06\n",
      "Epoch: 894600, elapsed: 1.13e+01, train loss: 1.26804e-06, val loss: 2.04261e-06, min loss: 1.19735e-06\n",
      "Epoch: 894700, elapsed: 1.13e+01, train loss: 1.20324e-06, val loss: 2.01704e-06, min loss: 1.19735e-06\n",
      "Epoch: 894800, elapsed: 1.16e+01, train loss: 1.25973e-06, val loss: 2.05124e-06, min loss: 1.19735e-06\n",
      "Epoch: 894900, elapsed: 1.14e+01, train loss: 1.91760e-06, val loss: 2.82795e-06, min loss: 1.19735e-06\n",
      "Epoch: 895000, elapsed: 1.14e+01, train loss: 2.34952e-06, val loss: 3.13788e-06, min loss: 1.19735e-06\n",
      "Epoch: 895100, elapsed: 1.35e+01, train loss: 1.34108e-06, val loss: 2.21079e-06, min loss: 1.19735e-06\n",
      "Epoch: 895200, elapsed: 1.13e+01, train loss: 1.25142e-06, val loss: 2.09003e-06, min loss: 1.19735e-06\n",
      "Epoch: 895300, elapsed: 1.13e+01, train loss: 1.19363e-06, val loss: 2.00418e-06, min loss: 1.19363e-06\n",
      "Epoch: 895400, elapsed: 1.40e+01, train loss: 1.32972e-06, val loss: 2.46467e-06, min loss: 1.19363e-06\n",
      "Epoch: 895500, elapsed: 1.15e+01, train loss: 1.20289e-06, val loss: 2.00951e-06, min loss: 1.19363e-06\n",
      "Epoch: 895600, elapsed: 1.15e+01, train loss: 1.19529e-06, val loss: 2.00274e-06, min loss: 1.19363e-06\n",
      "Epoch: 895700, elapsed: 1.15e+01, train loss: 1.24199e-06, val loss: 2.03106e-06, min loss: 1.19363e-06\n",
      "Epoch: 895800, elapsed: 1.13e+01, train loss: 1.28216e-06, val loss: 2.08412e-06, min loss: 1.19363e-06\n",
      "Epoch: 895900, elapsed: 1.15e+01, train loss: 2.06190e-06, val loss: 3.03797e-06, min loss: 1.19363e-06\n",
      "Epoch: 896000, elapsed: 1.16e+01, train loss: 1.65219e-06, val loss: 2.57578e-06, min loss: 1.19363e-06\n",
      "Epoch: 896100, elapsed: 1.15e+01, train loss: 1.19190e-06, val loss: 1.99742e-06, min loss: 1.19190e-06\n",
      "Epoch: 896200, elapsed: 1.14e+01, train loss: 1.19421e-06, val loss: 1.98904e-06, min loss: 1.19190e-06\n",
      "Epoch: 896300, elapsed: 1.15e+01, train loss: 1.31048e-06, val loss: 2.06163e-06, min loss: 1.19190e-06\n",
      "Epoch: 896400, elapsed: 1.13e+01, train loss: 1.20006e-06, val loss: 1.98786e-06, min loss: 1.19190e-06\n",
      "Epoch: 896500, elapsed: 1.13e+01, train loss: 1.19321e-06, val loss: 1.99081e-06, min loss: 1.19190e-06\n",
      "Epoch: 896600, elapsed: 1.15e+01, train loss: 1.21299e-06, val loss: 1.98601e-06, min loss: 1.19190e-06\n",
      "Epoch: 896700, elapsed: 1.14e+01, train loss: 1.50475e-06, val loss: 2.31542e-06, min loss: 1.19190e-06\n",
      "Epoch: 896800, elapsed: 1.14e+01, train loss: 2.55611e-06, val loss: 3.39675e-06, min loss: 1.19190e-06\n",
      "Epoch: 896900, elapsed: 1.14e+01, train loss: 1.92034e-06, val loss: 2.18161e-06, min loss: 1.19190e-06\n",
      "Epoch: 897000, elapsed: 1.13e+01, train loss: 1.29603e-06, val loss: 2.15780e-06, min loss: 1.19190e-06\n",
      "Epoch: 897100, elapsed: 1.41e+01, train loss: 3.23780e-06, val loss: 3.94396e-06, min loss: 1.19190e-06\n",
      "Epoch: 897200, elapsed: 1.17e+01, train loss: 1.26451e-06, val loss: 2.16462e-06, min loss: 1.19190e-06\n",
      "Epoch: 897300, elapsed: 1.14e+01, train loss: 1.22370e-06, val loss: 1.99128e-06, min loss: 1.19190e-06\n",
      "Epoch: 897400, elapsed: 1.15e+01, train loss: 1.19717e-06, val loss: 1.99438e-06, min loss: 1.19190e-06\n",
      "Epoch: 897500, elapsed: 1.17e+01, train loss: 1.24843e-06, val loss: 2.07494e-06, min loss: 1.19190e-06\n",
      "Epoch: 897600, elapsed: 1.14e+01, train loss: 1.19344e-06, val loss: 1.98641e-06, min loss: 1.19190e-06\n",
      "Epoch: 897700, elapsed: 1.13e+01, train loss: 1.19177e-06, val loss: 1.99938e-06, min loss: 1.19177e-06\n",
      "Epoch: 897800, elapsed: 1.14e+01, train loss: 1.19501e-06, val loss: 1.99511e-06, min loss: 1.19177e-06\n",
      "Epoch: 897900, elapsed: 1.11e+01, train loss: 1.86380e-06, val loss: 2.41718e-06, min loss: 1.19177e-06\n",
      "Epoch: 898000, elapsed: 1.12e+01, train loss: 1.52076e-06, val loss: 2.58946e-06, min loss: 1.19177e-06\n",
      "Epoch: 898100, elapsed: 1.13e+01, train loss: 1.20451e-06, val loss: 2.00394e-06, min loss: 1.19177e-06\n",
      "Epoch: 898200, elapsed: 1.14e+01, train loss: 1.19269e-06, val loss: 1.99229e-06, min loss: 1.19177e-06\n",
      "Epoch: 898300, elapsed: 1.15e+01, train loss: 1.20025e-06, val loss: 2.02443e-06, min loss: 1.19177e-06\n",
      "Epoch: 898400, elapsed: 1.15e+01, train loss: 1.24401e-06, val loss: 2.08556e-06, min loss: 1.19177e-06\n",
      "Epoch: 898500, elapsed: 1.14e+01, train loss: 1.30026e-06, val loss: 2.08402e-06, min loss: 1.19177e-06\n",
      "Epoch: 898600, elapsed: 1.12e+01, train loss: 1.20003e-06, val loss: 2.02366e-06, min loss: 1.19177e-06\n",
      "Epoch: 898700, elapsed: 1.13e+01, train loss: 1.23075e-06, val loss: 2.05123e-06, min loss: 1.19177e-06\n",
      "Epoch: 898800, elapsed: 1.37e+01, train loss: 1.21138e-06, val loss: 1.99076e-06, min loss: 1.19177e-06\n",
      "Epoch: 898900, elapsed: 1.15e+01, train loss: 1.43442e-06, val loss: 2.23294e-06, min loss: 1.19177e-06\n",
      "Epoch: 899000, elapsed: 1.12e+01, train loss: 1.25048e-06, val loss: 2.00802e-06, min loss: 1.19177e-06\n",
      "Epoch: 899100, elapsed: 1.15e+01, train loss: 1.19694e-06, val loss: 1.98479e-06, min loss: 1.19177e-06\n",
      "Epoch: 899200, elapsed: 1.14e+01, train loss: 1.28617e-06, val loss: 2.03141e-06, min loss: 1.19177e-06\n",
      "Epoch: 899300, elapsed: 1.14e+01, train loss: 1.19690e-06, val loss: 2.02140e-06, min loss: 1.19177e-06\n",
      "Epoch: 899400, elapsed: 1.12e+01, train loss: 1.18786e-06, val loss: 1.98963e-06, min loss: 1.18786e-06\n",
      "Epoch: 899500, elapsed: 1.15e+01, train loss: 1.85734e-06, val loss: 2.46383e-06, min loss: 1.18786e-06\n",
      "Epoch: 899600, elapsed: 1.12e+01, train loss: 1.18775e-06, val loss: 1.99100e-06, min loss: 1.18775e-06\n",
      "Epoch: 899700, elapsed: 1.13e+01, train loss: 1.33466e-06, val loss: 2.29939e-06, min loss: 1.18775e-06\n",
      "Epoch: 899800, elapsed: 1.15e+01, train loss: 1.41761e-06, val loss: 2.42574e-06, min loss: 1.18775e-06\n",
      "Epoch: 899900, elapsed: 1.13e+01, train loss: 1.18689e-06, val loss: 1.98476e-06, min loss: 1.18689e-06\n",
      "Epoch: 900000, elapsed: 1.14e+01, train loss: 1.19225e-06, val loss: 1.99777e-06, min loss: 1.18689e-06\n",
      "Epoch: 900100, elapsed: 1.33e+01, train loss: 1.21151e-06, val loss: 2.01892e-06, min loss: 1.18689e-06\n",
      "Epoch: 900200, elapsed: 1.12e+01, train loss: 1.30268e-06, val loss: 2.07929e-06, min loss: 1.18689e-06\n",
      "Epoch: 900300, elapsed: 1.13e+01, train loss: 1.19650e-06, val loss: 1.97939e-06, min loss: 1.18689e-06\n",
      "Epoch: 900400, elapsed: 1.12e+01, train loss: 1.60729e-06, val loss: 2.35328e-06, min loss: 1.18689e-06\n",
      "Epoch: 900500, elapsed: 1.39e+01, train loss: 1.33958e-06, val loss: 2.19025e-06, min loss: 1.18689e-06\n",
      "Epoch: 900600, elapsed: 1.14e+01, train loss: 1.29643e-06, val loss: 2.02426e-06, min loss: 1.18689e-06\n",
      "Epoch: 900700, elapsed: 1.13e+01, train loss: 1.38252e-06, val loss: 2.23207e-06, min loss: 1.18689e-06\n",
      "Epoch: 900800, elapsed: 1.15e+01, train loss: 1.24470e-06, val loss: 2.05137e-06, min loss: 1.18689e-06\n",
      "Epoch: 900900, elapsed: 1.13e+01, train loss: 1.45964e-06, val loss: 2.81015e-06, min loss: 1.18689e-06\n",
      "Epoch: 901000, elapsed: 1.12e+01, train loss: 1.18387e-06, val loss: 1.98319e-06, min loss: 1.18387e-06\n",
      "Epoch: 901100, elapsed: 1.15e+01, train loss: 1.19320e-06, val loss: 1.98875e-06, min loss: 1.18387e-06\n",
      "Epoch: 901200, elapsed: 1.13e+01, train loss: 1.24355e-06, val loss: 2.11488e-06, min loss: 1.18387e-06\n",
      "Epoch: 901300, elapsed: 1.11e+01, train loss: 1.19332e-06, val loss: 1.99539e-06, min loss: 1.18387e-06\n",
      "Epoch: 901400, elapsed: 1.15e+01, train loss: 4.38066e-06, val loss: 5.67526e-06, min loss: 1.18387e-06\n",
      "Epoch: 901500, elapsed: 1.13e+01, train loss: 1.24047e-06, val loss: 2.03407e-06, min loss: 1.18387e-06\n",
      "Epoch: 901600, elapsed: 1.13e+01, train loss: 1.18304e-06, val loss: 1.98085e-06, min loss: 1.18304e-06\n",
      "Epoch: 901700, elapsed: 1.14e+01, train loss: 1.31624e-06, val loss: 2.04008e-06, min loss: 1.18304e-06\n",
      "Epoch: 901800, elapsed: 1.15e+01, train loss: 1.23896e-06, val loss: 2.41805e-06, min loss: 1.18304e-06\n",
      "Epoch: 901900, elapsed: 1.13e+01, train loss: 1.29583e-06, val loss: 2.04668e-06, min loss: 1.18304e-06\n",
      "Epoch: 902000, elapsed: 1.13e+01, train loss: 1.18679e-06, val loss: 1.97950e-06, min loss: 1.18304e-06\n",
      "Epoch: 902100, elapsed: 1.13e+01, train loss: 1.26182e-06, val loss: 2.06078e-06, min loss: 1.18304e-06\n",
      "Epoch: 902200, elapsed: 1.42e+01, train loss: 1.31376e-06, val loss: 2.29878e-06, min loss: 1.18304e-06\n",
      "Epoch: 902300, elapsed: 1.13e+01, train loss: 1.98669e-06, val loss: 2.49730e-06, min loss: 1.18304e-06\n",
      "Epoch: 902400, elapsed: 1.14e+01, train loss: 1.36300e-06, val loss: 2.15239e-06, min loss: 1.18304e-06\n",
      "Epoch: 902500, elapsed: 1.13e+01, train loss: 1.18276e-06, val loss: 1.98066e-06, min loss: 1.18276e-06\n",
      "Epoch: 902600, elapsed: 1.12e+01, train loss: 1.18664e-06, val loss: 1.98589e-06, min loss: 1.18276e-06\n",
      "Epoch: 902700, elapsed: 1.13e+01, train loss: 1.22999e-06, val loss: 2.03686e-06, min loss: 1.18276e-06\n",
      "Epoch: 902800, elapsed: 1.12e+01, train loss: 2.17602e-06, val loss: 3.24978e-06, min loss: 1.18276e-06\n",
      "Epoch: 902900, elapsed: 1.13e+01, train loss: 1.18120e-06, val loss: 1.97841e-06, min loss: 1.18120e-06\n",
      "Epoch: 903000, elapsed: 1.12e+01, train loss: 1.22688e-06, val loss: 2.07860e-06, min loss: 1.18120e-06\n",
      "Epoch: 903100, elapsed: 1.14e+01, train loss: 1.22643e-06, val loss: 1.96520e-06, min loss: 1.18120e-06\n",
      "Epoch: 903200, elapsed: 1.12e+01, train loss: 1.18185e-06, val loss: 1.98242e-06, min loss: 1.18120e-06\n",
      "Epoch: 903300, elapsed: 1.13e+01, train loss: 1.18961e-06, val loss: 1.98727e-06, min loss: 1.18120e-06\n",
      "Epoch: 903400, elapsed: 1.12e+01, train loss: 1.20258e-06, val loss: 2.00411e-06, min loss: 1.18120e-06\n",
      "Epoch: 903500, elapsed: 1.13e+01, train loss: 1.37655e-06, val loss: 2.16994e-06, min loss: 1.18120e-06\n",
      "Epoch: 903600, elapsed: 1.13e+01, train loss: 1.44901e-06, val loss: 2.37183e-06, min loss: 1.18120e-06\n",
      "Epoch: 903700, elapsed: 1.13e+01, train loss: 1.34886e-06, val loss: 2.10106e-06, min loss: 1.18120e-06\n",
      "Epoch: 903800, elapsed: 1.13e+01, train loss: 1.20396e-06, val loss: 1.99759e-06, min loss: 1.18120e-06\n",
      "Epoch: 903900, elapsed: 1.39e+01, train loss: 1.18205e-06, val loss: 1.96757e-06, min loss: 1.18120e-06\n",
      "Epoch: 904000, elapsed: 1.15e+01, train loss: 1.18343e-06, val loss: 1.97729e-06, min loss: 1.18120e-06\n",
      "Epoch: 904100, elapsed: 1.17e+01, train loss: 1.42934e-06, val loss: 2.19076e-06, min loss: 1.18120e-06\n",
      "Epoch: 904200, elapsed: 1.15e+01, train loss: 1.17901e-06, val loss: 1.97512e-06, min loss: 1.17901e-06\n",
      "Epoch: 904300, elapsed: 1.13e+01, train loss: 1.18710e-06, val loss: 1.98649e-06, min loss: 1.17901e-06\n",
      "Epoch: 904400, elapsed: 1.14e+01, train loss: 1.39417e-06, val loss: 2.12146e-06, min loss: 1.17901e-06\n",
      "Epoch: 904500, elapsed: 1.14e+01, train loss: 1.26885e-06, val loss: 2.06945e-06, min loss: 1.17901e-06\n",
      "Epoch: 904600, elapsed: 1.13e+01, train loss: 1.94892e-06, val loss: 2.89315e-06, min loss: 1.17901e-06\n",
      "Epoch: 904700, elapsed: 1.13e+01, train loss: 1.34361e-06, val loss: 1.99586e-06, min loss: 1.17901e-06\n",
      "Epoch: 904800, elapsed: 1.14e+01, train loss: 1.24059e-06, val loss: 2.06262e-06, min loss: 1.17901e-06\n",
      "Epoch: 904900, elapsed: 1.13e+01, train loss: 1.63922e-06, val loss: 2.67293e-06, min loss: 1.17901e-06\n",
      "Epoch: 905000, elapsed: 1.15e+01, train loss: 1.17855e-06, val loss: 1.97459e-06, min loss: 1.17855e-06\n",
      "Epoch: 905100, elapsed: 1.34e+01, train loss: 1.18157e-06, val loss: 1.96898e-06, min loss: 1.17855e-06\n",
      "Epoch: 905200, elapsed: 1.12e+01, train loss: 1.17946e-06, val loss: 1.97429e-06, min loss: 1.17855e-06\n",
      "Epoch: 905300, elapsed: 1.13e+01, train loss: 1.18627e-06, val loss: 1.97786e-06, min loss: 1.17855e-06\n",
      "Epoch: 905400, elapsed: 1.13e+01, train loss: 4.95173e-06, val loss: 5.53470e-06, min loss: 1.17855e-06\n",
      "Epoch: 905500, elapsed: 1.12e+01, train loss: 4.73704e-06, val loss: 5.17354e-06, min loss: 1.17855e-06\n",
      "Epoch: 905600, elapsed: 1.40e+01, train loss: 4.98821e-06, val loss: 5.76207e-06, min loss: 1.17855e-06\n",
      "Epoch: 905700, elapsed: 1.14e+01, train loss: 1.62785e-06, val loss: 2.41823e-06, min loss: 1.17855e-06\n",
      "Epoch: 905800, elapsed: 1.12e+01, train loss: 1.18735e-06, val loss: 1.98860e-06, min loss: 1.17855e-06\n",
      "Epoch: 905900, elapsed: 1.13e+01, train loss: 1.76360e-06, val loss: 2.49432e-06, min loss: 1.17855e-06\n",
      "Epoch: 906000, elapsed: 1.13e+01, train loss: 1.18581e-06, val loss: 1.97779e-06, min loss: 1.17855e-06\n",
      "Epoch: 906100, elapsed: 1.13e+01, train loss: 1.19369e-06, val loss: 1.97688e-06, min loss: 1.17855e-06\n",
      "Epoch: 906200, elapsed: 1.13e+01, train loss: 4.83814e-06, val loss: 4.26239e-06, min loss: 1.17855e-06\n",
      "Epoch: 906300, elapsed: 1.13e+01, train loss: 2.04436e-06, val loss: 2.72132e-06, min loss: 1.17855e-06\n",
      "Epoch: 906400, elapsed: 1.13e+01, train loss: 2.74402e-06, val loss: 3.70968e-06, min loss: 1.17855e-06\n",
      "Epoch: 906500, elapsed: 1.13e+01, train loss: 5.05644e-06, val loss: 5.43025e-06, min loss: 1.17855e-06\n",
      "Epoch: 906600, elapsed: 1.12e+01, train loss: 1.22035e-06, val loss: 1.99016e-06, min loss: 1.17855e-06\n",
      "Epoch: 906700, elapsed: 1.14e+01, train loss: 1.19224e-06, val loss: 1.99556e-06, min loss: 1.17855e-06\n",
      "Epoch: 906800, elapsed: 1.13e+01, train loss: 2.26013e-06, val loss: 2.49120e-06, min loss: 1.17855e-06\n",
      "Epoch: 906900, elapsed: 1.14e+01, train loss: 2.00899e-06, val loss: 2.53877e-06, min loss: 1.17855e-06\n",
      "Epoch: 907000, elapsed: 1.13e+01, train loss: 1.42338e-06, val loss: 2.15202e-06, min loss: 1.17855e-06\n",
      "Epoch: 907100, elapsed: 1.13e+01, train loss: 1.25880e-06, val loss: 2.10314e-06, min loss: 1.17855e-06\n",
      "Epoch: 907200, elapsed: 1.12e+01, train loss: 1.34175e-06, val loss: 2.14604e-06, min loss: 1.17855e-06\n",
      "Epoch: 907300, elapsed: 1.39e+01, train loss: 1.29505e-06, val loss: 2.13135e-06, min loss: 1.17855e-06\n",
      "Epoch: 907400, elapsed: 1.15e+01, train loss: 2.72742e-06, val loss: 3.44125e-06, min loss: 1.17855e-06\n",
      "Epoch: 907500, elapsed: 1.13e+01, train loss: 1.24274e-06, val loss: 2.06528e-06, min loss: 1.17855e-06\n",
      "Epoch: 907600, elapsed: 1.14e+01, train loss: 1.20162e-06, val loss: 1.97087e-06, min loss: 1.17855e-06\n",
      "Epoch: 907700, elapsed: 1.12e+01, train loss: 1.17799e-06, val loss: 1.97715e-06, min loss: 1.17799e-06\n",
      "Epoch: 907800, elapsed: 1.13e+01, train loss: 1.20626e-06, val loss: 1.98836e-06, min loss: 1.17799e-06\n",
      "Epoch: 907900, elapsed: 1.15e+01, train loss: 1.17761e-06, val loss: 1.96886e-06, min loss: 1.17761e-06\n",
      "Epoch: 908000, elapsed: 1.14e+01, train loss: 1.17993e-06, val loss: 1.96634e-06, min loss: 1.17761e-06\n",
      "Epoch: 908100, elapsed: 1.12e+01, train loss: 3.44592e-06, val loss: 4.07596e-06, min loss: 1.17761e-06\n",
      "Epoch: 908200, elapsed: 1.14e+01, train loss: 1.17727e-06, val loss: 1.96166e-06, min loss: 1.17727e-06\n",
      "Epoch: 908300, elapsed: 1.14e+01, train loss: 2.06989e-06, val loss: 2.51628e-06, min loss: 1.17727e-06\n",
      "Epoch: 908400, elapsed: 1.12e+01, train loss: 1.47555e-06, val loss: 2.05839e-06, min loss: 1.17727e-06\n",
      "Epoch: 908500, elapsed: 1.13e+01, train loss: 1.57583e-06, val loss: 2.17989e-06, min loss: 1.17727e-06\n",
      "Epoch: 908600, elapsed: 1.13e+01, train loss: 1.18331e-06, val loss: 1.96465e-06, min loss: 1.17727e-06\n",
      "Epoch: 908700, elapsed: 1.12e+01, train loss: 1.17872e-06, val loss: 1.98357e-06, min loss: 1.17727e-06\n",
      "Epoch: 908800, elapsed: 1.12e+01, train loss: 1.20379e-06, val loss: 1.96693e-06, min loss: 1.17727e-06\n",
      "Epoch: 908900, elapsed: 1.09e+01, train loss: 1.38420e-06, val loss: 2.14685e-06, min loss: 1.17727e-06\n",
      "Epoch: 909000, elapsed: 1.39e+01, train loss: 1.67518e-06, val loss: 2.59313e-06, min loss: 1.17727e-06\n",
      "Epoch: 909100, elapsed: 1.15e+01, train loss: 1.35688e-06, val loss: 2.13717e-06, min loss: 1.17727e-06\n",
      "Epoch: 909200, elapsed: 1.14e+01, train loss: 1.48409e-06, val loss: 2.19082e-06, min loss: 1.17727e-06\n",
      "Epoch: 909300, elapsed: 1.12e+01, train loss: 2.03476e-06, val loss: 2.51825e-06, min loss: 1.17727e-06\n",
      "Epoch: 909400, elapsed: 1.13e+01, train loss: 1.17244e-06, val loss: 1.96447e-06, min loss: 1.17244e-06\n",
      "Epoch: 909500, elapsed: 1.13e+01, train loss: 1.80366e-06, val loss: 2.21462e-06, min loss: 1.17244e-06\n",
      "Epoch: 909600, elapsed: 1.15e+01, train loss: 1.17143e-06, val loss: 1.96553e-06, min loss: 1.17143e-06\n",
      "Epoch: 909700, elapsed: 1.13e+01, train loss: 1.17557e-06, val loss: 1.95689e-06, min loss: 1.17143e-06\n",
      "Epoch: 909800, elapsed: 1.13e+01, train loss: 1.17386e-06, val loss: 1.96513e-06, min loss: 1.17143e-06\n",
      "Epoch: 909900, elapsed: 1.13e+01, train loss: 1.26136e-06, val loss: 2.03368e-06, min loss: 1.17143e-06\n",
      "Epoch: 910000, elapsed: 1.13e+01, train loss: 3.57933e-06, val loss: 5.37999e-06, min loss: 1.17143e-06\n",
      "Epoch: 910100, elapsed: 1.37e+01, train loss: 1.17059e-06, val loss: 1.96399e-06, min loss: 1.17059e-06\n",
      "Epoch: 910200, elapsed: 1.12e+01, train loss: 1.40003e-06, val loss: 2.19745e-06, min loss: 1.17059e-06\n",
      "Epoch: 910300, elapsed: 1.13e+01, train loss: 1.17012e-06, val loss: 1.96057e-06, min loss: 1.17012e-06\n",
      "Epoch: 910400, elapsed: 1.14e+01, train loss: 1.48177e-06, val loss: 3.68276e-06, min loss: 1.17012e-06\n",
      "Epoch: 910500, elapsed: 1.13e+01, train loss: 1.16981e-06, val loss: 1.95989e-06, min loss: 1.16981e-06\n",
      "Epoch: 910600, elapsed: 1.12e+01, train loss: 3.93844e-06, val loss: 4.80139e-06, min loss: 1.16981e-06\n",
      "Epoch: 910700, elapsed: 1.39e+01, train loss: 3.60575e-06, val loss: 4.83250e-06, min loss: 1.16981e-06\n",
      "Epoch: 910800, elapsed: 1.15e+01, train loss: 2.07426e-06, val loss: 2.55445e-06, min loss: 1.16981e-06\n",
      "Epoch: 910900, elapsed: 1.13e+01, train loss: 1.29356e-06, val loss: 2.04118e-06, min loss: 1.16981e-06\n",
      "Epoch: 911000, elapsed: 1.12e+01, train loss: 1.20586e-06, val loss: 1.96414e-06, min loss: 1.16981e-06\n",
      "Epoch: 911100, elapsed: 1.13e+01, train loss: 1.46175e-06, val loss: 2.33299e-06, min loss: 1.16981e-06\n",
      "Epoch: 911200, elapsed: 1.12e+01, train loss: 1.22445e-06, val loss: 1.97759e-06, min loss: 1.16981e-06\n",
      "Epoch: 911300, elapsed: 1.14e+01, train loss: 1.20820e-06, val loss: 1.97184e-06, min loss: 1.16981e-06\n",
      "Epoch: 911400, elapsed: 1.12e+01, train loss: 1.27262e-06, val loss: 2.02734e-06, min loss: 1.16981e-06\n",
      "Epoch: 911500, elapsed: 1.11e+01, train loss: 1.19290e-06, val loss: 2.02375e-06, min loss: 1.16981e-06\n",
      "Epoch: 911600, elapsed: 1.15e+01, train loss: 1.17111e-06, val loss: 1.95592e-06, min loss: 1.16981e-06\n",
      "Epoch: 911700, elapsed: 1.12e+01, train loss: 1.17811e-06, val loss: 1.97162e-06, min loss: 1.16981e-06\n",
      "Epoch: 911800, elapsed: 1.13e+01, train loss: 1.18478e-06, val loss: 1.96336e-06, min loss: 1.16981e-06\n",
      "Epoch: 911900, elapsed: 1.12e+01, train loss: 1.36081e-06, val loss: 2.09022e-06, min loss: 1.16981e-06\n",
      "Epoch: 912000, elapsed: 1.12e+01, train loss: 1.19882e-06, val loss: 1.97285e-06, min loss: 1.16981e-06\n",
      "Epoch: 912100, elapsed: 1.12e+01, train loss: 1.17428e-06, val loss: 1.97972e-06, min loss: 1.16981e-06\n",
      "Epoch: 912200, elapsed: 1.11e+01, train loss: 1.17089e-06, val loss: 1.96217e-06, min loss: 1.16981e-06\n",
      "Epoch: 912300, elapsed: 1.12e+01, train loss: 1.19400e-06, val loss: 1.99022e-06, min loss: 1.16981e-06\n",
      "Epoch: 912400, elapsed: 1.38e+01, train loss: 1.26075e-06, val loss: 2.08162e-06, min loss: 1.16981e-06\n",
      "Epoch: 912500, elapsed: 1.14e+01, train loss: 3.07538e-06, val loss: 3.90066e-06, min loss: 1.16981e-06\n",
      "Epoch: 912600, elapsed: 1.13e+01, train loss: 1.22069e-06, val loss: 2.06967e-06, min loss: 1.16981e-06\n",
      "Epoch: 912700, elapsed: 1.12e+01, train loss: 2.75511e-06, val loss: 3.72793e-06, min loss: 1.16981e-06\n",
      "Epoch: 912800, elapsed: 1.14e+01, train loss: 3.09347e-06, val loss: 4.18799e-06, min loss: 1.16981e-06\n",
      "Epoch: 912900, elapsed: 1.12e+01, train loss: 1.18291e-06, val loss: 1.99241e-06, min loss: 1.16981e-06\n",
      "Epoch: 913000, elapsed: 1.12e+01, train loss: 1.16775e-06, val loss: 1.95584e-06, min loss: 1.16775e-06\n",
      "Epoch: 913100, elapsed: 1.12e+01, train loss: 1.17653e-06, val loss: 2.00099e-06, min loss: 1.16775e-06\n",
      "Epoch: 913200, elapsed: 1.13e+01, train loss: 1.87117e-06, val loss: 2.30771e-06, min loss: 1.16775e-06\n",
      "Epoch: 913300, elapsed: 1.15e+01, train loss: 1.26090e-06, val loss: 2.04643e-06, min loss: 1.16775e-06\n",
      "Epoch: 913400, elapsed: 1.13e+01, train loss: 1.31752e-06, val loss: 1.98485e-06, min loss: 1.16775e-06\n",
      "Epoch: 913500, elapsed: 1.14e+01, train loss: 1.43539e-06, val loss: 2.13480e-06, min loss: 1.16775e-06\n",
      "Epoch: 913600, elapsed: 1.14e+01, train loss: 1.46924e-06, val loss: 2.20410e-06, min loss: 1.16775e-06\n",
      "Epoch: 913700, elapsed: 1.12e+01, train loss: 1.18600e-06, val loss: 1.95246e-06, min loss: 1.16775e-06\n",
      "Epoch: 913800, elapsed: 1.14e+01, train loss: 1.17836e-06, val loss: 1.96784e-06, min loss: 1.16775e-06\n",
      "Epoch: 913900, elapsed: 1.13e+01, train loss: 1.32394e-06, val loss: 2.13306e-06, min loss: 1.16775e-06\n",
      "Epoch: 914000, elapsed: 1.13e+01, train loss: 3.56535e-06, val loss: 4.25101e-06, min loss: 1.16775e-06\n",
      "Epoch: 914100, elapsed: 1.13e+01, train loss: 2.24781e-06, val loss: 2.71458e-06, min loss: 1.16775e-06\n",
      "Epoch: 914200, elapsed: 1.39e+01, train loss: 1.57699e-06, val loss: 2.45370e-06, min loss: 1.16775e-06\n",
      "Epoch: 914300, elapsed: 1.13e+01, train loss: 1.22189e-06, val loss: 1.97493e-06, min loss: 1.16775e-06\n",
      "Epoch: 914400, elapsed: 1.14e+01, train loss: 1.18229e-06, val loss: 1.95429e-06, min loss: 1.16775e-06\n",
      "Epoch: 914500, elapsed: 1.14e+01, train loss: 1.96433e-06, val loss: 2.79322e-06, min loss: 1.16775e-06\n",
      "Epoch: 914600, elapsed: 1.15e+01, train loss: 1.16971e-06, val loss: 1.95020e-06, min loss: 1.16775e-06\n",
      "Epoch: 914700, elapsed: 1.15e+01, train loss: 1.16373e-06, val loss: 1.95166e-06, min loss: 1.16373e-06\n",
      "Epoch: 914800, elapsed: 1.14e+01, train loss: 1.17904e-06, val loss: 1.94687e-06, min loss: 1.16373e-06\n",
      "Epoch: 914900, elapsed: 1.14e+01, train loss: 1.16997e-06, val loss: 1.94327e-06, min loss: 1.16373e-06\n",
      "Epoch: 915000, elapsed: 1.14e+01, train loss: 1.16461e-06, val loss: 1.95256e-06, min loss: 1.16373e-06\n",
      "Epoch: 915100, elapsed: 1.35e+01, train loss: 1.85023e-06, val loss: 2.56546e-06, min loss: 1.16373e-06\n",
      "Epoch: 915200, elapsed: 1.14e+01, train loss: 1.16767e-06, val loss: 1.94224e-06, min loss: 1.16373e-06\n",
      "Epoch: 915300, elapsed: 1.16e+01, train loss: 1.17861e-06, val loss: 1.99289e-06, min loss: 1.16373e-06\n",
      "Epoch: 915400, elapsed: 1.13e+01, train loss: 1.16301e-06, val loss: 1.95170e-06, min loss: 1.16301e-06\n",
      "Epoch: 915500, elapsed: 1.14e+01, train loss: 1.53710e-06, val loss: 2.67236e-06, min loss: 1.16301e-06\n",
      "Epoch: 915600, elapsed: 1.12e+01, train loss: 1.18966e-06, val loss: 1.99277e-06, min loss: 1.16301e-06\n",
      "Epoch: 915700, elapsed: 1.14e+01, train loss: 1.50220e-06, val loss: 2.44710e-06, min loss: 1.16301e-06\n",
      "Epoch: 915800, elapsed: 1.13e+01, train loss: 1.29156e-06, val loss: 2.13995e-06, min loss: 1.16301e-06\n",
      "Epoch: 915900, elapsed: 1.40e+01, train loss: 1.23104e-06, val loss: 2.01113e-06, min loss: 1.16301e-06\n",
      "Epoch: 916000, elapsed: 1.15e+01, train loss: 1.20088e-06, val loss: 1.97597e-06, min loss: 1.16301e-06\n",
      "Epoch: 916100, elapsed: 1.15e+01, train loss: 1.16805e-06, val loss: 1.96422e-06, min loss: 1.16301e-06\n",
      "Epoch: 916200, elapsed: 1.13e+01, train loss: 1.19418e-06, val loss: 1.95166e-06, min loss: 1.16301e-06\n",
      "Epoch: 916300, elapsed: 1.14e+01, train loss: 1.30946e-06, val loss: 2.07115e-06, min loss: 1.16301e-06\n",
      "Epoch: 916400, elapsed: 1.14e+01, train loss: 1.39784e-06, val loss: 2.07806e-06, min loss: 1.16301e-06\n",
      "Epoch: 916500, elapsed: 1.16e+01, train loss: 1.63804e-06, val loss: 2.47895e-06, min loss: 1.16301e-06\n",
      "Epoch: 916600, elapsed: 1.13e+01, train loss: 1.43091e-06, val loss: 2.05832e-06, min loss: 1.16301e-06\n",
      "Epoch: 916700, elapsed: 1.14e+01, train loss: 1.18904e-06, val loss: 1.99033e-06, min loss: 1.16301e-06\n",
      "Epoch: 916800, elapsed: 1.13e+01, train loss: 1.19408e-06, val loss: 1.96973e-06, min loss: 1.16301e-06\n",
      "Epoch: 916900, elapsed: 1.12e+01, train loss: 3.22453e-06, val loss: 3.39976e-06, min loss: 1.16301e-06\n",
      "Epoch: 917000, elapsed: 1.12e+01, train loss: 1.72866e-06, val loss: 2.86446e-06, min loss: 1.16301e-06\n",
      "Epoch: 917100, elapsed: 1.13e+01, train loss: 1.40484e-06, val loss: 1.98347e-06, min loss: 1.16301e-06\n",
      "Epoch: 917200, elapsed: 1.12e+01, train loss: 1.59335e-06, val loss: 2.27407e-06, min loss: 1.16301e-06\n",
      "Epoch: 917300, elapsed: 1.12e+01, train loss: 1.27058e-06, val loss: 2.00894e-06, min loss: 1.16301e-06\n",
      "Epoch: 917400, elapsed: 1.13e+01, train loss: 1.68986e-06, val loss: 2.28042e-06, min loss: 1.16301e-06\n",
      "Epoch: 917500, elapsed: 1.11e+01, train loss: 1.31063e-06, val loss: 2.18575e-06, min loss: 1.16301e-06\n",
      "Epoch: 917600, elapsed: 1.37e+01, train loss: 1.26301e-06, val loss: 2.00036e-06, min loss: 1.16301e-06\n",
      "Epoch: 917700, elapsed: 1.15e+01, train loss: 3.03790e-06, val loss: 4.31888e-06, min loss: 1.16301e-06\n",
      "Epoch: 917800, elapsed: 1.15e+01, train loss: 2.83952e-06, val loss: 3.69720e-06, min loss: 1.16301e-06\n",
      "Epoch: 917900, elapsed: 1.12e+01, train loss: 1.79724e-06, val loss: 2.66481e-06, min loss: 1.16301e-06\n",
      "Epoch: 918000, elapsed: 1.12e+01, train loss: 1.19686e-06, val loss: 1.99062e-06, min loss: 1.16301e-06\n",
      "Epoch: 918100, elapsed: 1.12e+01, train loss: 1.19290e-06, val loss: 1.96311e-06, min loss: 1.16301e-06\n",
      "Epoch: 918200, elapsed: 1.13e+01, train loss: 1.45381e-06, val loss: 2.32175e-06, min loss: 1.16301e-06\n",
      "Epoch: 918300, elapsed: 1.12e+01, train loss: 1.22142e-06, val loss: 1.96135e-06, min loss: 1.16301e-06\n",
      "Epoch: 918400, elapsed: 1.12e+01, train loss: 5.13343e-06, val loss: 6.04245e-06, min loss: 1.16301e-06\n",
      "Epoch: 918500, elapsed: 1.11e+01, train loss: 1.69742e-06, val loss: 2.52194e-06, min loss: 1.16301e-06\n",
      "Epoch: 918600, elapsed: 1.11e+01, train loss: 1.22395e-06, val loss: 2.04918e-06, min loss: 1.16301e-06\n",
      "Epoch: 918700, elapsed: 1.11e+01, train loss: 1.20240e-06, val loss: 2.01697e-06, min loss: 1.16301e-06\n",
      "Epoch: 918800, elapsed: 1.12e+01, train loss: 1.71942e-06, val loss: 2.05039e-06, min loss: 1.16301e-06\n",
      "Epoch: 918900, elapsed: 1.12e+01, train loss: 1.44296e-06, val loss: 2.23801e-06, min loss: 1.16301e-06\n",
      "Epoch: 919000, elapsed: 1.12e+01, train loss: 1.32195e-06, val loss: 2.11819e-06, min loss: 1.16301e-06\n",
      "Epoch: 919100, elapsed: 1.11e+01, train loss: 1.42210e-06, val loss: 2.26754e-06, min loss: 1.16301e-06\n",
      "Epoch: 919200, elapsed: 1.12e+01, train loss: 1.22270e-06, val loss: 2.02855e-06, min loss: 1.16301e-06\n",
      "Epoch: 919300, elapsed: 1.12e+01, train loss: 1.47691e-06, val loss: 2.22371e-06, min loss: 1.16301e-06\n",
      "Epoch: 919400, elapsed: 1.41e+01, train loss: 1.22457e-06, val loss: 2.05012e-06, min loss: 1.16301e-06\n",
      "Epoch: 919500, elapsed: 1.14e+01, train loss: 1.23988e-06, val loss: 2.13195e-06, min loss: 1.16301e-06\n",
      "Epoch: 919600, elapsed: 1.14e+01, train loss: 2.77367e-06, val loss: 3.65833e-06, min loss: 1.16301e-06\n",
      "Epoch: 919700, elapsed: 1.13e+01, train loss: 1.21296e-06, val loss: 1.99909e-06, min loss: 1.16301e-06\n",
      "Epoch: 919800, elapsed: 1.14e+01, train loss: 1.20652e-06, val loss: 2.01637e-06, min loss: 1.16301e-06\n",
      "Epoch: 919900, elapsed: 1.14e+01, train loss: 1.24362e-06, val loss: 1.99173e-06, min loss: 1.16301e-06\n",
      "Epoch: 920000, elapsed: 1.14e+01, train loss: 1.25847e-06, val loss: 2.08555e-06, min loss: 1.16301e-06\n",
      "Epoch: 920100, elapsed: 1.33e+01, train loss: 1.17087e-06, val loss: 1.98356e-06, min loss: 1.16301e-06\n",
      "Epoch: 920200, elapsed: 1.13e+01, train loss: 1.17572e-06, val loss: 1.97182e-06, min loss: 1.16301e-06\n",
      "Epoch: 920300, elapsed: 1.12e+01, train loss: 1.27511e-06, val loss: 2.09831e-06, min loss: 1.16301e-06\n",
      "Epoch: 920400, elapsed: 1.12e+01, train loss: 1.64518e-06, val loss: 2.29524e-06, min loss: 1.16301e-06\n",
      "Epoch: 920500, elapsed: 1.11e+01, train loss: 1.50035e-06, val loss: 2.01236e-06, min loss: 1.16301e-06\n",
      "Epoch: 920600, elapsed: 1.12e+01, train loss: 1.18452e-06, val loss: 1.92887e-06, min loss: 1.16301e-06\n",
      "Epoch: 920700, elapsed: 1.13e+01, train loss: 1.16332e-06, val loss: 1.93630e-06, min loss: 1.16301e-06\n",
      "Epoch: 920800, elapsed: 1.13e+01, train loss: 1.17486e-06, val loss: 1.93498e-06, min loss: 1.16301e-06\n",
      "Epoch: 920900, elapsed: 1.13e+01, train loss: 1.22178e-06, val loss: 1.95783e-06, min loss: 1.16301e-06\n",
      "Epoch: 921000, elapsed: 1.12e+01, train loss: 1.24407e-06, val loss: 1.99935e-06, min loss: 1.16301e-06\n",
      "Epoch: 921100, elapsed: 1.40e+01, train loss: 1.16644e-06, val loss: 1.95202e-06, min loss: 1.16301e-06\n",
      "Epoch: 921200, elapsed: 1.14e+01, train loss: 2.35664e-06, val loss: 3.11806e-06, min loss: 1.16301e-06\n",
      "Epoch: 921300, elapsed: 1.14e+01, train loss: 5.35605e-06, val loss: 6.14198e-06, min loss: 1.16301e-06\n",
      "Epoch: 921400, elapsed: 1.12e+01, train loss: 2.83106e-06, val loss: 3.27578e-06, min loss: 1.16301e-06\n",
      "Epoch: 921500, elapsed: 1.12e+01, train loss: 1.15592e-06, val loss: 1.93931e-06, min loss: 1.15592e-06\n",
      "Epoch: 921600, elapsed: 1.11e+01, train loss: 1.19342e-06, val loss: 1.94156e-06, min loss: 1.15592e-06\n",
      "Epoch: 921700, elapsed: 1.12e+01, train loss: 1.18689e-06, val loss: 1.99368e-06, min loss: 1.15592e-06\n",
      "Epoch: 921800, elapsed: 1.12e+01, train loss: 1.20862e-06, val loss: 1.94877e-06, min loss: 1.15592e-06\n",
      "Epoch: 921900, elapsed: 1.13e+01, train loss: 1.96452e-06, val loss: 2.45332e-06, min loss: 1.15592e-06\n",
      "Epoch: 922000, elapsed: 1.13e+01, train loss: 6.18849e-06, val loss: 5.96528e-06, min loss: 1.15592e-06\n",
      "Epoch: 922100, elapsed: 1.11e+01, train loss: 1.79180e-06, val loss: 2.40698e-06, min loss: 1.15592e-06\n",
      "Epoch: 922200, elapsed: 1.12e+01, train loss: 1.41313e-06, val loss: 2.25091e-06, min loss: 1.15592e-06\n",
      "Epoch: 922300, elapsed: 1.13e+01, train loss: 1.31707e-06, val loss: 2.01504e-06, min loss: 1.15592e-06\n",
      "Epoch: 922400, elapsed: 1.12e+01, train loss: 2.06674e-06, val loss: 3.38868e-06, min loss: 1.15592e-06\n",
      "Epoch: 922500, elapsed: 1.11e+01, train loss: 1.18312e-06, val loss: 1.98685e-06, min loss: 1.15592e-06\n",
      "Epoch: 922600, elapsed: 1.12e+01, train loss: 1.15457e-06, val loss: 1.94083e-06, min loss: 1.15457e-06\n",
      "Epoch: 922700, elapsed: 1.11e+01, train loss: 1.24053e-06, val loss: 1.98856e-06, min loss: 1.15457e-06\n",
      "Epoch: 922800, elapsed: 1.38e+01, train loss: 1.15290e-06, val loss: 1.93542e-06, min loss: 1.15290e-06\n",
      "Epoch: 922900, elapsed: 1.13e+01, train loss: 1.15777e-06, val loss: 1.95180e-06, min loss: 1.15290e-06\n",
      "Epoch: 923000, elapsed: 1.14e+01, train loss: 1.80049e-06, val loss: 2.25369e-06, min loss: 1.15290e-06\n",
      "Epoch: 923100, elapsed: 1.13e+01, train loss: 1.70115e-06, val loss: 2.44907e-06, min loss: 1.15290e-06\n",
      "Epoch: 923200, elapsed: 1.13e+01, train loss: 1.30059e-06, val loss: 2.02132e-06, min loss: 1.15290e-06\n",
      "Epoch: 923300, elapsed: 1.14e+01, train loss: 1.38817e-06, val loss: 2.08271e-06, min loss: 1.15290e-06\n",
      "Epoch: 923400, elapsed: 1.13e+01, train loss: 1.15514e-06, val loss: 1.94391e-06, min loss: 1.15290e-06\n",
      "Epoch: 923500, elapsed: 1.13e+01, train loss: 2.97055e-06, val loss: 2.44631e-06, min loss: 1.15290e-06\n",
      "Epoch: 923600, elapsed: 1.12e+01, train loss: 1.17975e-06, val loss: 2.00091e-06, min loss: 1.15290e-06\n",
      "Epoch: 923700, elapsed: 1.13e+01, train loss: 1.27414e-06, val loss: 1.99774e-06, min loss: 1.15290e-06\n",
      "Epoch: 923800, elapsed: 1.14e+01, train loss: 1.51472e-06, val loss: 2.39390e-06, min loss: 1.15290e-06\n",
      "Epoch: 923900, elapsed: 1.12e+01, train loss: 3.63168e-06, val loss: 3.54001e-06, min loss: 1.15290e-06\n",
      "Epoch: 924000, elapsed: 1.11e+01, train loss: 1.15754e-06, val loss: 1.94676e-06, min loss: 1.15290e-06\n",
      "Epoch: 924100, elapsed: 1.13e+01, train loss: 1.60855e-06, val loss: 2.53637e-06, min loss: 1.15290e-06\n",
      "Epoch: 924200, elapsed: 1.12e+01, train loss: 1.21329e-06, val loss: 2.01169e-06, min loss: 1.15290e-06\n",
      "Epoch: 924300, elapsed: 1.13e+01, train loss: 1.22022e-06, val loss: 1.97475e-06, min loss: 1.15290e-06\n",
      "Epoch: 924400, elapsed: 1.12e+01, train loss: 1.34895e-06, val loss: 2.24556e-06, min loss: 1.15290e-06\n",
      "Epoch: 924500, elapsed: 1.12e+01, train loss: 1.15125e-06, val loss: 1.93449e-06, min loss: 1.15125e-06\n",
      "Epoch: 924600, elapsed: 1.39e+01, train loss: 1.15808e-06, val loss: 1.93996e-06, min loss: 1.15125e-06\n",
      "Epoch: 924700, elapsed: 1.14e+01, train loss: 1.15816e-06, val loss: 1.93586e-06, min loss: 1.15125e-06\n",
      "Epoch: 924800, elapsed: 1.13e+01, train loss: 1.16498e-06, val loss: 1.94581e-06, min loss: 1.15125e-06\n",
      "Epoch: 924900, elapsed: 1.13e+01, train loss: 1.15290e-06, val loss: 1.94144e-06, min loss: 1.15125e-06\n",
      "Epoch: 925000, elapsed: 1.14e+01, train loss: 1.18340e-06, val loss: 1.94125e-06, min loss: 1.15125e-06\n",
      "Epoch: 925100, elapsed: 1.33e+01, train loss: 1.34513e-06, val loss: 2.11943e-06, min loss: 1.15125e-06\n",
      "Epoch: 925200, elapsed: 1.13e+01, train loss: 1.16477e-06, val loss: 1.93993e-06, min loss: 1.15125e-06\n",
      "Epoch: 925300, elapsed: 1.12e+01, train loss: 1.18605e-06, val loss: 1.98637e-06, min loss: 1.15125e-06\n",
      "Epoch: 925400, elapsed: 1.14e+01, train loss: 2.37413e-06, val loss: 3.30146e-06, min loss: 1.15125e-06\n",
      "Epoch: 925500, elapsed: 1.13e+01, train loss: 1.37320e-06, val loss: 2.14427e-06, min loss: 1.15125e-06\n",
      "Epoch: 925600, elapsed: 1.12e+01, train loss: 1.23924e-06, val loss: 2.04228e-06, min loss: 1.15125e-06\n",
      "Epoch: 925700, elapsed: 1.13e+01, train loss: 2.34122e-06, val loss: 3.63222e-06, min loss: 1.15125e-06\n",
      "Epoch: 925800, elapsed: 1.14e+01, train loss: 1.74707e-06, val loss: 2.41052e-06, min loss: 1.15125e-06\n",
      "Epoch: 925900, elapsed: 1.14e+01, train loss: 1.20137e-06, val loss: 1.95640e-06, min loss: 1.15125e-06\n",
      "Epoch: 926000, elapsed: 1.13e+01, train loss: 1.49152e-06, val loss: 2.31406e-06, min loss: 1.15125e-06\n",
      "Epoch: 926100, elapsed: 1.14e+01, train loss: 2.20705e-06, val loss: 3.08352e-06, min loss: 1.15125e-06\n",
      "Epoch: 926200, elapsed: 1.14e+01, train loss: 1.29563e-06, val loss: 2.01579e-06, min loss: 1.15125e-06\n",
      "Epoch: 926300, elapsed: 1.39e+01, train loss: 1.15752e-06, val loss: 1.96798e-06, min loss: 1.15125e-06\n",
      "Epoch: 926400, elapsed: 1.15e+01, train loss: 1.15141e-06, val loss: 1.93895e-06, min loss: 1.15125e-06\n",
      "Epoch: 926500, elapsed: 1.13e+01, train loss: 1.17363e-06, val loss: 1.93564e-06, min loss: 1.15125e-06\n",
      "Epoch: 926600, elapsed: 1.13e+01, train loss: 1.16395e-06, val loss: 1.93713e-06, min loss: 1.15125e-06\n",
      "Epoch: 926700, elapsed: 1.12e+01, train loss: 1.18761e-06, val loss: 1.96490e-06, min loss: 1.15125e-06\n",
      "Epoch: 926800, elapsed: 1.12e+01, train loss: 1.68700e-06, val loss: 2.12009e-06, min loss: 1.15125e-06\n",
      "Epoch: 926900, elapsed: 1.14e+01, train loss: 2.30191e-06, val loss: 2.99673e-06, min loss: 1.15125e-06\n",
      "Epoch: 927000, elapsed: 1.11e+01, train loss: 1.16727e-06, val loss: 1.93713e-06, min loss: 1.15125e-06\n",
      "Epoch: 927100, elapsed: 1.13e+01, train loss: 1.17371e-06, val loss: 1.94974e-06, min loss: 1.15125e-06\n",
      "Epoch: 927200, elapsed: 1.13e+01, train loss: 1.20375e-06, val loss: 2.01493e-06, min loss: 1.15125e-06\n",
      "Epoch: 927300, elapsed: 1.13e+01, train loss: 1.15295e-06, val loss: 1.93826e-06, min loss: 1.15125e-06\n",
      "Epoch: 927400, elapsed: 1.12e+01, train loss: 1.20704e-06, val loss: 2.01714e-06, min loss: 1.15125e-06\n",
      "Epoch: 927500, elapsed: 1.12e+01, train loss: 1.66541e-06, val loss: 2.59626e-06, min loss: 1.15125e-06\n",
      "Epoch: 927600, elapsed: 1.12e+01, train loss: 2.62009e-06, val loss: 2.76509e-06, min loss: 1.15125e-06\n",
      "Epoch: 927700, elapsed: 1.11e+01, train loss: 1.28635e-06, val loss: 2.03253e-06, min loss: 1.15125e-06\n",
      "Epoch: 927800, elapsed: 1.13e+01, train loss: 1.25006e-06, val loss: 1.95452e-06, min loss: 1.15125e-06\n",
      "Epoch: 927900, elapsed: 1.12e+01, train loss: 1.52815e-06, val loss: 2.69578e-06, min loss: 1.15125e-06\n",
      "Epoch: 928000, elapsed: 1.37e+01, train loss: 1.26003e-06, val loss: 2.20238e-06, min loss: 1.15125e-06\n",
      "Epoch: 928100, elapsed: 1.15e+01, train loss: 2.72771e-06, val loss: 3.01504e-06, min loss: 1.15125e-06\n",
      "Epoch: 928200, elapsed: 1.15e+01, train loss: 1.45768e-06, val loss: 2.35864e-06, min loss: 1.15125e-06\n",
      "Epoch: 928300, elapsed: 1.15e+01, train loss: 1.14704e-06, val loss: 1.92586e-06, min loss: 1.14704e-06\n",
      "Epoch: 928400, elapsed: 1.14e+01, train loss: 3.96100e-06, val loss: 5.21465e-06, min loss: 1.14704e-06\n",
      "Epoch: 928500, elapsed: 1.14e+01, train loss: 1.14548e-06, val loss: 1.92304e-06, min loss: 1.14548e-06\n",
      "Epoch: 928600, elapsed: 1.13e+01, train loss: 1.63340e-06, val loss: 2.25044e-06, min loss: 1.14548e-06\n",
      "Epoch: 928700, elapsed: 1.13e+01, train loss: 2.31548e-06, val loss: 3.21854e-06, min loss: 1.14548e-06\n",
      "Epoch: 928800, elapsed: 1.12e+01, train loss: 2.74119e-06, val loss: 2.96218e-06, min loss: 1.14548e-06\n",
      "Epoch: 928900, elapsed: 1.13e+01, train loss: 1.72439e-06, val loss: 2.89514e-06, min loss: 1.14548e-06\n",
      "Epoch: 929000, elapsed: 1.14e+01, train loss: 1.16822e-06, val loss: 1.93726e-06, min loss: 1.14548e-06\n",
      "Epoch: 929100, elapsed: 1.14e+01, train loss: 1.15469e-06, val loss: 1.93682e-06, min loss: 1.14548e-06\n",
      "Epoch: 929200, elapsed: 1.12e+01, train loss: 1.14604e-06, val loss: 1.91589e-06, min loss: 1.14548e-06\n",
      "Epoch: 929300, elapsed: 1.11e+01, train loss: 1.15474e-06, val loss: 1.90956e-06, min loss: 1.14548e-06\n",
      "Epoch: 929400, elapsed: 1.13e+01, train loss: 1.15744e-06, val loss: 1.94627e-06, min loss: 1.14548e-06\n",
      "Epoch: 929500, elapsed: 1.13e+01, train loss: 1.15562e-06, val loss: 1.97871e-06, min loss: 1.14548e-06\n",
      "Epoch: 929600, elapsed: 1.12e+01, train loss: 1.25100e-06, val loss: 1.94318e-06, min loss: 1.14548e-06\n",
      "Epoch: 929700, elapsed: 1.13e+01, train loss: 1.77813e-06, val loss: 2.74422e-06, min loss: 1.14548e-06\n",
      "Epoch: 929800, elapsed: 1.41e+01, train loss: 1.64412e-06, val loss: 2.18093e-06, min loss: 1.14548e-06\n",
      "Epoch: 929900, elapsed: 1.15e+01, train loss: 1.22616e-06, val loss: 1.98165e-06, min loss: 1.14548e-06\n",
      "Epoch: 930000, elapsed: 1.15e+01, train loss: 1.20185e-06, val loss: 1.99855e-06, min loss: 1.14548e-06\n",
      "Epoch: 930100, elapsed: 1.35e+01, train loss: 1.16241e-06, val loss: 1.91155e-06, min loss: 1.14548e-06\n",
      "Epoch: 930200, elapsed: 1.14e+01, train loss: 1.18709e-06, val loss: 1.97818e-06, min loss: 1.14548e-06\n",
      "Epoch: 930300, elapsed: 1.13e+01, train loss: 1.14345e-06, val loss: 1.91912e-06, min loss: 1.14345e-06\n",
      "Epoch: 930400, elapsed: 1.14e+01, train loss: 1.14762e-06, val loss: 1.92089e-06, min loss: 1.14345e-06\n",
      "Epoch: 930500, elapsed: 1.14e+01, train loss: 1.43862e-06, val loss: 2.17973e-06, min loss: 1.14345e-06\n",
      "Epoch: 930600, elapsed: 1.14e+01, train loss: 1.21215e-06, val loss: 1.97750e-06, min loss: 1.14345e-06\n",
      "Epoch: 930700, elapsed: 1.14e+01, train loss: 1.14889e-06, val loss: 1.91788e-06, min loss: 1.14345e-06\n",
      "Epoch: 930800, elapsed: 1.13e+01, train loss: 1.15203e-06, val loss: 1.96327e-06, min loss: 1.14345e-06\n",
      "Epoch: 930900, elapsed: 1.13e+01, train loss: 1.14750e-06, val loss: 1.94398e-06, min loss: 1.14345e-06\n",
      "Epoch: 931000, elapsed: 1.14e+01, train loss: 1.14512e-06, val loss: 1.91021e-06, min loss: 1.14345e-06\n",
      "Epoch: 931100, elapsed: 1.12e+01, train loss: 1.16964e-06, val loss: 1.94650e-06, min loss: 1.14345e-06\n",
      "Epoch: 931200, elapsed: 1.14e+01, train loss: 1.16320e-06, val loss: 1.94916e-06, min loss: 1.14345e-06\n",
      "Epoch: 931300, elapsed: 1.12e+01, train loss: 1.14278e-06, val loss: 1.92329e-06, min loss: 1.14278e-06\n",
      "Epoch: 931400, elapsed: 1.12e+01, train loss: 1.14648e-06, val loss: 1.91905e-06, min loss: 1.14278e-06\n",
      "Epoch: 931500, elapsed: 1.39e+01, train loss: 1.14322e-06, val loss: 1.91689e-06, min loss: 1.14278e-06\n",
      "Epoch: 931600, elapsed: 1.13e+01, train loss: 1.22570e-06, val loss: 2.03525e-06, min loss: 1.14278e-06\n",
      "Epoch: 931700, elapsed: 1.15e+01, train loss: 3.51241e-06, val loss: 4.04998e-06, min loss: 1.14278e-06\n",
      "Epoch: 931800, elapsed: 1.14e+01, train loss: 1.48631e-06, val loss: 2.37937e-06, min loss: 1.14278e-06\n",
      "Epoch: 931900, elapsed: 1.13e+01, train loss: 1.38112e-06, val loss: 2.02964e-06, min loss: 1.14278e-06\n",
      "Epoch: 932000, elapsed: 1.13e+01, train loss: 1.14167e-06, val loss: 1.92327e-06, min loss: 1.14167e-06\n",
      "Epoch: 932100, elapsed: 1.13e+01, train loss: 1.28945e-06, val loss: 1.96838e-06, min loss: 1.14167e-06\n",
      "Epoch: 932200, elapsed: 1.11e+01, train loss: 1.34299e-06, val loss: 2.24039e-06, min loss: 1.14167e-06\n",
      "Epoch: 932300, elapsed: 1.13e+01, train loss: 1.14548e-06, val loss: 1.92871e-06, min loss: 1.14167e-06\n",
      "Epoch: 932400, elapsed: 1.15e+01, train loss: 1.14150e-06, val loss: 1.91812e-06, min loss: 1.14150e-06\n",
      "Epoch: 932500, elapsed: 1.13e+01, train loss: 1.14875e-06, val loss: 1.93765e-06, min loss: 1.14150e-06\n",
      "Epoch: 932600, elapsed: 1.14e+01, train loss: 1.23672e-06, val loss: 2.04909e-06, min loss: 1.14150e-06\n",
      "Epoch: 932700, elapsed: 1.13e+01, train loss: 1.23450e-06, val loss: 2.06748e-06, min loss: 1.14150e-06\n",
      "Epoch: 932800, elapsed: 1.13e+01, train loss: 2.23478e-06, val loss: 2.97019e-06, min loss: 1.14150e-06\n",
      "Epoch: 932900, elapsed: 1.12e+01, train loss: 1.32398e-06, val loss: 2.10770e-06, min loss: 1.14150e-06\n",
      "Epoch: 933000, elapsed: 1.15e+01, train loss: 1.15178e-06, val loss: 1.92584e-06, min loss: 1.14150e-06\n",
      "Epoch: 933100, elapsed: 1.14e+01, train loss: 1.14930e-06, val loss: 1.91973e-06, min loss: 1.14150e-06\n",
      "Epoch: 933200, elapsed: 1.12e+01, train loss: 1.14728e-06, val loss: 1.91780e-06, min loss: 1.14150e-06\n",
      "Epoch: 933300, elapsed: 1.41e+01, train loss: 1.14554e-06, val loss: 1.92393e-06, min loss: 1.14150e-06\n",
      "Epoch: 933400, elapsed: 1.12e+01, train loss: 1.14620e-06, val loss: 1.92606e-06, min loss: 1.14150e-06\n",
      "Epoch: 933500, elapsed: 1.15e+01, train loss: 1.16860e-06, val loss: 1.93372e-06, min loss: 1.14150e-06\n",
      "Epoch: 933600, elapsed: 1.14e+01, train loss: 1.29617e-06, val loss: 1.93288e-06, min loss: 1.14150e-06\n",
      "Epoch: 933700, elapsed: 1.15e+01, train loss: 1.52403e-06, val loss: 2.64981e-06, min loss: 1.14150e-06\n",
      "Epoch: 933800, elapsed: 1.13e+01, train loss: 3.75103e-06, val loss: 4.91162e-06, min loss: 1.14150e-06\n",
      "Epoch: 933900, elapsed: 1.14e+01, train loss: 1.16126e-06, val loss: 1.93928e-06, min loss: 1.14150e-06\n",
      "Epoch: 934000, elapsed: 1.13e+01, train loss: 1.14087e-06, val loss: 1.91777e-06, min loss: 1.14087e-06\n",
      "Epoch: 934100, elapsed: 1.13e+01, train loss: 1.20974e-06, val loss: 2.15002e-06, min loss: 1.14087e-06\n",
      "Epoch: 934200, elapsed: 1.13e+01, train loss: 2.27530e-06, val loss: 3.13985e-06, min loss: 1.14087e-06\n",
      "Epoch: 934300, elapsed: 1.13e+01, train loss: 1.33710e-06, val loss: 2.17206e-06, min loss: 1.14087e-06\n",
      "Epoch: 934400, elapsed: 1.14e+01, train loss: 1.13852e-06, val loss: 1.91254e-06, min loss: 1.13852e-06\n",
      "Epoch: 934500, elapsed: 1.13e+01, train loss: 1.26756e-06, val loss: 1.94615e-06, min loss: 1.13852e-06\n",
      "Epoch: 934600, elapsed: 1.11e+01, train loss: 2.24010e-06, val loss: 3.22901e-06, min loss: 1.13852e-06\n",
      "Epoch: 934700, elapsed: 1.15e+01, train loss: 1.70435e-06, val loss: 2.50057e-06, min loss: 1.13852e-06\n",
      "Epoch: 934800, elapsed: 1.15e+01, train loss: 1.34829e-06, val loss: 2.10117e-06, min loss: 1.13852e-06\n",
      "Epoch: 934900, elapsed: 1.13e+01, train loss: 1.58085e-06, val loss: 2.42515e-06, min loss: 1.13852e-06\n",
      "Epoch: 935000, elapsed: 1.39e+01, train loss: 1.20562e-06, val loss: 1.96819e-06, min loss: 1.13852e-06\n",
      "Epoch: 935100, elapsed: 1.35e+01, train loss: 1.21289e-06, val loss: 1.99005e-06, min loss: 1.13852e-06\n",
      "Epoch: 935200, elapsed: 1.16e+01, train loss: 1.16645e-06, val loss: 1.95874e-06, min loss: 1.13852e-06\n",
      "Epoch: 935300, elapsed: 1.14e+01, train loss: 1.34411e-06, val loss: 2.11336e-06, min loss: 1.13852e-06\n",
      "Epoch: 935400, elapsed: 1.14e+01, train loss: 1.15925e-06, val loss: 1.93190e-06, min loss: 1.13852e-06\n",
      "Epoch: 935500, elapsed: 1.14e+01, train loss: 1.67953e-06, val loss: 2.28548e-06, min loss: 1.13852e-06\n",
      "Epoch: 935600, elapsed: 1.13e+01, train loss: 2.79606e-06, val loss: 3.45130e-06, min loss: 1.13852e-06\n",
      "Epoch: 935700, elapsed: 1.13e+01, train loss: 1.24759e-06, val loss: 2.00326e-06, min loss: 1.13852e-06\n",
      "Epoch: 935800, elapsed: 1.13e+01, train loss: 1.16852e-06, val loss: 1.92753e-06, min loss: 1.13852e-06\n",
      "Epoch: 935900, elapsed: 1.13e+01, train loss: 1.17068e-06, val loss: 1.94074e-06, min loss: 1.13852e-06\n",
      "Epoch: 936000, elapsed: 1.13e+01, train loss: 1.32131e-06, val loss: 2.01672e-06, min loss: 1.13852e-06\n",
      "Epoch: 936100, elapsed: 1.13e+01, train loss: 1.19026e-06, val loss: 2.09731e-06, min loss: 1.13852e-06\n",
      "Epoch: 936200, elapsed: 1.14e+01, train loss: 1.94919e-06, val loss: 2.81838e-06, min loss: 1.13852e-06\n",
      "Epoch: 936300, elapsed: 1.13e+01, train loss: 1.15108e-06, val loss: 1.93576e-06, min loss: 1.13852e-06\n",
      "Epoch: 936400, elapsed: 1.14e+01, train loss: 1.13847e-06, val loss: 1.91612e-06, min loss: 1.13847e-06\n",
      "Epoch: 936500, elapsed: 1.13e+01, train loss: 1.14638e-06, val loss: 1.92131e-06, min loss: 1.13847e-06\n",
      "Epoch: 936600, elapsed: 1.11e+01, train loss: 1.15834e-06, val loss: 1.92384e-06, min loss: 1.13847e-06\n",
      "Epoch: 936700, elapsed: 1.12e+01, train loss: 1.31341e-06, val loss: 2.18958e-06, min loss: 1.13847e-06\n",
      "Epoch: 936800, elapsed: 1.41e+01, train loss: 1.13861e-06, val loss: 1.91426e-06, min loss: 1.13847e-06\n",
      "Epoch: 936900, elapsed: 1.15e+01, train loss: 1.87032e-06, val loss: 2.55632e-06, min loss: 1.13847e-06\n",
      "Epoch: 937000, elapsed: 1.14e+01, train loss: 1.32691e-06, val loss: 2.08620e-06, min loss: 1.13847e-06\n",
      "Epoch: 937100, elapsed: 1.13e+01, train loss: 1.26629e-06, val loss: 2.06647e-06, min loss: 1.13847e-06\n",
      "Epoch: 937200, elapsed: 1.15e+01, train loss: 1.16832e-06, val loss: 1.97117e-06, min loss: 1.13847e-06\n",
      "Epoch: 937300, elapsed: 1.15e+01, train loss: 1.67655e-06, val loss: 2.30613e-06, min loss: 1.13847e-06\n",
      "Epoch: 937400, elapsed: 1.13e+01, train loss: 1.13488e-06, val loss: 1.90912e-06, min loss: 1.13488e-06\n",
      "Epoch: 937500, elapsed: 1.13e+01, train loss: 1.14739e-06, val loss: 1.91906e-06, min loss: 1.13488e-06\n",
      "Epoch: 937600, elapsed: 1.13e+01, train loss: 2.29107e-06, val loss: 2.66783e-06, min loss: 1.13488e-06\n",
      "Epoch: 937700, elapsed: 1.13e+01, train loss: 4.04911e-06, val loss: 5.50755e-06, min loss: 1.13488e-06\n",
      "Epoch: 937800, elapsed: 1.12e+01, train loss: 1.80268e-06, val loss: 2.59152e-06, min loss: 1.13488e-06\n",
      "Epoch: 937900, elapsed: 1.12e+01, train loss: 1.38690e-06, val loss: 2.14348e-06, min loss: 1.13488e-06\n",
      "Epoch: 938000, elapsed: 1.13e+01, train loss: 1.31701e-06, val loss: 2.17029e-06, min loss: 1.13488e-06\n",
      "Epoch: 938100, elapsed: 1.11e+01, train loss: 3.44763e-06, val loss: 3.88966e-06, min loss: 1.13488e-06\n",
      "Epoch: 938200, elapsed: 1.12e+01, train loss: 1.13349e-06, val loss: 1.90553e-06, min loss: 1.13349e-06\n",
      "Epoch: 938300, elapsed: 1.13e+01, train loss: 1.16736e-06, val loss: 1.92511e-06, min loss: 1.13349e-06\n",
      "Epoch: 938400, elapsed: 1.13e+01, train loss: 1.13362e-06, val loss: 1.91260e-06, min loss: 1.13349e-06\n",
      "Epoch: 938500, elapsed: 1.12e+01, train loss: 1.14354e-06, val loss: 1.90791e-06, min loss: 1.13349e-06\n",
      "Epoch: 938600, elapsed: 1.42e+01, train loss: 6.95999e-06, val loss: 5.76964e-06, min loss: 1.13349e-06\n",
      "Epoch: 938700, elapsed: 1.15e+01, train loss: 1.13349e-06, val loss: 1.90738e-06, min loss: 1.13349e-06\n",
      "Epoch: 938800, elapsed: 1.13e+01, train loss: 1.14287e-06, val loss: 1.91219e-06, min loss: 1.13349e-06\n",
      "Epoch: 938900, elapsed: 1.14e+01, train loss: 1.61423e-06, val loss: 2.32718e-06, min loss: 1.13349e-06\n",
      "Epoch: 939000, elapsed: 1.14e+01, train loss: 1.14155e-06, val loss: 1.89831e-06, min loss: 1.13349e-06\n",
      "Epoch: 939100, elapsed: 1.12e+01, train loss: 1.16476e-06, val loss: 1.96413e-06, min loss: 1.13349e-06\n",
      "Epoch: 939200, elapsed: 1.14e+01, train loss: 1.15051e-06, val loss: 1.93593e-06, min loss: 1.13349e-06\n",
      "Epoch: 939300, elapsed: 1.16e+01, train loss: 1.58243e-06, val loss: 2.52714e-06, min loss: 1.13349e-06\n",
      "Epoch: 939400, elapsed: 1.14e+01, train loss: 1.18475e-06, val loss: 1.93669e-06, min loss: 1.13349e-06\n",
      "Epoch: 939500, elapsed: 1.13e+01, train loss: 1.13255e-06, val loss: 1.90102e-06, min loss: 1.13255e-06\n",
      "Epoch: 939600, elapsed: 1.13e+01, train loss: 1.53011e-06, val loss: 2.07162e-06, min loss: 1.13255e-06\n",
      "Epoch: 939700, elapsed: 1.14e+01, train loss: 2.46407e-06, val loss: 2.64318e-06, min loss: 1.13255e-06\n",
      "Epoch: 939800, elapsed: 1.14e+01, train loss: 3.70226e-06, val loss: 5.18126e-06, min loss: 1.13255e-06\n",
      "Epoch: 939900, elapsed: 1.14e+01, train loss: 1.13159e-06, val loss: 1.90218e-06, min loss: 1.13159e-06\n",
      "Epoch: 940000, elapsed: 1.12e+01, train loss: 1.15502e-06, val loss: 1.91430e-06, min loss: 1.13159e-06\n",
      "Epoch: 940100, elapsed: 1.34e+01, train loss: 1.17032e-06, val loss: 1.93549e-06, min loss: 1.13159e-06\n",
      "Epoch: 940200, elapsed: 1.12e+01, train loss: 1.16124e-06, val loss: 1.93695e-06, min loss: 1.13159e-06\n",
      "Epoch: 940300, elapsed: 1.42e+01, train loss: 1.17084e-06, val loss: 1.96170e-06, min loss: 1.13159e-06\n",
      "Epoch: 940400, elapsed: 1.15e+01, train loss: 1.14016e-06, val loss: 1.92120e-06, min loss: 1.13159e-06\n",
      "Epoch: 940500, elapsed: 1.13e+01, train loss: 1.64410e-06, val loss: 2.53601e-06, min loss: 1.13159e-06\n",
      "Epoch: 940600, elapsed: 1.14e+01, train loss: 3.78442e-06, val loss: 4.01757e-06, min loss: 1.13159e-06\n",
      "Epoch: 940700, elapsed: 1.12e+01, train loss: 1.41777e-06, val loss: 1.95255e-06, min loss: 1.13159e-06\n",
      "Epoch: 940800, elapsed: 1.13e+01, train loss: 1.17563e-06, val loss: 1.96732e-06, min loss: 1.13159e-06\n",
      "Epoch: 940900, elapsed: 1.13e+01, train loss: 1.94782e-06, val loss: 2.76109e-06, min loss: 1.13159e-06\n",
      "Epoch: 941000, elapsed: 1.15e+01, train loss: 1.77448e-06, val loss: 2.41666e-06, min loss: 1.13159e-06\n",
      "Epoch: 941100, elapsed: 1.14e+01, train loss: 1.26792e-06, val loss: 2.41786e-06, min loss: 1.13159e-06\n",
      "Epoch: 941200, elapsed: 1.12e+01, train loss: 1.40161e-06, val loss: 2.31833e-06, min loss: 1.13159e-06\n",
      "Epoch: 941300, elapsed: 1.14e+01, train loss: 2.75735e-06, val loss: 3.68318e-06, min loss: 1.13159e-06\n",
      "Epoch: 941400, elapsed: 1.11e+01, train loss: 4.84167e-06, val loss: 5.56676e-06, min loss: 1.13159e-06\n",
      "Epoch: 941500, elapsed: 1.13e+01, train loss: 3.30660e-06, val loss: 4.75586e-06, min loss: 1.13159e-06\n",
      "Epoch: 941600, elapsed: 1.14e+01, train loss: 1.13143e-06, val loss: 1.90520e-06, min loss: 1.13143e-06\n",
      "Epoch: 941700, elapsed: 1.12e+01, train loss: 1.13314e-06, val loss: 1.89863e-06, min loss: 1.13143e-06\n",
      "Epoch: 941800, elapsed: 1.12e+01, train loss: 1.14148e-06, val loss: 1.91208e-06, min loss: 1.13143e-06\n",
      "Epoch: 941900, elapsed: 1.13e+01, train loss: 1.29465e-06, val loss: 2.05158e-06, min loss: 1.13143e-06\n",
      "Epoch: 942000, elapsed: 1.12e+01, train loss: 3.01767e-06, val loss: 3.07462e-06, min loss: 1.13143e-06\n",
      "Epoch: 942100, elapsed: 1.40e+01, train loss: 1.82683e-06, val loss: 2.51532e-06, min loss: 1.13143e-06\n",
      "Epoch: 942200, elapsed: 1.13e+01, train loss: 3.14551e-06, val loss: 4.91808e-06, min loss: 1.13143e-06\n",
      "Epoch: 942300, elapsed: 1.15e+01, train loss: 1.12880e-06, val loss: 1.89896e-06, min loss: 1.12880e-06\n",
      "Epoch: 942400, elapsed: 1.14e+01, train loss: 1.28808e-06, val loss: 2.41672e-06, min loss: 1.12880e-06\n",
      "Epoch: 942500, elapsed: 1.13e+01, train loss: 1.13308e-06, val loss: 1.88886e-06, min loss: 1.12880e-06\n",
      "Epoch: 942600, elapsed: 1.13e+01, train loss: 1.17860e-06, val loss: 1.91890e-06, min loss: 1.12880e-06\n",
      "Epoch: 942700, elapsed: 1.13e+01, train loss: 1.14649e-06, val loss: 1.91608e-06, min loss: 1.12880e-06\n",
      "Epoch: 942800, elapsed: 1.14e+01, train loss: 1.38980e-06, val loss: 2.17204e-06, min loss: 1.12880e-06\n",
      "Epoch: 942900, elapsed: 1.13e+01, train loss: 1.14929e-06, val loss: 1.93831e-06, min loss: 1.12880e-06\n",
      "Epoch: 943000, elapsed: 1.12e+01, train loss: 1.12864e-06, val loss: 1.89298e-06, min loss: 1.12864e-06\n",
      "Epoch: 943100, elapsed: 1.10e+01, train loss: 1.23598e-06, val loss: 2.03140e-06, min loss: 1.12864e-06\n",
      "Epoch: 943200, elapsed: 1.12e+01, train loss: 1.13867e-06, val loss: 1.90511e-06, min loss: 1.12864e-06\n",
      "Epoch: 943300, elapsed: 1.12e+01, train loss: 1.14309e-06, val loss: 1.88480e-06, min loss: 1.12864e-06\n",
      "Epoch: 943400, elapsed: 1.13e+01, train loss: 1.13265e-06, val loss: 1.89932e-06, min loss: 1.12864e-06\n",
      "Epoch: 943500, elapsed: 1.12e+01, train loss: 1.13009e-06, val loss: 1.90162e-06, min loss: 1.12864e-06\n",
      "Epoch: 943600, elapsed: 1.13e+01, train loss: 1.13659e-06, val loss: 1.90110e-06, min loss: 1.12864e-06\n",
      "Epoch: 943700, elapsed: 1.11e+01, train loss: 1.29121e-06, val loss: 2.02914e-06, min loss: 1.12864e-06\n",
      "Epoch: 943800, elapsed: 1.11e+01, train loss: 1.14020e-06, val loss: 1.92463e-06, min loss: 1.12864e-06\n",
      "Epoch: 943900, elapsed: 1.41e+01, train loss: 1.14149e-06, val loss: 1.95304e-06, min loss: 1.12864e-06\n",
      "Epoch: 944000, elapsed: 1.14e+01, train loss: 1.13552e-06, val loss: 1.89660e-06, min loss: 1.12864e-06\n",
      "Epoch: 944100, elapsed: 1.16e+01, train loss: 1.13034e-06, val loss: 1.91020e-06, min loss: 1.12864e-06\n",
      "Epoch: 944200, elapsed: 1.14e+01, train loss: 1.89606e-06, val loss: 2.36887e-06, min loss: 1.12864e-06\n",
      "Epoch: 944300, elapsed: 1.13e+01, train loss: 1.42674e-06, val loss: 2.39362e-06, min loss: 1.12864e-06\n",
      "Epoch: 944400, elapsed: 1.13e+01, train loss: 1.12659e-06, val loss: 1.89510e-06, min loss: 1.12659e-06\n",
      "Epoch: 944500, elapsed: 1.13e+01, train loss: 1.13212e-06, val loss: 1.88794e-06, min loss: 1.12659e-06\n",
      "Epoch: 944600, elapsed: 1.15e+01, train loss: 1.14616e-06, val loss: 1.91607e-06, min loss: 1.12659e-06\n",
      "Epoch: 944700, elapsed: 1.13e+01, train loss: 1.12603e-06, val loss: 1.89884e-06, min loss: 1.12603e-06\n",
      "Epoch: 944800, elapsed: 1.13e+01, train loss: 1.15166e-06, val loss: 1.93324e-06, min loss: 1.12603e-06\n",
      "Epoch: 944900, elapsed: 1.12e+01, train loss: 1.13416e-06, val loss: 1.92175e-06, min loss: 1.12603e-06\n",
      "Epoch: 945000, elapsed: 1.13e+01, train loss: 1.37278e-06, val loss: 2.04115e-06, min loss: 1.12603e-06\n",
      "Epoch: 945100, elapsed: 1.32e+01, train loss: 1.40595e-06, val loss: 2.13132e-06, min loss: 1.12603e-06\n",
      "Epoch: 945200, elapsed: 1.13e+01, train loss: 1.69478e-06, val loss: 2.44641e-06, min loss: 1.12603e-06\n",
      "Epoch: 945300, elapsed: 1.11e+01, train loss: 1.70863e-06, val loss: 2.18028e-06, min loss: 1.12603e-06\n",
      "Epoch: 945400, elapsed: 1.11e+01, train loss: 5.37388e-06, val loss: 6.16913e-06, min loss: 1.12603e-06\n",
      "Epoch: 945500, elapsed: 1.10e+01, train loss: 1.20182e-06, val loss: 1.92501e-06, min loss: 1.12603e-06\n",
      "Epoch: 945600, elapsed: 1.40e+01, train loss: 1.12576e-06, val loss: 1.89677e-06, min loss: 1.12576e-06\n",
      "Epoch: 945700, elapsed: 1.15e+01, train loss: 1.14851e-06, val loss: 1.93130e-06, min loss: 1.12576e-06\n",
      "Epoch: 945800, elapsed: 1.12e+01, train loss: 1.12481e-06, val loss: 1.89649e-06, min loss: 1.12481e-06\n",
      "Epoch: 945900, elapsed: 1.14e+01, train loss: 1.79857e-06, val loss: 2.71973e-06, min loss: 1.12481e-06\n",
      "Epoch: 946000, elapsed: 1.13e+01, train loss: 1.12458e-06, val loss: 1.89228e-06, min loss: 1.12458e-06\n",
      "Epoch: 946100, elapsed: 1.13e+01, train loss: 1.14101e-06, val loss: 1.89821e-06, min loss: 1.12458e-06\n",
      "Epoch: 946200, elapsed: 1.12e+01, train loss: 1.18483e-06, val loss: 1.92096e-06, min loss: 1.12458e-06\n",
      "Epoch: 946300, elapsed: 1.12e+01, train loss: 1.20942e-06, val loss: 1.96695e-06, min loss: 1.12458e-06\n",
      "Epoch: 946400, elapsed: 1.11e+01, train loss: 4.26059e-06, val loss: 4.91614e-06, min loss: 1.12458e-06\n",
      "Epoch: 946500, elapsed: 1.14e+01, train loss: 2.16524e-06, val loss: 2.98903e-06, min loss: 1.12458e-06\n",
      "Epoch: 946600, elapsed: 1.12e+01, train loss: 5.14701e-06, val loss: 5.99951e-06, min loss: 1.12458e-06\n",
      "Epoch: 946700, elapsed: 1.13e+01, train loss: 3.12990e-06, val loss: 3.92362e-06, min loss: 1.12458e-06\n",
      "Epoch: 946800, elapsed: 1.13e+01, train loss: 4.28991e-06, val loss: 5.41969e-06, min loss: 1.12458e-06\n",
      "Epoch: 946900, elapsed: 1.14e+01, train loss: 1.82483e-06, val loss: 2.89986e-06, min loss: 1.12458e-06\n",
      "Epoch: 947000, elapsed: 1.13e+01, train loss: 1.22376e-06, val loss: 1.98773e-06, min loss: 1.12458e-06\n",
      "Epoch: 947100, elapsed: 1.13e+01, train loss: 1.13405e-06, val loss: 1.91781e-06, min loss: 1.12458e-06\n",
      "Epoch: 947200, elapsed: 1.12e+01, train loss: 1.13011e-06, val loss: 1.90233e-06, min loss: 1.12458e-06\n",
      "Epoch: 947300, elapsed: 1.13e+01, train loss: 1.43855e-06, val loss: 2.03564e-06, min loss: 1.12458e-06\n",
      "Epoch: 947400, elapsed: 1.42e+01, train loss: 1.32214e-06, val loss: 2.05388e-06, min loss: 1.12458e-06\n",
      "Epoch: 947500, elapsed: 1.14e+01, train loss: 1.21724e-06, val loss: 1.91601e-06, min loss: 1.12458e-06\n",
      "Epoch: 947600, elapsed: 1.17e+01, train loss: 1.21866e-06, val loss: 2.02148e-06, min loss: 1.12458e-06\n",
      "Epoch: 947700, elapsed: 1.13e+01, train loss: 1.22475e-06, val loss: 1.99500e-06, min loss: 1.12458e-06\n",
      "Epoch: 947800, elapsed: 1.13e+01, train loss: 1.12339e-06, val loss: 1.89299e-06, min loss: 1.12339e-06\n",
      "Epoch: 947900, elapsed: 1.14e+01, train loss: 1.38296e-06, val loss: 1.98168e-06, min loss: 1.12339e-06\n",
      "Epoch: 948000, elapsed: 1.13e+01, train loss: 1.17248e-06, val loss: 1.97576e-06, min loss: 1.12339e-06\n",
      "Epoch: 948100, elapsed: 1.16e+01, train loss: 2.07969e-06, val loss: 2.93606e-06, min loss: 1.12339e-06\n",
      "Epoch: 948200, elapsed: 1.14e+01, train loss: 1.24219e-06, val loss: 2.05035e-06, min loss: 1.12339e-06\n",
      "Epoch: 948300, elapsed: 1.16e+01, train loss: 1.12220e-06, val loss: 1.88871e-06, min loss: 1.12220e-06\n",
      "Epoch: 948400, elapsed: 1.13e+01, train loss: 1.14064e-06, val loss: 1.89720e-06, min loss: 1.12220e-06\n",
      "Epoch: 948500, elapsed: 1.14e+01, train loss: 2.44844e-06, val loss: 2.62802e-06, min loss: 1.12220e-06\n",
      "Epoch: 948600, elapsed: 1.12e+01, train loss: 3.14314e-06, val loss: 4.54183e-06, min loss: 1.12220e-06\n",
      "Epoch: 948700, elapsed: 1.12e+01, train loss: 1.12190e-06, val loss: 1.89276e-06, min loss: 1.12190e-06\n",
      "Epoch: 948800, elapsed: 1.12e+01, train loss: 1.13918e-06, val loss: 1.92374e-06, min loss: 1.12190e-06\n",
      "Epoch: 948900, elapsed: 1.13e+01, train loss: 6.82682e-06, val loss: 8.51343e-06, min loss: 1.12190e-06\n",
      "Epoch: 949000, elapsed: 1.13e+01, train loss: 1.12120e-06, val loss: 1.88893e-06, min loss: 1.12120e-06\n",
      "Epoch: 949100, elapsed: 1.12e+01, train loss: 1.18932e-06, val loss: 1.92616e-06, min loss: 1.12120e-06\n",
      "Epoch: 949200, elapsed: 1.42e+01, train loss: 2.04153e-06, val loss: 3.21582e-06, min loss: 1.12120e-06\n",
      "Epoch: 949300, elapsed: 1.14e+01, train loss: 1.12598e-06, val loss: 1.89808e-06, min loss: 1.12120e-06\n",
      "Epoch: 949400, elapsed: 1.13e+01, train loss: 1.12056e-06, val loss: 1.88562e-06, min loss: 1.12056e-06\n",
      "Epoch: 949500, elapsed: 1.13e+01, train loss: 1.14073e-06, val loss: 1.93970e-06, min loss: 1.12056e-06\n",
      "Epoch: 949600, elapsed: 1.11e+01, train loss: 1.24172e-06, val loss: 1.99591e-06, min loss: 1.12056e-06\n",
      "Epoch: 949700, elapsed: 1.12e+01, train loss: 1.26916e-06, val loss: 1.97814e-06, min loss: 1.12056e-06\n",
      "Epoch: 949800, elapsed: 1.13e+01, train loss: 1.21419e-06, val loss: 2.04692e-06, min loss: 1.12056e-06\n",
      "Epoch: 949900, elapsed: 1.13e+01, train loss: 1.32297e-06, val loss: 1.91623e-06, min loss: 1.12056e-06\n",
      "Epoch: 950000, elapsed: 1.12e+01, train loss: 4.03982e-06, val loss: 3.51990e-06, min loss: 1.12056e-06\n",
      "Epoch: 950100, elapsed: 1.33e+01, train loss: 1.20828e-06, val loss: 2.00926e-06, min loss: 1.12056e-06\n",
      "Epoch: 950200, elapsed: 1.14e+01, train loss: 1.12151e-06, val loss: 1.88491e-06, min loss: 1.12056e-06\n",
      "Epoch: 950300, elapsed: 1.15e+01, train loss: 1.12116e-06, val loss: 1.88090e-06, min loss: 1.12056e-06\n",
      "Epoch: 950400, elapsed: 1.14e+01, train loss: 1.15566e-06, val loss: 1.87536e-06, min loss: 1.12056e-06\n",
      "Epoch: 950500, elapsed: 1.13e+01, train loss: 1.20475e-06, val loss: 1.92217e-06, min loss: 1.12056e-06\n",
      "Epoch: 950600, elapsed: 1.14e+01, train loss: 1.35167e-06, val loss: 2.18714e-06, min loss: 1.12056e-06\n",
      "Epoch: 950700, elapsed: 1.13e+01, train loss: 1.13932e-06, val loss: 1.92654e-06, min loss: 1.12056e-06\n",
      "Epoch: 950800, elapsed: 1.11e+01, train loss: 1.11926e-06, val loss: 1.88232e-06, min loss: 1.11926e-06\n",
      "Epoch: 950900, elapsed: 1.14e+01, train loss: 1.16581e-06, val loss: 1.89593e-06, min loss: 1.11926e-06\n",
      "Epoch: 951000, elapsed: 1.41e+01, train loss: 1.13930e-06, val loss: 1.90097e-06, min loss: 1.11926e-06\n",
      "Epoch: 951100, elapsed: 1.15e+01, train loss: 1.12313e-06, val loss: 2.04309e-06, min loss: 1.11926e-06\n",
      "Epoch: 951200, elapsed: 1.15e+01, train loss: 1.49237e-06, val loss: 2.33553e-06, min loss: 1.11926e-06\n",
      "Epoch: 951300, elapsed: 1.14e+01, train loss: 1.39327e-06, val loss: 2.13318e-06, min loss: 1.11926e-06\n",
      "Epoch: 951400, elapsed: 1.15e+01, train loss: 4.72612e-06, val loss: 6.33041e-06, min loss: 1.11926e-06\n",
      "Epoch: 951500, elapsed: 1.14e+01, train loss: 1.15237e-06, val loss: 1.90539e-06, min loss: 1.11926e-06\n",
      "Epoch: 951600, elapsed: 1.13e+01, train loss: 1.12462e-06, val loss: 1.88255e-06, min loss: 1.11926e-06\n",
      "Epoch: 951700, elapsed: 1.13e+01, train loss: 1.11911e-06, val loss: 1.88302e-06, min loss: 1.11911e-06\n",
      "Epoch: 951800, elapsed: 1.13e+01, train loss: 1.12939e-06, val loss: 1.89227e-06, min loss: 1.11911e-06\n",
      "Epoch: 951900, elapsed: 1.15e+01, train loss: 1.15831e-06, val loss: 1.90452e-06, min loss: 1.11911e-06\n",
      "Epoch: 952000, elapsed: 1.14e+01, train loss: 1.32865e-06, val loss: 2.06185e-06, min loss: 1.11911e-06\n",
      "Epoch: 952100, elapsed: 1.13e+01, train loss: 1.36079e-06, val loss: 2.55169e-06, min loss: 1.11911e-06\n",
      "Epoch: 952200, elapsed: 1.13e+01, train loss: 1.40198e-06, val loss: 2.14047e-06, min loss: 1.11911e-06\n",
      "Epoch: 952300, elapsed: 1.13e+01, train loss: 1.13513e-06, val loss: 1.90715e-06, min loss: 1.11911e-06\n",
      "Epoch: 952400, elapsed: 1.14e+01, train loss: 1.12703e-06, val loss: 1.88204e-06, min loss: 1.11911e-06\n",
      "Epoch: 952500, elapsed: 1.13e+01, train loss: 1.48093e-06, val loss: 2.22146e-06, min loss: 1.11911e-06\n",
      "Epoch: 952600, elapsed: 1.12e+01, train loss: 1.17540e-06, val loss: 1.95758e-06, min loss: 1.11911e-06\n",
      "Epoch: 952700, elapsed: 1.14e+01, train loss: 2.88461e-06, val loss: 3.63781e-06, min loss: 1.11911e-06\n",
      "Epoch: 952800, elapsed: 1.41e+01, train loss: 1.11694e-06, val loss: 1.88343e-06, min loss: 1.11694e-06\n",
      "Epoch: 952900, elapsed: 1.15e+01, train loss: 1.12190e-06, val loss: 1.89554e-06, min loss: 1.11694e-06\n",
      "Epoch: 953000, elapsed: 1.14e+01, train loss: 1.42346e-06, val loss: 2.18339e-06, min loss: 1.11694e-06\n",
      "Epoch: 953100, elapsed: 1.15e+01, train loss: 4.28332e-06, val loss: 4.57345e-06, min loss: 1.11694e-06\n",
      "Epoch: 953200, elapsed: 1.16e+01, train loss: 1.23275e-06, val loss: 2.07948e-06, min loss: 1.11694e-06\n",
      "Epoch: 953300, elapsed: 1.14e+01, train loss: 1.11825e-06, val loss: 1.87853e-06, min loss: 1.11694e-06\n",
      "Epoch: 953400, elapsed: 1.13e+01, train loss: 2.76983e-06, val loss: 4.06258e-06, min loss: 1.11694e-06\n",
      "Epoch: 953500, elapsed: 1.14e+01, train loss: 1.11684e-06, val loss: 1.87779e-06, min loss: 1.11684e-06\n",
      "Epoch: 953600, elapsed: 1.13e+01, train loss: 1.11784e-06, val loss: 1.88789e-06, min loss: 1.11684e-06\n",
      "Epoch: 953700, elapsed: 1.13e+01, train loss: 1.47101e-06, val loss: 2.68894e-06, min loss: 1.11684e-06\n",
      "Epoch: 953800, elapsed: 1.13e+01, train loss: 1.11953e-06, val loss: 1.87847e-06, min loss: 1.11684e-06\n",
      "Epoch: 953900, elapsed: 1.14e+01, train loss: 1.11857e-06, val loss: 1.87717e-06, min loss: 1.11684e-06\n",
      "Epoch: 954000, elapsed: 1.12e+01, train loss: 1.17144e-06, val loss: 1.93543e-06, min loss: 1.11684e-06\n",
      "Epoch: 954100, elapsed: 1.14e+01, train loss: 1.14647e-06, val loss: 1.91224e-06, min loss: 1.11684e-06\n",
      "Epoch: 954200, elapsed: 1.13e+01, train loss: 1.14573e-06, val loss: 1.90351e-06, min loss: 1.11684e-06\n",
      "Epoch: 954300, elapsed: 1.12e+01, train loss: 1.13150e-06, val loss: 1.87651e-06, min loss: 1.11684e-06\n",
      "Epoch: 954400, elapsed: 1.13e+01, train loss: 1.13323e-06, val loss: 1.87569e-06, min loss: 1.11684e-06\n",
      "Epoch: 954500, elapsed: 1.13e+01, train loss: 1.12626e-06, val loss: 1.89727e-06, min loss: 1.11684e-06\n",
      "Epoch: 954600, elapsed: 1.41e+01, train loss: 1.67531e-06, val loss: 2.62191e-06, min loss: 1.11684e-06\n",
      "Epoch: 954700, elapsed: 1.15e+01, train loss: 2.21177e-06, val loss: 2.56380e-06, min loss: 1.11684e-06\n",
      "Epoch: 954800, elapsed: 1.15e+01, train loss: 3.65926e-06, val loss: 4.30265e-06, min loss: 1.11684e-06\n",
      "Epoch: 954900, elapsed: 1.15e+01, train loss: 1.73955e-06, val loss: 2.24930e-06, min loss: 1.11684e-06\n",
      "Epoch: 955000, elapsed: 1.14e+01, train loss: 2.78637e-06, val loss: 2.53264e-06, min loss: 1.11684e-06\n",
      "Epoch: 955100, elapsed: 1.34e+01, train loss: 1.11806e-06, val loss: 1.89202e-06, min loss: 1.11684e-06\n",
      "Epoch: 955200, elapsed: 1.14e+01, train loss: 1.11734e-06, val loss: 1.88081e-06, min loss: 1.11684e-06\n",
      "Epoch: 955300, elapsed: 1.12e+01, train loss: 1.16310e-06, val loss: 1.91395e-06, min loss: 1.11684e-06\n",
      "Epoch: 955400, elapsed: 1.13e+01, train loss: 1.56039e-06, val loss: 2.53527e-06, min loss: 1.11684e-06\n",
      "Epoch: 955500, elapsed: 1.14e+01, train loss: 1.11857e-06, val loss: 1.89039e-06, min loss: 1.11684e-06\n",
      "Epoch: 955600, elapsed: 1.15e+01, train loss: 1.11578e-06, val loss: 1.87492e-06, min loss: 1.11578e-06\n",
      "Epoch: 955700, elapsed: 1.14e+01, train loss: 1.13576e-06, val loss: 1.89950e-06, min loss: 1.11578e-06\n",
      "Epoch: 955800, elapsed: 1.14e+01, train loss: 1.39073e-06, val loss: 1.99926e-06, min loss: 1.11578e-06\n",
      "Epoch: 955900, elapsed: 1.13e+01, train loss: 5.15153e-06, val loss: 6.10463e-06, min loss: 1.11578e-06\n",
      "Epoch: 956000, elapsed: 1.12e+01, train loss: 2.17455e-06, val loss: 3.15158e-06, min loss: 1.11578e-06\n",
      "Epoch: 956100, elapsed: 1.14e+01, train loss: 1.11565e-06, val loss: 1.88377e-06, min loss: 1.11565e-06\n",
      "Epoch: 956200, elapsed: 1.13e+01, train loss: 1.11583e-06, val loss: 1.87058e-06, min loss: 1.11565e-06\n",
      "Epoch: 956300, elapsed: 1.43e+01, train loss: 1.12070e-06, val loss: 1.88193e-06, min loss: 1.11565e-06\n",
      "Epoch: 956400, elapsed: 1.14e+01, train loss: 1.12619e-06, val loss: 1.89606e-06, min loss: 1.11565e-06\n",
      "Epoch: 956500, elapsed: 1.15e+01, train loss: 1.96975e-06, val loss: 2.29350e-06, min loss: 1.11565e-06\n",
      "Epoch: 956600, elapsed: 1.15e+01, train loss: 4.79811e-06, val loss: 4.36683e-06, min loss: 1.11565e-06\n",
      "Epoch: 956700, elapsed: 1.13e+01, train loss: 1.16820e-06, val loss: 1.95921e-06, min loss: 1.11565e-06\n",
      "Epoch: 956800, elapsed: 1.12e+01, train loss: 1.21092e-06, val loss: 1.96498e-06, min loss: 1.11565e-06\n",
      "Epoch: 956900, elapsed: 1.15e+01, train loss: 1.12667e-06, val loss: 1.91964e-06, min loss: 1.11565e-06\n",
      "Epoch: 957000, elapsed: 1.14e+01, train loss: 1.11306e-06, val loss: 1.87353e-06, min loss: 1.11306e-06\n",
      "Epoch: 957100, elapsed: 1.13e+01, train loss: 1.12822e-06, val loss: 1.87694e-06, min loss: 1.11306e-06\n",
      "Epoch: 957200, elapsed: 1.13e+01, train loss: 4.82303e-06, val loss: 4.85917e-06, min loss: 1.11306e-06\n",
      "Epoch: 957300, elapsed: 1.15e+01, train loss: 1.11156e-06, val loss: 1.87357e-06, min loss: 1.11156e-06\n",
      "Epoch: 957400, elapsed: 1.14e+01, train loss: 1.77974e-06, val loss: 2.50635e-06, min loss: 1.11156e-06\n",
      "Epoch: 957500, elapsed: 1.13e+01, train loss: 1.37092e-06, val loss: 2.11058e-06, min loss: 1.11156e-06\n",
      "Epoch: 957600, elapsed: 1.14e+01, train loss: 1.61077e-06, val loss: 2.48426e-06, min loss: 1.11156e-06\n",
      "Epoch: 957700, elapsed: 1.12e+01, train loss: 2.00380e-06, val loss: 2.50192e-06, min loss: 1.11156e-06\n",
      "Epoch: 957800, elapsed: 1.12e+01, train loss: 1.11308e-06, val loss: 1.87807e-06, min loss: 1.11156e-06\n",
      "Epoch: 957900, elapsed: 1.12e+01, train loss: 1.11394e-06, val loss: 1.87446e-06, min loss: 1.11156e-06\n",
      "Epoch: 958000, elapsed: 1.13e+01, train loss: 1.11715e-06, val loss: 1.87518e-06, min loss: 1.11156e-06\n",
      "Epoch: 958100, elapsed: 1.42e+01, train loss: 1.13560e-06, val loss: 1.87887e-06, min loss: 1.11156e-06\n",
      "Epoch: 958200, elapsed: 1.13e+01, train loss: 1.69648e-06, val loss: 2.60009e-06, min loss: 1.11156e-06\n",
      "Epoch: 958300, elapsed: 1.12e+01, train loss: 1.14407e-06, val loss: 1.90004e-06, min loss: 1.11156e-06\n",
      "Epoch: 958400, elapsed: 1.14e+01, train loss: 1.89403e-06, val loss: 2.43695e-06, min loss: 1.11156e-06\n",
      "Epoch: 958500, elapsed: 1.14e+01, train loss: 1.18624e-06, val loss: 1.92370e-06, min loss: 1.11156e-06\n",
      "Epoch: 958600, elapsed: 1.13e+01, train loss: 1.17699e-06, val loss: 1.95068e-06, min loss: 1.11156e-06\n",
      "Epoch: 958700, elapsed: 1.15e+01, train loss: 1.85420e-06, val loss: 2.66597e-06, min loss: 1.11156e-06\n",
      "Epoch: 958800, elapsed: 1.17e+01, train loss: 1.11789e-06, val loss: 1.88464e-06, min loss: 1.11156e-06\n",
      "Epoch: 958900, elapsed: 1.14e+01, train loss: 1.14691e-06, val loss: 1.89320e-06, min loss: 1.11156e-06\n",
      "Epoch: 959000, elapsed: 1.14e+01, train loss: 1.86685e-06, val loss: 2.73627e-06, min loss: 1.11156e-06\n",
      "Epoch: 959100, elapsed: 1.14e+01, train loss: 1.23490e-06, val loss: 1.95334e-06, min loss: 1.11156e-06\n",
      "Epoch: 959200, elapsed: 1.14e+01, train loss: 1.11024e-06, val loss: 1.87315e-06, min loss: 1.11024e-06\n",
      "Epoch: 959300, elapsed: 1.12e+01, train loss: 1.12312e-06, val loss: 1.86931e-06, min loss: 1.11024e-06\n",
      "Epoch: 959400, elapsed: 1.14e+01, train loss: 1.14785e-06, val loss: 1.86847e-06, min loss: 1.11024e-06\n",
      "Epoch: 959500, elapsed: 1.12e+01, train loss: 2.65023e-06, val loss: 3.60953e-06, min loss: 1.11024e-06\n",
      "Epoch: 959600, elapsed: 1.13e+01, train loss: 1.27862e-06, val loss: 2.12226e-06, min loss: 1.11024e-06\n",
      "Epoch: 959700, elapsed: 1.11e+01, train loss: 1.94846e-06, val loss: 2.49657e-06, min loss: 1.11024e-06\n",
      "Epoch: 959800, elapsed: 1.14e+01, train loss: 1.12842e-06, val loss: 1.90401e-06, min loss: 1.11024e-06\n",
      "Epoch: 959900, elapsed: 1.41e+01, train loss: 2.70367e-06, val loss: 2.83104e-06, min loss: 1.11024e-06\n",
      "Epoch: 960000, elapsed: 1.15e+01, train loss: 1.11198e-06, val loss: 1.88001e-06, min loss: 1.11024e-06\n",
      "Epoch: 960100, elapsed: 1.34e+01, train loss: 1.11210e-06, val loss: 1.87554e-06, min loss: 1.11024e-06\n",
      "Epoch: 960200, elapsed: 1.15e+01, train loss: 1.26946e-06, val loss: 1.88181e-06, min loss: 1.11024e-06\n",
      "Epoch: 960300, elapsed: 1.14e+01, train loss: 2.16673e-06, val loss: 2.61067e-06, min loss: 1.11024e-06\n",
      "Epoch: 960400, elapsed: 1.15e+01, train loss: 1.12535e-06, val loss: 1.87251e-06, min loss: 1.11024e-06\n",
      "Epoch: 960500, elapsed: 1.15e+01, train loss: 1.29651e-06, val loss: 1.99991e-06, min loss: 1.11024e-06\n",
      "Epoch: 960600, elapsed: 1.13e+01, train loss: 1.67142e-06, val loss: 2.64930e-06, min loss: 1.11024e-06\n",
      "Epoch: 960700, elapsed: 1.13e+01, train loss: 1.33814e-06, val loss: 2.08882e-06, min loss: 1.11024e-06\n",
      "Epoch: 960800, elapsed: 1.13e+01, train loss: 1.11263e-06, val loss: 1.87646e-06, min loss: 1.11024e-06\n",
      "Epoch: 960900, elapsed: 1.13e+01, train loss: 1.19889e-06, val loss: 2.10422e-06, min loss: 1.11024e-06\n",
      "Epoch: 961000, elapsed: 1.12e+01, train loss: 1.22271e-06, val loss: 2.02205e-06, min loss: 1.11024e-06\n",
      "Epoch: 961100, elapsed: 1.13e+01, train loss: 1.66529e-06, val loss: 2.21317e-06, min loss: 1.11024e-06\n",
      "Epoch: 961200, elapsed: 1.13e+01, train loss: 1.22990e-06, val loss: 1.91185e-06, min loss: 1.11024e-06\n",
      "Epoch: 961300, elapsed: 1.13e+01, train loss: 1.35598e-06, val loss: 2.02381e-06, min loss: 1.11024e-06\n",
      "Epoch: 961400, elapsed: 1.13e+01, train loss: 1.41324e-06, val loss: 2.26215e-06, min loss: 1.11024e-06\n",
      "Epoch: 961500, elapsed: 1.13e+01, train loss: 1.11472e-06, val loss: 1.86617e-06, min loss: 1.11024e-06\n",
      "Epoch: 961600, elapsed: 1.12e+01, train loss: 1.80897e-06, val loss: 2.47120e-06, min loss: 1.11024e-06\n",
      "Epoch: 961700, elapsed: 1.41e+01, train loss: 1.55402e-06, val loss: 2.31888e-06, min loss: 1.11024e-06\n",
      "Epoch: 961800, elapsed: 1.14e+01, train loss: 1.23119e-06, val loss: 1.88305e-06, min loss: 1.11024e-06\n",
      "Epoch: 961900, elapsed: 1.12e+01, train loss: 3.96807e-06, val loss: 4.78402e-06, min loss: 1.11024e-06\n",
      "Epoch: 962000, elapsed: 1.15e+01, train loss: 1.13737e-06, val loss: 2.04878e-06, min loss: 1.11024e-06\n",
      "Epoch: 962100, elapsed: 1.15e+01, train loss: 2.80427e-06, val loss: 4.18755e-06, min loss: 1.11024e-06\n",
      "Epoch: 962200, elapsed: 1.14e+01, train loss: 1.10630e-06, val loss: 1.86755e-06, min loss: 1.10630e-06\n",
      "Epoch: 962300, elapsed: 1.13e+01, train loss: 1.15944e-06, val loss: 1.94357e-06, min loss: 1.10630e-06\n",
      "Epoch: 962400, elapsed: 1.14e+01, train loss: 1.28105e-06, val loss: 1.95817e-06, min loss: 1.10630e-06\n",
      "Epoch: 962500, elapsed: 1.13e+01, train loss: 3.63181e-06, val loss: 5.42411e-06, min loss: 1.10630e-06\n",
      "Epoch: 962600, elapsed: 1.12e+01, train loss: 1.62174e-06, val loss: 2.44157e-06, min loss: 1.10630e-06\n",
      "Epoch: 962700, elapsed: 1.14e+01, train loss: 1.17312e-06, val loss: 1.94234e-06, min loss: 1.10630e-06\n",
      "Epoch: 962800, elapsed: 1.13e+01, train loss: 1.17254e-06, val loss: 1.92125e-06, min loss: 1.10630e-06\n",
      "Epoch: 962900, elapsed: 1.12e+01, train loss: 1.18204e-06, val loss: 2.05530e-06, min loss: 1.10630e-06\n",
      "Epoch: 963000, elapsed: 1.13e+01, train loss: 3.94290e-06, val loss: 5.52744e-06, min loss: 1.10630e-06\n",
      "Epoch: 963100, elapsed: 1.13e+01, train loss: 1.40377e-06, val loss: 2.29363e-06, min loss: 1.10630e-06\n",
      "Epoch: 963200, elapsed: 1.12e+01, train loss: 1.10951e-06, val loss: 1.86398e-06, min loss: 1.10630e-06\n",
      "Epoch: 963300, elapsed: 1.12e+01, train loss: 1.10565e-06, val loss: 1.86635e-06, min loss: 1.10565e-06\n",
      "Epoch: 963400, elapsed: 1.14e+01, train loss: 1.39253e-06, val loss: 2.11497e-06, min loss: 1.10565e-06\n",
      "Epoch: 963500, elapsed: 1.41e+01, train loss: 1.67523e-06, val loss: 2.66699e-06, min loss: 1.10565e-06\n",
      "Epoch: 963600, elapsed: 1.15e+01, train loss: 1.81127e-06, val loss: 2.84512e-06, min loss: 1.10565e-06\n",
      "Epoch: 963700, elapsed: 1.16e+01, train loss: 1.13734e-06, val loss: 1.93214e-06, min loss: 1.10565e-06\n",
      "Epoch: 963800, elapsed: 1.15e+01, train loss: 1.11446e-06, val loss: 1.87020e-06, min loss: 1.10565e-06\n",
      "Epoch: 963900, elapsed: 1.14e+01, train loss: 1.12778e-06, val loss: 1.89374e-06, min loss: 1.10565e-06\n",
      "Epoch: 964000, elapsed: 1.14e+01, train loss: 1.12229e-06, val loss: 1.89396e-06, min loss: 1.10565e-06\n",
      "Epoch: 964100, elapsed: 1.13e+01, train loss: 2.58250e-06, val loss: 2.82565e-06, min loss: 1.10565e-06\n",
      "Epoch: 964200, elapsed: 1.15e+01, train loss: 1.10433e-06, val loss: 1.86280e-06, min loss: 1.10433e-06\n",
      "Epoch: 964300, elapsed: 1.15e+01, train loss: 1.12479e-06, val loss: 1.87490e-06, min loss: 1.10433e-06\n",
      "Epoch: 964400, elapsed: 1.11e+01, train loss: 2.44472e-06, val loss: 3.55651e-06, min loss: 1.10433e-06\n",
      "Epoch: 964500, elapsed: 1.12e+01, train loss: 1.10475e-06, val loss: 1.86106e-06, min loss: 1.10433e-06\n",
      "Epoch: 964600, elapsed: 1.12e+01, train loss: 1.11677e-06, val loss: 1.88292e-06, min loss: 1.10433e-06\n",
      "Epoch: 964700, elapsed: 1.12e+01, train loss: 1.15750e-06, val loss: 1.92976e-06, min loss: 1.10433e-06\n",
      "Epoch: 964800, elapsed: 1.12e+01, train loss: 2.29566e-06, val loss: 2.82683e-06, min loss: 1.10433e-06\n",
      "Epoch: 964900, elapsed: 1.13e+01, train loss: 1.11723e-06, val loss: 1.87183e-06, min loss: 1.10433e-06\n",
      "Epoch: 965000, elapsed: 1.12e+01, train loss: 1.10943e-06, val loss: 1.87149e-06, min loss: 1.10433e-06\n",
      "Epoch: 965100, elapsed: 1.33e+01, train loss: 1.13299e-06, val loss: 1.87951e-06, min loss: 1.10433e-06\n",
      "Epoch: 965200, elapsed: 1.11e+01, train loss: 1.61630e-06, val loss: 1.88181e-06, min loss: 1.10433e-06\n",
      "Epoch: 965300, elapsed: 1.41e+01, train loss: 1.22372e-06, val loss: 1.96875e-06, min loss: 1.10433e-06\n",
      "Epoch: 965400, elapsed: 1.14e+01, train loss: 1.10794e-06, val loss: 1.87171e-06, min loss: 1.10433e-06\n",
      "Epoch: 965500, elapsed: 1.14e+01, train loss: 1.10981e-06, val loss: 1.85983e-06, min loss: 1.10433e-06\n",
      "Epoch: 965600, elapsed: 1.13e+01, train loss: 1.12569e-06, val loss: 1.87198e-06, min loss: 1.10433e-06\n",
      "Epoch: 965700, elapsed: 1.14e+01, train loss: 1.13339e-06, val loss: 1.86106e-06, min loss: 1.10433e-06\n",
      "Epoch: 965800, elapsed: 1.12e+01, train loss: 1.15688e-06, val loss: 2.06290e-06, min loss: 1.10433e-06\n",
      "Epoch: 965900, elapsed: 1.13e+01, train loss: 1.71623e-06, val loss: 2.47484e-06, min loss: 1.10433e-06\n",
      "Epoch: 966000, elapsed: 1.14e+01, train loss: 1.12059e-06, val loss: 1.85718e-06, min loss: 1.10433e-06\n",
      "Epoch: 966100, elapsed: 1.13e+01, train loss: 1.84461e-06, val loss: 2.39696e-06, min loss: 1.10433e-06\n",
      "Epoch: 966200, elapsed: 1.13e+01, train loss: 1.10690e-06, val loss: 1.86409e-06, min loss: 1.10433e-06\n",
      "Epoch: 966300, elapsed: 1.13e+01, train loss: 1.10398e-06, val loss: 1.85905e-06, min loss: 1.10398e-06\n",
      "Epoch: 966400, elapsed: 1.12e+01, train loss: 1.10298e-06, val loss: 1.85854e-06, min loss: 1.10298e-06\n",
      "Epoch: 966500, elapsed: 1.15e+01, train loss: 1.13370e-06, val loss: 1.96182e-06, min loss: 1.10298e-06\n",
      "Epoch: 966600, elapsed: 1.12e+01, train loss: 1.10258e-06, val loss: 1.86725e-06, min loss: 1.10258e-06\n",
      "Epoch: 966700, elapsed: 1.12e+01, train loss: 1.10281e-06, val loss: 1.86209e-06, min loss: 1.10258e-06\n",
      "Epoch: 966800, elapsed: 1.11e+01, train loss: 1.12147e-06, val loss: 1.87585e-06, min loss: 1.10258e-06\n",
      "Epoch: 966900, elapsed: 1.13e+01, train loss: 1.70054e-06, val loss: 2.30636e-06, min loss: 1.10258e-06\n",
      "Epoch: 967000, elapsed: 1.14e+01, train loss: 1.47511e-06, val loss: 2.32167e-06, min loss: 1.10258e-06\n",
      "Epoch: 967100, elapsed: 1.40e+01, train loss: 1.33316e-06, val loss: 2.12125e-06, min loss: 1.10258e-06\n",
      "Epoch: 967200, elapsed: 1.13e+01, train loss: 1.21573e-06, val loss: 2.14003e-06, min loss: 1.10258e-06\n",
      "Epoch: 967300, elapsed: 1.14e+01, train loss: 1.28956e-06, val loss: 2.04797e-06, min loss: 1.10258e-06\n",
      "Epoch: 967400, elapsed: 1.14e+01, train loss: 1.10475e-06, val loss: 1.85973e-06, min loss: 1.10258e-06\n",
      "Epoch: 967500, elapsed: 1.16e+01, train loss: 1.10342e-06, val loss: 1.86683e-06, min loss: 1.10258e-06\n",
      "Epoch: 967600, elapsed: 1.13e+01, train loss: 1.10312e-06, val loss: 1.86455e-06, min loss: 1.10258e-06\n",
      "Epoch: 967700, elapsed: 1.13e+01, train loss: 1.17758e-06, val loss: 1.90045e-06, min loss: 1.10258e-06\n",
      "Epoch: 967800, elapsed: 1.13e+01, train loss: 1.61124e-06, val loss: 2.14215e-06, min loss: 1.10258e-06\n",
      "Epoch: 967900, elapsed: 1.13e+01, train loss: 1.33870e-06, val loss: 2.22198e-06, min loss: 1.10258e-06\n",
      "Epoch: 968000, elapsed: 1.12e+01, train loss: 1.34628e-06, val loss: 2.07726e-06, min loss: 1.10258e-06\n",
      "Epoch: 968100, elapsed: 1.12e+01, train loss: 1.42010e-06, val loss: 1.96523e-06, min loss: 1.10258e-06\n",
      "Epoch: 968200, elapsed: 1.15e+01, train loss: 1.83355e-06, val loss: 2.21876e-06, min loss: 1.10258e-06\n",
      "Epoch: 968300, elapsed: 1.11e+01, train loss: 1.19921e-06, val loss: 1.91005e-06, min loss: 1.10258e-06\n",
      "Epoch: 968400, elapsed: 1.14e+01, train loss: 1.13043e-06, val loss: 1.88935e-06, min loss: 1.10258e-06\n",
      "Epoch: 968500, elapsed: 1.13e+01, train loss: 1.19011e-06, val loss: 1.96592e-06, min loss: 1.10258e-06\n",
      "Epoch: 968600, elapsed: 1.13e+01, train loss: 1.15721e-06, val loss: 1.94295e-06, min loss: 1.10258e-06\n",
      "Epoch: 968700, elapsed: 1.12e+01, train loss: 2.04599e-06, val loss: 2.66826e-06, min loss: 1.10258e-06\n",
      "Epoch: 968800, elapsed: 1.12e+01, train loss: 1.70444e-06, val loss: 2.36322e-06, min loss: 1.10258e-06\n",
      "Epoch: 968900, elapsed: 1.12e+01, train loss: 1.14257e-06, val loss: 1.87495e-06, min loss: 1.10258e-06\n",
      "Epoch: 969000, elapsed: 1.41e+01, train loss: 1.10332e-06, val loss: 1.87463e-06, min loss: 1.10258e-06\n",
      "Epoch: 969100, elapsed: 1.14e+01, train loss: 1.23034e-06, val loss: 1.96471e-06, min loss: 1.10258e-06\n",
      "Epoch: 969200, elapsed: 1.13e+01, train loss: 2.93354e-06, val loss: 2.69629e-06, min loss: 1.10258e-06\n",
      "Epoch: 969300, elapsed: 1.14e+01, train loss: 1.18877e-06, val loss: 1.87213e-06, min loss: 1.10258e-06\n",
      "Epoch: 969400, elapsed: 1.13e+01, train loss: 1.19076e-06, val loss: 1.93104e-06, min loss: 1.10258e-06\n",
      "Epoch: 969500, elapsed: 1.14e+01, train loss: 2.46310e-06, val loss: 2.25190e-06, min loss: 1.10258e-06\n",
      "Epoch: 969600, elapsed: 1.16e+01, train loss: 1.57814e-06, val loss: 2.41682e-06, min loss: 1.10258e-06\n",
      "Epoch: 969700, elapsed: 1.14e+01, train loss: 1.16022e-06, val loss: 1.92718e-06, min loss: 1.10258e-06\n",
      "Epoch: 969800, elapsed: 1.13e+01, train loss: 1.10232e-06, val loss: 1.84653e-06, min loss: 1.10232e-06\n",
      "Epoch: 969900, elapsed: 1.12e+01, train loss: 1.26985e-06, val loss: 2.07465e-06, min loss: 1.10232e-06\n",
      "Epoch: 970000, elapsed: 1.13e+01, train loss: 1.32064e-06, val loss: 2.15786e-06, min loss: 1.10232e-06\n",
      "Epoch: 970100, elapsed: 1.36e+01, train loss: 1.13623e-06, val loss: 1.91860e-06, min loss: 1.10232e-06\n",
      "Epoch: 970200, elapsed: 1.13e+01, train loss: 1.10529e-06, val loss: 1.85552e-06, min loss: 1.10232e-06\n",
      "Epoch: 970300, elapsed: 1.12e+01, train loss: 1.10453e-06, val loss: 1.88427e-06, min loss: 1.10232e-06\n",
      "Epoch: 970400, elapsed: 1.13e+01, train loss: 1.12477e-06, val loss: 1.89461e-06, min loss: 1.10232e-06\n",
      "Epoch: 970500, elapsed: 1.12e+01, train loss: 1.15921e-06, val loss: 1.89458e-06, min loss: 1.10232e-06\n",
      "Epoch: 970600, elapsed: 1.12e+01, train loss: 2.09149e-06, val loss: 3.05717e-06, min loss: 1.10232e-06\n",
      "Epoch: 970700, elapsed: 1.14e+01, train loss: 4.58404e-06, val loss: 5.46832e-06, min loss: 1.10232e-06\n",
      "Epoch: 970800, elapsed: 1.42e+01, train loss: 1.80324e-06, val loss: 2.95485e-06, min loss: 1.10232e-06\n",
      "Epoch: 970900, elapsed: 1.14e+01, train loss: 1.09796e-06, val loss: 1.85044e-06, min loss: 1.09796e-06\n",
      "Epoch: 971000, elapsed: 1.12e+01, train loss: 1.10469e-06, val loss: 1.85620e-06, min loss: 1.09796e-06\n",
      "Epoch: 971100, elapsed: 1.15e+01, train loss: 1.18576e-06, val loss: 1.93442e-06, min loss: 1.09796e-06\n",
      "Epoch: 971200, elapsed: 1.14e+01, train loss: 2.65215e-06, val loss: 3.83285e-06, min loss: 1.09796e-06\n",
      "Epoch: 971300, elapsed: 1.15e+01, train loss: 1.76614e-06, val loss: 2.49605e-06, min loss: 1.09796e-06\n",
      "Epoch: 971400, elapsed: 1.16e+01, train loss: 1.13539e-06, val loss: 1.90419e-06, min loss: 1.09796e-06\n",
      "Epoch: 971500, elapsed: 1.15e+01, train loss: 1.37175e-06, val loss: 2.15653e-06, min loss: 1.09796e-06\n",
      "Epoch: 971600, elapsed: 1.13e+01, train loss: 1.11008e-06, val loss: 1.86978e-06, min loss: 1.09796e-06\n",
      "Epoch: 971700, elapsed: 1.15e+01, train loss: 1.10054e-06, val loss: 1.86206e-06, min loss: 1.09796e-06\n",
      "Epoch: 971800, elapsed: 1.14e+01, train loss: 1.09979e-06, val loss: 1.84856e-06, min loss: 1.09796e-06\n",
      "Epoch: 971900, elapsed: 1.14e+01, train loss: 1.11166e-06, val loss: 1.87554e-06, min loss: 1.09796e-06\n",
      "Epoch: 972000, elapsed: 1.13e+01, train loss: 1.11355e-06, val loss: 1.89376e-06, min loss: 1.09796e-06\n",
      "Epoch: 972100, elapsed: 1.12e+01, train loss: 1.37787e-06, val loss: 2.09388e-06, min loss: 1.09796e-06\n",
      "Epoch: 972200, elapsed: 1.13e+01, train loss: 2.49073e-06, val loss: 3.47131e-06, min loss: 1.09796e-06\n",
      "Epoch: 972300, elapsed: 1.12e+01, train loss: 1.09608e-06, val loss: 1.85079e-06, min loss: 1.09608e-06\n",
      "Epoch: 972400, elapsed: 1.14e+01, train loss: 1.11865e-06, val loss: 1.86053e-06, min loss: 1.09608e-06\n",
      "Epoch: 972500, elapsed: 1.15e+01, train loss: 1.14093e-06, val loss: 1.88461e-06, min loss: 1.09608e-06\n",
      "Epoch: 972600, elapsed: 1.41e+01, train loss: 1.23778e-06, val loss: 2.01016e-06, min loss: 1.09608e-06\n",
      "Epoch: 972700, elapsed: 1.13e+01, train loss: 1.10521e-06, val loss: 1.86485e-06, min loss: 1.09608e-06\n",
      "Epoch: 972800, elapsed: 1.15e+01, train loss: 1.12103e-06, val loss: 1.86024e-06, min loss: 1.09608e-06\n",
      "Epoch: 972900, elapsed: 1.15e+01, train loss: 1.87216e-06, val loss: 2.56091e-06, min loss: 1.09608e-06\n",
      "Epoch: 973000, elapsed: 1.13e+01, train loss: 1.16371e-06, val loss: 1.88719e-06, min loss: 1.09608e-06\n",
      "Epoch: 973100, elapsed: 1.14e+01, train loss: 1.16344e-06, val loss: 1.89681e-06, min loss: 1.09608e-06\n",
      "Epoch: 973200, elapsed: 1.13e+01, train loss: 2.05047e-06, val loss: 2.74430e-06, min loss: 1.09608e-06\n",
      "Epoch: 973300, elapsed: 1.14e+01, train loss: 2.87697e-06, val loss: 3.62875e-06, min loss: 1.09608e-06\n",
      "Epoch: 973400, elapsed: 1.13e+01, train loss: 1.11390e-06, val loss: 1.87697e-06, min loss: 1.09608e-06\n",
      "Epoch: 973500, elapsed: 1.11e+01, train loss: 1.09511e-06, val loss: 1.84616e-06, min loss: 1.09511e-06\n",
      "Epoch: 973600, elapsed: 1.13e+01, train loss: 1.20335e-06, val loss: 1.96695e-06, min loss: 1.09511e-06\n",
      "Epoch: 973700, elapsed: 1.14e+01, train loss: 2.11580e-06, val loss: 2.75185e-06, min loss: 1.09511e-06\n",
      "Epoch: 973800, elapsed: 1.13e+01, train loss: 3.55115e-06, val loss: 4.46285e-06, min loss: 1.09511e-06\n",
      "Epoch: 973900, elapsed: 1.12e+01, train loss: 1.39104e-06, val loss: 2.15148e-06, min loss: 1.09511e-06\n",
      "Epoch: 974000, elapsed: 1.13e+01, train loss: 1.09471e-06, val loss: 1.85346e-06, min loss: 1.09471e-06\n",
      "Epoch: 974100, elapsed: 1.13e+01, train loss: 1.24800e-06, val loss: 1.91904e-06, min loss: 1.09471e-06\n",
      "Epoch: 974200, elapsed: 1.13e+01, train loss: 1.28152e-06, val loss: 2.24284e-06, min loss: 1.09471e-06\n",
      "Epoch: 974300, elapsed: 1.11e+01, train loss: 1.09365e-06, val loss: 1.85009e-06, min loss: 1.09365e-06\n",
      "Epoch: 974400, elapsed: 1.40e+01, train loss: 1.18866e-06, val loss: 1.93499e-06, min loss: 1.09365e-06\n",
      "Epoch: 974500, elapsed: 1.13e+01, train loss: 2.14541e-06, val loss: 2.15643e-06, min loss: 1.09365e-06\n",
      "Epoch: 974600, elapsed: 1.14e+01, train loss: 1.09325e-06, val loss: 1.85109e-06, min loss: 1.09325e-06\n",
      "Epoch: 974700, elapsed: 1.12e+01, train loss: 1.10360e-06, val loss: 1.84669e-06, min loss: 1.09325e-06\n",
      "Epoch: 974800, elapsed: 1.14e+01, train loss: 2.09772e-06, val loss: 3.31010e-06, min loss: 1.09325e-06\n",
      "Epoch: 974900, elapsed: 1.14e+01, train loss: 1.10829e-06, val loss: 1.86208e-06, min loss: 1.09325e-06\n",
      "Epoch: 975000, elapsed: 1.13e+01, train loss: 1.09473e-06, val loss: 1.84907e-06, min loss: 1.09325e-06\n",
      "Epoch: 975100, elapsed: 1.34e+01, train loss: 1.11251e-06, val loss: 1.91890e-06, min loss: 1.09325e-06\n",
      "Epoch: 975200, elapsed: 1.13e+01, train loss: 1.12190e-06, val loss: 1.90949e-06, min loss: 1.09325e-06\n",
      "Epoch: 975300, elapsed: 1.11e+01, train loss: 1.18115e-06, val loss: 1.96427e-06, min loss: 1.09325e-06\n",
      "Epoch: 975400, elapsed: 1.13e+01, train loss: 1.09405e-06, val loss: 1.85117e-06, min loss: 1.09325e-06\n",
      "Epoch: 975500, elapsed: 1.13e+01, train loss: 1.15271e-06, val loss: 1.92112e-06, min loss: 1.09325e-06\n",
      "Epoch: 975600, elapsed: 1.14e+01, train loss: 2.37761e-06, val loss: 3.38752e-06, min loss: 1.09325e-06\n",
      "Epoch: 975700, elapsed: 1.12e+01, train loss: 1.65104e-06, val loss: 2.38273e-06, min loss: 1.09325e-06\n",
      "Epoch: 975800, elapsed: 1.13e+01, train loss: 1.16884e-06, val loss: 1.90461e-06, min loss: 1.09325e-06\n",
      "Epoch: 975900, elapsed: 1.14e+01, train loss: 1.11907e-06, val loss: 1.88988e-06, min loss: 1.09325e-06\n",
      "Epoch: 976000, elapsed: 1.13e+01, train loss: 1.11648e-06, val loss: 1.85597e-06, min loss: 1.09325e-06\n",
      "Epoch: 976100, elapsed: 1.12e+01, train loss: 1.43942e-06, val loss: 2.23871e-06, min loss: 1.09325e-06\n",
      "Epoch: 976200, elapsed: 1.41e+01, train loss: 1.84645e-06, val loss: 2.52204e-06, min loss: 1.09325e-06\n",
      "Epoch: 976300, elapsed: 1.15e+01, train loss: 1.44315e-06, val loss: 2.28153e-06, min loss: 1.09325e-06\n",
      "Epoch: 976400, elapsed: 1.15e+01, train loss: 1.84396e-06, val loss: 2.51873e-06, min loss: 1.09325e-06\n",
      "Epoch: 976500, elapsed: 1.15e+01, train loss: 6.39786e-06, val loss: 5.29316e-06, min loss: 1.09325e-06\n",
      "Epoch: 976600, elapsed: 1.13e+01, train loss: 1.20200e-06, val loss: 2.06857e-06, min loss: 1.09325e-06\n",
      "Epoch: 976700, elapsed: 1.13e+01, train loss: 1.14735e-06, val loss: 1.93248e-06, min loss: 1.09325e-06\n",
      "Epoch: 976800, elapsed: 1.13e+01, train loss: 1.10182e-06, val loss: 1.89315e-06, min loss: 1.09325e-06\n",
      "Epoch: 976900, elapsed: 1.12e+01, train loss: 2.32054e-06, val loss: 2.65971e-06, min loss: 1.09325e-06\n",
      "Epoch: 977000, elapsed: 1.14e+01, train loss: 1.22423e-06, val loss: 2.04093e-06, min loss: 1.09325e-06\n",
      "Epoch: 977100, elapsed: 1.12e+01, train loss: 1.15278e-06, val loss: 1.89371e-06, min loss: 1.09325e-06\n",
      "Epoch: 977200, elapsed: 1.12e+01, train loss: 1.18112e-06, val loss: 1.94504e-06, min loss: 1.09325e-06\n",
      "Epoch: 977300, elapsed: 1.12e+01, train loss: 2.07753e-06, val loss: 2.90029e-06, min loss: 1.09325e-06\n",
      "Epoch: 977400, elapsed: 1.13e+01, train loss: 1.13736e-06, val loss: 1.93289e-06, min loss: 1.09325e-06\n",
      "Epoch: 977500, elapsed: 1.12e+01, train loss: 1.09091e-06, val loss: 1.84973e-06, min loss: 1.09091e-06\n",
      "Epoch: 977600, elapsed: 1.12e+01, train loss: 1.11025e-06, val loss: 1.87231e-06, min loss: 1.09091e-06\n",
      "Epoch: 977700, elapsed: 1.13e+01, train loss: 4.69520e-06, val loss: 4.78905e-06, min loss: 1.09091e-06\n",
      "Epoch: 977800, elapsed: 1.12e+01, train loss: 1.22963e-06, val loss: 2.11934e-06, min loss: 1.09091e-06\n",
      "Epoch: 977900, elapsed: 1.12e+01, train loss: 1.09041e-06, val loss: 1.84311e-06, min loss: 1.09041e-06\n",
      "Epoch: 978000, elapsed: 1.13e+01, train loss: 1.11274e-06, val loss: 1.85463e-06, min loss: 1.09041e-06\n",
      "Epoch: 978100, elapsed: 1.41e+01, train loss: 2.07846e-06, val loss: 2.34582e-06, min loss: 1.09041e-06\n",
      "Epoch: 978200, elapsed: 1.15e+01, train loss: 1.27498e-06, val loss: 2.09807e-06, min loss: 1.09041e-06\n",
      "Epoch: 978300, elapsed: 1.16e+01, train loss: 1.09041e-06, val loss: 1.84696e-06, min loss: 1.09041e-06\n",
      "Epoch: 978400, elapsed: 1.13e+01, train loss: 1.17540e-06, val loss: 1.96459e-06, min loss: 1.09041e-06\n",
      "Epoch: 978500, elapsed: 1.14e+01, train loss: 1.10281e-06, val loss: 1.86641e-06, min loss: 1.09041e-06\n",
      "Epoch: 978600, elapsed: 1.14e+01, train loss: 1.09149e-06, val loss: 1.84610e-06, min loss: 1.09041e-06\n",
      "Epoch: 978700, elapsed: 1.12e+01, train loss: 3.52380e-06, val loss: 4.97733e-06, min loss: 1.09041e-06\n",
      "Epoch: 978800, elapsed: 1.13e+01, train loss: 1.09006e-06, val loss: 1.84409e-06, min loss: 1.09006e-06\n",
      "Epoch: 978900, elapsed: 1.13e+01, train loss: 1.09007e-06, val loss: 1.84382e-06, min loss: 1.09006e-06\n",
      "Epoch: 979000, elapsed: 1.13e+01, train loss: 1.20685e-06, val loss: 1.92143e-06, min loss: 1.09006e-06\n",
      "Epoch: 979100, elapsed: 1.14e+01, train loss: 1.24407e-06, val loss: 1.94672e-06, min loss: 1.09006e-06\n",
      "Epoch: 979200, elapsed: 1.13e+01, train loss: 2.13576e-06, val loss: 3.05366e-06, min loss: 1.09006e-06\n",
      "Epoch: 979300, elapsed: 1.13e+01, train loss: 1.11475e-06, val loss: 1.91691e-06, min loss: 1.09006e-06\n",
      "Epoch: 979400, elapsed: 1.11e+01, train loss: 1.10686e-06, val loss: 1.84748e-06, min loss: 1.09006e-06\n",
      "Epoch: 979500, elapsed: 1.13e+01, train loss: 1.16168e-06, val loss: 1.90226e-06, min loss: 1.09006e-06\n",
      "Epoch: 979600, elapsed: 1.13e+01, train loss: 1.66367e-06, val loss: 3.06954e-06, min loss: 1.09006e-06\n",
      "Epoch: 979700, elapsed: 1.13e+01, train loss: 1.11586e-06, val loss: 1.89983e-06, min loss: 1.09006e-06\n",
      "Epoch: 979800, elapsed: 1.14e+01, train loss: 1.08846e-06, val loss: 1.84140e-06, min loss: 1.08846e-06\n",
      "Epoch: 979900, elapsed: 1.43e+01, train loss: 1.09228e-06, val loss: 1.84941e-06, min loss: 1.08846e-06\n",
      "Epoch: 980000, elapsed: 1.14e+01, train loss: 1.14515e-06, val loss: 2.00451e-06, min loss: 1.08846e-06\n",
      "Epoch: 980100, elapsed: 1.35e+01, train loss: 1.08778e-06, val loss: 1.84121e-06, min loss: 1.08778e-06\n",
      "Epoch: 980200, elapsed: 1.14e+01, train loss: 1.09615e-06, val loss: 1.84626e-06, min loss: 1.08778e-06\n",
      "Epoch: 980300, elapsed: 1.15e+01, train loss: 1.83317e-06, val loss: 1.94493e-06, min loss: 1.08778e-06\n",
      "Epoch: 980400, elapsed: 1.14e+01, train loss: 1.46621e-06, val loss: 2.20426e-06, min loss: 1.08778e-06\n",
      "Epoch: 980500, elapsed: 1.14e+01, train loss: 1.11587e-06, val loss: 1.88784e-06, min loss: 1.08778e-06\n",
      "Epoch: 980600, elapsed: 1.14e+01, train loss: 1.13055e-06, val loss: 1.91353e-06, min loss: 1.08778e-06\n",
      "Epoch: 980700, elapsed: 1.13e+01, train loss: 1.08956e-06, val loss: 1.84480e-06, min loss: 1.08778e-06\n",
      "Epoch: 980800, elapsed: 1.13e+01, train loss: 1.09261e-06, val loss: 1.83805e-06, min loss: 1.08778e-06\n",
      "Epoch: 980900, elapsed: 1.12e+01, train loss: 1.20081e-06, val loss: 1.96925e-06, min loss: 1.08778e-06\n",
      "Epoch: 981000, elapsed: 1.13e+01, train loss: 1.11822e-06, val loss: 1.83368e-06, min loss: 1.08778e-06\n",
      "Epoch: 981100, elapsed: 1.14e+01, train loss: 1.40546e-06, val loss: 2.11837e-06, min loss: 1.08778e-06\n",
      "Epoch: 981200, elapsed: 1.13e+01, train loss: 1.13271e-06, val loss: 1.87225e-06, min loss: 1.08778e-06\n",
      "Epoch: 981300, elapsed: 1.12e+01, train loss: 1.10968e-06, val loss: 1.87733e-06, min loss: 1.08778e-06\n",
      "Epoch: 981400, elapsed: 1.13e+01, train loss: 1.10435e-06, val loss: 1.86687e-06, min loss: 1.08778e-06\n",
      "Epoch: 981500, elapsed: 1.13e+01, train loss: 1.10605e-06, val loss: 1.83844e-06, min loss: 1.08778e-06\n",
      "Epoch: 981600, elapsed: 1.13e+01, train loss: 1.15462e-06, val loss: 1.92534e-06, min loss: 1.08778e-06\n",
      "Epoch: 981700, elapsed: 1.40e+01, train loss: 1.09128e-06, val loss: 1.85299e-06, min loss: 1.08778e-06\n",
      "Epoch: 981800, elapsed: 1.16e+01, train loss: 1.09316e-06, val loss: 1.86856e-06, min loss: 1.08778e-06\n",
      "Epoch: 981900, elapsed: 1.14e+01, train loss: 1.12478e-06, val loss: 1.89492e-06, min loss: 1.08778e-06\n",
      "Epoch: 982000, elapsed: 1.13e+01, train loss: 1.25406e-06, val loss: 2.08048e-06, min loss: 1.08778e-06\n",
      "Epoch: 982100, elapsed: 1.12e+01, train loss: 1.22886e-06, val loss: 1.85268e-06, min loss: 1.08778e-06\n",
      "Epoch: 982200, elapsed: 1.14e+01, train loss: 1.09354e-06, val loss: 1.86622e-06, min loss: 1.08778e-06\n",
      "Epoch: 982300, elapsed: 1.12e+01, train loss: 1.14411e-06, val loss: 1.90675e-06, min loss: 1.08778e-06\n",
      "Epoch: 982400, elapsed: 1.13e+01, train loss: 1.08823e-06, val loss: 1.84328e-06, min loss: 1.08778e-06\n",
      "Epoch: 982500, elapsed: 1.13e+01, train loss: 1.09399e-06, val loss: 1.84437e-06, min loss: 1.08778e-06\n",
      "Epoch: 982600, elapsed: 1.12e+01, train loss: 1.08759e-06, val loss: 1.83620e-06, min loss: 1.08759e-06\n",
      "Epoch: 982700, elapsed: 1.14e+01, train loss: 1.10126e-06, val loss: 1.86469e-06, min loss: 1.08759e-06\n",
      "Epoch: 982800, elapsed: 1.15e+01, train loss: 1.49445e-06, val loss: 2.08802e-06, min loss: 1.08759e-06\n",
      "Epoch: 982900, elapsed: 1.13e+01, train loss: 1.08508e-06, val loss: 1.83811e-06, min loss: 1.08508e-06\n",
      "Epoch: 983000, elapsed: 1.13e+01, train loss: 1.09282e-06, val loss: 1.84235e-06, min loss: 1.08508e-06\n",
      "Epoch: 983100, elapsed: 1.13e+01, train loss: 1.67040e-06, val loss: 2.36218e-06, min loss: 1.08508e-06\n",
      "Epoch: 983200, elapsed: 1.14e+01, train loss: 3.33424e-06, val loss: 4.24477e-06, min loss: 1.08508e-06\n",
      "Epoch: 983300, elapsed: 1.13e+01, train loss: 1.08530e-06, val loss: 1.83732e-06, min loss: 1.08508e-06\n",
      "Epoch: 983400, elapsed: 1.12e+01, train loss: 1.08703e-06, val loss: 1.83606e-06, min loss: 1.08508e-06\n",
      "Epoch: 983500, elapsed: 1.13e+01, train loss: 1.31721e-06, val loss: 2.08149e-06, min loss: 1.08508e-06\n",
      "Epoch: 983600, elapsed: 1.41e+01, train loss: 1.22319e-06, val loss: 1.91477e-06, min loss: 1.08508e-06\n",
      "Epoch: 983700, elapsed: 1.15e+01, train loss: 1.38890e-06, val loss: 2.53145e-06, min loss: 1.08508e-06\n",
      "Epoch: 983800, elapsed: 1.16e+01, train loss: 1.59008e-06, val loss: 2.30500e-06, min loss: 1.08508e-06\n",
      "Epoch: 983900, elapsed: 1.13e+01, train loss: 1.09269e-06, val loss: 1.84970e-06, min loss: 1.08508e-06\n",
      "Epoch: 984000, elapsed: 1.14e+01, train loss: 2.07715e-06, val loss: 2.75577e-06, min loss: 1.08508e-06\n",
      "Epoch: 984100, elapsed: 1.16e+01, train loss: 1.08430e-06, val loss: 1.84281e-06, min loss: 1.08430e-06\n",
      "Epoch: 984200, elapsed: 1.13e+01, train loss: 1.08642e-06, val loss: 1.84592e-06, min loss: 1.08430e-06\n",
      "Epoch: 984300, elapsed: 1.14e+01, train loss: 1.10573e-06, val loss: 1.90124e-06, min loss: 1.08430e-06\n",
      "Epoch: 984400, elapsed: 1.13e+01, train loss: 2.05662e-06, val loss: 3.08213e-06, min loss: 1.08430e-06\n",
      "Epoch: 984500, elapsed: 1.13e+01, train loss: 2.79698e-06, val loss: 2.72756e-06, min loss: 1.08430e-06\n",
      "Epoch: 984600, elapsed: 1.12e+01, train loss: 1.30787e-06, val loss: 2.13089e-06, min loss: 1.08430e-06\n",
      "Epoch: 984700, elapsed: 1.13e+01, train loss: 1.15164e-06, val loss: 1.84267e-06, min loss: 1.08430e-06\n",
      "Epoch: 984800, elapsed: 1.12e+01, train loss: 1.08731e-06, val loss: 1.84669e-06, min loss: 1.08430e-06\n",
      "Epoch: 984900, elapsed: 1.12e+01, train loss: 1.33538e-06, val loss: 2.04154e-06, min loss: 1.08430e-06\n",
      "Epoch: 985000, elapsed: 1.12e+01, train loss: 1.10265e-06, val loss: 1.84862e-06, min loss: 1.08430e-06\n",
      "Epoch: 985100, elapsed: 1.35e+01, train loss: 1.09150e-06, val loss: 1.85670e-06, min loss: 1.08430e-06\n",
      "Epoch: 985200, elapsed: 1.15e+01, train loss: 1.08668e-06, val loss: 1.85091e-06, min loss: 1.08430e-06\n",
      "Epoch: 985300, elapsed: 1.13e+01, train loss: 4.94751e-06, val loss: 2.87017e-06, min loss: 1.08430e-06\n",
      "Epoch: 985400, elapsed: 1.43e+01, train loss: 1.08253e-06, val loss: 1.83499e-06, min loss: 1.08253e-06\n",
      "Epoch: 985500, elapsed: 1.15e+01, train loss: 2.05200e-06, val loss: 2.51882e-06, min loss: 1.08253e-06\n",
      "Epoch: 985600, elapsed: 1.15e+01, train loss: 1.17726e-06, val loss: 1.92758e-06, min loss: 1.08253e-06\n",
      "Epoch: 985700, elapsed: 1.14e+01, train loss: 1.09594e-06, val loss: 1.85152e-06, min loss: 1.08253e-06\n",
      "Epoch: 985800, elapsed: 1.13e+01, train loss: 1.14323e-06, val loss: 1.87427e-06, min loss: 1.08253e-06\n",
      "Epoch: 985900, elapsed: 1.13e+01, train loss: 1.11384e-06, val loss: 1.83707e-06, min loss: 1.08253e-06\n",
      "Epoch: 986000, elapsed: 1.13e+01, train loss: 1.08624e-06, val loss: 1.85328e-06, min loss: 1.08253e-06\n",
      "Epoch: 986100, elapsed: 1.13e+01, train loss: 1.37063e-06, val loss: 2.07243e-06, min loss: 1.08253e-06\n",
      "Epoch: 986200, elapsed: 1.12e+01, train loss: 1.23838e-06, val loss: 2.04530e-06, min loss: 1.08253e-06\n",
      "Epoch: 986300, elapsed: 1.11e+01, train loss: 1.10095e-06, val loss: 1.85785e-06, min loss: 1.08253e-06\n",
      "Epoch: 986400, elapsed: 1.12e+01, train loss: 1.82445e-06, val loss: 2.39285e-06, min loss: 1.08253e-06\n",
      "Epoch: 986500, elapsed: 1.13e+01, train loss: 1.77929e-06, val loss: 2.69123e-06, min loss: 1.08253e-06\n",
      "Epoch: 986600, elapsed: 1.11e+01, train loss: 1.60098e-06, val loss: 2.55046e-06, min loss: 1.08253e-06\n",
      "Epoch: 986700, elapsed: 1.12e+01, train loss: 2.31719e-06, val loss: 2.83914e-06, min loss: 1.08253e-06\n",
      "Epoch: 986800, elapsed: 1.14e+01, train loss: 2.72106e-06, val loss: 2.94440e-06, min loss: 1.08253e-06\n",
      "Epoch: 986900, elapsed: 1.12e+01, train loss: 1.85227e-06, val loss: 2.77910e-06, min loss: 1.08253e-06\n",
      "Epoch: 987000, elapsed: 1.12e+01, train loss: 1.37378e-06, val loss: 2.23608e-06, min loss: 1.08253e-06\n",
      "Epoch: 987100, elapsed: 1.12e+01, train loss: 1.26875e-06, val loss: 2.13934e-06, min loss: 1.08253e-06\n",
      "Epoch: 987200, elapsed: 1.12e+01, train loss: 1.08211e-06, val loss: 1.84200e-06, min loss: 1.08211e-06\n",
      "Epoch: 987300, elapsed: 1.43e+01, train loss: 1.49502e-06, val loss: 2.43145e-06, min loss: 1.08211e-06\n",
      "Epoch: 987400, elapsed: 1.15e+01, train loss: 1.08171e-06, val loss: 1.83985e-06, min loss: 1.08171e-06\n",
      "Epoch: 987500, elapsed: 1.14e+01, train loss: 1.09046e-06, val loss: 1.84567e-06, min loss: 1.08171e-06\n",
      "Epoch: 987600, elapsed: 1.13e+01, train loss: 1.08658e-06, val loss: 1.84618e-06, min loss: 1.08171e-06\n",
      "Epoch: 987700, elapsed: 1.16e+01, train loss: 1.09733e-06, val loss: 1.85008e-06, min loss: 1.08171e-06\n",
      "Epoch: 987800, elapsed: 1.15e+01, train loss: 3.57203e-06, val loss: 4.30592e-06, min loss: 1.08171e-06\n",
      "Epoch: 987900, elapsed: 1.14e+01, train loss: 1.40822e-06, val loss: 2.23130e-06, min loss: 1.08171e-06\n",
      "Epoch: 988000, elapsed: 1.13e+01, train loss: 1.11206e-06, val loss: 1.85613e-06, min loss: 1.08171e-06\n",
      "Epoch: 988100, elapsed: 1.12e+01, train loss: 1.08890e-06, val loss: 1.83270e-06, min loss: 1.08171e-06\n",
      "Epoch: 988200, elapsed: 1.14e+01, train loss: 1.33882e-06, val loss: 1.85287e-06, min loss: 1.08171e-06\n",
      "Epoch: 988300, elapsed: 1.15e+01, train loss: 1.08853e-06, val loss: 1.86994e-06, min loss: 1.08171e-06\n",
      "Epoch: 988400, elapsed: 1.14e+01, train loss: 1.19293e-06, val loss: 1.87117e-06, min loss: 1.08171e-06\n",
      "Epoch: 988500, elapsed: 1.13e+01, train loss: 1.25482e-06, val loss: 1.90812e-06, min loss: 1.08171e-06\n",
      "Epoch: 988600, elapsed: 1.12e+01, train loss: 1.72757e-06, val loss: 2.43451e-06, min loss: 1.08171e-06\n",
      "Epoch: 988700, elapsed: 1.12e+01, train loss: 1.39736e-06, val loss: 2.07233e-06, min loss: 1.08171e-06\n",
      "Epoch: 988800, elapsed: 1.13e+01, train loss: 1.35277e-06, val loss: 2.09303e-06, min loss: 1.08171e-06\n",
      "Epoch: 988900, elapsed: 1.13e+01, train loss: 1.16695e-06, val loss: 1.89554e-06, min loss: 1.08171e-06\n",
      "Epoch: 989000, elapsed: 1.13e+01, train loss: 1.13272e-06, val loss: 1.90392e-06, min loss: 1.08171e-06\n",
      "Epoch: 989100, elapsed: 1.41e+01, train loss: 1.26942e-06, val loss: 2.07501e-06, min loss: 1.08171e-06\n",
      "Epoch: 989200, elapsed: 1.16e+01, train loss: 1.09745e-06, val loss: 1.85334e-06, min loss: 1.08171e-06\n",
      "Epoch: 989300, elapsed: 1.15e+01, train loss: 1.07900e-06, val loss: 1.83150e-06, min loss: 1.07900e-06\n",
      "Epoch: 989400, elapsed: 1.13e+01, train loss: 1.14888e-06, val loss: 2.09302e-06, min loss: 1.07900e-06\n",
      "Epoch: 989500, elapsed: 1.14e+01, train loss: 1.26810e-06, val loss: 2.11990e-06, min loss: 1.07900e-06\n",
      "Epoch: 989600, elapsed: 1.14e+01, train loss: 3.31682e-06, val loss: 4.18398e-06, min loss: 1.07900e-06\n",
      "Epoch: 989700, elapsed: 1.15e+01, train loss: 1.22204e-06, val loss: 1.92520e-06, min loss: 1.07900e-06\n",
      "Epoch: 989800, elapsed: 1.12e+01, train loss: 1.08013e-06, val loss: 1.82584e-06, min loss: 1.07900e-06\n",
      "Epoch: 989900, elapsed: 1.13e+01, train loss: 1.50967e-06, val loss: 2.23624e-06, min loss: 1.07900e-06\n",
      "Epoch: 990000, elapsed: 1.14e+01, train loss: 1.16197e-06, val loss: 2.29338e-06, min loss: 1.07900e-06\n",
      "Epoch: 990100, elapsed: 1.34e+01, train loss: 1.58957e-06, val loss: 2.53362e-06, min loss: 1.07900e-06\n",
      "Epoch: 990200, elapsed: 1.13e+01, train loss: 2.25314e-06, val loss: 3.35426e-06, min loss: 1.07900e-06\n",
      "Epoch: 990300, elapsed: 1.14e+01, train loss: 2.33582e-06, val loss: 2.72091e-06, min loss: 1.07900e-06\n",
      "Epoch: 990400, elapsed: 1.13e+01, train loss: 1.13757e-06, val loss: 1.91734e-06, min loss: 1.07900e-06\n",
      "Epoch: 990500, elapsed: 1.14e+01, train loss: 1.16106e-06, val loss: 1.94483e-06, min loss: 1.07900e-06\n",
      "Epoch: 990600, elapsed: 1.12e+01, train loss: 2.41393e-06, val loss: 3.44284e-06, min loss: 1.07900e-06\n",
      "Epoch: 990700, elapsed: 1.14e+01, train loss: 4.62318e-06, val loss: 4.42647e-06, min loss: 1.07900e-06\n",
      "Epoch: 990800, elapsed: 1.14e+01, train loss: 1.09474e-06, val loss: 1.83544e-06, min loss: 1.07900e-06\n",
      "Epoch: 990900, elapsed: 1.14e+01, train loss: 1.13935e-06, val loss: 1.95202e-06, min loss: 1.07900e-06\n",
      "Epoch: 991000, elapsed: 1.44e+01, train loss: 1.10053e-06, val loss: 1.87724e-06, min loss: 1.07900e-06\n",
      "Epoch: 991100, elapsed: 1.15e+01, train loss: 1.49376e-06, val loss: 2.01686e-06, min loss: 1.07900e-06\n",
      "Epoch: 991200, elapsed: 1.16e+01, train loss: 1.11986e-06, val loss: 1.87249e-06, min loss: 1.07900e-06\n",
      "Epoch: 991300, elapsed: 1.15e+01, train loss: 1.07759e-06, val loss: 1.82817e-06, min loss: 1.07759e-06\n",
      "Epoch: 991400, elapsed: 1.14e+01, train loss: 1.09531e-06, val loss: 1.85077e-06, min loss: 1.07759e-06\n",
      "Epoch: 991500, elapsed: 1.14e+01, train loss: 1.45552e-06, val loss: 2.15956e-06, min loss: 1.07759e-06\n",
      "Epoch: 991600, elapsed: 1.13e+01, train loss: 1.47271e-06, val loss: 2.13957e-06, min loss: 1.07759e-06\n",
      "Epoch: 991700, elapsed: 1.14e+01, train loss: 3.05290e-06, val loss: 4.48895e-06, min loss: 1.07759e-06\n",
      "Epoch: 991800, elapsed: 1.13e+01, train loss: 1.08623e-06, val loss: 1.83313e-06, min loss: 1.07759e-06\n",
      "Epoch: 991900, elapsed: 1.15e+01, train loss: 1.09262e-06, val loss: 1.84778e-06, min loss: 1.07759e-06\n",
      "Epoch: 992000, elapsed: 1.14e+01, train loss: 1.08538e-06, val loss: 1.84032e-06, min loss: 1.07759e-06\n",
      "Epoch: 992100, elapsed: 1.14e+01, train loss: 1.08404e-06, val loss: 1.84512e-06, min loss: 1.07759e-06\n",
      "Epoch: 992200, elapsed: 1.14e+01, train loss: 1.12471e-06, val loss: 1.85156e-06, min loss: 1.07759e-06\n",
      "Epoch: 992300, elapsed: 1.13e+01, train loss: 1.15319e-06, val loss: 1.90685e-06, min loss: 1.07759e-06\n",
      "Epoch: 992400, elapsed: 1.13e+01, train loss: 1.15952e-06, val loss: 1.81544e-06, min loss: 1.07759e-06\n",
      "Epoch: 992500, elapsed: 1.12e+01, train loss: 1.62012e-06, val loss: 2.06483e-06, min loss: 1.07759e-06\n",
      "Epoch: 992600, elapsed: 1.11e+01, train loss: 1.39199e-06, val loss: 2.31370e-06, min loss: 1.07759e-06\n",
      "Epoch: 992700, elapsed: 1.13e+01, train loss: 1.09843e-06, val loss: 1.87511e-06, min loss: 1.07759e-06\n",
      "Epoch: 992800, elapsed: 1.42e+01, train loss: 1.15974e-06, val loss: 1.92026e-06, min loss: 1.07759e-06\n",
      "Epoch: 992900, elapsed: 1.14e+01, train loss: 1.14624e-06, val loss: 1.94686e-06, min loss: 1.07759e-06\n",
      "Epoch: 993000, elapsed: 1.13e+01, train loss: 1.10708e-06, val loss: 1.83438e-06, min loss: 1.07759e-06\n",
      "Epoch: 993100, elapsed: 1.14e+01, train loss: 1.10064e-06, val loss: 1.84145e-06, min loss: 1.07759e-06\n",
      "Epoch: 993200, elapsed: 1.13e+01, train loss: 1.08221e-06, val loss: 1.84259e-06, min loss: 1.07759e-06\n",
      "Epoch: 993300, elapsed: 1.13e+01, train loss: 1.07830e-06, val loss: 1.82375e-06, min loss: 1.07759e-06\n",
      "Epoch: 993400, elapsed: 1.13e+01, train loss: 1.07976e-06, val loss: 1.83411e-06, min loss: 1.07759e-06\n",
      "Epoch: 993500, elapsed: 1.13e+01, train loss: 1.17960e-06, val loss: 1.86220e-06, min loss: 1.07759e-06\n",
      "Epoch: 993600, elapsed: 1.11e+01, train loss: 1.19005e-06, val loss: 1.95137e-06, min loss: 1.07759e-06\n",
      "Epoch: 993700, elapsed: 1.12e+01, train loss: 1.55394e-06, val loss: 2.33990e-06, min loss: 1.07759e-06\n",
      "Epoch: 993800, elapsed: 1.12e+01, train loss: 1.08899e-06, val loss: 1.83024e-06, min loss: 1.07759e-06\n",
      "Epoch: 993900, elapsed: 1.12e+01, train loss: 1.76824e-06, val loss: 2.67940e-06, min loss: 1.07759e-06\n",
      "Epoch: 994000, elapsed: 1.13e+01, train loss: 3.84457e-06, val loss: 4.94578e-06, min loss: 1.07759e-06\n",
      "Epoch: 994100, elapsed: 1.12e+01, train loss: 1.16944e-06, val loss: 1.92339e-06, min loss: 1.07759e-06\n",
      "Epoch: 994200, elapsed: 1.11e+01, train loss: 1.09602e-06, val loss: 1.83160e-06, min loss: 1.07759e-06\n",
      "Epoch: 994300, elapsed: 1.12e+01, train loss: 1.17113e-06, val loss: 1.90505e-06, min loss: 1.07759e-06\n",
      "Epoch: 994400, elapsed: 1.12e+01, train loss: 1.09339e-06, val loss: 1.85245e-06, min loss: 1.07759e-06\n",
      "Epoch: 994500, elapsed: 1.12e+01, train loss: 2.87663e-06, val loss: 3.37420e-06, min loss: 1.07759e-06\n",
      "Epoch: 994600, elapsed: 1.11e+01, train loss: 1.78869e-06, val loss: 2.56072e-06, min loss: 1.07759e-06\n",
      "Epoch: 994700, elapsed: 1.43e+01, train loss: 1.11866e-06, val loss: 1.89265e-06, min loss: 1.07759e-06\n",
      "Epoch: 994800, elapsed: 1.16e+01, train loss: 1.09347e-06, val loss: 1.86312e-06, min loss: 1.07759e-06\n",
      "Epoch: 994900, elapsed: 1.16e+01, train loss: 1.07398e-06, val loss: 1.82449e-06, min loss: 1.07398e-06\n",
      "Epoch: 995000, elapsed: 1.14e+01, train loss: 1.08357e-06, val loss: 1.83052e-06, min loss: 1.07398e-06\n",
      "Epoch: 995100, elapsed: 1.37e+01, train loss: 2.93578e-06, val loss: 3.34345e-06, min loss: 1.07398e-06\n",
      "Epoch: 995200, elapsed: 1.15e+01, train loss: 1.57716e-06, val loss: 2.36618e-06, min loss: 1.07398e-06\n",
      "Epoch: 995300, elapsed: 1.12e+01, train loss: 1.10799e-06, val loss: 1.87775e-06, min loss: 1.07398e-06\n",
      "Epoch: 995400, elapsed: 1.14e+01, train loss: 1.61928e-06, val loss: 2.50017e-06, min loss: 1.07398e-06\n",
      "Epoch: 995500, elapsed: 1.14e+01, train loss: 1.42222e-06, val loss: 2.21549e-06, min loss: 1.07398e-06\n",
      "Epoch: 995600, elapsed: 1.14e+01, train loss: 1.21304e-06, val loss: 1.94412e-06, min loss: 1.07398e-06\n",
      "Epoch: 995700, elapsed: 1.14e+01, train loss: 1.92769e-06, val loss: 2.54685e-06, min loss: 1.07398e-06\n",
      "Epoch: 995800, elapsed: 1.15e+01, train loss: 1.41529e-06, val loss: 2.17958e-06, min loss: 1.07398e-06\n",
      "Epoch: 995900, elapsed: 1.14e+01, train loss: 1.63981e-06, val loss: 2.44492e-06, min loss: 1.07398e-06\n",
      "Epoch: 996000, elapsed: 1.13e+01, train loss: 1.19329e-06, val loss: 1.95279e-06, min loss: 1.07398e-06\n",
      "Epoch: 996100, elapsed: 1.13e+01, train loss: 1.11005e-06, val loss: 1.87340e-06, min loss: 1.07398e-06\n",
      "Epoch: 996200, elapsed: 1.14e+01, train loss: 1.11595e-06, val loss: 1.85121e-06, min loss: 1.07398e-06\n",
      "Epoch: 996300, elapsed: 1.13e+01, train loss: 1.13120e-06, val loss: 1.84911e-06, min loss: 1.07398e-06\n",
      "Epoch: 996400, elapsed: 1.14e+01, train loss: 3.00413e-06, val loss: 3.48783e-06, min loss: 1.07398e-06\n",
      "Epoch: 996500, elapsed: 1.43e+01, train loss: 1.59166e-06, val loss: 2.49539e-06, min loss: 1.07398e-06\n",
      "Epoch: 996600, elapsed: 1.16e+01, train loss: 1.93085e-06, val loss: 2.85278e-06, min loss: 1.07398e-06\n",
      "Epoch: 996700, elapsed: 1.13e+01, train loss: 1.15207e-06, val loss: 1.90438e-06, min loss: 1.07398e-06\n",
      "Epoch: 996800, elapsed: 1.13e+01, train loss: 1.07515e-06, val loss: 1.83021e-06, min loss: 1.07398e-06\n",
      "Epoch: 996900, elapsed: 1.14e+01, train loss: 1.16779e-06, val loss: 1.96286e-06, min loss: 1.07398e-06\n",
      "Epoch: 997000, elapsed: 1.14e+01, train loss: 1.15304e-06, val loss: 1.89611e-06, min loss: 1.07398e-06\n",
      "Epoch: 997100, elapsed: 1.15e+01, train loss: 1.10502e-06, val loss: 1.88674e-06, min loss: 1.07398e-06\n",
      "Epoch: 997200, elapsed: 1.14e+01, train loss: 1.26180e-06, val loss: 2.02464e-06, min loss: 1.07398e-06\n",
      "Epoch: 997300, elapsed: 1.13e+01, train loss: 1.07287e-06, val loss: 1.81596e-06, min loss: 1.07287e-06\n",
      "Epoch: 997400, elapsed: 1.13e+01, train loss: 1.07475e-06, val loss: 1.81968e-06, min loss: 1.07287e-06\n",
      "Epoch: 997500, elapsed: 1.13e+01, train loss: 1.15120e-06, val loss: 1.86963e-06, min loss: 1.07287e-06\n",
      "Epoch: 997600, elapsed: 1.13e+01, train loss: 1.67198e-06, val loss: 2.59456e-06, min loss: 1.07287e-06\n",
      "Epoch: 997700, elapsed: 1.13e+01, train loss: 1.34843e-06, val loss: 2.14485e-06, min loss: 1.07287e-06\n",
      "Epoch: 997800, elapsed: 1.11e+01, train loss: 1.07062e-06, val loss: 1.82051e-06, min loss: 1.07062e-06\n",
      "Epoch: 997900, elapsed: 1.14e+01, train loss: 1.07552e-06, val loss: 1.83119e-06, min loss: 1.07062e-06\n",
      "Epoch: 998000, elapsed: 1.14e+01, train loss: 1.18592e-06, val loss: 1.94769e-06, min loss: 1.07062e-06\n",
      "Epoch: 998100, elapsed: 1.14e+01, train loss: 1.07515e-06, val loss: 1.83060e-06, min loss: 1.07062e-06\n",
      "Epoch: 998200, elapsed: 1.11e+01, train loss: 1.07170e-06, val loss: 1.82334e-06, min loss: 1.07062e-06\n",
      "Epoch: 998300, elapsed: 1.13e+01, train loss: 1.14690e-06, val loss: 2.02612e-06, min loss: 1.07062e-06\n",
      "Epoch: 998400, elapsed: 1.42e+01, train loss: 1.08593e-06, val loss: 1.83558e-06, min loss: 1.07062e-06\n",
      "Epoch: 998500, elapsed: 1.14e+01, train loss: 1.07150e-06, val loss: 1.82036e-06, min loss: 1.07062e-06\n",
      "Epoch: 998600, elapsed: 1.15e+01, train loss: 1.17460e-06, val loss: 1.89245e-06, min loss: 1.07062e-06\n",
      "Epoch: 998700, elapsed: 1.14e+01, train loss: 1.22626e-06, val loss: 1.98729e-06, min loss: 1.07062e-06\n",
      "Epoch: 998800, elapsed: 1.13e+01, train loss: 1.07035e-06, val loss: 1.82398e-06, min loss: 1.07035e-06\n",
      "Epoch: 998900, elapsed: 1.12e+01, train loss: 2.13527e-06, val loss: 3.12552e-06, min loss: 1.07035e-06\n",
      "Epoch: 999000, elapsed: 1.14e+01, train loss: 1.51230e-06, val loss: 2.18738e-06, min loss: 1.07035e-06\n",
      "Epoch: 999100, elapsed: 1.14e+01, train loss: 1.18965e-06, val loss: 1.90709e-06, min loss: 1.07035e-06\n",
      "Epoch: 999200, elapsed: 1.14e+01, train loss: 1.88148e-06, val loss: 2.31265e-06, min loss: 1.07035e-06\n",
      "Epoch: 999300, elapsed: 1.14e+01, train loss: 1.07209e-06, val loss: 1.82708e-06, min loss: 1.07035e-06\n",
      "Epoch: 999400, elapsed: 1.14e+01, train loss: 1.07735e-06, val loss: 1.85453e-06, min loss: 1.07035e-06\n",
      "Epoch: 999500, elapsed: 1.12e+01, train loss: 2.02962e-06, val loss: 2.83226e-06, min loss: 1.07035e-06\n",
      "Epoch: 999600, elapsed: 1.14e+01, train loss: 1.23073e-06, val loss: 2.19622e-06, min loss: 1.07035e-06\n",
      "Epoch: 999700, elapsed: 1.13e+01, train loss: 1.27874e-06, val loss: 2.22739e-06, min loss: 1.07035e-06\n",
      "Epoch: 999800, elapsed: 1.16e+01, train loss: 1.14118e-06, val loss: 1.91723e-06, min loss: 1.07035e-06\n",
      "Epoch: 999900, elapsed: 1.15e+01, train loss: 2.57110e-06, val loss: 2.37139e-06, min loss: 1.07035e-06\n",
      "Epoch: 1000000, elapsed: 1.13e+01, train loss: 1.09447e-06, val loss: 1.83855e-06, min loss: 1.07035e-06\n",
      "Epoch: 1000100, elapsed: 1.34e+01, train loss: 1.07271e-06, val loss: 1.80873e-06, min loss: 1.07035e-06\n",
      "Epoch: 1000200, elapsed: 1.42e+01, train loss: 1.07153e-06, val loss: 1.82831e-06, min loss: 1.07035e-06\n",
      "Epoch: 1000300, elapsed: 1.14e+01, train loss: 1.09723e-06, val loss: 1.88508e-06, min loss: 1.07035e-06\n",
      "Epoch: 1000400, elapsed: 1.15e+01, train loss: 1.82403e-06, val loss: 2.00984e-06, min loss: 1.07035e-06\n",
      "Epoch: 1000500, elapsed: 1.15e+01, train loss: 1.25179e-06, val loss: 1.98719e-06, min loss: 1.07035e-06\n",
      "Epoch: 1000600, elapsed: 1.14e+01, train loss: 1.06993e-06, val loss: 1.81762e-06, min loss: 1.06993e-06\n",
      "Epoch: 1000700, elapsed: 1.14e+01, train loss: 4.81548e-06, val loss: 2.86099e-06, min loss: 1.06993e-06\n",
      "Epoch: 1000800, elapsed: 1.14e+01, train loss: 1.06804e-06, val loss: 1.81815e-06, min loss: 1.06804e-06\n",
      "Epoch: 1000900, elapsed: 1.14e+01, train loss: 1.14659e-06, val loss: 1.93590e-06, min loss: 1.06804e-06\n",
      "Epoch: 1001000, elapsed: 1.12e+01, train loss: 1.06836e-06, val loss: 1.81482e-06, min loss: 1.06804e-06\n",
      "Epoch: 1001100, elapsed: 1.12e+01, train loss: 1.13317e-06, val loss: 1.99677e-06, min loss: 1.06804e-06\n",
      "Epoch: 1001200, elapsed: 1.13e+01, train loss: 1.52631e-06, val loss: 2.15578e-06, min loss: 1.06804e-06\n",
      "Epoch: 1001300, elapsed: 1.14e+01, train loss: 1.85234e-06, val loss: 2.45021e-06, min loss: 1.06804e-06\n",
      "Epoch: 1001400, elapsed: 1.13e+01, train loss: 2.34451e-06, val loss: 3.27529e-06, min loss: 1.06804e-06\n",
      "Epoch: 1001500, elapsed: 1.13e+01, train loss: 1.07643e-06, val loss: 1.84029e-06, min loss: 1.06804e-06\n",
      "Epoch: 1001600, elapsed: 1.13e+01, train loss: 1.40961e-06, val loss: 2.58030e-06, min loss: 1.06804e-06\n",
      "Epoch: 1001700, elapsed: 1.14e+01, train loss: 1.08571e-06, val loss: 1.84180e-06, min loss: 1.06804e-06\n",
      "Epoch: 1001800, elapsed: 1.13e+01, train loss: 1.08208e-06, val loss: 1.82820e-06, min loss: 1.06804e-06\n",
      "Epoch: 1001900, elapsed: 1.12e+01, train loss: 3.04067e-06, val loss: 3.41076e-06, min loss: 1.06804e-06\n",
      "Epoch: 1002000, elapsed: 1.12e+01, train loss: 1.47493e-06, val loss: 1.85728e-06, min loss: 1.06804e-06\n",
      "Epoch: 1002100, elapsed: 1.41e+01, train loss: 1.06675e-06, val loss: 1.81530e-06, min loss: 1.06675e-06\n",
      "Epoch: 1002200, elapsed: 1.15e+01, train loss: 1.07055e-06, val loss: 1.81860e-06, min loss: 1.06675e-06\n",
      "Epoch: 1002300, elapsed: 1.15e+01, train loss: 1.10536e-06, val loss: 1.88209e-06, min loss: 1.06675e-06\n",
      "Epoch: 1002400, elapsed: 1.13e+01, train loss: 1.09014e-06, val loss: 1.81536e-06, min loss: 1.06675e-06\n",
      "Epoch: 1002500, elapsed: 1.13e+01, train loss: 4.65606e-06, val loss: 4.48362e-06, min loss: 1.06675e-06\n",
      "Epoch: 1002600, elapsed: 1.11e+01, train loss: 1.22236e-06, val loss: 1.97556e-06, min loss: 1.06675e-06\n",
      "Epoch: 1002700, elapsed: 1.14e+01, train loss: 1.38193e-06, val loss: 2.01007e-06, min loss: 1.06675e-06\n",
      "Epoch: 1002800, elapsed: 1.14e+01, train loss: 1.17922e-06, val loss: 1.86495e-06, min loss: 1.06675e-06\n",
      "Epoch: 1002900, elapsed: 1.14e+01, train loss: 1.17888e-06, val loss: 1.95978e-06, min loss: 1.06675e-06\n",
      "Epoch: 1003000, elapsed: 1.13e+01, train loss: 1.06600e-06, val loss: 1.81702e-06, min loss: 1.06600e-06\n",
      "Epoch: 1003100, elapsed: 1.13e+01, train loss: 1.21385e-06, val loss: 1.88631e-06, min loss: 1.06600e-06\n",
      "Epoch: 1003200, elapsed: 1.11e+01, train loss: 1.27681e-06, val loss: 1.97529e-06, min loss: 1.06600e-06\n",
      "Epoch: 1003300, elapsed: 1.12e+01, train loss: 2.37603e-06, val loss: 2.30711e-06, min loss: 1.06600e-06\n",
      "Epoch: 1003400, elapsed: 1.14e+01, train loss: 1.10100e-06, val loss: 1.87622e-06, min loss: 1.06600e-06\n",
      "Epoch: 1003500, elapsed: 1.13e+01, train loss: 1.36441e-06, val loss: 2.25561e-06, min loss: 1.06600e-06\n",
      "Epoch: 1003600, elapsed: 1.14e+01, train loss: 1.08997e-06, val loss: 1.82806e-06, min loss: 1.06600e-06\n",
      "Epoch: 1003700, elapsed: 1.13e+01, train loss: 1.08452e-06, val loss: 1.82014e-06, min loss: 1.06600e-06\n",
      "Epoch: 1003800, elapsed: 1.12e+01, train loss: 1.29484e-06, val loss: 2.03940e-06, min loss: 1.06600e-06\n",
      "Epoch: 1003900, elapsed: 1.12e+01, train loss: 2.80793e-06, val loss: 3.60987e-06, min loss: 1.06600e-06\n",
      "Epoch: 1004000, elapsed: 1.41e+01, train loss: 1.20562e-06, val loss: 1.97916e-06, min loss: 1.06600e-06\n",
      "Epoch: 1004100, elapsed: 1.14e+01, train loss: 1.08514e-06, val loss: 1.81984e-06, min loss: 1.06600e-06\n",
      "Epoch: 1004200, elapsed: 1.12e+01, train loss: 1.06828e-06, val loss: 1.80737e-06, min loss: 1.06600e-06\n",
      "Epoch: 1004300, elapsed: 1.14e+01, train loss: 1.08976e-06, val loss: 1.87064e-06, min loss: 1.06600e-06\n",
      "Epoch: 1004400, elapsed: 1.13e+01, train loss: 1.12373e-06, val loss: 1.89713e-06, min loss: 1.06600e-06\n",
      "Epoch: 1004500, elapsed: 1.12e+01, train loss: 1.15832e-06, val loss: 1.82645e-06, min loss: 1.06600e-06\n",
      "Epoch: 1004600, elapsed: 1.13e+01, train loss: 1.41630e-06, val loss: 2.16339e-06, min loss: 1.06600e-06\n",
      "Epoch: 1004700, elapsed: 1.13e+01, train loss: 1.24683e-06, val loss: 1.99111e-06, min loss: 1.06600e-06\n",
      "Epoch: 1004800, elapsed: 1.13e+01, train loss: 1.24982e-06, val loss: 2.13643e-06, min loss: 1.06600e-06\n",
      "Epoch: 1004900, elapsed: 1.12e+01, train loss: 1.26927e-06, val loss: 1.99235e-06, min loss: 1.06600e-06\n",
      "Epoch: 1005000, elapsed: 1.12e+01, train loss: 1.28349e-06, val loss: 2.77801e-06, min loss: 1.06600e-06\n",
      "Epoch: 1005100, elapsed: 1.33e+01, train loss: 1.26644e-06, val loss: 1.99722e-06, min loss: 1.06600e-06\n",
      "Epoch: 1005200, elapsed: 1.12e+01, train loss: 1.10131e-06, val loss: 1.87878e-06, min loss: 1.06600e-06\n",
      "Epoch: 1005300, elapsed: 1.11e+01, train loss: 3.44322e-06, val loss: 3.85924e-06, min loss: 1.06600e-06\n",
      "Epoch: 1005400, elapsed: 1.12e+01, train loss: 1.51498e-06, val loss: 2.10496e-06, min loss: 1.06600e-06\n",
      "Epoch: 1005500, elapsed: 1.12e+01, train loss: 1.07021e-06, val loss: 1.83382e-06, min loss: 1.06600e-06\n",
      "Epoch: 1005600, elapsed: 1.10e+01, train loss: 1.67895e-06, val loss: 2.82167e-06, min loss: 1.06600e-06\n",
      "Epoch: 1005700, elapsed: 1.13e+01, train loss: 1.06475e-06, val loss: 1.81878e-06, min loss: 1.06475e-06\n",
      "Epoch: 1005800, elapsed: 1.11e+01, train loss: 1.06483e-06, val loss: 1.81366e-06, min loss: 1.06475e-06\n",
      "Epoch: 1005900, elapsed: 1.43e+01, train loss: 1.75396e-06, val loss: 2.74207e-06, min loss: 1.06475e-06\n",
      "Epoch: 1006000, elapsed: 1.14e+01, train loss: 1.06312e-06, val loss: 1.81389e-06, min loss: 1.06312e-06\n",
      "Epoch: 1006100, elapsed: 1.13e+01, train loss: 1.06668e-06, val loss: 1.81205e-06, min loss: 1.06312e-06\n",
      "Epoch: 1006200, elapsed: 1.12e+01, train loss: 1.11826e-06, val loss: 1.91152e-06, min loss: 1.06312e-06\n",
      "Epoch: 1006300, elapsed: 1.10e+01, train loss: 1.15324e-06, val loss: 1.87361e-06, min loss: 1.06312e-06\n",
      "Epoch: 1006400, elapsed: 1.15e+01, train loss: 1.54733e-06, val loss: 2.42055e-06, min loss: 1.06312e-06\n",
      "Epoch: 1006500, elapsed: 1.12e+01, train loss: 1.41957e-06, val loss: 2.23293e-06, min loss: 1.06312e-06\n",
      "Epoch: 1006600, elapsed: 1.13e+01, train loss: 1.17116e-06, val loss: 2.35634e-06, min loss: 1.06312e-06\n",
      "Epoch: 1006700, elapsed: 1.13e+01, train loss: 2.17251e-06, val loss: 2.87698e-06, min loss: 1.06312e-06\n",
      "Epoch: 1006800, elapsed: 1.13e+01, train loss: 1.06989e-06, val loss: 1.81690e-06, min loss: 1.06312e-06\n",
      "Epoch: 1006900, elapsed: 1.13e+01, train loss: 1.27136e-06, val loss: 1.96210e-06, min loss: 1.06312e-06\n",
      "Epoch: 1007000, elapsed: 1.12e+01, train loss: 1.14689e-06, val loss: 1.88721e-06, min loss: 1.06312e-06\n",
      "Epoch: 1007100, elapsed: 1.11e+01, train loss: 1.30093e-06, val loss: 1.95495e-06, min loss: 1.06312e-06\n",
      "Epoch: 1007200, elapsed: 1.11e+01, train loss: 2.77010e-06, val loss: 3.10171e-06, min loss: 1.06312e-06\n",
      "Epoch: 1007300, elapsed: 1.12e+01, train loss: 1.09151e-06, val loss: 1.84634e-06, min loss: 1.06312e-06\n",
      "Epoch: 1007400, elapsed: 1.13e+01, train loss: 1.06469e-06, val loss: 1.80631e-06, min loss: 1.06312e-06\n",
      "Epoch: 1007500, elapsed: 1.13e+01, train loss: 1.07779e-06, val loss: 1.82049e-06, min loss: 1.06312e-06\n",
      "Epoch: 1007600, elapsed: 1.14e+01, train loss: 1.31126e-06, val loss: 1.99543e-06, min loss: 1.06312e-06\n",
      "Epoch: 1007700, elapsed: 1.12e+01, train loss: 2.69207e-06, val loss: 3.83219e-06, min loss: 1.06312e-06\n",
      "Epoch: 1007800, elapsed: 1.46e+01, train loss: 2.55438e-06, val loss: 3.66038e-06, min loss: 1.06312e-06\n",
      "Epoch: 1007900, elapsed: 1.17e+01, train loss: 3.77134e-06, val loss: 5.16262e-06, min loss: 1.06312e-06\n",
      "Epoch: 1008000, elapsed: 1.15e+01, train loss: 1.70276e-06, val loss: 2.55574e-06, min loss: 1.06312e-06\n",
      "Epoch: 1008100, elapsed: 1.15e+01, train loss: 4.18289e-06, val loss: 3.85252e-06, min loss: 1.06312e-06\n",
      "Epoch: 1008200, elapsed: 1.13e+01, train loss: 1.56155e-06, val loss: 2.22897e-06, min loss: 1.06312e-06\n",
      "Epoch: 1008300, elapsed: 1.13e+01, train loss: 4.45897e-06, val loss: 5.20233e-06, min loss: 1.06312e-06\n",
      "Epoch: 1008400, elapsed: 1.14e+01, train loss: 1.39216e-06, val loss: 2.03268e-06, min loss: 1.06312e-06\n",
      "Epoch: 1008500, elapsed: 1.14e+01, train loss: 1.28559e-06, val loss: 1.85976e-06, min loss: 1.06312e-06\n",
      "Epoch: 1008600, elapsed: 1.14e+01, train loss: 1.09073e-06, val loss: 1.84013e-06, min loss: 1.06312e-06\n",
      "Epoch: 1008700, elapsed: 1.14e+01, train loss: 1.08712e-06, val loss: 1.85258e-06, min loss: 1.06312e-06\n",
      "Epoch: 1008800, elapsed: 1.13e+01, train loss: 1.14812e-06, val loss: 1.93709e-06, min loss: 1.06312e-06\n",
      "Epoch: 1008900, elapsed: 1.13e+01, train loss: 1.89782e-06, val loss: 2.12807e-06, min loss: 1.06312e-06\n",
      "Epoch: 1009000, elapsed: 1.16e+01, train loss: 1.77332e-06, val loss: 2.73699e-06, min loss: 1.06312e-06\n",
      "Epoch: 1009100, elapsed: 1.12e+01, train loss: 1.51664e-06, val loss: 2.35386e-06, min loss: 1.06312e-06\n",
      "Epoch: 1009200, elapsed: 1.14e+01, train loss: 1.06072e-06, val loss: 1.80730e-06, min loss: 1.06072e-06\n",
      "Epoch: 1009300, elapsed: 1.14e+01, train loss: 1.07811e-06, val loss: 1.82254e-06, min loss: 1.06072e-06\n",
      "Epoch: 1009400, elapsed: 1.15e+01, train loss: 1.34191e-06, val loss: 2.14599e-06, min loss: 1.06072e-06\n",
      "Epoch: 1009500, elapsed: 1.15e+01, train loss: 1.05987e-06, val loss: 1.80936e-06, min loss: 1.05987e-06\n",
      "Epoch: 1009600, elapsed: 1.13e+01, train loss: 1.08963e-06, val loss: 1.84433e-06, min loss: 1.05987e-06\n",
      "Epoch: 1009700, elapsed: 1.44e+01, train loss: 1.07550e-06, val loss: 1.80797e-06, min loss: 1.05987e-06\n",
      "Epoch: 1009800, elapsed: 1.15e+01, train loss: 1.08854e-06, val loss: 1.87576e-06, min loss: 1.05987e-06\n",
      "Epoch: 1009900, elapsed: 1.13e+01, train loss: 1.07017e-06, val loss: 1.81270e-06, min loss: 1.05987e-06\n",
      "Epoch: 1010000, elapsed: 1.14e+01, train loss: 1.57350e-06, val loss: 2.32720e-06, min loss: 1.05987e-06\n",
      "Epoch: 1010100, elapsed: 1.36e+01, train loss: 1.33905e-06, val loss: 2.01966e-06, min loss: 1.05987e-06\n",
      "Epoch: 1010200, elapsed: 1.14e+01, train loss: 1.09404e-06, val loss: 1.82592e-06, min loss: 1.05987e-06\n",
      "Epoch: 1010300, elapsed: 1.16e+01, train loss: 1.69572e-06, val loss: 2.27793e-06, min loss: 1.05987e-06\n",
      "Epoch: 1010400, elapsed: 1.13e+01, train loss: 1.06098e-06, val loss: 1.80765e-06, min loss: 1.05987e-06\n",
      "Epoch: 1010500, elapsed: 1.14e+01, train loss: 1.51859e-06, val loss: 2.48958e-06, min loss: 1.05987e-06\n",
      "Epoch: 1010600, elapsed: 1.15e+01, train loss: 1.05944e-06, val loss: 1.81277e-06, min loss: 1.05944e-06\n",
      "Epoch: 1010700, elapsed: 1.15e+01, train loss: 1.09747e-06, val loss: 1.80448e-06, min loss: 1.05944e-06\n",
      "Epoch: 1010800, elapsed: 1.12e+01, train loss: 1.08802e-06, val loss: 1.86867e-06, min loss: 1.05944e-06\n",
      "Epoch: 1010900, elapsed: 1.14e+01, train loss: 1.05966e-06, val loss: 1.80832e-06, min loss: 1.05944e-06\n",
      "Epoch: 1011000, elapsed: 1.13e+01, train loss: 2.34710e-06, val loss: 2.59142e-06, min loss: 1.05944e-06\n",
      "Epoch: 1011100, elapsed: 1.14e+01, train loss: 1.38714e-06, val loss: 2.30323e-06, min loss: 1.05944e-06\n",
      "Epoch: 1011200, elapsed: 1.13e+01, train loss: 1.06813e-06, val loss: 1.82254e-06, min loss: 1.05944e-06\n",
      "Epoch: 1011300, elapsed: 1.14e+01, train loss: 2.30534e-06, val loss: 3.34446e-06, min loss: 1.05944e-06\n",
      "Epoch: 1011400, elapsed: 1.14e+01, train loss: 1.30565e-06, val loss: 1.84446e-06, min loss: 1.05944e-06\n",
      "Epoch: 1011500, elapsed: 1.43e+01, train loss: 2.05334e-06, val loss: 3.00390e-06, min loss: 1.05944e-06\n",
      "Epoch: 1011600, elapsed: 1.14e+01, train loss: 1.72522e-06, val loss: 2.94826e-06, min loss: 1.05944e-06\n",
      "Epoch: 1011700, elapsed: 1.13e+01, train loss: 3.01275e-06, val loss: 4.11014e-06, min loss: 1.05944e-06\n",
      "Epoch: 1011800, elapsed: 1.15e+01, train loss: 1.06790e-06, val loss: 1.82406e-06, min loss: 1.05944e-06\n",
      "Epoch: 1011900, elapsed: 1.13e+01, train loss: 1.49736e-06, val loss: 2.14345e-06, min loss: 1.05944e-06\n",
      "Epoch: 1012000, elapsed: 1.13e+01, train loss: 1.06549e-06, val loss: 1.80984e-06, min loss: 1.05944e-06\n",
      "Epoch: 1012100, elapsed: 1.15e+01, train loss: 1.36374e-06, val loss: 2.14610e-06, min loss: 1.05944e-06\n",
      "Epoch: 1012200, elapsed: 1.13e+01, train loss: 1.06970e-06, val loss: 1.81584e-06, min loss: 1.05944e-06\n",
      "Epoch: 1012300, elapsed: 1.15e+01, train loss: 1.21486e-06, val loss: 1.91163e-06, min loss: 1.05944e-06\n",
      "Epoch: 1012400, elapsed: 1.13e+01, train loss: 1.08271e-06, val loss: 1.83419e-06, min loss: 1.05944e-06\n",
      "Epoch: 1012500, elapsed: 1.13e+01, train loss: 1.06480e-06, val loss: 1.81151e-06, min loss: 1.05944e-06\n",
      "Epoch: 1012600, elapsed: 1.13e+01, train loss: 1.14519e-06, val loss: 1.88447e-06, min loss: 1.05944e-06\n",
      "Epoch: 1012700, elapsed: 1.15e+01, train loss: 1.08514e-06, val loss: 1.85658e-06, min loss: 1.05944e-06\n",
      "Epoch: 1012800, elapsed: 1.12e+01, train loss: 1.09631e-06, val loss: 1.85101e-06, min loss: 1.05944e-06\n",
      "Epoch: 1012900, elapsed: 1.12e+01, train loss: 1.80339e-06, val loss: 2.67446e-06, min loss: 1.05944e-06\n",
      "Epoch: 1013000, elapsed: 1.13e+01, train loss: 1.13967e-06, val loss: 1.87358e-06, min loss: 1.05944e-06\n",
      "Epoch: 1013100, elapsed: 1.12e+01, train loss: 1.09456e-06, val loss: 1.83872e-06, min loss: 1.05944e-06\n",
      "Epoch: 1013200, elapsed: 1.14e+01, train loss: 1.17913e-06, val loss: 1.93007e-06, min loss: 1.05944e-06\n",
      "Epoch: 1013300, elapsed: 1.14e+01, train loss: 1.08680e-06, val loss: 1.83672e-06, min loss: 1.05944e-06\n",
      "Epoch: 1013400, elapsed: 1.43e+01, train loss: 1.06150e-06, val loss: 1.81190e-06, min loss: 1.05944e-06\n",
      "Epoch: 1013500, elapsed: 1.15e+01, train loss: 2.58861e-06, val loss: 3.52612e-06, min loss: 1.05944e-06\n",
      "Epoch: 1013600, elapsed: 1.14e+01, train loss: 1.46728e-06, val loss: 2.37956e-06, min loss: 1.05944e-06\n",
      "Epoch: 1013700, elapsed: 1.15e+01, train loss: 1.79927e-06, val loss: 2.56886e-06, min loss: 1.05944e-06\n",
      "Epoch: 1013800, elapsed: 1.13e+01, train loss: 1.05881e-06, val loss: 1.80629e-06, min loss: 1.05881e-06\n",
      "Epoch: 1013900, elapsed: 1.14e+01, train loss: 1.06100e-06, val loss: 1.80517e-06, min loss: 1.05881e-06\n",
      "Epoch: 1014000, elapsed: 1.14e+01, train loss: 1.15110e-06, val loss: 1.88770e-06, min loss: 1.05881e-06\n",
      "Epoch: 1014100, elapsed: 1.15e+01, train loss: 2.02986e-06, val loss: 2.87108e-06, min loss: 1.05881e-06\n",
      "Epoch: 1014200, elapsed: 1.15e+01, train loss: 2.69258e-06, val loss: 2.58183e-06, min loss: 1.05881e-06\n",
      "Epoch: 1014300, elapsed: 1.13e+01, train loss: 1.22926e-06, val loss: 2.00401e-06, min loss: 1.05881e-06\n",
      "Epoch: 1014400, elapsed: 1.13e+01, train loss: 1.17801e-06, val loss: 2.02459e-06, min loss: 1.05881e-06\n",
      "Epoch: 1014500, elapsed: 1.13e+01, train loss: 1.95877e-06, val loss: 2.64876e-06, min loss: 1.05881e-06\n",
      "Epoch: 1014600, elapsed: 1.13e+01, train loss: 1.18249e-06, val loss: 1.95383e-06, min loss: 1.05881e-06\n",
      "Epoch: 1014700, elapsed: 1.13e+01, train loss: 1.14093e-06, val loss: 1.91969e-06, min loss: 1.05881e-06\n",
      "Epoch: 1014800, elapsed: 1.14e+01, train loss: 1.31461e-06, val loss: 1.98575e-06, min loss: 1.05881e-06\n",
      "Epoch: 1014900, elapsed: 1.14e+01, train loss: 1.05566e-06, val loss: 1.80282e-06, min loss: 1.05566e-06\n",
      "Epoch: 1015000, elapsed: 1.14e+01, train loss: 1.08989e-06, val loss: 1.82175e-06, min loss: 1.05566e-06\n",
      "Epoch: 1015100, elapsed: 1.34e+01, train loss: 1.12916e-06, val loss: 1.82927e-06, min loss: 1.05566e-06\n",
      "Epoch: 1015200, elapsed: 1.13e+01, train loss: 1.08701e-06, val loss: 1.81664e-06, min loss: 1.05566e-06\n",
      "Epoch: 1015300, elapsed: 1.45e+01, train loss: 1.06410e-06, val loss: 1.80035e-06, min loss: 1.05566e-06\n",
      "Epoch: 1015400, elapsed: 1.15e+01, train loss: 1.05832e-06, val loss: 1.80662e-06, min loss: 1.05566e-06\n",
      "Epoch: 1015500, elapsed: 1.14e+01, train loss: 1.06251e-06, val loss: 1.81779e-06, min loss: 1.05566e-06\n",
      "Epoch: 1015600, elapsed: 1.16e+01, train loss: 1.39415e-06, val loss: 2.06601e-06, min loss: 1.05566e-06\n",
      "Epoch: 1015700, elapsed: 1.15e+01, train loss: 5.60707e-06, val loss: 5.87754e-06, min loss: 1.05566e-06\n",
      "Epoch: 1015800, elapsed: 1.14e+01, train loss: 1.05506e-06, val loss: 1.80570e-06, min loss: 1.05506e-06\n",
      "Epoch: 1015900, elapsed: 1.14e+01, train loss: 1.05521e-06, val loss: 1.80921e-06, min loss: 1.05506e-06\n",
      "Epoch: 1016000, elapsed: 1.14e+01, train loss: 1.06305e-06, val loss: 1.80868e-06, min loss: 1.05506e-06\n",
      "Epoch: 1016100, elapsed: 1.13e+01, train loss: 1.10211e-06, val loss: 1.92100e-06, min loss: 1.05506e-06\n",
      "Epoch: 1016200, elapsed: 1.15e+01, train loss: 2.06151e-06, val loss: 1.98218e-06, min loss: 1.05506e-06\n",
      "Epoch: 1016300, elapsed: 1.14e+01, train loss: 1.13215e-06, val loss: 1.94200e-06, min loss: 1.05506e-06\n",
      "Epoch: 1016400, elapsed: 1.13e+01, train loss: 1.11322e-06, val loss: 1.90069e-06, min loss: 1.05506e-06\n",
      "Epoch: 1016500, elapsed: 1.13e+01, train loss: 1.59561e-06, val loss: 2.10290e-06, min loss: 1.05506e-06\n",
      "Epoch: 1016600, elapsed: 1.14e+01, train loss: 1.41927e-06, val loss: 2.08947e-06, min loss: 1.05506e-06\n",
      "Epoch: 1016700, elapsed: 1.13e+01, train loss: 1.38509e-06, val loss: 2.02515e-06, min loss: 1.05506e-06\n",
      "Epoch: 1016800, elapsed: 1.13e+01, train loss: 1.17934e-06, val loss: 1.90557e-06, min loss: 1.05506e-06\n",
      "Epoch: 1016900, elapsed: 1.13e+01, train loss: 1.15227e-06, val loss: 1.85912e-06, min loss: 1.05506e-06\n",
      "Epoch: 1017000, elapsed: 1.14e+01, train loss: 1.05973e-06, val loss: 1.80073e-06, min loss: 1.05506e-06\n",
      "Epoch: 1017100, elapsed: 1.13e+01, train loss: 1.13166e-06, val loss: 1.90567e-06, min loss: 1.05506e-06\n",
      "Epoch: 1017200, elapsed: 1.42e+01, train loss: 2.68224e-06, val loss: 3.10007e-06, min loss: 1.05506e-06\n",
      "Epoch: 1017300, elapsed: 1.15e+01, train loss: 1.27807e-06, val loss: 1.96043e-06, min loss: 1.05506e-06\n",
      "Epoch: 1017400, elapsed: 1.14e+01, train loss: 1.10348e-06, val loss: 1.83749e-06, min loss: 1.05506e-06\n",
      "Epoch: 1017500, elapsed: 1.14e+01, train loss: 1.76805e-06, val loss: 2.52069e-06, min loss: 1.05506e-06\n",
      "Epoch: 1017600, elapsed: 1.13e+01, train loss: 3.59209e-06, val loss: 3.51188e-06, min loss: 1.05506e-06\n",
      "Epoch: 1017700, elapsed: 1.13e+01, train loss: 4.45958e-06, val loss: 4.34202e-06, min loss: 1.05506e-06\n",
      "Epoch: 1017800, elapsed: 1.12e+01, train loss: 2.66196e-06, val loss: 3.28525e-06, min loss: 1.05506e-06\n",
      "Epoch: 1017900, elapsed: 1.13e+01, train loss: 1.08863e-06, val loss: 1.86319e-06, min loss: 1.05506e-06\n",
      "Epoch: 1018000, elapsed: 1.13e+01, train loss: 3.45406e-06, val loss: 4.06202e-06, min loss: 1.05506e-06\n",
      "Epoch: 1018100, elapsed: 1.13e+01, train loss: 1.34973e-06, val loss: 2.14163e-06, min loss: 1.05506e-06\n",
      "Epoch: 1018200, elapsed: 1.13e+01, train loss: 1.07537e-06, val loss: 1.85628e-06, min loss: 1.05506e-06\n",
      "Epoch: 1018300, elapsed: 1.12e+01, train loss: 1.05330e-06, val loss: 1.79745e-06, min loss: 1.05330e-06\n",
      "Epoch: 1018400, elapsed: 1.13e+01, train loss: 1.06397e-06, val loss: 1.83332e-06, min loss: 1.05330e-06\n",
      "Epoch: 1018500, elapsed: 1.12e+01, train loss: 1.08630e-06, val loss: 1.86760e-06, min loss: 1.05330e-06\n",
      "Epoch: 1018600, elapsed: 1.13e+01, train loss: 1.11003e-06, val loss: 1.84188e-06, min loss: 1.05330e-06\n",
      "Epoch: 1018700, elapsed: 1.12e+01, train loss: 1.07088e-06, val loss: 1.81836e-06, min loss: 1.05330e-06\n",
      "Epoch: 1018800, elapsed: 1.13e+01, train loss: 1.07889e-06, val loss: 1.79828e-06, min loss: 1.05330e-06\n",
      "Epoch: 1018900, elapsed: 1.11e+01, train loss: 1.51611e-06, val loss: 2.55983e-06, min loss: 1.05330e-06\n",
      "Epoch: 1019000, elapsed: 1.10e+01, train loss: 5.08742e-06, val loss: 5.59228e-06, min loss: 1.05330e-06\n",
      "Epoch: 1019100, elapsed: 1.43e+01, train loss: 1.07740e-06, val loss: 1.86738e-06, min loss: 1.05330e-06\n",
      "Epoch: 1019200, elapsed: 1.15e+01, train loss: 1.05600e-06, val loss: 1.81279e-06, min loss: 1.05330e-06\n",
      "Epoch: 1019300, elapsed: 1.13e+01, train loss: 1.05552e-06, val loss: 1.81666e-06, min loss: 1.05330e-06\n",
      "Epoch: 1019400, elapsed: 1.13e+01, train loss: 1.06704e-06, val loss: 1.79552e-06, min loss: 1.05330e-06\n",
      "Epoch: 1019500, elapsed: 1.13e+01, train loss: 1.61251e-06, val loss: 2.16737e-06, min loss: 1.05330e-06\n",
      "Epoch: 1019600, elapsed: 1.13e+01, train loss: 1.28630e-06, val loss: 1.89381e-06, min loss: 1.05330e-06\n",
      "Epoch: 1019700, elapsed: 1.13e+01, train loss: 2.10461e-06, val loss: 2.95208e-06, min loss: 1.05330e-06\n",
      "Epoch: 1019800, elapsed: 1.12e+01, train loss: 2.65714e-06, val loss: 3.62764e-06, min loss: 1.05330e-06\n",
      "Epoch: 1019900, elapsed: 1.12e+01, train loss: 1.17484e-06, val loss: 1.93531e-06, min loss: 1.05330e-06\n",
      "Epoch: 1020000, elapsed: 1.12e+01, train loss: 1.05083e-06, val loss: 1.79995e-06, min loss: 1.05083e-06\n",
      "Epoch: 1020100, elapsed: 1.34e+01, train loss: 1.08539e-06, val loss: 1.80529e-06, min loss: 1.05083e-06\n",
      "Epoch: 1020200, elapsed: 1.12e+01, train loss: 1.84248e-06, val loss: 2.61309e-06, min loss: 1.05083e-06\n",
      "Epoch: 1020300, elapsed: 1.11e+01, train loss: 1.50289e-06, val loss: 2.41258e-06, min loss: 1.05083e-06\n",
      "Epoch: 1020400, elapsed: 1.11e+01, train loss: 1.05297e-06, val loss: 1.80380e-06, min loss: 1.05083e-06\n",
      "Epoch: 1020500, elapsed: 1.12e+01, train loss: 1.05213e-06, val loss: 1.80297e-06, min loss: 1.05083e-06\n",
      "Epoch: 1020600, elapsed: 1.12e+01, train loss: 1.06311e-06, val loss: 1.79541e-06, min loss: 1.05083e-06\n",
      "Epoch: 1020700, elapsed: 1.13e+01, train loss: 1.89613e-06, val loss: 2.69474e-06, min loss: 1.05083e-06\n",
      "Epoch: 1020800, elapsed: 1.12e+01, train loss: 1.45613e-06, val loss: 2.35276e-06, min loss: 1.05083e-06\n",
      "Epoch: 1020900, elapsed: 1.13e+01, train loss: 4.82590e-06, val loss: 6.11791e-06, min loss: 1.05083e-06\n",
      "Epoch: 1021000, elapsed: 1.44e+01, train loss: 1.66232e-06, val loss: 2.66426e-06, min loss: 1.05083e-06\n",
      "Epoch: 1021100, elapsed: 1.14e+01, train loss: 1.06288e-06, val loss: 1.81133e-06, min loss: 1.05083e-06\n",
      "Epoch: 1021200, elapsed: 1.15e+01, train loss: 1.06506e-06, val loss: 1.84813e-06, min loss: 1.05083e-06\n",
      "Epoch: 1021300, elapsed: 1.15e+01, train loss: 6.72760e-06, val loss: 7.29685e-06, min loss: 1.05083e-06\n",
      "Epoch: 1021400, elapsed: 1.12e+01, train loss: 1.20415e-06, val loss: 1.85448e-06, min loss: 1.05083e-06\n",
      "Epoch: 1021500, elapsed: 1.15e+01, train loss: 1.05369e-06, val loss: 1.79462e-06, min loss: 1.05083e-06\n",
      "Epoch: 1021600, elapsed: 1.12e+01, train loss: 1.05224e-06, val loss: 1.80557e-06, min loss: 1.05083e-06\n",
      "Epoch: 1021700, elapsed: 1.13e+01, train loss: 1.05021e-06, val loss: 1.79958e-06, min loss: 1.05021e-06\n",
      "Epoch: 1021800, elapsed: 1.13e+01, train loss: 1.20201e-06, val loss: 1.93988e-06, min loss: 1.05021e-06\n",
      "Epoch: 1021900, elapsed: 1.13e+01, train loss: 5.31648e-06, val loss: 6.47265e-06, min loss: 1.05021e-06\n",
      "Epoch: 1022000, elapsed: 1.13e+01, train loss: 1.14188e-06, val loss: 1.93622e-06, min loss: 1.05021e-06\n",
      "Epoch: 1022100, elapsed: 1.13e+01, train loss: 1.19571e-06, val loss: 1.85991e-06, min loss: 1.05021e-06\n",
      "Epoch: 1022200, elapsed: 1.12e+01, train loss: 1.05907e-06, val loss: 1.84021e-06, min loss: 1.05021e-06\n",
      "Epoch: 1022300, elapsed: 1.12e+01, train loss: 1.22513e-06, val loss: 1.96809e-06, min loss: 1.05021e-06\n",
      "Epoch: 1022400, elapsed: 1.12e+01, train loss: 1.11868e-06, val loss: 1.83385e-06, min loss: 1.05021e-06\n",
      "Epoch: 1022500, elapsed: 1.12e+01, train loss: 1.32399e-06, val loss: 2.22656e-06, min loss: 1.05021e-06\n",
      "Epoch: 1022600, elapsed: 1.12e+01, train loss: 1.04858e-06, val loss: 1.80002e-06, min loss: 1.04858e-06\n",
      "Epoch: 1022700, elapsed: 1.13e+01, train loss: 1.04985e-06, val loss: 1.79987e-06, min loss: 1.04858e-06\n",
      "Epoch: 1022800, elapsed: 1.14e+01, train loss: 1.17234e-06, val loss: 1.81958e-06, min loss: 1.04858e-06\n",
      "Epoch: 1022900, elapsed: 1.42e+01, train loss: 1.13272e-06, val loss: 1.85225e-06, min loss: 1.04858e-06\n",
      "Epoch: 1023000, elapsed: 1.14e+01, train loss: 1.14570e-06, val loss: 1.88889e-06, min loss: 1.04858e-06\n",
      "Epoch: 1023100, elapsed: 1.14e+01, train loss: 1.15148e-06, val loss: 1.84867e-06, min loss: 1.04858e-06\n",
      "Epoch: 1023200, elapsed: 1.13e+01, train loss: 1.18310e-06, val loss: 2.03007e-06, min loss: 1.04858e-06\n",
      "Epoch: 1023300, elapsed: 1.15e+01, train loss: 1.53097e-06, val loss: 1.97900e-06, min loss: 1.04858e-06\n",
      "Epoch: 1023400, elapsed: 1.14e+01, train loss: 1.82817e-06, val loss: 3.06831e-06, min loss: 1.04858e-06\n",
      "Epoch: 1023500, elapsed: 1.14e+01, train loss: 1.06605e-06, val loss: 1.83481e-06, min loss: 1.04858e-06\n",
      "Epoch: 1023600, elapsed: 1.13e+01, train loss: 1.04983e-06, val loss: 1.80668e-06, min loss: 1.04858e-06\n",
      "Epoch: 1023700, elapsed: 1.13e+01, train loss: 1.05000e-06, val loss: 1.80396e-06, min loss: 1.04858e-06\n",
      "Epoch: 1023800, elapsed: 1.13e+01, train loss: 1.25574e-06, val loss: 2.08275e-06, min loss: 1.04858e-06\n",
      "Epoch: 1023900, elapsed: 1.14e+01, train loss: 1.04730e-06, val loss: 1.79998e-06, min loss: 1.04730e-06\n",
      "Epoch: 1024000, elapsed: 1.12e+01, train loss: 1.05013e-06, val loss: 1.80206e-06, min loss: 1.04730e-06\n",
      "Epoch: 1024100, elapsed: 1.12e+01, train loss: 1.06272e-06, val loss: 1.85013e-06, min loss: 1.04730e-06\n",
      "Epoch: 1024200, elapsed: 1.12e+01, train loss: 1.32386e-06, val loss: 2.17207e-06, min loss: 1.04730e-06\n",
      "Epoch: 1024300, elapsed: 1.14e+01, train loss: 1.10441e-06, val loss: 1.92576e-06, min loss: 1.04730e-06\n",
      "Epoch: 1024400, elapsed: 1.12e+01, train loss: 3.29303e-06, val loss: 4.65423e-06, min loss: 1.04730e-06\n",
      "Epoch: 1024500, elapsed: 1.13e+01, train loss: 1.05553e-06, val loss: 1.80025e-06, min loss: 1.04730e-06\n",
      "Epoch: 1024600, elapsed: 1.12e+01, train loss: 1.85379e-06, val loss: 2.85869e-06, min loss: 1.04730e-06\n",
      "Epoch: 1024700, elapsed: 1.12e+01, train loss: 3.97942e-06, val loss: 4.54194e-06, min loss: 1.04730e-06\n",
      "Epoch: 1024800, elapsed: 1.41e+01, train loss: 1.17449e-06, val loss: 2.36541e-06, min loss: 1.04730e-06\n",
      "Epoch: 1024900, elapsed: 1.13e+01, train loss: 2.47039e-06, val loss: 2.46865e-06, min loss: 1.04730e-06\n",
      "Epoch: 1025000, elapsed: 1.14e+01, train loss: 1.27246e-06, val loss: 2.01771e-06, min loss: 1.04730e-06\n",
      "Epoch: 1025100, elapsed: 1.35e+01, train loss: 1.04856e-06, val loss: 1.79569e-06, min loss: 1.04730e-06\n",
      "Epoch: 1025200, elapsed: 1.13e+01, train loss: 1.64893e-06, val loss: 2.47867e-06, min loss: 1.04730e-06\n",
      "Epoch: 1025300, elapsed: 1.13e+01, train loss: 1.72826e-06, val loss: 2.46805e-06, min loss: 1.04730e-06\n",
      "Epoch: 1025400, elapsed: 1.15e+01, train loss: 1.23740e-06, val loss: 1.99334e-06, min loss: 1.04730e-06\n",
      "Epoch: 1025500, elapsed: 1.13e+01, train loss: 1.04909e-06, val loss: 1.79451e-06, min loss: 1.04730e-06\n",
      "Epoch: 1025600, elapsed: 1.14e+01, train loss: 1.05042e-06, val loss: 1.78634e-06, min loss: 1.04730e-06\n",
      "Epoch: 1025700, elapsed: 1.14e+01, train loss: 1.05454e-06, val loss: 1.80795e-06, min loss: 1.04730e-06\n",
      "Epoch: 1025800, elapsed: 1.13e+01, train loss: 1.04708e-06, val loss: 1.79492e-06, min loss: 1.04708e-06\n",
      "Epoch: 1025900, elapsed: 1.13e+01, train loss: 1.05462e-06, val loss: 1.80048e-06, min loss: 1.04708e-06\n",
      "Epoch: 1026000, elapsed: 1.12e+01, train loss: 1.07785e-06, val loss: 1.84849e-06, min loss: 1.04708e-06\n",
      "Epoch: 1026100, elapsed: 1.13e+01, train loss: 1.26811e-06, val loss: 1.92831e-06, min loss: 1.04708e-06\n",
      "Epoch: 1026200, elapsed: 1.13e+01, train loss: 2.93793e-06, val loss: 3.17627e-06, min loss: 1.04708e-06\n",
      "Epoch: 1026300, elapsed: 1.12e+01, train loss: 1.56812e-06, val loss: 2.41830e-06, min loss: 1.04708e-06\n",
      "Epoch: 1026400, elapsed: 1.12e+01, train loss: 1.33298e-06, val loss: 2.31822e-06, min loss: 1.04708e-06\n",
      "Epoch: 1026500, elapsed: 1.15e+01, train loss: 4.30514e-06, val loss: 5.35496e-06, min loss: 1.04708e-06\n",
      "Epoch: 1026600, elapsed: 1.12e+01, train loss: 1.13798e-06, val loss: 1.93963e-06, min loss: 1.04708e-06\n",
      "Epoch: 1026700, elapsed: 1.41e+01, train loss: 1.04655e-06, val loss: 1.79576e-06, min loss: 1.04655e-06\n",
      "Epoch: 1026800, elapsed: 1.15e+01, train loss: 1.06263e-06, val loss: 1.78827e-06, min loss: 1.04655e-06\n",
      "Epoch: 1026900, elapsed: 1.14e+01, train loss: 1.07359e-06, val loss: 1.85000e-06, min loss: 1.04655e-06\n",
      "Epoch: 1027000, elapsed: 1.16e+01, train loss: 1.23593e-06, val loss: 2.01362e-06, min loss: 1.04655e-06\n",
      "Epoch: 1027100, elapsed: 1.14e+01, train loss: 1.07032e-06, val loss: 1.82274e-06, min loss: 1.04655e-06\n",
      "Epoch: 1027200, elapsed: 1.15e+01, train loss: 1.07562e-06, val loss: 1.79867e-06, min loss: 1.04655e-06\n",
      "Epoch: 1027300, elapsed: 1.13e+01, train loss: 1.05583e-06, val loss: 1.79906e-06, min loss: 1.04655e-06\n",
      "Epoch: 1027400, elapsed: 1.14e+01, train loss: 1.11913e-06, val loss: 1.86737e-06, min loss: 1.04655e-06\n",
      "Epoch: 1027500, elapsed: 1.13e+01, train loss: 1.99206e-06, val loss: 2.83489e-06, min loss: 1.04655e-06\n",
      "Epoch: 1027600, elapsed: 1.12e+01, train loss: 5.22379e-06, val loss: 6.53138e-06, min loss: 1.04655e-06\n",
      "Epoch: 1027700, elapsed: 1.11e+01, train loss: 1.21274e-06, val loss: 1.86736e-06, min loss: 1.04655e-06\n",
      "Epoch: 1027800, elapsed: 1.12e+01, train loss: 3.10914e-06, val loss: 3.85722e-06, min loss: 1.04655e-06\n",
      "Epoch: 1027900, elapsed: 1.12e+01, train loss: 2.97584e-06, val loss: 3.11665e-06, min loss: 1.04655e-06\n",
      "Epoch: 1028000, elapsed: 1.11e+01, train loss: 1.78466e-06, val loss: 2.38536e-06, min loss: 1.04655e-06\n",
      "Epoch: 1028100, elapsed: 1.12e+01, train loss: 1.09215e-06, val loss: 1.81510e-06, min loss: 1.04655e-06\n",
      "Epoch: 1028200, elapsed: 1.12e+01, train loss: 1.12266e-06, val loss: 1.80638e-06, min loss: 1.04655e-06\n",
      "Epoch: 1028300, elapsed: 1.13e+01, train loss: 1.05592e-06, val loss: 1.79353e-06, min loss: 1.04655e-06\n",
      "Epoch: 1028400, elapsed: 1.12e+01, train loss: 1.05036e-06, val loss: 1.80702e-06, min loss: 1.04655e-06\n",
      "Epoch: 1028500, elapsed: 1.11e+01, train loss: 1.30940e-06, val loss: 2.01860e-06, min loss: 1.04655e-06\n",
      "Epoch: 1028600, elapsed: 1.41e+01, train loss: 1.09240e-06, val loss: 1.85817e-06, min loss: 1.04655e-06\n",
      "Epoch: 1028700, elapsed: 1.13e+01, train loss: 1.14678e-06, val loss: 1.88716e-06, min loss: 1.04655e-06\n",
      "Epoch: 1028800, elapsed: 1.13e+01, train loss: 1.21199e-06, val loss: 1.96439e-06, min loss: 1.04655e-06\n",
      "Epoch: 1028900, elapsed: 1.13e+01, train loss: 1.74611e-06, val loss: 2.65811e-06, min loss: 1.04655e-06\n",
      "Epoch: 1029000, elapsed: 1.14e+01, train loss: 3.46105e-06, val loss: 4.62406e-06, min loss: 1.04655e-06\n",
      "Epoch: 1029100, elapsed: 1.16e+01, train loss: 2.33617e-06, val loss: 2.84368e-06, min loss: 1.04655e-06\n",
      "Epoch: 1029200, elapsed: 1.14e+01, train loss: 1.13426e-06, val loss: 1.83330e-06, min loss: 1.04655e-06\n",
      "Epoch: 1029300, elapsed: 1.14e+01, train loss: 1.04382e-06, val loss: 1.79492e-06, min loss: 1.04382e-06\n",
      "Epoch: 1029400, elapsed: 1.13e+01, train loss: 1.10888e-06, val loss: 1.81874e-06, min loss: 1.04382e-06\n",
      "Epoch: 1029500, elapsed: 1.12e+01, train loss: 1.06507e-06, val loss: 1.80055e-06, min loss: 1.04382e-06\n",
      "Epoch: 1029600, elapsed: 1.12e+01, train loss: 1.04354e-06, val loss: 1.79334e-06, min loss: 1.04354e-06\n",
      "Epoch: 1029700, elapsed: 1.14e+01, train loss: 1.22372e-06, val loss: 2.11244e-06, min loss: 1.04354e-06\n",
      "Epoch: 1029800, elapsed: 1.15e+01, train loss: 1.04265e-06, val loss: 1.79325e-06, min loss: 1.04265e-06\n",
      "Epoch: 1029900, elapsed: 1.11e+01, train loss: 1.04418e-06, val loss: 1.79760e-06, min loss: 1.04265e-06\n",
      "Epoch: 1030000, elapsed: 1.12e+01, train loss: 1.11040e-06, val loss: 1.80856e-06, min loss: 1.04265e-06\n",
      "Epoch: 1030100, elapsed: 1.34e+01, train loss: 1.25179e-06, val loss: 1.92905e-06, min loss: 1.04265e-06\n",
      "Epoch: 1030200, elapsed: 1.13e+01, train loss: 1.18844e-06, val loss: 1.84733e-06, min loss: 1.04265e-06\n",
      "Epoch: 1030300, elapsed: 1.15e+01, train loss: 1.11927e-06, val loss: 1.90485e-06, min loss: 1.04265e-06\n",
      "Epoch: 1030400, elapsed: 1.13e+01, train loss: 1.50179e-06, val loss: 2.48302e-06, min loss: 1.04265e-06\n",
      "Epoch: 1030500, elapsed: 1.43e+01, train loss: 2.20161e-06, val loss: 2.68991e-06, min loss: 1.04265e-06\n",
      "Epoch: 1030600, elapsed: 1.16e+01, train loss: 2.11937e-06, val loss: 2.71412e-06, min loss: 1.04265e-06\n",
      "Epoch: 1030700, elapsed: 1.15e+01, train loss: 1.13521e-06, val loss: 1.88038e-06, min loss: 1.04265e-06\n",
      "Epoch: 1030800, elapsed: 1.16e+01, train loss: 1.05980e-06, val loss: 1.79453e-06, min loss: 1.04265e-06\n",
      "Epoch: 1030900, elapsed: 1.14e+01, train loss: 1.04959e-06, val loss: 1.79902e-06, min loss: 1.04265e-06\n",
      "Epoch: 1031000, elapsed: 1.13e+01, train loss: 5.68021e-06, val loss: 6.85778e-06, min loss: 1.04265e-06\n",
      "Epoch: 1031100, elapsed: 1.14e+01, train loss: 1.04204e-06, val loss: 1.79342e-06, min loss: 1.04204e-06\n",
      "Epoch: 1031200, elapsed: 1.14e+01, train loss: 1.04966e-06, val loss: 1.80218e-06, min loss: 1.04204e-06\n",
      "Epoch: 1031300, elapsed: 1.14e+01, train loss: 1.06873e-06, val loss: 1.81458e-06, min loss: 1.04204e-06\n",
      "Epoch: 1031400, elapsed: 1.12e+01, train loss: 1.45742e-06, val loss: 2.09027e-06, min loss: 1.04204e-06\n",
      "Epoch: 1031500, elapsed: 1.11e+01, train loss: 1.32522e-06, val loss: 1.98553e-06, min loss: 1.04204e-06\n",
      "Epoch: 1031600, elapsed: 1.13e+01, train loss: 3.25142e-06, val loss: 3.77846e-06, min loss: 1.04204e-06\n",
      "Epoch: 1031700, elapsed: 1.12e+01, train loss: 1.06677e-06, val loss: 1.79662e-06, min loss: 1.04204e-06\n",
      "Epoch: 1031800, elapsed: 1.12e+01, train loss: 1.05479e-06, val loss: 1.81741e-06, min loss: 1.04204e-06\n",
      "Epoch: 1031900, elapsed: 1.14e+01, train loss: 1.05181e-06, val loss: 1.80286e-06, min loss: 1.04204e-06\n",
      "Epoch: 1032000, elapsed: 1.14e+01, train loss: 1.14367e-06, val loss: 1.87872e-06, min loss: 1.04204e-06\n",
      "Epoch: 1032100, elapsed: 1.14e+01, train loss: 1.56894e-06, val loss: 2.10982e-06, min loss: 1.04204e-06\n",
      "Epoch: 1032200, elapsed: 1.14e+01, train loss: 1.07898e-06, val loss: 1.85574e-06, min loss: 1.04204e-06\n",
      "Epoch: 1032300, elapsed: 1.13e+01, train loss: 1.04090e-06, val loss: 1.78780e-06, min loss: 1.04090e-06\n",
      "Epoch: 1032400, elapsed: 1.14e+01, train loss: 1.05246e-06, val loss: 1.79855e-06, min loss: 1.04090e-06\n",
      "Epoch: 1032500, elapsed: 1.43e+01, train loss: 1.42114e-06, val loss: 2.29686e-06, min loss: 1.04090e-06\n",
      "Epoch: 1032600, elapsed: 1.14e+01, train loss: 1.04115e-06, val loss: 1.79640e-06, min loss: 1.04090e-06\n",
      "Epoch: 1032700, elapsed: 1.13e+01, train loss: 1.04617e-06, val loss: 1.80603e-06, min loss: 1.04090e-06\n",
      "Epoch: 1032800, elapsed: 1.14e+01, train loss: 1.90236e-06, val loss: 2.40281e-06, min loss: 1.04090e-06\n",
      "Epoch: 1032900, elapsed: 1.14e+01, train loss: 1.03991e-06, val loss: 1.79176e-06, min loss: 1.03991e-06\n",
      "Epoch: 1033000, elapsed: 1.13e+01, train loss: 1.04846e-06, val loss: 1.79550e-06, min loss: 1.03991e-06\n",
      "Epoch: 1033100, elapsed: 1.14e+01, train loss: 1.26477e-06, val loss: 2.04601e-06, min loss: 1.03991e-06\n",
      "Epoch: 1033200, elapsed: 1.14e+01, train loss: 1.36509e-06, val loss: 2.22915e-06, min loss: 1.03991e-06\n",
      "Epoch: 1033300, elapsed: 1.13e+01, train loss: 1.29183e-06, val loss: 2.24323e-06, min loss: 1.03991e-06\n",
      "Epoch: 1033400, elapsed: 1.11e+01, train loss: 2.94385e-06, val loss: 4.23129e-06, min loss: 1.03991e-06\n",
      "Epoch: 1033500, elapsed: 1.14e+01, train loss: 1.04251e-06, val loss: 1.78862e-06, min loss: 1.03991e-06\n",
      "Epoch: 1033600, elapsed: 1.12e+01, train loss: 1.04038e-06, val loss: 1.79507e-06, min loss: 1.03991e-06\n",
      "Epoch: 1033700, elapsed: 1.13e+01, train loss: 1.05480e-06, val loss: 1.79403e-06, min loss: 1.03991e-06\n",
      "Epoch: 1033800, elapsed: 1.13e+01, train loss: 1.38105e-06, val loss: 1.87322e-06, min loss: 1.03991e-06\n",
      "Epoch: 1033900, elapsed: 1.16e+01, train loss: 1.10857e-06, val loss: 1.85455e-06, min loss: 1.03991e-06\n",
      "Epoch: 1034000, elapsed: 1.15e+01, train loss: 1.03975e-06, val loss: 1.79546e-06, min loss: 1.03975e-06\n",
      "Epoch: 1034100, elapsed: 1.13e+01, train loss: 1.14030e-06, val loss: 1.82982e-06, min loss: 1.03975e-06\n",
      "Epoch: 1034200, elapsed: 1.15e+01, train loss: 1.03958e-06, val loss: 1.79069e-06, min loss: 1.03958e-06\n",
      "Epoch: 1034300, elapsed: 1.12e+01, train loss: 1.04470e-06, val loss: 1.78937e-06, min loss: 1.03958e-06\n",
      "Epoch: 1034400, elapsed: 1.39e+01, train loss: 2.47156e-06, val loss: 2.93795e-06, min loss: 1.03958e-06\n",
      "Epoch: 1034500, elapsed: 1.15e+01, train loss: 1.11079e-06, val loss: 1.92152e-06, min loss: 1.03958e-06\n",
      "Epoch: 1034600, elapsed: 1.15e+01, train loss: 1.03846e-06, val loss: 1.79460e-06, min loss: 1.03846e-06\n",
      "Epoch: 1034700, elapsed: 1.14e+01, train loss: 1.60483e-06, val loss: 2.45669e-06, min loss: 1.03846e-06\n",
      "Epoch: 1034800, elapsed: 1.15e+01, train loss: 1.14567e-06, val loss: 1.93138e-06, min loss: 1.03846e-06\n",
      "Epoch: 1034900, elapsed: 1.14e+01, train loss: 1.04297e-06, val loss: 1.81263e-06, min loss: 1.03846e-06\n",
      "Epoch: 1035000, elapsed: 1.14e+01, train loss: 2.97270e-06, val loss: 3.66888e-06, min loss: 1.03846e-06\n",
      "Epoch: 1035100, elapsed: 1.34e+01, train loss: 1.12577e-06, val loss: 1.97389e-06, min loss: 1.03846e-06\n",
      "Epoch: 1035200, elapsed: 1.14e+01, train loss: 1.49556e-06, val loss: 2.22545e-06, min loss: 1.03846e-06\n",
      "Epoch: 1035300, elapsed: 1.12e+01, train loss: 5.72033e-06, val loss: 5.30639e-06, min loss: 1.03846e-06\n",
      "Epoch: 1035400, elapsed: 1.12e+01, train loss: 1.10500e-06, val loss: 1.83600e-06, min loss: 1.03846e-06\n",
      "Epoch: 1035500, elapsed: 1.14e+01, train loss: 1.05264e-06, val loss: 1.79616e-06, min loss: 1.03846e-06\n",
      "Epoch: 1035600, elapsed: 1.14e+01, train loss: 1.08852e-06, val loss: 1.80985e-06, min loss: 1.03846e-06\n",
      "Epoch: 1035700, elapsed: 1.13e+01, train loss: 1.38064e-06, val loss: 1.98014e-06, min loss: 1.03846e-06\n",
      "Epoch: 1035800, elapsed: 1.13e+01, train loss: 1.06088e-06, val loss: 1.79644e-06, min loss: 1.03846e-06\n",
      "Epoch: 1035900, elapsed: 1.13e+01, train loss: 2.87405e-06, val loss: 3.20947e-06, min loss: 1.03846e-06\n",
      "Epoch: 1036000, elapsed: 1.12e+01, train loss: 1.61364e-06, val loss: 2.46036e-06, min loss: 1.03846e-06\n",
      "Epoch: 1036100, elapsed: 1.12e+01, train loss: 1.17697e-06, val loss: 1.84910e-06, min loss: 1.03846e-06\n",
      "Epoch: 1036200, elapsed: 1.14e+01, train loss: 1.35601e-06, val loss: 2.37861e-06, min loss: 1.03846e-06\n",
      "Epoch: 1036300, elapsed: 1.44e+01, train loss: 1.83345e-06, val loss: 1.91195e-06, min loss: 1.03846e-06\n",
      "Epoch: 1036400, elapsed: 1.16e+01, train loss: 1.42038e-06, val loss: 2.37314e-06, min loss: 1.03846e-06\n",
      "Epoch: 1036500, elapsed: 1.14e+01, train loss: 1.14836e-06, val loss: 1.95250e-06, min loss: 1.03846e-06\n",
      "Epoch: 1036600, elapsed: 1.16e+01, train loss: 2.17923e-06, val loss: 2.80606e-06, min loss: 1.03846e-06\n",
      "Epoch: 1036700, elapsed: 1.14e+01, train loss: 1.04714e-06, val loss: 1.84159e-06, min loss: 1.03846e-06\n",
      "Epoch: 1036800, elapsed: 1.15e+01, train loss: 1.06738e-06, val loss: 1.82481e-06, min loss: 1.03846e-06\n",
      "Epoch: 1036900, elapsed: 1.14e+01, train loss: 2.27753e-06, val loss: 2.25980e-06, min loss: 1.03846e-06\n",
      "Epoch: 1037000, elapsed: 1.15e+01, train loss: 4.45848e-06, val loss: 5.07799e-06, min loss: 1.03846e-06\n",
      "Epoch: 1037100, elapsed: 1.13e+01, train loss: 3.30046e-06, val loss: 4.72158e-06, min loss: 1.03846e-06\n",
      "Epoch: 1037200, elapsed: 1.10e+01, train loss: 1.12900e-06, val loss: 1.85711e-06, min loss: 1.03846e-06\n",
      "Epoch: 1037300, elapsed: 1.15e+01, train loss: 1.04062e-06, val loss: 1.79027e-06, min loss: 1.03846e-06\n",
      "Epoch: 1037400, elapsed: 1.14e+01, train loss: 1.12485e-06, val loss: 1.80988e-06, min loss: 1.03846e-06\n",
      "Epoch: 1037500, elapsed: 1.13e+01, train loss: 1.13290e-06, val loss: 1.83730e-06, min loss: 1.03846e-06\n",
      "Epoch: 1037600, elapsed: 1.13e+01, train loss: 1.54241e-06, val loss: 2.74662e-06, min loss: 1.03846e-06\n",
      "Epoch: 1037700, elapsed: 1.13e+01, train loss: 1.03643e-06, val loss: 1.79268e-06, min loss: 1.03643e-06\n",
      "Epoch: 1037800, elapsed: 1.11e+01, train loss: 1.03695e-06, val loss: 1.78706e-06, min loss: 1.03643e-06\n",
      "Epoch: 1037900, elapsed: 1.13e+01, train loss: 1.04709e-06, val loss: 1.80462e-06, min loss: 1.03643e-06\n",
      "Epoch: 1038000, elapsed: 1.13e+01, train loss: 1.15354e-06, val loss: 1.88504e-06, min loss: 1.03643e-06\n",
      "Epoch: 1038100, elapsed: 1.12e+01, train loss: 1.47920e-06, val loss: 2.30766e-06, min loss: 1.03643e-06\n",
      "Epoch: 1038200, elapsed: 1.11e+01, train loss: 1.03675e-06, val loss: 1.79363e-06, min loss: 1.03643e-06\n",
      "Epoch: 1038300, elapsed: 1.44e+01, train loss: 1.03701e-06, val loss: 1.78819e-06, min loss: 1.03643e-06\n",
      "Epoch: 1038400, elapsed: 1.16e+01, train loss: 1.03561e-06, val loss: 1.79399e-06, min loss: 1.03561e-06\n",
      "Epoch: 1038500, elapsed: 1.14e+01, train loss: 1.77823e-06, val loss: 2.35580e-06, min loss: 1.03561e-06\n",
      "Epoch: 1038600, elapsed: 1.14e+01, train loss: 1.03690e-06, val loss: 1.79506e-06, min loss: 1.03561e-06\n",
      "Epoch: 1038700, elapsed: 1.14e+01, train loss: 1.03674e-06, val loss: 1.79122e-06, min loss: 1.03561e-06\n",
      "Epoch: 1038800, elapsed: 1.14e+01, train loss: 1.05881e-06, val loss: 1.78533e-06, min loss: 1.03561e-06\n",
      "Epoch: 1038900, elapsed: 1.14e+01, train loss: 1.03503e-06, val loss: 1.78869e-06, min loss: 1.03503e-06\n",
      "Epoch: 1039000, elapsed: 1.14e+01, train loss: 1.03637e-06, val loss: 1.79153e-06, min loss: 1.03503e-06\n",
      "Epoch: 1039100, elapsed: 1.13e+01, train loss: 1.34901e-06, val loss: 2.15374e-06, min loss: 1.03503e-06\n",
      "Epoch: 1039200, elapsed: 1.12e+01, train loss: 1.03489e-06, val loss: 1.78703e-06, min loss: 1.03489e-06\n",
      "Epoch: 1039300, elapsed: 1.13e+01, train loss: 1.03625e-06, val loss: 1.78904e-06, min loss: 1.03489e-06\n",
      "Epoch: 1039400, elapsed: 1.12e+01, train loss: 1.91810e-06, val loss: 2.49983e-06, min loss: 1.03489e-06\n",
      "Epoch: 1039500, elapsed: 1.13e+01, train loss: 2.33432e-06, val loss: 3.09677e-06, min loss: 1.03489e-06\n",
      "Epoch: 1039600, elapsed: 1.11e+01, train loss: 1.69914e-06, val loss: 2.26176e-06, min loss: 1.03489e-06\n",
      "Epoch: 1039700, elapsed: 1.13e+01, train loss: 1.04641e-06, val loss: 1.82586e-06, min loss: 1.03489e-06\n",
      "Epoch: 1039800, elapsed: 1.12e+01, train loss: 1.04220e-06, val loss: 1.80459e-06, min loss: 1.03489e-06\n",
      "Epoch: 1039900, elapsed: 1.13e+01, train loss: 1.03978e-06, val loss: 1.80394e-06, min loss: 1.03489e-06\n",
      "Epoch: 1040000, elapsed: 1.13e+01, train loss: 1.07928e-06, val loss: 1.86921e-06, min loss: 1.03489e-06\n",
      "Epoch: 1040100, elapsed: 1.35e+01, train loss: 2.42827e-06, val loss: 2.37998e-06, min loss: 1.03489e-06\n",
      "Epoch: 1040200, elapsed: 1.45e+01, train loss: 1.04775e-06, val loss: 1.81664e-06, min loss: 1.03489e-06\n",
      "Epoch: 1040300, elapsed: 1.15e+01, train loss: 1.03440e-06, val loss: 1.78852e-06, min loss: 1.03440e-06\n",
      "Epoch: 1040400, elapsed: 1.17e+01, train loss: 1.12589e-06, val loss: 1.85622e-06, min loss: 1.03440e-06\n",
      "Epoch: 1040500, elapsed: 1.13e+01, train loss: 2.90345e-06, val loss: 4.02833e-06, min loss: 1.03440e-06\n",
      "Epoch: 1040600, elapsed: 1.14e+01, train loss: 1.03398e-06, val loss: 1.79123e-06, min loss: 1.03398e-06\n",
      "Epoch: 1040700, elapsed: 1.12e+01, train loss: 1.03621e-06, val loss: 1.80022e-06, min loss: 1.03398e-06\n",
      "Epoch: 1040800, elapsed: 1.12e+01, train loss: 3.81058e-06, val loss: 2.96821e-06, min loss: 1.03398e-06\n",
      "Epoch: 1040900, elapsed: 1.12e+01, train loss: 1.03808e-06, val loss: 1.79988e-06, min loss: 1.03398e-06\n",
      "Epoch: 1041000, elapsed: 1.12e+01, train loss: 1.03534e-06, val loss: 1.78391e-06, min loss: 1.03398e-06\n",
      "Epoch: 1041100, elapsed: 1.12e+01, train loss: 1.16711e-06, val loss: 1.94259e-06, min loss: 1.03398e-06\n",
      "Epoch: 1041200, elapsed: 1.10e+01, train loss: 1.05249e-06, val loss: 1.82525e-06, min loss: 1.03398e-06\n",
      "Epoch: 1041300, elapsed: 1.13e+01, train loss: 1.11792e-06, val loss: 1.81239e-06, min loss: 1.03398e-06\n",
      "Epoch: 1041400, elapsed: 1.12e+01, train loss: 1.15153e-06, val loss: 2.09605e-06, min loss: 1.03398e-06\n",
      "Epoch: 1041500, elapsed: 1.12e+01, train loss: 1.12890e-06, val loss: 1.88539e-06, min loss: 1.03398e-06\n",
      "Epoch: 1041600, elapsed: 1.12e+01, train loss: 1.03312e-06, val loss: 1.78926e-06, min loss: 1.03312e-06\n",
      "Epoch: 1041700, elapsed: 1.12e+01, train loss: 1.19910e-06, val loss: 1.87307e-06, min loss: 1.03312e-06\n",
      "Epoch: 1041800, elapsed: 1.11e+01, train loss: 1.19306e-06, val loss: 1.99919e-06, min loss: 1.03312e-06\n",
      "Epoch: 1041900, elapsed: 1.13e+01, train loss: 1.03938e-06, val loss: 1.81016e-06, min loss: 1.03312e-06\n",
      "Epoch: 1042000, elapsed: 1.15e+01, train loss: 1.09499e-06, val loss: 1.80231e-06, min loss: 1.03312e-06\n",
      "Epoch: 1042100, elapsed: 1.45e+01, train loss: 1.30690e-06, val loss: 1.99179e-06, min loss: 1.03312e-06\n",
      "Epoch: 1042200, elapsed: 1.15e+01, train loss: 2.73616e-06, val loss: 3.18952e-06, min loss: 1.03312e-06\n",
      "Epoch: 1042300, elapsed: 1.15e+01, train loss: 1.06105e-06, val loss: 1.82979e-06, min loss: 1.03312e-06\n",
      "Epoch: 1042400, elapsed: 1.14e+01, train loss: 1.03878e-06, val loss: 1.80623e-06, min loss: 1.03312e-06\n",
      "Epoch: 1042500, elapsed: 1.14e+01, train loss: 1.03369e-06, val loss: 1.79543e-06, min loss: 1.03312e-06\n",
      "Epoch: 1042600, elapsed: 1.13e+01, train loss: 1.06650e-06, val loss: 1.78498e-06, min loss: 1.03312e-06\n",
      "Epoch: 1042700, elapsed: 1.14e+01, train loss: 5.34289e-06, val loss: 5.16146e-06, min loss: 1.03312e-06\n",
      "Epoch: 1042800, elapsed: 1.13e+01, train loss: 1.07403e-06, val loss: 1.91362e-06, min loss: 1.03312e-06\n",
      "Epoch: 1042900, elapsed: 1.14e+01, train loss: 1.04474e-06, val loss: 1.81402e-06, min loss: 1.03312e-06\n",
      "Epoch: 1043000, elapsed: 1.12e+01, train loss: 1.04435e-06, val loss: 1.79532e-06, min loss: 1.03312e-06\n",
      "Epoch: 1043100, elapsed: 1.13e+01, train loss: 1.04168e-06, val loss: 1.80339e-06, min loss: 1.03312e-06\n",
      "Epoch: 1043200, elapsed: 1.12e+01, train loss: 1.42424e-06, val loss: 2.10901e-06, min loss: 1.03312e-06\n",
      "Epoch: 1043300, elapsed: 1.13e+01, train loss: 2.41162e-06, val loss: 3.42463e-06, min loss: 1.03312e-06\n",
      "Epoch: 1043400, elapsed: 1.13e+01, train loss: 1.35747e-06, val loss: 2.14528e-06, min loss: 1.03312e-06\n",
      "Epoch: 1043500, elapsed: 1.12e+01, train loss: 1.15467e-06, val loss: 1.87931e-06, min loss: 1.03312e-06\n",
      "Epoch: 1043600, elapsed: 1.13e+01, train loss: 1.11577e-06, val loss: 1.86809e-06, min loss: 1.03312e-06\n",
      "Epoch: 1043700, elapsed: 1.14e+01, train loss: 1.23386e-06, val loss: 1.94393e-06, min loss: 1.03312e-06\n",
      "Epoch: 1043800, elapsed: 1.14e+01, train loss: 1.56678e-06, val loss: 2.18971e-06, min loss: 1.03312e-06\n",
      "Epoch: 1043900, elapsed: 1.13e+01, train loss: 1.41894e-06, val loss: 1.96289e-06, min loss: 1.03312e-06\n",
      "Epoch: 1044000, elapsed: 1.12e+01, train loss: 1.03475e-06, val loss: 1.79473e-06, min loss: 1.03312e-06\n",
      "Epoch: 1044100, elapsed: 1.44e+01, train loss: 1.03497e-06, val loss: 1.79639e-06, min loss: 1.03312e-06\n",
      "Epoch: 1044200, elapsed: 1.14e+01, train loss: 1.63412e-06, val loss: 2.33511e-06, min loss: 1.03312e-06\n",
      "Epoch: 1044300, elapsed: 1.13e+01, train loss: 1.47641e-06, val loss: 2.42024e-06, min loss: 1.03312e-06\n",
      "Epoch: 1044400, elapsed: 1.14e+01, train loss: 1.03205e-06, val loss: 1.78936e-06, min loss: 1.03205e-06\n",
      "Epoch: 1044500, elapsed: 1.15e+01, train loss: 1.03226e-06, val loss: 1.78444e-06, min loss: 1.03205e-06\n",
      "Epoch: 1044600, elapsed: 1.14e+01, train loss: 1.04031e-06, val loss: 1.79665e-06, min loss: 1.03205e-06\n",
      "Epoch: 1044700, elapsed: 1.13e+01, train loss: 1.06493e-06, val loss: 1.84071e-06, min loss: 1.03205e-06\n",
      "Epoch: 1044800, elapsed: 1.12e+01, train loss: 1.07234e-06, val loss: 1.80021e-06, min loss: 1.03205e-06\n",
      "Epoch: 1044900, elapsed: 1.13e+01, train loss: 1.03243e-06, val loss: 1.78356e-06, min loss: 1.03205e-06\n",
      "Epoch: 1045000, elapsed: 1.12e+01, train loss: 1.11485e-06, val loss: 1.82882e-06, min loss: 1.03205e-06\n",
      "Epoch: 1045100, elapsed: 1.33e+01, train loss: 1.09033e-06, val loss: 1.85211e-06, min loss: 1.03205e-06\n",
      "Epoch: 1045200, elapsed: 1.13e+01, train loss: 1.85624e-06, val loss: 3.07309e-06, min loss: 1.03205e-06\n",
      "Epoch: 1045300, elapsed: 1.12e+01, train loss: 1.03031e-06, val loss: 1.78228e-06, min loss: 1.03031e-06\n",
      "Epoch: 1045400, elapsed: 1.12e+01, train loss: 1.08626e-06, val loss: 1.84320e-06, min loss: 1.03031e-06\n",
      "Epoch: 1045500, elapsed: 1.12e+01, train loss: 1.73459e-06, val loss: 2.69928e-06, min loss: 1.03031e-06\n",
      "Epoch: 1045600, elapsed: 1.13e+01, train loss: 1.02989e-06, val loss: 1.78726e-06, min loss: 1.02989e-06\n",
      "Epoch: 1045700, elapsed: 1.14e+01, train loss: 1.06925e-06, val loss: 1.84043e-06, min loss: 1.02989e-06\n",
      "Epoch: 1045800, elapsed: 1.14e+01, train loss: 1.11086e-06, val loss: 1.82022e-06, min loss: 1.02989e-06\n",
      "Epoch: 1045900, elapsed: 1.13e+01, train loss: 2.83241e-06, val loss: 2.95138e-06, min loss: 1.02989e-06\n",
      "Epoch: 1046000, elapsed: 1.46e+01, train loss: 1.17433e-06, val loss: 1.83950e-06, min loss: 1.02989e-06\n",
      "Epoch: 1046100, elapsed: 1.17e+01, train loss: 1.19166e-06, val loss: 1.95955e-06, min loss: 1.02989e-06\n",
      "Epoch: 1046200, elapsed: 1.15e+01, train loss: 1.09837e-06, val loss: 1.90600e-06, min loss: 1.02989e-06\n",
      "Epoch: 1046300, elapsed: 1.14e+01, train loss: 1.03489e-06, val loss: 1.80693e-06, min loss: 1.02989e-06\n",
      "Epoch: 1046400, elapsed: 1.13e+01, train loss: 1.03251e-06, val loss: 1.78907e-06, min loss: 1.02989e-06\n",
      "Epoch: 1046500, elapsed: 1.13e+01, train loss: 1.31758e-06, val loss: 2.01818e-06, min loss: 1.02989e-06\n",
      "Epoch: 1046600, elapsed: 1.13e+01, train loss: 1.02883e-06, val loss: 1.78591e-06, min loss: 1.02883e-06\n",
      "Epoch: 1046700, elapsed: 1.13e+01, train loss: 1.03245e-06, val loss: 1.80165e-06, min loss: 1.02883e-06\n",
      "Epoch: 1046800, elapsed: 1.12e+01, train loss: 1.09902e-06, val loss: 1.86678e-06, min loss: 1.02883e-06\n",
      "Epoch: 1046900, elapsed: 1.11e+01, train loss: 1.02902e-06, val loss: 1.78797e-06, min loss: 1.02883e-06\n",
      "Epoch: 1047000, elapsed: 1.15e+01, train loss: 1.69584e-06, val loss: 2.60431e-06, min loss: 1.02883e-06\n",
      "Epoch: 1047100, elapsed: 1.15e+01, train loss: 1.14268e-06, val loss: 1.93823e-06, min loss: 1.02883e-06\n",
      "Epoch: 1047200, elapsed: 1.13e+01, train loss: 1.03121e-06, val loss: 1.78283e-06, min loss: 1.02883e-06\n",
      "Epoch: 1047300, elapsed: 1.13e+01, train loss: 1.02964e-06, val loss: 1.77819e-06, min loss: 1.02883e-06\n",
      "Epoch: 1047400, elapsed: 1.12e+01, train loss: 1.03143e-06, val loss: 1.77993e-06, min loss: 1.02883e-06\n",
      "Epoch: 1047500, elapsed: 1.12e+01, train loss: 1.03244e-06, val loss: 1.79515e-06, min loss: 1.02883e-06\n",
      "Epoch: 1047600, elapsed: 1.12e+01, train loss: 1.03121e-06, val loss: 1.79178e-06, min loss: 1.02883e-06\n",
      "Epoch: 1047700, elapsed: 1.13e+01, train loss: 1.84260e-06, val loss: 2.37704e-06, min loss: 1.02883e-06\n",
      "Epoch: 1047800, elapsed: 1.11e+01, train loss: 3.75612e-06, val loss: 3.53003e-06, min loss: 1.02883e-06\n",
      "Epoch: 1047900, elapsed: 1.43e+01, train loss: 1.79221e-06, val loss: 2.85275e-06, min loss: 1.02883e-06\n",
      "Epoch: 1048000, elapsed: 1.15e+01, train loss: 1.02824e-06, val loss: 1.78293e-06, min loss: 1.02824e-06\n",
      "Epoch: 1048100, elapsed: 1.14e+01, train loss: 1.14938e-06, val loss: 1.88557e-06, min loss: 1.02824e-06\n",
      "Epoch: 1048200, elapsed: 1.14e+01, train loss: 1.02776e-06, val loss: 1.78712e-06, min loss: 1.02776e-06\n",
      "Epoch: 1048300, elapsed: 1.14e+01, train loss: 1.03002e-06, val loss: 1.78129e-06, min loss: 1.02776e-06\n",
      "Epoch: 1048400, elapsed: 1.15e+01, train loss: 1.03143e-06, val loss: 1.79241e-06, min loss: 1.02776e-06\n",
      "Epoch: 1048500, elapsed: 1.12e+01, train loss: 1.02832e-06, val loss: 1.78474e-06, min loss: 1.02776e-06\n",
      "Epoch: 1048600, elapsed: 1.12e+01, train loss: 2.85713e-06, val loss: 4.16119e-06, min loss: 1.02776e-06\n",
      "Epoch: 1048700, elapsed: 1.13e+01, train loss: 1.03140e-06, val loss: 1.79543e-06, min loss: 1.02776e-06\n",
      "Epoch: 1048800, elapsed: 1.13e+01, train loss: 1.02788e-06, val loss: 1.78222e-06, min loss: 1.02776e-06\n",
      "Epoch: 1048900, elapsed: 1.11e+01, train loss: 1.06427e-06, val loss: 1.81842e-06, min loss: 1.02776e-06\n",
      "Epoch: 1049000, elapsed: 1.15e+01, train loss: 2.72281e-06, val loss: 3.80709e-06, min loss: 1.02776e-06\n",
      "Epoch: 1049100, elapsed: 1.14e+01, train loss: 1.02828e-06, val loss: 1.78234e-06, min loss: 1.02776e-06\n",
      "Epoch: 1049200, elapsed: 1.13e+01, train loss: 1.02952e-06, val loss: 1.78135e-06, min loss: 1.02776e-06\n",
      "Epoch: 1049300, elapsed: 1.13e+01, train loss: 1.03709e-06, val loss: 1.80048e-06, min loss: 1.02776e-06\n",
      "Epoch: 1049400, elapsed: 1.12e+01, train loss: 1.24096e-06, val loss: 1.86996e-06, min loss: 1.02776e-06\n",
      "Epoch: 1049500, elapsed: 1.14e+01, train loss: 5.39828e-06, val loss: 6.40724e-06, min loss: 1.02776e-06\n",
      "Epoch: 1049600, elapsed: 1.15e+01, train loss: 1.21980e-06, val loss: 2.08460e-06, min loss: 1.02776e-06\n",
      "Epoch: 1049700, elapsed: 1.14e+01, train loss: 3.68875e-06, val loss: 4.36036e-06, min loss: 1.02776e-06\n",
      "Epoch: 1049800, elapsed: 1.14e+01, train loss: 2.09193e-06, val loss: 2.53578e-06, min loss: 1.02776e-06\n",
      "Epoch: 1049900, elapsed: 1.44e+01, train loss: 1.06082e-06, val loss: 1.79203e-06, min loss: 1.02776e-06\n",
      "Epoch: 1050000, elapsed: 1.14e+01, train loss: 1.35401e-06, val loss: 1.91821e-06, min loss: 1.02776e-06\n",
      "Epoch: 1050100, elapsed: 1.35e+01, train loss: 1.12040e-06, val loss: 1.83709e-06, min loss: 1.02776e-06\n",
      "Epoch: 1050200, elapsed: 1.14e+01, train loss: 1.75550e-06, val loss: 2.49555e-06, min loss: 1.02776e-06\n",
      "Epoch: 1050300, elapsed: 1.14e+01, train loss: 1.30990e-06, val loss: 1.90031e-06, min loss: 1.02776e-06\n",
      "Epoch: 1050400, elapsed: 1.15e+01, train loss: 1.66859e-06, val loss: 2.08820e-06, min loss: 1.02776e-06\n",
      "Epoch: 1050500, elapsed: 1.14e+01, train loss: 2.67944e-06, val loss: 2.51053e-06, min loss: 1.02776e-06\n",
      "Epoch: 1050600, elapsed: 1.14e+01, train loss: 1.03028e-06, val loss: 1.77495e-06, min loss: 1.02776e-06\n",
      "Epoch: 1050700, elapsed: 1.13e+01, train loss: 1.02637e-06, val loss: 1.78298e-06, min loss: 1.02637e-06\n",
      "Epoch: 1050800, elapsed: 1.14e+01, train loss: 1.03879e-06, val loss: 1.79605e-06, min loss: 1.02637e-06\n",
      "Epoch: 1050900, elapsed: 1.14e+01, train loss: 1.10490e-06, val loss: 2.06855e-06, min loss: 1.02637e-06\n",
      "Epoch: 1051000, elapsed: 1.13e+01, train loss: 1.08598e-06, val loss: 1.83873e-06, min loss: 1.02637e-06\n",
      "Epoch: 1051100, elapsed: 1.13e+01, train loss: 1.30416e-06, val loss: 2.13664e-06, min loss: 1.02637e-06\n",
      "Epoch: 1051200, elapsed: 1.14e+01, train loss: 1.74977e-06, val loss: 1.92827e-06, min loss: 1.02637e-06\n",
      "Epoch: 1051300, elapsed: 1.15e+01, train loss: 1.53078e-06, val loss: 2.43619e-06, min loss: 1.02637e-06\n",
      "Epoch: 1051400, elapsed: 1.14e+01, train loss: 1.38575e-06, val loss: 2.14210e-06, min loss: 1.02637e-06\n",
      "Epoch: 1051500, elapsed: 1.13e+01, train loss: 1.09877e-06, val loss: 1.87519e-06, min loss: 1.02637e-06\n",
      "Epoch: 1051600, elapsed: 1.14e+01, train loss: 1.29352e-06, val loss: 2.37748e-06, min loss: 1.02637e-06\n",
      "Epoch: 1051700, elapsed: 1.14e+01, train loss: 3.47090e-06, val loss: 3.51430e-06, min loss: 1.02637e-06\n",
      "Epoch: 1051800, elapsed: 1.41e+01, train loss: 5.41340e-06, val loss: 5.56011e-06, min loss: 1.02637e-06\n",
      "Epoch: 1051900, elapsed: 1.15e+01, train loss: 1.11059e-06, val loss: 1.85894e-06, min loss: 1.02637e-06\n",
      "Epoch: 1052000, elapsed: 1.16e+01, train loss: 1.02919e-06, val loss: 1.78337e-06, min loss: 1.02637e-06\n",
      "Epoch: 1052100, elapsed: 1.18e+01, train loss: 1.03695e-06, val loss: 1.79830e-06, min loss: 1.02637e-06\n",
      "Epoch: 1052200, elapsed: 1.15e+01, train loss: 1.03620e-06, val loss: 1.78730e-06, min loss: 1.02637e-06\n",
      "Epoch: 1052300, elapsed: 1.14e+01, train loss: 1.05095e-06, val loss: 1.81663e-06, min loss: 1.02637e-06\n",
      "Epoch: 1052400, elapsed: 1.14e+01, train loss: 1.71689e-06, val loss: 2.55535e-06, min loss: 1.02637e-06\n",
      "Epoch: 1052500, elapsed: 1.12e+01, train loss: 1.13838e-06, val loss: 1.89202e-06, min loss: 1.02637e-06\n",
      "Epoch: 1052600, elapsed: 1.13e+01, train loss: 2.21519e-06, val loss: 3.19246e-06, min loss: 1.02637e-06\n",
      "Epoch: 1052700, elapsed: 1.15e+01, train loss: 1.11547e-06, val loss: 1.90437e-06, min loss: 1.02637e-06\n",
      "Epoch: 1052800, elapsed: 1.14e+01, train loss: 1.15419e-06, val loss: 1.92347e-06, min loss: 1.02637e-06\n",
      "Epoch: 1052900, elapsed: 1.13e+01, train loss: 1.36658e-06, val loss: 2.24201e-06, min loss: 1.02637e-06\n",
      "Epoch: 1053000, elapsed: 1.11e+01, train loss: 1.25113e-06, val loss: 1.93867e-06, min loss: 1.02637e-06\n",
      "Epoch: 1053100, elapsed: 1.13e+01, train loss: 1.13960e-06, val loss: 2.25235e-06, min loss: 1.02637e-06\n",
      "Epoch: 1053200, elapsed: 1.14e+01, train loss: 2.54433e-06, val loss: 2.65823e-06, min loss: 1.02637e-06\n",
      "Epoch: 1053300, elapsed: 1.13e+01, train loss: 5.12189e-06, val loss: 5.78207e-06, min loss: 1.02637e-06\n",
      "Epoch: 1053400, elapsed: 1.15e+01, train loss: 1.09795e-06, val loss: 1.84933e-06, min loss: 1.02637e-06\n",
      "Epoch: 1053500, elapsed: 1.13e+01, train loss: 1.09959e-06, val loss: 1.86983e-06, min loss: 1.02637e-06\n",
      "Epoch: 1053600, elapsed: 1.11e+01, train loss: 1.23738e-06, val loss: 2.08935e-06, min loss: 1.02637e-06\n",
      "Epoch: 1053700, elapsed: 1.12e+01, train loss: 1.63521e-06, val loss: 2.51298e-06, min loss: 1.02637e-06\n",
      "Epoch: 1053800, elapsed: 1.46e+01, train loss: 1.16307e-06, val loss: 1.85298e-06, min loss: 1.02637e-06\n",
      "Epoch: 1053900, elapsed: 1.15e+01, train loss: 1.05158e-06, val loss: 1.79353e-06, min loss: 1.02637e-06\n",
      "Epoch: 1054000, elapsed: 1.14e+01, train loss: 1.09256e-06, val loss: 1.79622e-06, min loss: 1.02637e-06\n",
      "Epoch: 1054100, elapsed: 1.14e+01, train loss: 2.61615e-06, val loss: 2.79563e-06, min loss: 1.02637e-06\n",
      "Epoch: 1054200, elapsed: 1.13e+01, train loss: 1.19138e-06, val loss: 1.99918e-06, min loss: 1.02637e-06\n",
      "Epoch: 1054300, elapsed: 1.13e+01, train loss: 1.02509e-06, val loss: 1.78916e-06, min loss: 1.02509e-06\n",
      "Epoch: 1054400, elapsed: 1.14e+01, train loss: 1.02747e-06, val loss: 1.77677e-06, min loss: 1.02509e-06\n",
      "Epoch: 1054500, elapsed: 1.13e+01, train loss: 1.04117e-06, val loss: 1.82624e-06, min loss: 1.02509e-06\n",
      "Epoch: 1054600, elapsed: 1.13e+01, train loss: 1.14973e-06, val loss: 1.91777e-06, min loss: 1.02509e-06\n",
      "Epoch: 1054700, elapsed: 1.15e+01, train loss: 1.10532e-06, val loss: 2.24448e-06, min loss: 1.02509e-06\n",
      "Epoch: 1054800, elapsed: 1.15e+01, train loss: 1.04323e-06, val loss: 1.84367e-06, min loss: 1.02509e-06\n",
      "Epoch: 1054900, elapsed: 1.15e+01, train loss: 2.40864e-06, val loss: 3.04925e-06, min loss: 1.02509e-06\n",
      "Epoch: 1055000, elapsed: 1.13e+01, train loss: 1.05569e-06, val loss: 1.81208e-06, min loss: 1.02509e-06\n",
      "Epoch: 1055100, elapsed: 1.33e+01, train loss: 1.13947e-06, val loss: 1.88750e-06, min loss: 1.02509e-06\n",
      "Epoch: 1055200, elapsed: 1.12e+01, train loss: 1.04866e-06, val loss: 1.82860e-06, min loss: 1.02509e-06\n",
      "Epoch: 1055300, elapsed: 1.14e+01, train loss: 5.34072e-06, val loss: 5.36365e-06, min loss: 1.02509e-06\n",
      "Epoch: 1055400, elapsed: 1.11e+01, train loss: 1.03567e-06, val loss: 1.81009e-06, min loss: 1.02509e-06\n",
      "Epoch: 1055500, elapsed: 1.12e+01, train loss: 1.02460e-06, val loss: 1.77994e-06, min loss: 1.02460e-06\n",
      "Epoch: 1055600, elapsed: 1.13e+01, train loss: 1.02801e-06, val loss: 1.79103e-06, min loss: 1.02460e-06\n",
      "Epoch: 1055700, elapsed: 1.43e+01, train loss: 1.81130e-06, val loss: 2.67841e-06, min loss: 1.02460e-06\n",
      "Epoch: 1055800, elapsed: 1.18e+01, train loss: 1.24171e-06, val loss: 2.06000e-06, min loss: 1.02460e-06\n",
      "Epoch: 1055900, elapsed: 1.16e+01, train loss: 1.07314e-06, val loss: 1.81802e-06, min loss: 1.02460e-06\n",
      "Epoch: 1056000, elapsed: 1.16e+01, train loss: 1.04063e-06, val loss: 1.78855e-06, min loss: 1.02460e-06\n",
      "Epoch: 1056100, elapsed: 1.15e+01, train loss: 2.32021e-06, val loss: 2.49759e-06, min loss: 1.02460e-06\n",
      "Epoch: 1056200, elapsed: 1.13e+01, train loss: 1.56502e-06, val loss: 2.57724e-06, min loss: 1.02460e-06\n",
      "Epoch: 1056300, elapsed: 1.14e+01, train loss: 1.04156e-06, val loss: 1.79126e-06, min loss: 1.02460e-06\n",
      "Epoch: 1056400, elapsed: 1.13e+01, train loss: 1.02485e-06, val loss: 1.78796e-06, min loss: 1.02460e-06\n",
      "Epoch: 1056500, elapsed: 1.13e+01, train loss: 1.02937e-06, val loss: 1.80314e-06, min loss: 1.02460e-06\n",
      "Epoch: 1056600, elapsed: 1.15e+01, train loss: 1.09498e-06, val loss: 1.81436e-06, min loss: 1.02460e-06\n",
      "Epoch: 1056700, elapsed: 1.14e+01, train loss: 1.21619e-06, val loss: 1.94723e-06, min loss: 1.02460e-06\n",
      "Epoch: 1056800, elapsed: 1.12e+01, train loss: 1.03941e-06, val loss: 1.83067e-06, min loss: 1.02460e-06\n",
      "Epoch: 1056900, elapsed: 1.14e+01, train loss: 1.04053e-06, val loss: 1.81359e-06, min loss: 1.02460e-06\n",
      "Epoch: 1057000, elapsed: 1.12e+01, train loss: 1.02705e-06, val loss: 1.78489e-06, min loss: 1.02460e-06\n",
      "Epoch: 1057100, elapsed: 1.13e+01, train loss: 1.04170e-06, val loss: 1.78482e-06, min loss: 1.02460e-06\n",
      "Epoch: 1057200, elapsed: 1.11e+01, train loss: 1.02788e-06, val loss: 1.80257e-06, min loss: 1.02460e-06\n",
      "Epoch: 1057300, elapsed: 1.14e+01, train loss: 1.03748e-06, val loss: 1.78138e-06, min loss: 1.02460e-06\n",
      "Epoch: 1057400, elapsed: 1.14e+01, train loss: 1.09253e-06, val loss: 1.95291e-06, min loss: 1.02460e-06\n",
      "Epoch: 1057500, elapsed: 1.15e+01, train loss: 1.14175e-06, val loss: 2.05497e-06, min loss: 1.02460e-06\n",
      "Epoch: 1057600, elapsed: 1.13e+01, train loss: 1.37670e-06, val loss: 2.25642e-06, min loss: 1.02460e-06\n",
      "Epoch: 1057700, elapsed: 1.44e+01, train loss: 1.05762e-06, val loss: 1.80443e-06, min loss: 1.02460e-06\n",
      "Epoch: 1057800, elapsed: 1.15e+01, train loss: 1.03058e-06, val loss: 1.80386e-06, min loss: 1.02460e-06\n",
      "Epoch: 1057900, elapsed: 1.13e+01, train loss: 1.02222e-06, val loss: 1.78586e-06, min loss: 1.02222e-06\n",
      "Epoch: 1058000, elapsed: 1.14e+01, train loss: 1.02621e-06, val loss: 1.79146e-06, min loss: 1.02222e-06\n",
      "Epoch: 1058100, elapsed: 1.12e+01, train loss: 1.04882e-06, val loss: 1.78487e-06, min loss: 1.02222e-06\n",
      "Epoch: 1058200, elapsed: 1.13e+01, train loss: 3.64882e-06, val loss: 4.35202e-06, min loss: 1.02222e-06\n",
      "Epoch: 1058300, elapsed: 1.13e+01, train loss: 4.44966e-06, val loss: 4.84990e-06, min loss: 1.02222e-06\n",
      "Epoch: 1058400, elapsed: 1.15e+01, train loss: 2.33110e-06, val loss: 3.48082e-06, min loss: 1.02222e-06\n",
      "Epoch: 1058500, elapsed: 1.14e+01, train loss: 1.77578e-06, val loss: 2.65681e-06, min loss: 1.02222e-06\n",
      "Epoch: 1058600, elapsed: 1.15e+01, train loss: 1.25377e-06, val loss: 1.99753e-06, min loss: 1.02222e-06\n",
      "Epoch: 1058700, elapsed: 1.12e+01, train loss: 1.02658e-06, val loss: 1.79680e-06, min loss: 1.02222e-06\n",
      "Epoch: 1058800, elapsed: 1.13e+01, train loss: 1.04032e-06, val loss: 1.78105e-06, min loss: 1.02222e-06\n",
      "Epoch: 1058900, elapsed: 1.13e+01, train loss: 1.42395e-06, val loss: 2.22511e-06, min loss: 1.02222e-06\n",
      "Epoch: 1059000, elapsed: 1.12e+01, train loss: 1.46587e-06, val loss: 2.29011e-06, min loss: 1.02222e-06\n",
      "Epoch: 1059100, elapsed: 1.12e+01, train loss: 1.53302e-06, val loss: 2.27466e-06, min loss: 1.02222e-06\n",
      "Epoch: 1059200, elapsed: 1.13e+01, train loss: 1.04287e-06, val loss: 1.82300e-06, min loss: 1.02222e-06\n",
      "Epoch: 1059300, elapsed: 1.13e+01, train loss: 1.30745e-06, val loss: 2.54359e-06, min loss: 1.02222e-06\n",
      "Epoch: 1059400, elapsed: 1.13e+01, train loss: 1.02451e-06, val loss: 1.80540e-06, min loss: 1.02222e-06\n",
      "Epoch: 1059500, elapsed: 1.13e+01, train loss: 1.02810e-06, val loss: 1.78949e-06, min loss: 1.02222e-06\n",
      "Epoch: 1059600, elapsed: 1.14e+01, train loss: 1.08039e-06, val loss: 1.84202e-06, min loss: 1.02222e-06\n",
      "Epoch: 1059700, elapsed: 1.42e+01, train loss: 1.67682e-06, val loss: 2.10923e-06, min loss: 1.02222e-06\n",
      "Epoch: 1059800, elapsed: 1.15e+01, train loss: 1.22912e-06, val loss: 2.08727e-06, min loss: 1.02222e-06\n",
      "Epoch: 1059900, elapsed: 1.15e+01, train loss: 1.02029e-06, val loss: 1.77817e-06, min loss: 1.02029e-06\n",
      "Epoch: 1060000, elapsed: 1.15e+01, train loss: 1.38155e-06, val loss: 2.32103e-06, min loss: 1.02029e-06\n",
      "Epoch: 1060100, elapsed: 1.36e+01, train loss: 1.77117e-06, val loss: 2.49799e-06, min loss: 1.02029e-06\n",
      "Epoch: 1060200, elapsed: 1.13e+01, train loss: 1.10680e-06, val loss: 1.96153e-06, min loss: 1.02029e-06\n",
      "Epoch: 1060300, elapsed: 1.14e+01, train loss: 1.33206e-06, val loss: 2.05395e-06, min loss: 1.02029e-06\n",
      "Epoch: 1060400, elapsed: 1.15e+01, train loss: 1.30944e-06, val loss: 2.03740e-06, min loss: 1.02029e-06\n",
      "Epoch: 1060500, elapsed: 1.13e+01, train loss: 3.25179e-06, val loss: 2.47698e-06, min loss: 1.02029e-06\n",
      "Epoch: 1060600, elapsed: 1.15e+01, train loss: 1.01818e-06, val loss: 1.77924e-06, min loss: 1.01818e-06\n",
      "Epoch: 1060700, elapsed: 1.14e+01, train loss: 1.03159e-06, val loss: 1.80609e-06, min loss: 1.01818e-06\n",
      "Epoch: 1060800, elapsed: 1.13e+01, train loss: 1.01974e-06, val loss: 1.79209e-06, min loss: 1.01818e-06\n",
      "Epoch: 1060900, elapsed: 1.14e+01, train loss: 1.02051e-06, val loss: 1.77472e-06, min loss: 1.01818e-06\n",
      "Epoch: 1061000, elapsed: 1.13e+01, train loss: 1.03800e-06, val loss: 1.78750e-06, min loss: 1.01818e-06\n",
      "Epoch: 1061100, elapsed: 1.13e+01, train loss: 1.31467e-06, val loss: 2.19414e-06, min loss: 1.01818e-06\n",
      "Epoch: 1061200, elapsed: 1.13e+01, train loss: 1.82841e-06, val loss: 2.54743e-06, min loss: 1.01818e-06\n",
      "Epoch: 1061300, elapsed: 1.13e+01, train loss: 1.15062e-06, val loss: 1.80489e-06, min loss: 1.01818e-06\n",
      "Epoch: 1061400, elapsed: 1.15e+01, train loss: 1.06466e-06, val loss: 1.80328e-06, min loss: 1.01818e-06\n",
      "Epoch: 1061500, elapsed: 1.13e+01, train loss: 1.02350e-06, val loss: 1.77704e-06, min loss: 1.01818e-06\n",
      "Epoch: 1061600, elapsed: 1.11e+01, train loss: 1.07465e-06, val loss: 1.82823e-06, min loss: 1.01818e-06\n",
      "Epoch: 1061700, elapsed: 1.42e+01, train loss: 1.02176e-06, val loss: 1.79445e-06, min loss: 1.01818e-06\n",
      "Epoch: 1061800, elapsed: 1.14e+01, train loss: 1.07332e-06, val loss: 1.86728e-06, min loss: 1.01818e-06\n",
      "Epoch: 1061900, elapsed: 1.16e+01, train loss: 1.67626e-06, val loss: 2.24235e-06, min loss: 1.01818e-06\n",
      "Epoch: 1062000, elapsed: 1.13e+01, train loss: 1.43862e-06, val loss: 2.12705e-06, min loss: 1.01818e-06\n",
      "Epoch: 1062100, elapsed: 1.14e+01, train loss: 5.97662e-06, val loss: 6.56839e-06, min loss: 1.01818e-06\n",
      "Epoch: 1062200, elapsed: 1.13e+01, train loss: 1.01770e-06, val loss: 1.78569e-06, min loss: 1.01770e-06\n",
      "Epoch: 1062300, elapsed: 1.14e+01, train loss: 2.28110e-06, val loss: 3.27006e-06, min loss: 1.01770e-06\n",
      "Epoch: 1062400, elapsed: 1.16e+01, train loss: 1.13085e-06, val loss: 1.84071e-06, min loss: 1.01770e-06\n",
      "Epoch: 1062500, elapsed: 1.14e+01, train loss: 1.07235e-06, val loss: 1.83787e-06, min loss: 1.01770e-06\n",
      "Epoch: 1062600, elapsed: 1.14e+01, train loss: 1.20331e-06, val loss: 1.90236e-06, min loss: 1.01770e-06\n",
      "Epoch: 1062700, elapsed: 1.11e+01, train loss: 2.71082e-06, val loss: 3.05776e-06, min loss: 1.01770e-06\n",
      "Epoch: 1062800, elapsed: 1.13e+01, train loss: 1.12143e-06, val loss: 1.80636e-06, min loss: 1.01770e-06\n",
      "Epoch: 1062900, elapsed: 1.13e+01, train loss: 1.35196e-06, val loss: 2.17206e-06, min loss: 1.01770e-06\n",
      "Epoch: 1063000, elapsed: 1.12e+01, train loss: 1.33369e-06, val loss: 2.07306e-06, min loss: 1.01770e-06\n",
      "Epoch: 1063100, elapsed: 1.14e+01, train loss: 1.35812e-06, val loss: 2.26150e-06, min loss: 1.01770e-06\n",
      "Epoch: 1063200, elapsed: 1.13e+01, train loss: 1.09286e-06, val loss: 1.88341e-06, min loss: 1.01770e-06\n",
      "Epoch: 1063300, elapsed: 1.14e+01, train loss: 1.04449e-06, val loss: 1.84451e-06, min loss: 1.01770e-06\n",
      "Epoch: 1063400, elapsed: 1.13e+01, train loss: 3.13890e-06, val loss: 4.58424e-06, min loss: 1.01770e-06\n",
      "Epoch: 1063500, elapsed: 1.14e+01, train loss: 1.01607e-06, val loss: 1.78145e-06, min loss: 1.01607e-06\n",
      "Epoch: 1063600, elapsed: 1.44e+01, train loss: 1.01802e-06, val loss: 1.79510e-06, min loss: 1.01607e-06\n",
      "Epoch: 1063700, elapsed: 1.15e+01, train loss: 1.05408e-06, val loss: 1.84505e-06, min loss: 1.01607e-06\n",
      "Epoch: 1063800, elapsed: 1.15e+01, train loss: 1.06020e-06, val loss: 1.84914e-06, min loss: 1.01607e-06\n",
      "Epoch: 1063900, elapsed: 1.13e+01, train loss: 1.03120e-06, val loss: 1.79958e-06, min loss: 1.01607e-06\n",
      "Epoch: 1064000, elapsed: 1.15e+01, train loss: 1.02523e-06, val loss: 1.82648e-06, min loss: 1.01607e-06\n",
      "Epoch: 1064100, elapsed: 1.15e+01, train loss: 1.09781e-06, val loss: 1.86587e-06, min loss: 1.01607e-06\n",
      "Epoch: 1064200, elapsed: 1.12e+01, train loss: 1.03628e-06, val loss: 1.78338e-06, min loss: 1.01607e-06\n",
      "Epoch: 1064300, elapsed: 1.14e+01, train loss: 1.04046e-06, val loss: 1.81352e-06, min loss: 1.01607e-06\n",
      "Epoch: 1064400, elapsed: 1.13e+01, train loss: 1.05143e-06, val loss: 1.78723e-06, min loss: 1.01607e-06\n",
      "Epoch: 1064500, elapsed: 1.13e+01, train loss: 1.02282e-06, val loss: 1.79969e-06, min loss: 1.01607e-06\n",
      "Epoch: 1064600, elapsed: 1.11e+01, train loss: 1.01754e-06, val loss: 1.78188e-06, min loss: 1.01607e-06\n",
      "Epoch: 1064700, elapsed: 1.13e+01, train loss: 1.03082e-06, val loss: 1.80386e-06, min loss: 1.01607e-06\n",
      "Epoch: 1064800, elapsed: 1.13e+01, train loss: 1.79027e-06, val loss: 2.79344e-06, min loss: 1.01607e-06\n",
      "Epoch: 1064900, elapsed: 1.12e+01, train loss: 2.80125e-06, val loss: 4.19630e-06, min loss: 1.01607e-06\n",
      "Epoch: 1065000, elapsed: 1.12e+01, train loss: 1.40830e-06, val loss: 1.98438e-06, min loss: 1.01607e-06\n",
      "Epoch: 1065100, elapsed: 1.32e+01, train loss: 1.04467e-06, val loss: 1.81173e-06, min loss: 1.01607e-06\n",
      "Epoch: 1065200, elapsed: 1.13e+01, train loss: 1.09048e-06, val loss: 1.86107e-06, min loss: 1.01607e-06\n",
      "Epoch: 1065300, elapsed: 1.11e+01, train loss: 1.12077e-06, val loss: 1.95759e-06, min loss: 1.01607e-06\n",
      "Epoch: 1065400, elapsed: 1.13e+01, train loss: 1.20280e-06, val loss: 1.95226e-06, min loss: 1.01607e-06\n",
      "Epoch: 1065500, elapsed: 1.12e+01, train loss: 1.03390e-06, val loss: 1.80736e-06, min loss: 1.01607e-06\n",
      "Epoch: 1065600, elapsed: 1.44e+01, train loss: 1.16079e-06, val loss: 2.04320e-06, min loss: 1.01607e-06\n",
      "Epoch: 1065700, elapsed: 1.17e+01, train loss: 2.43949e-06, val loss: 3.40993e-06, min loss: 1.01607e-06\n",
      "Epoch: 1065800, elapsed: 1.15e+01, train loss: 1.07308e-06, val loss: 1.82447e-06, min loss: 1.01607e-06\n",
      "Epoch: 1065900, elapsed: 1.16e+01, train loss: 1.42906e-06, val loss: 2.25719e-06, min loss: 1.01607e-06\n",
      "Epoch: 1066000, elapsed: 1.17e+01, train loss: 1.01583e-06, val loss: 1.78589e-06, min loss: 1.01583e-06\n",
      "Epoch: 1066100, elapsed: 1.14e+01, train loss: 1.01558e-06, val loss: 1.78878e-06, min loss: 1.01558e-06\n",
      "Epoch: 1066200, elapsed: 1.14e+01, train loss: 1.10990e-06, val loss: 1.84600e-06, min loss: 1.01558e-06\n",
      "Epoch: 1066300, elapsed: 1.12e+01, train loss: 1.04596e-06, val loss: 1.81567e-06, min loss: 1.01558e-06\n",
      "Epoch: 1066400, elapsed: 1.14e+01, train loss: 1.34978e-06, val loss: 2.09678e-06, min loss: 1.01558e-06\n",
      "Epoch: 1066500, elapsed: 1.14e+01, train loss: 1.17286e-06, val loss: 1.88042e-06, min loss: 1.01558e-06\n",
      "Epoch: 1066600, elapsed: 1.13e+01, train loss: 1.06118e-06, val loss: 1.80412e-06, min loss: 1.01558e-06\n",
      "Epoch: 1066700, elapsed: 1.13e+01, train loss: 1.02713e-06, val loss: 1.79667e-06, min loss: 1.01558e-06\n",
      "Epoch: 1066800, elapsed: 1.15e+01, train loss: 1.04996e-06, val loss: 1.84292e-06, min loss: 1.01558e-06\n",
      "Epoch: 1066900, elapsed: 1.14e+01, train loss: 1.02194e-06, val loss: 1.78014e-06, min loss: 1.01558e-06\n",
      "Epoch: 1067000, elapsed: 1.12e+01, train loss: 1.01623e-06, val loss: 1.77646e-06, min loss: 1.01558e-06\n",
      "Epoch: 1067100, elapsed: 1.12e+01, train loss: 1.02956e-06, val loss: 1.81733e-06, min loss: 1.01558e-06\n",
      "Epoch: 1067200, elapsed: 1.12e+01, train loss: 1.09477e-06, val loss: 1.85150e-06, min loss: 1.01558e-06\n",
      "Epoch: 1067300, elapsed: 1.14e+01, train loss: 1.93846e-06, val loss: 2.99571e-06, min loss: 1.01558e-06\n",
      "Epoch: 1067400, elapsed: 1.13e+01, train loss: 1.01364e-06, val loss: 1.78087e-06, min loss: 1.01364e-06\n",
      "Epoch: 1067500, elapsed: 1.14e+01, train loss: 1.04270e-06, val loss: 1.82857e-06, min loss: 1.01364e-06\n",
      "Epoch: 1067600, elapsed: 1.43e+01, train loss: 1.02664e-06, val loss: 1.78392e-06, min loss: 1.01364e-06\n",
      "Epoch: 1067700, elapsed: 1.14e+01, train loss: 1.01487e-06, val loss: 1.77710e-06, min loss: 1.01364e-06\n",
      "Epoch: 1067800, elapsed: 1.14e+01, train loss: 1.01533e-06, val loss: 1.78621e-06, min loss: 1.01364e-06\n",
      "Epoch: 1067900, elapsed: 1.13e+01, train loss: 1.07130e-06, val loss: 1.78411e-06, min loss: 1.01364e-06\n",
      "Epoch: 1068000, elapsed: 1.16e+01, train loss: 1.07542e-06, val loss: 1.80944e-06, min loss: 1.01364e-06\n",
      "Epoch: 1068100, elapsed: 1.12e+01, train loss: 1.04384e-06, val loss: 1.81755e-06, min loss: 1.01364e-06\n",
      "Epoch: 1068200, elapsed: 1.12e+01, train loss: 1.09665e-06, val loss: 1.82415e-06, min loss: 1.01364e-06\n",
      "Epoch: 1068300, elapsed: 1.13e+01, train loss: 1.07689e-06, val loss: 1.87700e-06, min loss: 1.01364e-06\n",
      "Epoch: 1068400, elapsed: 1.13e+01, train loss: 1.27628e-06, val loss: 2.03217e-06, min loss: 1.01364e-06\n",
      "Epoch: 1068500, elapsed: 1.12e+01, train loss: 1.01276e-06, val loss: 1.78086e-06, min loss: 1.01276e-06\n",
      "Epoch: 1068600, elapsed: 1.12e+01, train loss: 1.02722e-06, val loss: 1.77761e-06, min loss: 1.01276e-06\n",
      "Epoch: 1068700, elapsed: 1.12e+01, train loss: 1.31683e-06, val loss: 1.98045e-06, min loss: 1.01276e-06\n",
      "Epoch: 1068800, elapsed: 1.13e+01, train loss: 3.94391e-06, val loss: 3.52089e-06, min loss: 1.01276e-06\n",
      "Epoch: 1068900, elapsed: 1.12e+01, train loss: 1.38406e-06, val loss: 2.27626e-06, min loss: 1.01276e-06\n",
      "Epoch: 1069000, elapsed: 1.13e+01, train loss: 2.38836e-06, val loss: 3.53416e-06, min loss: 1.01276e-06\n",
      "Epoch: 1069100, elapsed: 1.13e+01, train loss: 1.01564e-06, val loss: 1.77556e-06, min loss: 1.01276e-06\n",
      "Epoch: 1069200, elapsed: 1.12e+01, train loss: 1.41350e-06, val loss: 2.08053e-06, min loss: 1.01276e-06\n",
      "Epoch: 1069300, elapsed: 1.12e+01, train loss: 1.03896e-06, val loss: 1.81516e-06, min loss: 1.01276e-06\n",
      "Epoch: 1069400, elapsed: 1.13e+01, train loss: 2.30013e-06, val loss: 2.41304e-06, min loss: 1.01276e-06\n",
      "Epoch: 1069500, elapsed: 1.13e+01, train loss: 1.15204e-06, val loss: 1.92826e-06, min loss: 1.01276e-06\n",
      "Epoch: 1069600, elapsed: 1.47e+01, train loss: 2.03879e-06, val loss: 2.84627e-06, min loss: 1.01276e-06\n",
      "Epoch: 1069700, elapsed: 1.15e+01, train loss: 1.62099e-06, val loss: 2.24333e-06, min loss: 1.01276e-06\n",
      "Epoch: 1069800, elapsed: 1.13e+01, train loss: 1.02354e-06, val loss: 1.81051e-06, min loss: 1.01276e-06\n",
      "Epoch: 1069900, elapsed: 1.14e+01, train loss: 1.38903e-06, val loss: 2.31847e-06, min loss: 1.01276e-06\n",
      "Epoch: 1070000, elapsed: 1.12e+01, train loss: 1.20270e-06, val loss: 2.07340e-06, min loss: 1.01276e-06\n",
      "Epoch: 1070100, elapsed: 1.33e+01, train loss: 1.02416e-06, val loss: 1.78609e-06, min loss: 1.01276e-06\n",
      "Epoch: 1070200, elapsed: 1.13e+01, train loss: 1.28049e-06, val loss: 1.83626e-06, min loss: 1.01276e-06\n",
      "Epoch: 1070300, elapsed: 1.13e+01, train loss: 2.33622e-06, val loss: 2.93152e-06, min loss: 1.01276e-06\n",
      "Epoch: 1070400, elapsed: 1.13e+01, train loss: 1.19958e-06, val loss: 1.87550e-06, min loss: 1.01276e-06\n",
      "Epoch: 1070500, elapsed: 1.14e+01, train loss: 1.62823e-06, val loss: 2.36989e-06, min loss: 1.01276e-06\n",
      "Epoch: 1070600, elapsed: 1.11e+01, train loss: 1.04180e-06, val loss: 1.79962e-06, min loss: 1.01276e-06\n",
      "Epoch: 1070700, elapsed: 1.11e+01, train loss: 1.11942e-06, val loss: 1.83858e-06, min loss: 1.01276e-06\n",
      "Epoch: 1070800, elapsed: 1.13e+01, train loss: 1.01870e-06, val loss: 1.78260e-06, min loss: 1.01276e-06\n",
      "Epoch: 1070900, elapsed: 1.14e+01, train loss: 1.01692e-06, val loss: 1.78988e-06, min loss: 1.01276e-06\n",
      "Epoch: 1071000, elapsed: 1.12e+01, train loss: 1.01419e-06, val loss: 1.77527e-06, min loss: 1.01276e-06\n",
      "Epoch: 1071100, elapsed: 1.13e+01, train loss: 1.02416e-06, val loss: 1.81729e-06, min loss: 1.01276e-06\n",
      "Epoch: 1071200, elapsed: 1.13e+01, train loss: 1.63729e-06, val loss: 2.78596e-06, min loss: 1.01276e-06\n",
      "Epoch: 1071300, elapsed: 1.13e+01, train loss: 1.10605e-06, val loss: 1.85756e-06, min loss: 1.01276e-06\n",
      "Epoch: 1071400, elapsed: 1.14e+01, train loss: 1.01144e-06, val loss: 1.78073e-06, min loss: 1.01144e-06\n",
      "Epoch: 1071500, elapsed: 1.13e+01, train loss: 1.29499e-06, val loss: 1.95413e-06, min loss: 1.01144e-06\n",
      "Epoch: 1071600, elapsed: 1.47e+01, train loss: 1.04514e-06, val loss: 1.79703e-06, min loss: 1.01144e-06\n",
      "Epoch: 1071700, elapsed: 1.16e+01, train loss: 1.28222e-06, val loss: 2.14979e-06, min loss: 1.01144e-06\n",
      "Epoch: 1071800, elapsed: 1.13e+01, train loss: 1.40514e-06, val loss: 1.88627e-06, min loss: 1.01144e-06\n",
      "Epoch: 1071900, elapsed: 1.13e+01, train loss: 1.48333e-06, val loss: 2.13379e-06, min loss: 1.01144e-06\n",
      "Epoch: 1072000, elapsed: 1.14e+01, train loss: 1.29829e-06, val loss: 1.92361e-06, min loss: 1.01144e-06\n",
      "Epoch: 1072100, elapsed: 1.14e+01, train loss: 2.57564e-06, val loss: 3.25118e-06, min loss: 1.01144e-06\n",
      "Epoch: 1072200, elapsed: 1.14e+01, train loss: 1.17418e-06, val loss: 1.94888e-06, min loss: 1.01144e-06\n",
      "Epoch: 1072300, elapsed: 1.12e+01, train loss: 3.58105e-06, val loss: 3.38305e-06, min loss: 1.01144e-06\n",
      "Epoch: 1072400, elapsed: 1.14e+01, train loss: 3.39314e-06, val loss: 4.49895e-06, min loss: 1.01144e-06\n",
      "Epoch: 1072500, elapsed: 1.14e+01, train loss: 1.01277e-06, val loss: 1.77522e-06, min loss: 1.01144e-06\n",
      "Epoch: 1072600, elapsed: 1.15e+01, train loss: 1.01159e-06, val loss: 1.78535e-06, min loss: 1.01144e-06\n",
      "Epoch: 1072700, elapsed: 1.16e+01, train loss: 1.15617e-06, val loss: 1.95570e-06, min loss: 1.01144e-06\n",
      "Epoch: 1072800, elapsed: 1.14e+01, train loss: 1.42334e-06, val loss: 2.28230e-06, min loss: 1.01144e-06\n",
      "Epoch: 1072900, elapsed: 1.14e+01, train loss: 1.52777e-06, val loss: 2.22946e-06, min loss: 1.01144e-06\n",
      "Epoch: 1073000, elapsed: 1.12e+01, train loss: 1.01404e-06, val loss: 1.80397e-06, min loss: 1.01144e-06\n",
      "Epoch: 1073100, elapsed: 1.13e+01, train loss: 1.41045e-06, val loss: 1.85261e-06, min loss: 1.01144e-06\n",
      "Epoch: 1073200, elapsed: 1.14e+01, train loss: 1.25481e-06, val loss: 1.92794e-06, min loss: 1.01144e-06\n",
      "Epoch: 1073300, elapsed: 1.13e+01, train loss: 1.12990e-06, val loss: 1.88394e-06, min loss: 1.01144e-06\n",
      "Epoch: 1073400, elapsed: 1.12e+01, train loss: 1.71298e-06, val loss: 2.50796e-06, min loss: 1.01144e-06\n",
      "Epoch: 1073500, elapsed: 1.12e+01, train loss: 1.00949e-06, val loss: 1.77399e-06, min loss: 1.00949e-06\n",
      "Epoch: 1073600, elapsed: 1.45e+01, train loss: 1.55293e-06, val loss: 2.38648e-06, min loss: 1.00949e-06\n",
      "Epoch: 1073700, elapsed: 1.15e+01, train loss: 1.11270e-06, val loss: 1.81255e-06, min loss: 1.00949e-06\n",
      "Epoch: 1073800, elapsed: 1.13e+01, train loss: 1.01169e-06, val loss: 1.77082e-06, min loss: 1.00949e-06\n",
      "Epoch: 1073900, elapsed: 1.14e+01, train loss: 1.37446e-06, val loss: 2.22666e-06, min loss: 1.00949e-06\n",
      "Epoch: 1074000, elapsed: 1.13e+01, train loss: 1.27985e-06, val loss: 2.08206e-06, min loss: 1.00949e-06\n",
      "Epoch: 1074100, elapsed: 1.13e+01, train loss: 1.90505e-06, val loss: 2.60842e-06, min loss: 1.00949e-06\n",
      "Epoch: 1074200, elapsed: 1.13e+01, train loss: 1.23813e-06, val loss: 1.94817e-06, min loss: 1.00949e-06\n",
      "Epoch: 1074300, elapsed: 1.13e+01, train loss: 1.17834e-06, val loss: 1.82930e-06, min loss: 1.00949e-06\n",
      "Epoch: 1074400, elapsed: 1.13e+01, train loss: 1.06621e-06, val loss: 1.85274e-06, min loss: 1.00949e-06\n",
      "Epoch: 1074500, elapsed: 1.15e+01, train loss: 1.17359e-06, val loss: 2.13043e-06, min loss: 1.00949e-06\n",
      "Epoch: 1074600, elapsed: 1.14e+01, train loss: 1.02839e-06, val loss: 1.81684e-06, min loss: 1.00949e-06\n",
      "Epoch: 1074700, elapsed: 1.14e+01, train loss: 1.00906e-06, val loss: 1.78080e-06, min loss: 1.00906e-06\n",
      "Epoch: 1074800, elapsed: 1.15e+01, train loss: 1.08955e-06, val loss: 1.90019e-06, min loss: 1.00906e-06\n",
      "Epoch: 1074900, elapsed: 1.15e+01, train loss: 1.95717e-06, val loss: 2.88495e-06, min loss: 1.00906e-06\n",
      "Epoch: 1075000, elapsed: 1.14e+01, train loss: 1.17287e-06, val loss: 1.99611e-06, min loss: 1.00906e-06\n",
      "Epoch: 1075100, elapsed: 1.34e+01, train loss: 1.02627e-06, val loss: 1.77199e-06, min loss: 1.00906e-06\n",
      "Epoch: 1075200, elapsed: 1.13e+01, train loss: 1.01162e-06, val loss: 1.77898e-06, min loss: 1.00906e-06\n",
      "Epoch: 1075300, elapsed: 1.14e+01, train loss: 1.04017e-06, val loss: 1.76947e-06, min loss: 1.00906e-06\n",
      "Epoch: 1075400, elapsed: 1.13e+01, train loss: 1.10354e-06, val loss: 1.85794e-06, min loss: 1.00906e-06\n",
      "Epoch: 1075500, elapsed: 1.44e+01, train loss: 1.02415e-06, val loss: 2.11119e-06, min loss: 1.00906e-06\n",
      "Epoch: 1075600, elapsed: 1.16e+01, train loss: 1.03671e-06, val loss: 1.78751e-06, min loss: 1.00906e-06\n",
      "Epoch: 1075700, elapsed: 1.16e+01, train loss: 1.00706e-06, val loss: 1.77457e-06, min loss: 1.00706e-06\n",
      "Epoch: 1075800, elapsed: 1.14e+01, train loss: 1.04871e-06, val loss: 1.79024e-06, min loss: 1.00706e-06\n",
      "Epoch: 1075900, elapsed: 1.14e+01, train loss: 1.29600e-06, val loss: 2.02493e-06, min loss: 1.00706e-06\n",
      "Epoch: 1076000, elapsed: 1.14e+01, train loss: 1.06205e-06, val loss: 1.77646e-06, min loss: 1.00706e-06\n",
      "Epoch: 1076100, elapsed: 1.14e+01, train loss: 2.68373e-06, val loss: 3.63395e-06, min loss: 1.00706e-06\n",
      "Epoch: 1076200, elapsed: 1.13e+01, train loss: 1.11639e-06, val loss: 1.92594e-06, min loss: 1.00706e-06\n",
      "Epoch: 1076300, elapsed: 1.14e+01, train loss: 1.01254e-06, val loss: 1.78032e-06, min loss: 1.00706e-06\n",
      "Epoch: 1076400, elapsed: 1.14e+01, train loss: 1.00970e-06, val loss: 1.77948e-06, min loss: 1.00706e-06\n",
      "Epoch: 1076500, elapsed: 1.13e+01, train loss: 1.05862e-06, val loss: 1.79367e-06, min loss: 1.00706e-06\n",
      "Epoch: 1076600, elapsed: 1.12e+01, train loss: 1.17409e-06, val loss: 1.94128e-06, min loss: 1.00706e-06\n",
      "Epoch: 1076700, elapsed: 1.13e+01, train loss: 1.01419e-06, val loss: 1.79178e-06, min loss: 1.00706e-06\n",
      "Epoch: 1076800, elapsed: 1.13e+01, train loss: 1.30767e-06, val loss: 1.92112e-06, min loss: 1.00706e-06\n",
      "Epoch: 1076900, elapsed: 1.12e+01, train loss: 2.04927e-06, val loss: 3.14951e-06, min loss: 1.00706e-06\n",
      "Epoch: 1077000, elapsed: 1.11e+01, train loss: 3.32547e-06, val loss: 3.99118e-06, min loss: 1.00706e-06\n",
      "Epoch: 1077100, elapsed: 1.13e+01, train loss: 1.18491e-06, val loss: 2.07778e-06, min loss: 1.00706e-06\n",
      "Epoch: 1077200, elapsed: 1.13e+01, train loss: 1.12515e-06, val loss: 1.90716e-06, min loss: 1.00706e-06\n",
      "Epoch: 1077300, elapsed: 1.13e+01, train loss: 1.04490e-06, val loss: 1.80602e-06, min loss: 1.00706e-06\n",
      "Epoch: 1077400, elapsed: 1.12e+01, train loss: 1.10308e-06, val loss: 1.89113e-06, min loss: 1.00706e-06\n",
      "Epoch: 1077500, elapsed: 1.42e+01, train loss: 1.73266e-06, val loss: 2.36106e-06, min loss: 1.00706e-06\n",
      "Epoch: 1077600, elapsed: 1.15e+01, train loss: 1.09962e-06, val loss: 1.96460e-06, min loss: 1.00706e-06\n",
      "Epoch: 1077700, elapsed: 1.15e+01, train loss: 1.10660e-06, val loss: 1.93738e-06, min loss: 1.00706e-06\n",
      "Epoch: 1077800, elapsed: 1.14e+01, train loss: 1.05718e-06, val loss: 1.88935e-06, min loss: 1.00706e-06\n",
      "Epoch: 1077900, elapsed: 1.14e+01, train loss: 1.07569e-06, val loss: 1.86261e-06, min loss: 1.00706e-06\n",
      "Epoch: 1078000, elapsed: 1.14e+01, train loss: 1.00940e-06, val loss: 1.77334e-06, min loss: 1.00706e-06\n",
      "Epoch: 1078100, elapsed: 1.14e+01, train loss: 1.08157e-06, val loss: 1.83578e-06, min loss: 1.00706e-06\n",
      "Epoch: 1078200, elapsed: 1.13e+01, train loss: 1.48305e-06, val loss: 2.24632e-06, min loss: 1.00706e-06\n",
      "Epoch: 1078300, elapsed: 1.16e+01, train loss: 1.50362e-06, val loss: 2.38493e-06, min loss: 1.00706e-06\n",
      "Epoch: 1078400, elapsed: 1.14e+01, train loss: 1.11631e-06, val loss: 1.90334e-06, min loss: 1.00706e-06\n",
      "Epoch: 1078500, elapsed: 1.16e+01, train loss: 1.06271e-06, val loss: 1.86136e-06, min loss: 1.00706e-06\n",
      "Epoch: 1078600, elapsed: 1.13e+01, train loss: 1.05318e-06, val loss: 1.82479e-06, min loss: 1.00706e-06\n",
      "Epoch: 1078700, elapsed: 1.13e+01, train loss: 1.02115e-06, val loss: 1.81157e-06, min loss: 1.00706e-06\n",
      "Epoch: 1078800, elapsed: 1.12e+01, train loss: 1.01479e-06, val loss: 1.79325e-06, min loss: 1.00706e-06\n",
      "Epoch: 1078900, elapsed: 1.12e+01, train loss: 1.01679e-06, val loss: 1.76982e-06, min loss: 1.00706e-06\n",
      "Epoch: 1079000, elapsed: 1.13e+01, train loss: 1.28347e-06, val loss: 2.28864e-06, min loss: 1.00706e-06\n",
      "Epoch: 1079100, elapsed: 1.11e+01, train loss: 1.18590e-06, val loss: 1.98707e-06, min loss: 1.00706e-06\n",
      "Epoch: 1079200, elapsed: 1.12e+01, train loss: 1.07862e-06, val loss: 1.82040e-06, min loss: 1.00706e-06\n",
      "Epoch: 1079300, elapsed: 1.15e+01, train loss: 1.39585e-06, val loss: 2.19956e-06, min loss: 1.00706e-06\n",
      "Epoch: 1079400, elapsed: 1.14e+01, train loss: 1.01552e-06, val loss: 1.77982e-06, min loss: 1.00706e-06\n",
      "Epoch: 1079500, elapsed: 1.12e+01, train loss: 1.02112e-06, val loss: 1.77598e-06, min loss: 1.00706e-06\n",
      "Epoch: 1079600, elapsed: 1.46e+01, train loss: 1.02185e-06, val loss: 1.79958e-06, min loss: 1.00706e-06\n",
      "Epoch: 1079700, elapsed: 1.16e+01, train loss: 2.33926e-06, val loss: 3.39004e-06, min loss: 1.00706e-06\n",
      "Epoch: 1079800, elapsed: 1.14e+01, train loss: 1.51478e-06, val loss: 2.37784e-06, min loss: 1.00706e-06\n",
      "Epoch: 1079900, elapsed: 1.13e+01, train loss: 1.30594e-06, val loss: 1.93725e-06, min loss: 1.00706e-06\n",
      "Epoch: 1080000, elapsed: 1.14e+01, train loss: 2.85790e-06, val loss: 3.70498e-06, min loss: 1.00706e-06\n",
      "Epoch: 1080100, elapsed: 1.33e+01, train loss: 2.31980e-06, val loss: 3.49467e-06, min loss: 1.00706e-06\n",
      "Epoch: 1080200, elapsed: 1.14e+01, train loss: 1.82688e-06, val loss: 2.76773e-06, min loss: 1.00706e-06\n",
      "Epoch: 1080300, elapsed: 1.15e+01, train loss: 2.40463e-06, val loss: 3.61199e-06, min loss: 1.00706e-06\n",
      "Epoch: 1080400, elapsed: 1.14e+01, train loss: 1.21057e-06, val loss: 1.92680e-06, min loss: 1.00706e-06\n",
      "Epoch: 1080500, elapsed: 1.13e+01, train loss: 1.03012e-06, val loss: 1.82160e-06, min loss: 1.00706e-06\n",
      "Epoch: 1080600, elapsed: 1.13e+01, train loss: 1.08347e-06, val loss: 1.91897e-06, min loss: 1.00706e-06\n",
      "Epoch: 1080700, elapsed: 1.13e+01, train loss: 1.00543e-06, val loss: 1.78110e-06, min loss: 1.00543e-06\n",
      "Epoch: 1080800, elapsed: 1.13e+01, train loss: 1.00443e-06, val loss: 1.77166e-06, min loss: 1.00443e-06\n",
      "Epoch: 1080900, elapsed: 1.12e+01, train loss: 1.00432e-06, val loss: 1.78566e-06, min loss: 1.00432e-06\n",
      "Epoch: 1081000, elapsed: 1.13e+01, train loss: 1.00880e-06, val loss: 1.78253e-06, min loss: 1.00432e-06\n",
      "Epoch: 1081100, elapsed: 1.12e+01, train loss: 1.02624e-06, val loss: 1.80609e-06, min loss: 1.00432e-06\n",
      "Epoch: 1081200, elapsed: 1.13e+01, train loss: 1.04402e-06, val loss: 1.78905e-06, min loss: 1.00432e-06\n",
      "Epoch: 1081300, elapsed: 1.13e+01, train loss: 1.00476e-06, val loss: 1.77660e-06, min loss: 1.00432e-06\n",
      "Epoch: 1081400, elapsed: 1.13e+01, train loss: 1.10065e-06, val loss: 1.95601e-06, min loss: 1.00432e-06\n",
      "Epoch: 1081500, elapsed: 1.45e+01, train loss: 2.13034e-06, val loss: 2.26880e-06, min loss: 1.00432e-06\n",
      "Epoch: 1081600, elapsed: 1.16e+01, train loss: 1.03529e-06, val loss: 1.80121e-06, min loss: 1.00432e-06\n",
      "Epoch: 1081700, elapsed: 1.14e+01, train loss: 1.00430e-06, val loss: 1.78257e-06, min loss: 1.00430e-06\n",
      "Epoch: 1081800, elapsed: 1.13e+01, train loss: 1.00669e-06, val loss: 1.78110e-06, min loss: 1.00430e-06\n",
      "Epoch: 1081900, elapsed: 1.14e+01, train loss: 1.08151e-06, val loss: 1.84019e-06, min loss: 1.00430e-06\n",
      "Epoch: 1082000, elapsed: 1.12e+01, train loss: 3.24410e-06, val loss: 2.76869e-06, min loss: 1.00430e-06\n",
      "Epoch: 1082100, elapsed: 1.14e+01, train loss: 1.21107e-06, val loss: 1.91167e-06, min loss: 1.00430e-06\n",
      "Epoch: 1082200, elapsed: 1.14e+01, train loss: 1.02497e-06, val loss: 1.79985e-06, min loss: 1.00430e-06\n",
      "Epoch: 1082300, elapsed: 1.14e+01, train loss: 1.00879e-06, val loss: 1.79444e-06, min loss: 1.00430e-06\n",
      "Epoch: 1082400, elapsed: 1.14e+01, train loss: 1.00939e-06, val loss: 1.78159e-06, min loss: 1.00430e-06\n",
      "Epoch: 1082500, elapsed: 1.15e+01, train loss: 1.00878e-06, val loss: 1.78628e-06, min loss: 1.00430e-06\n",
      "Epoch: 1082600, elapsed: 1.14e+01, train loss: 1.14329e-06, val loss: 1.88255e-06, min loss: 1.00430e-06\n",
      "Epoch: 1082700, elapsed: 1.14e+01, train loss: 1.03497e-06, val loss: 1.79053e-06, min loss: 1.00430e-06\n",
      "Epoch: 1082800, elapsed: 1.13e+01, train loss: 1.05910e-06, val loss: 1.81813e-06, min loss: 1.00430e-06\n",
      "Epoch: 1082900, elapsed: 1.13e+01, train loss: 1.45576e-06, val loss: 1.97906e-06, min loss: 1.00430e-06\n",
      "Epoch: 1083000, elapsed: 1.13e+01, train loss: 1.18481e-06, val loss: 1.99540e-06, min loss: 1.00430e-06\n",
      "Epoch: 1083100, elapsed: 1.12e+01, train loss: 1.00630e-06, val loss: 1.77574e-06, min loss: 1.00430e-06\n",
      "Epoch: 1083200, elapsed: 1.14e+01, train loss: 1.00943e-06, val loss: 1.77670e-06, min loss: 1.00430e-06\n",
      "Epoch: 1083300, elapsed: 1.13e+01, train loss: 1.04644e-06, val loss: 1.81357e-06, min loss: 1.00430e-06\n",
      "Epoch: 1083400, elapsed: 1.13e+01, train loss: 1.00171e-06, val loss: 1.77789e-06, min loss: 1.00171e-06\n",
      "Epoch: 1083500, elapsed: 1.12e+01, train loss: 1.08074e-06, val loss: 1.93068e-06, min loss: 1.00171e-06\n",
      "Epoch: 1083600, elapsed: 1.47e+01, train loss: 1.78670e-06, val loss: 2.45039e-06, min loss: 1.00171e-06\n",
      "Epoch: 1083700, elapsed: 1.14e+01, train loss: 1.02998e-06, val loss: 1.78564e-06, min loss: 1.00171e-06\n",
      "Epoch: 1083800, elapsed: 1.12e+01, train loss: 2.56346e-06, val loss: 3.76293e-06, min loss: 1.00171e-06\n",
      "Epoch: 1083900, elapsed: 1.14e+01, train loss: 1.38799e-06, val loss: 2.37045e-06, min loss: 1.00171e-06\n",
      "Epoch: 1084000, elapsed: 1.13e+01, train loss: 1.00139e-06, val loss: 1.77595e-06, min loss: 1.00139e-06\n",
      "Epoch: 1084100, elapsed: 1.12e+01, train loss: 1.01833e-06, val loss: 1.80161e-06, min loss: 1.00139e-06\n",
      "Epoch: 1084200, elapsed: 1.13e+01, train loss: 1.09127e-06, val loss: 2.00318e-06, min loss: 1.00139e-06\n",
      "Epoch: 1084300, elapsed: 1.13e+01, train loss: 1.46730e-06, val loss: 2.20408e-06, min loss: 1.00139e-06\n",
      "Epoch: 1084400, elapsed: 1.14e+01, train loss: 1.00224e-06, val loss: 1.77887e-06, min loss: 1.00139e-06\n",
      "Epoch: 1084500, elapsed: 1.14e+01, train loss: 1.00571e-06, val loss: 1.78294e-06, min loss: 1.00139e-06\n",
      "Epoch: 1084600, elapsed: 1.13e+01, train loss: 1.00367e-06, val loss: 1.77891e-06, min loss: 1.00139e-06\n",
      "Epoch: 1084700, elapsed: 1.13e+01, train loss: 1.00471e-06, val loss: 1.78276e-06, min loss: 1.00139e-06\n",
      "Epoch: 1084800, elapsed: 1.13e+01, train loss: 1.04383e-06, val loss: 1.79967e-06, min loss: 1.00139e-06\n",
      "Epoch: 1084900, elapsed: 1.13e+01, train loss: 2.27490e-06, val loss: 2.19930e-06, min loss: 1.00139e-06\n",
      "Epoch: 1085000, elapsed: 1.13e+01, train loss: 1.00677e-06, val loss: 1.78090e-06, min loss: 1.00139e-06\n",
      "Epoch: 1085100, elapsed: 1.32e+01, train loss: 1.24479e-06, val loss: 1.93110e-06, min loss: 1.00139e-06\n",
      "Epoch: 1085200, elapsed: 1.13e+01, train loss: 1.10878e-06, val loss: 1.85417e-06, min loss: 1.00139e-06\n",
      "Epoch: 1085300, elapsed: 1.16e+01, train loss: 1.23810e-06, val loss: 2.02720e-06, min loss: 1.00139e-06\n",
      "Epoch: 1085400, elapsed: 1.14e+01, train loss: 1.52347e-06, val loss: 2.51076e-06, min loss: 1.00139e-06\n",
      "Epoch: 1085500, elapsed: 1.42e+01, train loss: 1.31082e-06, val loss: 2.10136e-06, min loss: 1.00139e-06\n",
      "Epoch: 1085600, elapsed: 1.16e+01, train loss: 1.03572e-06, val loss: 1.81513e-06, min loss: 1.00139e-06\n",
      "Epoch: 1085700, elapsed: 1.14e+01, train loss: 1.03328e-06, val loss: 1.85456e-06, min loss: 1.00139e-06\n",
      "Epoch: 1085800, elapsed: 1.14e+01, train loss: 1.01139e-06, val loss: 1.77816e-06, min loss: 1.00139e-06\n",
      "Epoch: 1085900, elapsed: 1.15e+01, train loss: 1.01884e-06, val loss: 1.78615e-06, min loss: 1.00139e-06\n",
      "Epoch: 1086000, elapsed: 1.15e+01, train loss: 1.00359e-06, val loss: 1.78097e-06, min loss: 1.00139e-06\n",
      "Epoch: 1086100, elapsed: 1.13e+01, train loss: 1.64191e-06, val loss: 2.00252e-06, min loss: 1.00139e-06\n",
      "Epoch: 1086200, elapsed: 1.13e+01, train loss: 3.01276e-06, val loss: 4.47453e-06, min loss: 1.00139e-06\n",
      "Epoch: 1086300, elapsed: 1.13e+01, train loss: 1.27409e-06, val loss: 2.19503e-06, min loss: 1.00139e-06\n",
      "Epoch: 1086400, elapsed: 1.14e+01, train loss: 1.04092e-06, val loss: 1.80562e-06, min loss: 1.00139e-06\n",
      "Epoch: 1086500, elapsed: 1.14e+01, train loss: 1.38204e-06, val loss: 2.00012e-06, min loss: 1.00139e-06\n",
      "Epoch: 1086600, elapsed: 1.14e+01, train loss: 1.06012e-06, val loss: 1.83239e-06, min loss: 1.00139e-06\n",
      "Epoch: 1086700, elapsed: 1.14e+01, train loss: 1.07186e-06, val loss: 1.81928e-06, min loss: 1.00139e-06\n",
      "Epoch: 1086800, elapsed: 1.12e+01, train loss: 1.07776e-06, val loss: 1.89518e-06, min loss: 1.00139e-06\n",
      "Epoch: 1086900, elapsed: 1.13e+01, train loss: 1.11961e-06, val loss: 1.89779e-06, min loss: 1.00139e-06\n",
      "Epoch: 1087000, elapsed: 1.14e+01, train loss: 1.28395e-06, val loss: 1.77742e-06, min loss: 1.00139e-06\n",
      "Epoch: 1087100, elapsed: 1.14e+01, train loss: 1.02502e-06, val loss: 1.82664e-06, min loss: 1.00139e-06\n",
      "Epoch: 1087200, elapsed: 1.14e+01, train loss: 5.11405e-06, val loss: 5.38425e-06, min loss: 1.00139e-06\n",
      "Epoch: 1087300, elapsed: 1.13e+01, train loss: 2.33749e-06, val loss: 3.04240e-06, min loss: 1.00139e-06\n",
      "Epoch: 1087400, elapsed: 1.14e+01, train loss: 1.84942e-06, val loss: 2.60516e-06, min loss: 1.00139e-06\n",
      "Epoch: 1087500, elapsed: 1.12e+01, train loss: 1.60164e-06, val loss: 2.30301e-06, min loss: 1.00139e-06\n",
      "Epoch: 1087600, elapsed: 1.44e+01, train loss: 2.24257e-06, val loss: 3.27304e-06, min loss: 1.00139e-06\n",
      "Epoch: 1087700, elapsed: 1.15e+01, train loss: 1.41220e-06, val loss: 2.08648e-06, min loss: 1.00139e-06\n",
      "Epoch: 1087800, elapsed: 1.13e+01, train loss: 5.11968e-06, val loss: 7.02265e-06, min loss: 1.00139e-06\n",
      "Epoch: 1087900, elapsed: 1.13e+01, train loss: 1.32080e-06, val loss: 1.98393e-06, min loss: 1.00139e-06\n",
      "Epoch: 1088000, elapsed: 1.13e+01, train loss: 3.27354e-06, val loss: 3.23759e-06, min loss: 1.00139e-06\n",
      "Epoch: 1088100, elapsed: 1.14e+01, train loss: 2.01385e-06, val loss: 2.62466e-06, min loss: 1.00139e-06\n",
      "Epoch: 1088200, elapsed: 1.13e+01, train loss: 1.72401e-06, val loss: 2.14503e-06, min loss: 1.00139e-06\n",
      "Epoch: 1088300, elapsed: 1.15e+01, train loss: 3.45197e-06, val loss: 3.00192e-06, min loss: 1.00139e-06\n",
      "Epoch: 1088400, elapsed: 1.13e+01, train loss: 1.22638e-06, val loss: 1.88959e-06, min loss: 1.00139e-06\n",
      "Epoch: 1088500, elapsed: 1.14e+01, train loss: 1.20247e-06, val loss: 1.87710e-06, min loss: 1.00139e-06\n",
      "Epoch: 1088600, elapsed: 1.13e+01, train loss: 1.22442e-06, val loss: 1.87391e-06, min loss: 1.00139e-06\n",
      "Epoch: 1088700, elapsed: 1.13e+01, train loss: 1.17902e-06, val loss: 1.85702e-06, min loss: 1.00139e-06\n",
      "Epoch: 1088800, elapsed: 1.14e+01, train loss: 1.17257e-06, val loss: 1.85230e-06, min loss: 1.00139e-06\n",
      "Epoch: 1088900, elapsed: 1.12e+01, train loss: 1.16920e-06, val loss: 1.84610e-06, min loss: 1.00139e-06\n",
      "Epoch: 1089000, elapsed: 1.13e+01, train loss: 1.15546e-06, val loss: 1.84069e-06, min loss: 1.00139e-06\n",
      "Epoch: 1089100, elapsed: 1.13e+01, train loss: 1.60451e-06, val loss: 2.41067e-06, min loss: 1.00139e-06\n",
      "Epoch: 1089200, elapsed: 1.13e+01, train loss: 1.20867e-06, val loss: 1.88799e-06, min loss: 1.00139e-06\n",
      "Epoch: 1089300, elapsed: 1.14e+01, train loss: 3.01783e-06, val loss: 3.90204e-06, min loss: 1.00139e-06\n",
      "Epoch: 1089400, elapsed: 1.14e+01, train loss: 2.20669e-06, val loss: 3.00646e-06, min loss: 1.00139e-06\n",
      "Epoch: 1089500, elapsed: 1.13e+01, train loss: 1.18516e-06, val loss: 1.97850e-06, min loss: 1.00139e-06\n",
      "Epoch: 1089600, elapsed: 1.43e+01, train loss: 1.49516e-06, val loss: 1.90571e-06, min loss: 1.00139e-06\n",
      "Epoch: 1089700, elapsed: 1.16e+01, train loss: 1.33371e-06, val loss: 2.07879e-06, min loss: 1.00139e-06\n",
      "Epoch: 1089800, elapsed: 1.15e+01, train loss: 1.13859e-06, val loss: 1.85618e-06, min loss: 1.00139e-06\n",
      "Epoch: 1089900, elapsed: 1.14e+01, train loss: 1.19360e-06, val loss: 1.94276e-06, min loss: 1.00139e-06\n",
      "Epoch: 1090000, elapsed: 1.13e+01, train loss: 2.95388e-06, val loss: 3.39201e-06, min loss: 1.00139e-06\n",
      "Epoch: 1090100, elapsed: 1.34e+01, train loss: 1.13840e-06, val loss: 1.95209e-06, min loss: 1.00139e-06\n",
      "Epoch: 1090200, elapsed: 1.14e+01, train loss: 4.17743e-06, val loss: 5.47513e-06, min loss: 1.00139e-06\n",
      "Epoch: 1090300, elapsed: 1.14e+01, train loss: 1.14402e-06, val loss: 1.88927e-06, min loss: 1.00139e-06\n",
      "Epoch: 1090400, elapsed: 1.12e+01, train loss: 1.11547e-06, val loss: 1.86494e-06, min loss: 1.00139e-06\n",
      "Epoch: 1090500, elapsed: 1.15e+01, train loss: 1.19800e-06, val loss: 2.00256e-06, min loss: 1.00139e-06\n",
      "Epoch: 1090600, elapsed: 1.13e+01, train loss: 1.08811e-06, val loss: 1.83044e-06, min loss: 1.00139e-06\n",
      "Epoch: 1090700, elapsed: 1.14e+01, train loss: 1.09155e-06, val loss: 1.83942e-06, min loss: 1.00139e-06\n",
      "Epoch: 1090800, elapsed: 1.13e+01, train loss: 2.89952e-06, val loss: 3.31567e-06, min loss: 1.00139e-06\n",
      "Epoch: 1090900, elapsed: 1.14e+01, train loss: 1.09858e-06, val loss: 1.84628e-06, min loss: 1.00139e-06\n",
      "Epoch: 1091000, elapsed: 1.16e+01, train loss: 1.11316e-06, val loss: 1.90313e-06, min loss: 1.00139e-06\n",
      "Epoch: 1091100, elapsed: 1.13e+01, train loss: 1.09092e-06, val loss: 1.85524e-06, min loss: 1.00139e-06\n",
      "Epoch: 1091200, elapsed: 1.12e+01, train loss: 1.13788e-06, val loss: 1.89661e-06, min loss: 1.00139e-06\n",
      "Epoch: 1091300, elapsed: 1.14e+01, train loss: 1.08062e-06, val loss: 1.83002e-06, min loss: 1.00139e-06\n",
      "Epoch: 1091400, elapsed: 1.14e+01, train loss: 1.07825e-06, val loss: 1.84259e-06, min loss: 1.00139e-06\n",
      "Epoch: 1091500, elapsed: 1.12e+01, train loss: 1.07693e-06, val loss: 1.85459e-06, min loss: 1.00139e-06\n",
      "Epoch: 1091600, elapsed: 1.45e+01, train loss: 1.15360e-06, val loss: 1.90532e-06, min loss: 1.00139e-06\n",
      "Epoch: 1091700, elapsed: 1.17e+01, train loss: 1.13191e-06, val loss: 1.95449e-06, min loss: 1.00139e-06\n",
      "Epoch: 1091800, elapsed: 1.15e+01, train loss: 1.07998e-06, val loss: 1.85825e-06, min loss: 1.00139e-06\n",
      "Epoch: 1091900, elapsed: 1.14e+01, train loss: 1.14237e-06, val loss: 1.92272e-06, min loss: 1.00139e-06\n",
      "Epoch: 1092000, elapsed: 1.14e+01, train loss: 1.16255e-06, val loss: 1.88356e-06, min loss: 1.00139e-06\n",
      "Epoch: 1092100, elapsed: 1.15e+01, train loss: 1.40770e-06, val loss: 2.19207e-06, min loss: 1.00139e-06\n",
      "Epoch: 1092200, elapsed: 1.15e+01, train loss: 1.07017e-06, val loss: 1.83415e-06, min loss: 1.00139e-06\n",
      "Epoch: 1092300, elapsed: 1.13e+01, train loss: 1.05952e-06, val loss: 1.84179e-06, min loss: 1.00139e-06\n",
      "Epoch: 1092400, elapsed: 1.14e+01, train loss: 1.11367e-06, val loss: 1.85717e-06, min loss: 1.00139e-06\n",
      "Epoch: 1092500, elapsed: 1.12e+01, train loss: 1.06561e-06, val loss: 1.87460e-06, min loss: 1.00139e-06\n",
      "Epoch: 1092600, elapsed: 1.14e+01, train loss: 1.28613e-06, val loss: 2.09307e-06, min loss: 1.00139e-06\n",
      "Epoch: 1092700, elapsed: 1.13e+01, train loss: 1.13029e-06, val loss: 1.94272e-06, min loss: 1.00139e-06\n",
      "Epoch: 1092800, elapsed: 1.15e+01, train loss: 2.46086e-06, val loss: 3.62844e-06, min loss: 1.00139e-06\n",
      "Epoch: 1092900, elapsed: 1.13e+01, train loss: 1.07243e-06, val loss: 1.87575e-06, min loss: 1.00139e-06\n",
      "Epoch: 1093000, elapsed: 1.15e+01, train loss: 1.07744e-06, val loss: 1.87758e-06, min loss: 1.00139e-06\n",
      "Epoch: 1093100, elapsed: 1.13e+01, train loss: 1.09335e-06, val loss: 1.87677e-06, min loss: 1.00139e-06\n",
      "Epoch: 1093200, elapsed: 1.13e+01, train loss: 1.12603e-06, val loss: 1.91407e-06, min loss: 1.00139e-06\n",
      "Epoch: 1093300, elapsed: 1.15e+01, train loss: 1.08675e-06, val loss: 1.89346e-06, min loss: 1.00139e-06\n",
      "Epoch: 1093400, elapsed: 1.13e+01, train loss: 1.09258e-06, val loss: 1.87608e-06, min loss: 1.00139e-06\n",
      "Epoch: 1093500, elapsed: 1.13e+01, train loss: 1.05035e-06, val loss: 1.84809e-06, min loss: 1.00139e-06\n",
      "Epoch: 1093600, elapsed: 1.43e+01, train loss: 1.05835e-06, val loss: 1.84769e-06, min loss: 1.00139e-06\n",
      "Epoch: 1093700, elapsed: 1.15e+01, train loss: 1.04916e-06, val loss: 1.85423e-06, min loss: 1.00139e-06\n",
      "Epoch: 1093800, elapsed: 1.14e+01, train loss: 1.08138e-06, val loss: 1.88521e-06, min loss: 1.00139e-06\n",
      "Epoch: 1093900, elapsed: 1.15e+01, train loss: 3.61667e-06, val loss: 4.32928e-06, min loss: 1.00139e-06\n",
      "Epoch: 1094000, elapsed: 1.15e+01, train loss: 1.31301e-06, val loss: 2.40319e-06, min loss: 1.00139e-06\n",
      "Epoch: 1094100, elapsed: 1.15e+01, train loss: 1.42581e-06, val loss: 2.06419e-06, min loss: 1.00139e-06\n",
      "Epoch: 1094200, elapsed: 1.14e+01, train loss: 1.14666e-06, val loss: 1.94708e-06, min loss: 1.00139e-06\n",
      "Epoch: 1094300, elapsed: 1.13e+01, train loss: 2.44747e-06, val loss: 3.41031e-06, min loss: 1.00139e-06\n",
      "Epoch: 1094400, elapsed: 1.14e+01, train loss: 1.31537e-06, val loss: 2.17576e-06, min loss: 1.00139e-06\n",
      "Epoch: 1094500, elapsed: 1.14e+01, train loss: 1.66726e-06, val loss: 2.25004e-06, min loss: 1.00139e-06\n",
      "Epoch: 1094600, elapsed: 1.14e+01, train loss: 1.41813e-06, val loss: 2.44475e-06, min loss: 1.00139e-06\n",
      "Epoch: 1094700, elapsed: 1.13e+01, train loss: 1.51580e-06, val loss: 2.05515e-06, min loss: 1.00139e-06\n",
      "Epoch: 1094800, elapsed: 1.15e+01, train loss: 1.14573e-06, val loss: 1.97440e-06, min loss: 1.00139e-06\n",
      "Epoch: 1094900, elapsed: 1.13e+01, train loss: 1.05190e-06, val loss: 1.84451e-06, min loss: 1.00139e-06\n",
      "Epoch: 1095000, elapsed: 1.12e+01, train loss: 1.08055e-06, val loss: 1.91167e-06, min loss: 1.00139e-06\n",
      "Epoch: 1095100, elapsed: 1.33e+01, train loss: 1.10435e-06, val loss: 1.93810e-06, min loss: 1.00139e-06\n",
      "Epoch: 1095200, elapsed: 1.13e+01, train loss: 1.05757e-06, val loss: 1.86765e-06, min loss: 1.00139e-06\n",
      "Epoch: 1095300, elapsed: 1.12e+01, train loss: 1.08572e-06, val loss: 1.89022e-06, min loss: 1.00139e-06\n",
      "Epoch: 1095400, elapsed: 1.13e+01, train loss: 1.42197e-06, val loss: 2.31319e-06, min loss: 1.00139e-06\n",
      "Epoch: 1095500, elapsed: 1.13e+01, train loss: 2.40570e-06, val loss: 3.06553e-06, min loss: 1.00139e-06\n",
      "Epoch: 1095600, elapsed: 1.47e+01, train loss: 1.05719e-06, val loss: 1.87541e-06, min loss: 1.00139e-06\n",
      "Epoch: 1095700, elapsed: 1.15e+01, train loss: 1.09069e-06, val loss: 1.90488e-06, min loss: 1.00139e-06\n",
      "Epoch: 1095800, elapsed: 1.15e+01, train loss: 1.04107e-06, val loss: 1.87154e-06, min loss: 1.00139e-06\n",
      "Epoch: 1095900, elapsed: 1.14e+01, train loss: 1.04162e-06, val loss: 1.87237e-06, min loss: 1.00139e-06\n",
      "Epoch: 1096000, elapsed: 1.16e+01, train loss: 1.09981e-06, val loss: 1.92047e-06, min loss: 1.00139e-06\n",
      "Epoch: 1096100, elapsed: 1.15e+01, train loss: 1.65050e-06, val loss: 2.67055e-06, min loss: 1.00139e-06\n",
      "Epoch: 1096200, elapsed: 1.13e+01, train loss: 1.05359e-06, val loss: 1.90653e-06, min loss: 1.00139e-06\n",
      "Epoch: 1096300, elapsed: 1.14e+01, train loss: 1.10720e-06, val loss: 1.89377e-06, min loss: 1.00139e-06\n",
      "Epoch: 1096400, elapsed: 1.15e+01, train loss: 1.07938e-06, val loss: 1.89609e-06, min loss: 1.00139e-06\n",
      "Epoch: 1096500, elapsed: 1.14e+01, train loss: 1.09559e-06, val loss: 1.90253e-06, min loss: 1.00139e-06\n",
      "Epoch: 1096600, elapsed: 1.15e+01, train loss: 1.04753e-06, val loss: 1.88014e-06, min loss: 1.00139e-06\n",
      "Epoch: 1096700, elapsed: 1.15e+01, train loss: 1.07511e-06, val loss: 1.89342e-06, min loss: 1.00139e-06\n",
      "Epoch: 1096800, elapsed: 1.14e+01, train loss: 1.06095e-06, val loss: 1.91106e-06, min loss: 1.00139e-06\n",
      "Epoch: 1096900, elapsed: 1.15e+01, train loss: 1.03699e-06, val loss: 1.88558e-06, min loss: 1.00139e-06\n",
      "Epoch: 1097000, elapsed: 1.16e+01, train loss: 1.03850e-06, val loss: 1.86609e-06, min loss: 1.00139e-06\n",
      "Epoch: 1097100, elapsed: 1.13e+01, train loss: 1.03570e-06, val loss: 1.85571e-06, min loss: 1.00139e-06\n",
      "Epoch: 1097200, elapsed: 1.15e+01, train loss: 1.15652e-06, val loss: 1.96038e-06, min loss: 1.00139e-06\n",
      "Epoch: 1097300, elapsed: 1.14e+01, train loss: 1.13447e-06, val loss: 1.97603e-06, min loss: 1.00139e-06\n",
      "Epoch: 1097400, elapsed: 1.13e+01, train loss: 1.07408e-06, val loss: 1.90826e-06, min loss: 1.00139e-06\n",
      "Epoch: 1097500, elapsed: 1.13e+01, train loss: 1.17894e-06, val loss: 1.96119e-06, min loss: 1.00139e-06\n",
      "Epoch: 1097600, elapsed: 1.13e+01, train loss: 1.27205e-06, val loss: 2.10788e-06, min loss: 1.00139e-06\n",
      "Epoch: 1097700, elapsed: 1.45e+01, train loss: 1.08654e-06, val loss: 1.90242e-06, min loss: 1.00139e-06\n",
      "Epoch: 1097800, elapsed: 1.14e+01, train loss: 1.06771e-06, val loss: 1.88250e-06, min loss: 1.00139e-06\n",
      "Epoch: 1097900, elapsed: 1.14e+01, train loss: 1.04643e-06, val loss: 1.86714e-06, min loss: 1.00139e-06\n",
      "Epoch: 1098000, elapsed: 1.13e+01, train loss: 1.03581e-06, val loss: 1.87829e-06, min loss: 1.00139e-06\n",
      "Epoch: 1098100, elapsed: 1.14e+01, train loss: 1.03066e-06, val loss: 1.85436e-06, min loss: 1.00139e-06\n",
      "Epoch: 1098200, elapsed: 1.13e+01, train loss: 1.03519e-06, val loss: 1.86500e-06, min loss: 1.00139e-06\n",
      "Epoch: 1098300, elapsed: 1.14e+01, train loss: 1.12091e-06, val loss: 1.87882e-06, min loss: 1.00139e-06\n",
      "Epoch: 1098400, elapsed: 1.13e+01, train loss: 1.60957e-06, val loss: 2.44155e-06, min loss: 1.00139e-06\n",
      "Epoch: 1098500, elapsed: 1.13e+01, train loss: 2.34792e-06, val loss: 3.67440e-06, min loss: 1.00139e-06\n",
      "Epoch: 1098600, elapsed: 1.14e+01, train loss: 1.02766e-06, val loss: 1.86659e-06, min loss: 1.00139e-06\n",
      "Epoch: 1098700, elapsed: 1.14e+01, train loss: 1.08266e-06, val loss: 1.92218e-06, min loss: 1.00139e-06\n",
      "Epoch: 1098800, elapsed: 1.15e+01, train loss: 1.02491e-06, val loss: 1.86107e-06, min loss: 1.00139e-06\n",
      "Epoch: 1098900, elapsed: 1.14e+01, train loss: 1.33070e-06, val loss: 2.20478e-06, min loss: 1.00139e-06\n",
      "Epoch: 1099000, elapsed: 1.13e+01, train loss: 1.02425e-06, val loss: 1.86252e-06, min loss: 1.00139e-06\n",
      "Epoch: 1099100, elapsed: 1.13e+01, train loss: 1.14542e-06, val loss: 1.98646e-06, min loss: 1.00139e-06\n",
      "Epoch: 1099200, elapsed: 1.13e+01, train loss: 1.02376e-06, val loss: 1.86169e-06, min loss: 1.00139e-06\n",
      "Epoch: 1099300, elapsed: 1.13e+01, train loss: 1.18796e-06, val loss: 1.99353e-06, min loss: 1.00139e-06\n",
      "Epoch: 1099400, elapsed: 1.13e+01, train loss: 1.18382e-06, val loss: 2.01288e-06, min loss: 1.00139e-06\n",
      "Epoch: 1099500, elapsed: 1.13e+01, train loss: 1.34888e-06, val loss: 2.19021e-06, min loss: 1.00139e-06\n",
      "Epoch: 1099600, elapsed: 1.13e+01, train loss: 1.13729e-06, val loss: 1.97557e-06, min loss: 1.00139e-06\n",
      "Epoch: 1099700, elapsed: 1.45e+01, train loss: 1.17779e-06, val loss: 2.05251e-06, min loss: 1.00139e-06\n",
      "Epoch: 1099800, elapsed: 1.17e+01, train loss: 1.32893e-06, val loss: 2.15303e-06, min loss: 1.00139e-06\n",
      "Epoch: 1099900, elapsed: 1.14e+01, train loss: 1.31663e-06, val loss: 2.16406e-06, min loss: 1.00139e-06\n",
      "Epoch: 1100000, elapsed: 1.15e+01, train loss: 1.25713e-06, val loss: 2.19937e-06, min loss: 1.00139e-06\n",
      "Epoch: 1100100, elapsed: 1.35e+01, train loss: 1.09693e-06, val loss: 1.98268e-06, min loss: 1.00139e-06\n",
      "Epoch: 1100200, elapsed: 1.15e+01, train loss: 1.02347e-06, val loss: 1.85663e-06, min loss: 1.00139e-06\n",
      "Epoch: 1100300, elapsed: 1.12e+01, train loss: 1.02725e-06, val loss: 1.85803e-06, min loss: 1.00139e-06\n",
      "Epoch: 1100400, elapsed: 1.13e+01, train loss: 1.13004e-06, val loss: 2.03809e-06, min loss: 1.00139e-06\n",
      "Epoch: 1100500, elapsed: 1.13e+01, train loss: 1.10216e-06, val loss: 1.93627e-06, min loss: 1.00139e-06\n",
      "Epoch: 1100600, elapsed: 1.13e+01, train loss: 1.04076e-06, val loss: 1.85749e-06, min loss: 1.00139e-06\n",
      "Epoch: 1100700, elapsed: 1.13e+01, train loss: 1.07388e-06, val loss: 1.90884e-06, min loss: 1.00139e-06\n",
      "Epoch: 1100800, elapsed: 1.15e+01, train loss: 1.57521e-06, val loss: 2.30292e-06, min loss: 1.00139e-06\n",
      "Epoch: 1100900, elapsed: 1.15e+01, train loss: 1.01951e-06, val loss: 1.86207e-06, min loss: 1.00139e-06\n",
      "Epoch: 1101000, elapsed: 1.13e+01, train loss: 1.03082e-06, val loss: 1.87520e-06, min loss: 1.00139e-06\n",
      "Epoch: 1101100, elapsed: 1.13e+01, train loss: 2.47223e-06, val loss: 2.58505e-06, min loss: 1.00139e-06\n",
      "Epoch: 1101200, elapsed: 1.13e+01, train loss: 1.01944e-06, val loss: 1.86462e-06, min loss: 1.00139e-06\n",
      "Epoch: 1101300, elapsed: 1.15e+01, train loss: 1.02314e-06, val loss: 1.85861e-06, min loss: 1.00139e-06\n",
      "Epoch: 1101400, elapsed: 1.11e+01, train loss: 1.04010e-06, val loss: 1.87083e-06, min loss: 1.00139e-06\n",
      "Epoch: 1101500, elapsed: 1.12e+01, train loss: 1.12532e-06, val loss: 2.02346e-06, min loss: 1.00139e-06\n",
      "Epoch: 1101600, elapsed: 1.12e+01, train loss: 1.01930e-06, val loss: 1.86297e-06, min loss: 1.00139e-06\n",
      "Epoch: 1101700, elapsed: 1.12e+01, train loss: 1.01856e-06, val loss: 1.86060e-06, min loss: 1.00139e-06\n",
      "Epoch: 1101800, elapsed: 1.46e+01, train loss: 1.06656e-06, val loss: 1.89992e-06, min loss: 1.00139e-06\n",
      "Epoch: 1101900, elapsed: 1.13e+01, train loss: 1.64518e-06, val loss: 2.53515e-06, min loss: 1.00139e-06\n",
      "Epoch: 1102000, elapsed: 1.15e+01, train loss: 1.25696e-06, val loss: 2.11974e-06, min loss: 1.00139e-06\n",
      "Epoch: 1102100, elapsed: 1.15e+01, train loss: 1.17577e-06, val loss: 2.09437e-06, min loss: 1.00139e-06\n",
      "Epoch: 1102200, elapsed: 1.14e+01, train loss: 1.05057e-06, val loss: 1.87308e-06, min loss: 1.00139e-06\n",
      "Epoch: 1102300, elapsed: 1.13e+01, train loss: 1.04237e-06, val loss: 1.88421e-06, min loss: 1.00139e-06\n",
      "Epoch: 1102400, elapsed: 1.13e+01, train loss: 1.03586e-06, val loss: 1.87668e-06, min loss: 1.00139e-06\n",
      "Epoch: 1102500, elapsed: 1.13e+01, train loss: 1.02746e-06, val loss: 1.85473e-06, min loss: 1.00139e-06\n",
      "Epoch: 1102600, elapsed: 1.15e+01, train loss: 1.03030e-06, val loss: 1.86812e-06, min loss: 1.00139e-06\n",
      "Epoch: 1102700, elapsed: 1.13e+01, train loss: 3.45892e-06, val loss: 4.23421e-06, min loss: 1.00139e-06\n",
      "Epoch: 1102800, elapsed: 1.12e+01, train loss: 1.62822e-06, val loss: 2.54594e-06, min loss: 1.00139e-06\n",
      "Epoch: 1102900, elapsed: 1.13e+01, train loss: 1.12083e-06, val loss: 1.96618e-06, min loss: 1.00139e-06\n",
      "Epoch: 1103000, elapsed: 1.15e+01, train loss: 1.08355e-06, val loss: 1.93460e-06, min loss: 1.00139e-06\n",
      "Epoch: 1103100, elapsed: 1.12e+01, train loss: 2.51759e-06, val loss: 3.29268e-06, min loss: 1.00139e-06\n",
      "Epoch: 1103200, elapsed: 1.12e+01, train loss: 1.05274e-06, val loss: 1.93011e-06, min loss: 1.00139e-06\n",
      "Epoch: 1103300, elapsed: 1.11e+01, train loss: 1.01661e-06, val loss: 1.85659e-06, min loss: 1.00139e-06\n",
      "Epoch: 1103400, elapsed: 1.13e+01, train loss: 1.01885e-06, val loss: 1.85627e-06, min loss: 1.00139e-06\n",
      "Epoch: 1103500, elapsed: 1.12e+01, train loss: 1.12714e-06, val loss: 1.99404e-06, min loss: 1.00139e-06\n",
      "Epoch: 1103600, elapsed: 1.13e+01, train loss: 2.02394e-06, val loss: 3.27712e-06, min loss: 1.00139e-06\n",
      "Epoch: 1103700, elapsed: 1.12e+01, train loss: 1.01678e-06, val loss: 1.87043e-06, min loss: 1.00139e-06\n",
      "Epoch: 1103800, elapsed: 1.43e+01, train loss: 1.04363e-06, val loss: 1.91636e-06, min loss: 1.00139e-06\n",
      "Epoch: 1103900, elapsed: 1.16e+01, train loss: 5.59816e-06, val loss: 5.77031e-06, min loss: 1.00139e-06\n",
      "Epoch: 1104000, elapsed: 1.14e+01, train loss: 1.04447e-06, val loss: 1.89985e-06, min loss: 1.00139e-06\n",
      "Epoch: 1104100, elapsed: 1.13e+01, train loss: 1.03694e-06, val loss: 1.86624e-06, min loss: 1.00139e-06\n",
      "Epoch: 1104200, elapsed: 1.13e+01, train loss: 1.02141e-06, val loss: 1.87806e-06, min loss: 1.00139e-06\n",
      "Epoch: 1104300, elapsed: 1.14e+01, train loss: 1.01978e-06, val loss: 1.85813e-06, min loss: 1.00139e-06\n",
      "Epoch: 1104400, elapsed: 1.15e+01, train loss: 1.01505e-06, val loss: 1.86190e-06, min loss: 1.00139e-06\n",
      "Epoch: 1104500, elapsed: 1.14e+01, train loss: 1.03950e-06, val loss: 1.91048e-06, min loss: 1.00139e-06\n",
      "Epoch: 1104600, elapsed: 1.13e+01, train loss: 1.45838e-06, val loss: 2.39435e-06, min loss: 1.00139e-06\n",
      "Epoch: 1104700, elapsed: 1.15e+01, train loss: 1.53170e-06, val loss: 2.49438e-06, min loss: 1.00139e-06\n",
      "Epoch: 1104800, elapsed: 1.14e+01, train loss: 1.01233e-06, val loss: 1.86128e-06, min loss: 1.00139e-06\n",
      "Epoch: 1104900, elapsed: 1.14e+01, train loss: 1.03352e-06, val loss: 1.90059e-06, min loss: 1.00139e-06\n",
      "Epoch: 1105000, elapsed: 1.13e+01, train loss: 1.01467e-06, val loss: 1.87436e-06, min loss: 1.00139e-06\n",
      "Epoch: 1105100, elapsed: 1.33e+01, train loss: 1.10816e-06, val loss: 1.90618e-06, min loss: 1.00139e-06\n",
      "Epoch: 1105200, elapsed: 1.12e+01, train loss: 1.04698e-06, val loss: 1.88937e-06, min loss: 1.00139e-06\n",
      "Epoch: 1105300, elapsed: 1.13e+01, train loss: 1.08530e-06, val loss: 1.92906e-06, min loss: 1.00139e-06\n",
      "Epoch: 1105400, elapsed: 1.13e+01, train loss: 1.81359e-06, val loss: 2.75112e-06, min loss: 1.00139e-06\n",
      "Epoch: 1105500, elapsed: 1.14e+01, train loss: 1.01120e-06, val loss: 1.86461e-06, min loss: 1.00139e-06\n",
      "Epoch: 1105600, elapsed: 1.14e+01, train loss: 1.02448e-06, val loss: 1.87336e-06, min loss: 1.00139e-06\n",
      "Epoch: 1105700, elapsed: 1.12e+01, train loss: 1.01090e-06, val loss: 1.85854e-06, min loss: 1.00139e-06\n",
      "Epoch: 1105800, elapsed: 1.44e+01, train loss: 1.03048e-06, val loss: 2.01188e-06, min loss: 1.00139e-06\n",
      "Epoch: 1105900, elapsed: 1.15e+01, train loss: 1.01003e-06, val loss: 1.86459e-06, min loss: 1.00139e-06\n",
      "Epoch: 1106000, elapsed: 1.15e+01, train loss: 1.01204e-06, val loss: 1.86295e-06, min loss: 1.00139e-06\n",
      "Epoch: 1106100, elapsed: 1.13e+01, train loss: 1.54203e-06, val loss: 2.42924e-06, min loss: 1.00139e-06\n",
      "Epoch: 1106200, elapsed: 1.12e+01, train loss: 2.90485e-06, val loss: 3.94215e-06, min loss: 1.00139e-06\n",
      "Epoch: 1106300, elapsed: 1.14e+01, train loss: 1.16058e-06, val loss: 2.03051e-06, min loss: 1.00139e-06\n",
      "Epoch: 1106400, elapsed: 1.14e+01, train loss: 1.06866e-06, val loss: 1.89233e-06, min loss: 1.00139e-06\n",
      "Epoch: 1106500, elapsed: 1.13e+01, train loss: 1.02029e-06, val loss: 1.88968e-06, min loss: 1.00139e-06\n",
      "Epoch: 1106600, elapsed: 1.15e+01, train loss: 1.01267e-06, val loss: 1.86885e-06, min loss: 1.00139e-06\n",
      "Epoch: 1106700, elapsed: 1.14e+01, train loss: 1.01201e-06, val loss: 1.87035e-06, min loss: 1.00139e-06\n",
      "Epoch: 1106800, elapsed: 1.13e+01, train loss: 1.03362e-06, val loss: 1.92752e-06, min loss: 1.00139e-06\n",
      "Epoch: 1106900, elapsed: 1.14e+01, train loss: 1.04168e-06, val loss: 1.93080e-06, min loss: 1.00139e-06\n",
      "Epoch: 1107000, elapsed: 1.13e+01, train loss: 1.00991e-06, val loss: 1.87352e-06, min loss: 1.00139e-06\n",
      "Epoch: 1107100, elapsed: 1.14e+01, train loss: 1.02677e-06, val loss: 1.89510e-06, min loss: 1.00139e-06\n",
      "Epoch: 1107200, elapsed: 1.12e+01, train loss: 1.04468e-06, val loss: 1.89441e-06, min loss: 1.00139e-06\n",
      "Epoch: 1107300, elapsed: 1.12e+01, train loss: 1.00901e-06, val loss: 1.86024e-06, min loss: 1.00139e-06\n",
      "Epoch: 1107400, elapsed: 1.12e+01, train loss: 1.02871e-06, val loss: 1.89812e-06, min loss: 1.00139e-06\n",
      "Epoch: 1107500, elapsed: 1.13e+01, train loss: 1.08442e-06, val loss: 1.96155e-06, min loss: 1.00139e-06\n",
      "Epoch: 1107600, elapsed: 1.13e+01, train loss: 1.08887e-06, val loss: 2.01286e-06, min loss: 1.00139e-06\n",
      "Epoch: 1107700, elapsed: 1.13e+01, train loss: 1.13269e-06, val loss: 1.97126e-06, min loss: 1.00139e-06\n",
      "Epoch: 1107800, elapsed: 1.13e+01, train loss: 1.20890e-06, val loss: 2.06975e-06, min loss: 1.00139e-06\n",
      "Epoch: 1107900, elapsed: 1.45e+01, train loss: 1.00756e-06, val loss: 1.86219e-06, min loss: 1.00139e-06\n",
      "Epoch: 1108000, elapsed: 1.14e+01, train loss: 1.01450e-06, val loss: 1.86968e-06, min loss: 1.00139e-06\n",
      "Epoch: 1108100, elapsed: 1.16e+01, train loss: 6.58136e-06, val loss: 8.49420e-06, min loss: 1.00139e-06\n",
      "Epoch: 1108200, elapsed: 1.15e+01, train loss: 1.01247e-06, val loss: 1.86520e-06, min loss: 1.00139e-06\n",
      "Epoch: 1108300, elapsed: 1.14e+01, train loss: 1.06969e-06, val loss: 1.90989e-06, min loss: 1.00139e-06\n",
      "Epoch: 1108400, elapsed: 1.13e+01, train loss: 1.06898e-06, val loss: 1.88389e-06, min loss: 1.00139e-06\n",
      "Epoch: 1108500, elapsed: 1.14e+01, train loss: 1.10624e-06, val loss: 1.89517e-06, min loss: 1.00139e-06\n",
      "Epoch: 1108600, elapsed: 1.14e+01, train loss: 1.99917e-06, val loss: 3.02361e-06, min loss: 1.00139e-06\n",
      "Epoch: 1108700, elapsed: 1.14e+01, train loss: 1.01617e-06, val loss: 1.91482e-06, min loss: 1.00139e-06\n",
      "Epoch: 1108800, elapsed: 1.14e+01, train loss: 1.17748e-06, val loss: 1.95662e-06, min loss: 1.00139e-06\n",
      "Epoch: 1108900, elapsed: 1.13e+01, train loss: 1.91610e-06, val loss: 2.78386e-06, min loss: 1.00139e-06\n",
      "Epoch: 1109000, elapsed: 1.13e+01, train loss: 1.14271e-06, val loss: 2.08615e-06, min loss: 1.00139e-06\n",
      "Epoch: 1109100, elapsed: 1.13e+01, train loss: 1.05789e-06, val loss: 1.90588e-06, min loss: 1.00139e-06\n",
      "Epoch: 1109200, elapsed: 1.12e+01, train loss: 1.11307e-06, val loss: 1.99696e-06, min loss: 1.00139e-06\n",
      "Epoch: 1109300, elapsed: 1.15e+01, train loss: 1.01714e-06, val loss: 1.87943e-06, min loss: 1.00139e-06\n",
      "Epoch: 1109400, elapsed: 1.12e+01, train loss: 1.00545e-06, val loss: 1.86032e-06, min loss: 1.00139e-06\n",
      "Epoch: 1109500, elapsed: 1.14e+01, train loss: 1.01488e-06, val loss: 1.90767e-06, min loss: 1.00139e-06\n",
      "Epoch: 1109600, elapsed: 1.13e+01, train loss: 1.02176e-06, val loss: 1.89865e-06, min loss: 1.00139e-06\n",
      "Epoch: 1109700, elapsed: 1.15e+01, train loss: 1.01146e-06, val loss: 1.88168e-06, min loss: 1.00139e-06\n",
      "Epoch: 1109800, elapsed: 1.15e+01, train loss: 1.03717e-06, val loss: 1.86991e-06, min loss: 1.00139e-06\n",
      "Epoch: 1109900, elapsed: 1.14e+01, train loss: 1.67446e-06, val loss: 2.48447e-06, min loss: 1.00139e-06\n",
      "Epoch: 1110000, elapsed: 1.49e+01, train loss: 1.56866e-06, val loss: 2.42644e-06, min loss: 1.00139e-06\n",
      "Epoch: 1110100, elapsed: 1.37e+01, train loss: 1.11314e-06, val loss: 1.99103e-06, min loss: 1.00139e-06\n",
      "Epoch: 1110200, elapsed: 1.16e+01, train loss: 1.23087e-06, val loss: 2.03153e-06, min loss: 1.00139e-06\n",
      "Epoch: 1110300, elapsed: 1.15e+01, train loss: 1.10360e-06, val loss: 1.97943e-06, min loss: 1.00139e-06\n",
      "Epoch: 1110400, elapsed: 1.14e+01, train loss: 1.03065e-06, val loss: 1.90489e-06, min loss: 1.00139e-06\n",
      "Epoch: 1110500, elapsed: 1.14e+01, train loss: 1.02968e-06, val loss: 1.86960e-06, min loss: 1.00139e-06\n",
      "Epoch: 1110600, elapsed: 1.16e+01, train loss: 1.04921e-06, val loss: 1.95346e-06, min loss: 1.00139e-06\n",
      "Epoch: 1110700, elapsed: 1.12e+01, train loss: 1.04661e-06, val loss: 1.89051e-06, min loss: 1.00139e-06\n",
      "Epoch: 1110800, elapsed: 1.14e+01, train loss: 1.06679e-06, val loss: 1.95302e-06, min loss: 1.00139e-06\n",
      "Epoch: 1110900, elapsed: 1.13e+01, train loss: 1.06164e-06, val loss: 1.96423e-06, min loss: 1.00139e-06\n",
      "Epoch: 1111000, elapsed: 1.14e+01, train loss: 4.83504e-06, val loss: 4.15563e-06, min loss: 1.00139e-06\n",
      "Epoch: 1111100, elapsed: 1.13e+01, train loss: 1.00303e-06, val loss: 1.86325e-06, min loss: 1.00139e-06\n",
      "Epoch: 1111200, elapsed: 1.15e+01, train loss: 1.04146e-06, val loss: 1.93203e-06, min loss: 1.00139e-06\n",
      "Epoch: 1111300, elapsed: 1.14e+01, train loss: 1.58968e-06, val loss: 2.12312e-06, min loss: 1.00139e-06\n",
      "Epoch: 1111400, elapsed: 1.13e+01, train loss: 1.00228e-06, val loss: 1.86252e-06, min loss: 1.00139e-06\n",
      "Epoch: 1111500, elapsed: 1.13e+01, train loss: 1.04937e-06, val loss: 1.93146e-06, min loss: 1.00139e-06\n",
      "Epoch: 1111600, elapsed: 1.13e+01, train loss: 2.79496e-06, val loss: 3.63505e-06, min loss: 1.00139e-06\n",
      "Epoch: 1111700, elapsed: 1.14e+01, train loss: 1.10277e-06, val loss: 2.04753e-06, min loss: 1.00139e-06\n",
      "Epoch: 1111800, elapsed: 1.13e+01, train loss: 1.28993e-06, val loss: 2.11610e-06, min loss: 1.00139e-06\n",
      "Epoch: 1111900, elapsed: 1.14e+01, train loss: 1.08016e-06, val loss: 1.95847e-06, min loss: 1.00139e-06\n",
      "Epoch: 1112000, elapsed: 1.46e+01, train loss: 1.38906e-06, val loss: 2.35841e-06, min loss: 1.00139e-06\n",
      "Epoch: 1112100, elapsed: 1.15e+01, train loss: 1.00456e-06, val loss: 1.85722e-06, min loss: 1.00139e-06\n",
      "Epoch: 1112200, elapsed: 1.15e+01, train loss: 1.00543e-06, val loss: 1.86648e-06, min loss: 1.00139e-06\n",
      "Epoch: 1112300, elapsed: 1.15e+01, train loss: 1.01357e-06, val loss: 1.86869e-06, min loss: 1.00139e-06\n",
      "Epoch: 1112400, elapsed: 1.15e+01, train loss: 1.08209e-06, val loss: 1.99604e-06, min loss: 1.00139e-06\n",
      "Epoch: 1112500, elapsed: 1.15e+01, train loss: 2.38296e-06, val loss: 2.50826e-06, min loss: 1.00139e-06\n",
      "Epoch: 1112600, elapsed: 1.16e+01, train loss: 1.14478e-06, val loss: 2.09355e-06, min loss: 1.00139e-06\n",
      "Epoch: 1112700, elapsed: 1.14e+01, train loss: 1.02055e-06, val loss: 1.89134e-06, min loss: 1.00139e-06\n",
      "Epoch: 1112800, elapsed: 1.12e+01, train loss: 2.28984e-06, val loss: 2.97594e-06, min loss: 1.00139e-06\n",
      "Epoch: 1112900, elapsed: 1.14e+01, train loss: 1.08532e-06, val loss: 1.91786e-06, min loss: 1.00139e-06\n",
      "Epoch: 1113000, elapsed: 1.13e+01, train loss: 1.08806e-06, val loss: 1.99502e-06, min loss: 1.00139e-06\n",
      "Epoch: 1113100, elapsed: 1.13e+01, train loss: 1.01894e-06, val loss: 1.88114e-06, min loss: 1.00139e-06\n",
      "Epoch: 1113200, elapsed: 1.13e+01, train loss: 1.01454e-06, val loss: 1.88173e-06, min loss: 1.00139e-06\n",
      "Epoch: 1113300, elapsed: 1.12e+01, train loss: 1.04560e-06, val loss: 1.85498e-06, min loss: 1.00139e-06\n",
      "Epoch: 1113400, elapsed: 1.13e+01, train loss: 3.36958e-06, val loss: 4.49919e-06, min loss: 1.00139e-06\n",
      "Epoch: 1113500, elapsed: 1.14e+01, train loss: 4.28249e-06, val loss: 5.73800e-06, min loss: 1.00139e-06\n",
      "Epoch: 1113600, elapsed: 1.16e+01, train loss: 1.21320e-06, val loss: 2.09894e-06, min loss: 1.00139e-06\n",
      "Epoch: 1113700, elapsed: 1.14e+01, train loss: 1.01489e-06, val loss: 1.85613e-06, min loss: 1.00139e-06\n",
      "Epoch: 1113800, elapsed: 1.13e+01, train loss: 1.00192e-06, val loss: 1.85082e-06, min loss: 1.00139e-06\n",
      "Epoch: 1113900, elapsed: 1.12e+01, train loss: 1.27873e-06, val loss: 1.94261e-06, min loss: 1.00139e-06\n",
      "Epoch: 1114000, elapsed: 1.14e+01, train loss: 2.33238e-06, val loss: 3.27616e-06, min loss: 1.00139e-06\n",
      "Epoch: 1114100, elapsed: 1.47e+01, train loss: 3.47145e-06, val loss: 3.63638e-06, min loss: 1.00139e-06\n",
      "Epoch: 1114200, elapsed: 1.13e+01, train loss: 2.15810e-06, val loss: 2.25120e-06, min loss: 1.00139e-06\n",
      "Epoch: 1114300, elapsed: 1.15e+01, train loss: 2.03819e-06, val loss: 2.75334e-06, min loss: 1.00139e-06\n",
      "Epoch: 1114400, elapsed: 1.14e+01, train loss: 1.02793e-06, val loss: 1.91328e-06, min loss: 1.00139e-06\n",
      "Epoch: 1114500, elapsed: 1.16e+01, train loss: 1.00573e-06, val loss: 1.85398e-06, min loss: 1.00139e-06\n",
      "Epoch: 1114600, elapsed: 1.14e+01, train loss: 1.00976e-06, val loss: 1.86141e-06, min loss: 1.00139e-06\n",
      "Epoch: 1114700, elapsed: 1.12e+01, train loss: 1.21654e-06, val loss: 2.01859e-06, min loss: 1.00139e-06\n",
      "Epoch: 1114800, elapsed: 1.13e+01, train loss: 3.63793e-06, val loss: 3.90288e-06, min loss: 1.00139e-06\n",
      "Epoch: 1114900, elapsed: 1.13e+01, train loss: 1.85214e-06, val loss: 2.70951e-06, min loss: 1.00139e-06\n",
      "Epoch: 1115000, elapsed: 1.14e+01, train loss: 2.08168e-06, val loss: 3.03774e-06, min loss: 1.00139e-06\n",
      "Epoch: 1115100, elapsed: 1.35e+01, train loss: 1.09381e-06, val loss: 1.97417e-06, min loss: 1.00139e-06\n",
      "Epoch: 1115200, elapsed: 1.14e+01, train loss: 1.00070e-06, val loss: 1.86708e-06, min loss: 1.00070e-06\n",
      "Epoch: 1115300, elapsed: 1.15e+01, train loss: 1.03148e-06, val loss: 1.89205e-06, min loss: 1.00070e-06\n",
      "Epoch: 1115400, elapsed: 1.15e+01, train loss: 1.02756e-06, val loss: 1.96306e-06, min loss: 1.00070e-06\n",
      "Epoch: 1115500, elapsed: 1.14e+01, train loss: 1.12426e-06, val loss: 1.98049e-06, min loss: 1.00070e-06\n",
      "Epoch: 1115600, elapsed: 1.13e+01, train loss: 1.00261e-06, val loss: 1.87023e-06, min loss: 1.00070e-06\n",
      "Epoch: 1115700, elapsed: 1.13e+01, train loss: 1.00164e-06, val loss: 1.85873e-06, min loss: 1.00070e-06\n",
      "Epoch: 1115800, elapsed: 1.13e+01, train loss: 1.00010e-06, val loss: 1.85997e-06, min loss: 1.00010e-06\n",
      "Epoch: 1115900, elapsed: 1.12e+01, train loss: 1.00935e-06, val loss: 1.85516e-06, min loss: 1.00010e-06\n",
      "Epoch: 1116000, elapsed: 1.14e+01, train loss: 2.80665e-06, val loss: 3.58388e-06, min loss: 1.00010e-06\n",
      "Epoch: 1116100, elapsed: 1.45e+01, train loss: 1.00336e-06, val loss: 1.86202e-06, min loss: 1.00010e-06\n",
      "Epoch: 1116200, elapsed: 1.14e+01, train loss: 1.00444e-06, val loss: 1.84737e-06, min loss: 1.00010e-06\n",
      "Epoch: 1116300, elapsed: 1.13e+01, train loss: 1.07525e-06, val loss: 1.90730e-06, min loss: 1.00010e-06\n",
      "Epoch: 1116400, elapsed: 1.14e+01, train loss: 3.25766e-06, val loss: 3.82380e-06, min loss: 1.00010e-06\n",
      "Epoch: 1116500, elapsed: 1.13e+01, train loss: 1.74907e-06, val loss: 2.84229e-06, min loss: 1.00010e-06\n",
      "Epoch: 1116600, elapsed: 1.14e+01, train loss: 1.10053e-06, val loss: 1.98100e-06, min loss: 1.00010e-06\n",
      "Epoch: 1116700, elapsed: 1.13e+01, train loss: 1.35599e-06, val loss: 1.92278e-06, min loss: 1.00010e-06\n",
      "Epoch: 1116800, elapsed: 1.13e+01, train loss: 3.17721e-06, val loss: 3.09562e-06, min loss: 1.00010e-06\n",
      "Epoch: 1116900, elapsed: 1.14e+01, train loss: 1.41420e-06, val loss: 2.40685e-06, min loss: 1.00010e-06\n",
      "Epoch: 1117000, elapsed: 1.14e+01, train loss: 9.97786e-07, val loss: 1.86442e-06, min loss: 9.97786e-07\n",
      "Epoch: 1117100, elapsed: 1.14e+01, train loss: 1.00684e-06, val loss: 1.86628e-06, min loss: 9.97786e-07\n",
      "Epoch: 1117200, elapsed: 1.13e+01, train loss: 1.18466e-06, val loss: 2.19746e-06, min loss: 9.97786e-07\n",
      "Epoch: 1117300, elapsed: 1.15e+01, train loss: 1.25790e-06, val loss: 2.40024e-06, min loss: 9.97786e-07\n",
      "Epoch: 1117400, elapsed: 1.11e+01, train loss: 1.86443e-06, val loss: 2.67522e-06, min loss: 9.97786e-07\n",
      "Epoch: 1117500, elapsed: 1.13e+01, train loss: 1.01307e-06, val loss: 1.86183e-06, min loss: 9.97786e-07\n",
      "Epoch: 1117600, elapsed: 1.11e+01, train loss: 1.53397e-06, val loss: 2.13320e-06, min loss: 9.97786e-07\n",
      "Epoch: 1117700, elapsed: 1.12e+01, train loss: 1.81947e-06, val loss: 2.87790e-06, min loss: 9.97786e-07\n",
      "Epoch: 1117800, elapsed: 1.14e+01, train loss: 9.98185e-07, val loss: 1.85355e-06, min loss: 9.97786e-07\n",
      "Epoch: 1117900, elapsed: 1.13e+01, train loss: 1.27301e-06, val loss: 2.11135e-06, min loss: 9.97786e-07\n",
      "Epoch: 1118000, elapsed: 1.14e+01, train loss: 1.01298e-06, val loss: 1.86562e-06, min loss: 9.97786e-07\n",
      "Epoch: 1118100, elapsed: 1.12e+01, train loss: 9.95757e-07, val loss: 1.85645e-06, min loss: 9.95757e-07\n",
      "Epoch: 1118200, elapsed: 1.48e+01, train loss: 1.02382e-06, val loss: 1.89646e-06, min loss: 9.95757e-07\n",
      "Epoch: 1118300, elapsed: 1.14e+01, train loss: 1.00127e-06, val loss: 1.86895e-06, min loss: 9.95757e-07\n",
      "Epoch: 1118400, elapsed: 1.14e+01, train loss: 1.00941e-06, val loss: 1.86411e-06, min loss: 9.95757e-07\n",
      "Epoch: 1118500, elapsed: 1.14e+01, train loss: 1.00132e-06, val loss: 1.86182e-06, min loss: 9.95757e-07\n",
      "Epoch: 1118600, elapsed: 1.14e+01, train loss: 1.20988e-06, val loss: 1.98984e-06, min loss: 9.95757e-07\n",
      "Epoch: 1118700, elapsed: 1.13e+01, train loss: 4.60440e-06, val loss: 6.01637e-06, min loss: 9.95757e-07\n",
      "Epoch: 1118800, elapsed: 1.14e+01, train loss: 9.93537e-07, val loss: 1.86006e-06, min loss: 9.93537e-07\n",
      "Epoch: 1118900, elapsed: 1.14e+01, train loss: 1.00873e-06, val loss: 1.85773e-06, min loss: 9.93537e-07\n",
      "Epoch: 1119000, elapsed: 1.12e+01, train loss: 1.15643e-06, val loss: 2.09426e-06, min loss: 9.93537e-07\n",
      "Epoch: 1119100, elapsed: 1.14e+01, train loss: 1.08399e-06, val loss: 1.92187e-06, min loss: 9.93537e-07\n",
      "Epoch: 1119200, elapsed: 1.13e+01, train loss: 1.08182e-06, val loss: 1.99512e-06, min loss: 9.93537e-07\n",
      "Epoch: 1119300, elapsed: 1.13e+01, train loss: 1.25621e-06, val loss: 1.99288e-06, min loss: 9.93537e-07\n",
      "Epoch: 1119400, elapsed: 1.13e+01, train loss: 1.34618e-06, val loss: 2.05352e-06, min loss: 9.93537e-07\n",
      "Epoch: 1119500, elapsed: 1.13e+01, train loss: 2.71950e-06, val loss: 3.96909e-06, min loss: 9.93537e-07\n",
      "Epoch: 1119600, elapsed: 1.14e+01, train loss: 2.43458e-06, val loss: 2.73434e-06, min loss: 9.93537e-07\n",
      "Epoch: 1119700, elapsed: 1.13e+01, train loss: 2.02261e-06, val loss: 2.44157e-06, min loss: 9.93537e-07\n",
      "Epoch: 1119800, elapsed: 1.13e+01, train loss: 1.67010e-06, val loss: 2.31082e-06, min loss: 9.93537e-07\n",
      "Epoch: 1119900, elapsed: 1.13e+01, train loss: 1.04687e-06, val loss: 1.93664e-06, min loss: 9.93537e-07\n",
      "Epoch: 1120000, elapsed: 1.14e+01, train loss: 1.21187e-06, val loss: 2.18237e-06, min loss: 9.93537e-07\n",
      "Epoch: 1120100, elapsed: 1.35e+01, train loss: 1.17770e-06, val loss: 2.11507e-06, min loss: 9.93537e-07\n",
      "Epoch: 1120200, elapsed: 1.44e+01, train loss: 9.92406e-07, val loss: 1.85642e-06, min loss: 9.92406e-07\n",
      "Epoch: 1120300, elapsed: 1.15e+01, train loss: 1.01088e-06, val loss: 1.85820e-06, min loss: 9.92406e-07\n",
      "Epoch: 1120400, elapsed: 1.14e+01, train loss: 1.68826e-06, val loss: 2.14955e-06, min loss: 9.92406e-07\n",
      "Epoch: 1120500, elapsed: 1.13e+01, train loss: 1.08789e-06, val loss: 1.98443e-06, min loss: 9.92406e-07\n",
      "Epoch: 1120600, elapsed: 1.13e+01, train loss: 1.00465e-06, val loss: 1.88492e-06, min loss: 9.92406e-07\n",
      "Epoch: 1120700, elapsed: 1.15e+01, train loss: 1.84035e-06, val loss: 2.50402e-06, min loss: 9.92406e-07\n",
      "Epoch: 1120800, elapsed: 1.14e+01, train loss: 9.92354e-07, val loss: 1.86043e-06, min loss: 9.92354e-07\n",
      "Epoch: 1120900, elapsed: 1.14e+01, train loss: 9.95609e-07, val loss: 1.86342e-06, min loss: 9.92354e-07\n",
      "Epoch: 1121000, elapsed: 1.17e+01, train loss: 1.00294e-06, val loss: 1.84767e-06, min loss: 9.92354e-07\n",
      "Epoch: 1121100, elapsed: 1.15e+01, train loss: 6.55987e-06, val loss: 5.71918e-06, min loss: 9.92354e-07\n",
      "Epoch: 1121200, elapsed: 1.15e+01, train loss: 1.00592e-06, val loss: 1.87521e-06, min loss: 9.92354e-07\n",
      "Epoch: 1121300, elapsed: 1.13e+01, train loss: 1.00154e-06, val loss: 1.87236e-06, min loss: 9.92354e-07\n",
      "Epoch: 1121400, elapsed: 1.12e+01, train loss: 4.25454e-06, val loss: 4.92030e-06, min loss: 9.92354e-07\n",
      "Epoch: 1121500, elapsed: 1.12e+01, train loss: 2.11814e-06, val loss: 2.92949e-06, min loss: 9.92354e-07\n",
      "Epoch: 1121600, elapsed: 1.13e+01, train loss: 3.67437e-06, val loss: 3.74842e-06, min loss: 9.92354e-07\n",
      "Epoch: 1121700, elapsed: 1.13e+01, train loss: 1.10934e-06, val loss: 1.97899e-06, min loss: 9.92354e-07\n",
      "Epoch: 1121800, elapsed: 1.14e+01, train loss: 9.91083e-07, val loss: 1.85601e-06, min loss: 9.91083e-07\n",
      "Epoch: 1121900, elapsed: 1.13e+01, train loss: 1.03821e-06, val loss: 1.89496e-06, min loss: 9.91083e-07\n",
      "Epoch: 1122000, elapsed: 1.14e+01, train loss: 1.23753e-06, val loss: 2.52911e-06, min loss: 9.91083e-07\n",
      "Epoch: 1122100, elapsed: 1.14e+01, train loss: 1.52335e-06, val loss: 2.36536e-06, min loss: 9.91083e-07\n",
      "Epoch: 1122200, elapsed: 1.14e+01, train loss: 1.00354e-06, val loss: 1.85044e-06, min loss: 9.91083e-07\n",
      "Epoch: 1122300, elapsed: 1.46e+01, train loss: 1.09779e-06, val loss: 2.05652e-06, min loss: 9.91083e-07\n",
      "Epoch: 1122400, elapsed: 1.15e+01, train loss: 9.92687e-07, val loss: 1.86513e-06, min loss: 9.91083e-07\n",
      "Epoch: 1122500, elapsed: 1.14e+01, train loss: 1.00810e-06, val loss: 1.85393e-06, min loss: 9.91083e-07\n",
      "Epoch: 1122600, elapsed: 1.14e+01, train loss: 9.98482e-07, val loss: 1.86506e-06, min loss: 9.91083e-07\n",
      "Epoch: 1122700, elapsed: 1.15e+01, train loss: 9.90615e-07, val loss: 1.85441e-06, min loss: 9.90615e-07\n",
      "Epoch: 1122800, elapsed: 1.15e+01, train loss: 9.96672e-07, val loss: 1.86195e-06, min loss: 9.90615e-07\n",
      "Epoch: 1122900, elapsed: 1.15e+01, train loss: 2.41210e-06, val loss: 3.72917e-06, min loss: 9.90615e-07\n",
      "Epoch: 1123000, elapsed: 1.13e+01, train loss: 2.16983e-06, val loss: 3.17161e-06, min loss: 9.90615e-07\n",
      "Epoch: 1123100, elapsed: 1.13e+01, train loss: 1.24053e-06, val loss: 2.22906e-06, min loss: 9.90615e-07\n",
      "Epoch: 1123200, elapsed: 1.13e+01, train loss: 1.84241e-06, val loss: 2.41128e-06, min loss: 9.90615e-07\n",
      "Epoch: 1123300, elapsed: 1.12e+01, train loss: 3.81261e-06, val loss: 5.20282e-06, min loss: 9.90615e-07\n",
      "Epoch: 1123400, elapsed: 1.14e+01, train loss: 1.01269e-06, val loss: 1.87768e-06, min loss: 9.90615e-07\n",
      "Epoch: 1123500, elapsed: 1.14e+01, train loss: 9.92105e-07, val loss: 1.85754e-06, min loss: 9.90615e-07\n",
      "Epoch: 1123600, elapsed: 1.13e+01, train loss: 1.01479e-06, val loss: 1.88169e-06, min loss: 9.90615e-07\n",
      "Epoch: 1123700, elapsed: 1.15e+01, train loss: 1.14940e-06, val loss: 2.20603e-06, min loss: 9.90615e-07\n",
      "Epoch: 1123800, elapsed: 1.13e+01, train loss: 2.36169e-06, val loss: 3.37581e-06, min loss: 9.90615e-07\n",
      "Epoch: 1123900, elapsed: 1.12e+01, train loss: 1.04087e-06, val loss: 1.97609e-06, min loss: 9.90615e-07\n",
      "Epoch: 1124000, elapsed: 1.13e+01, train loss: 1.09805e-06, val loss: 1.99342e-06, min loss: 9.90615e-07\n",
      "Epoch: 1124100, elapsed: 1.15e+01, train loss: 1.17704e-06, val loss: 2.02241e-06, min loss: 9.90615e-07\n",
      "Epoch: 1124200, elapsed: 1.13e+01, train loss: 1.02872e-06, val loss: 1.86947e-06, min loss: 9.90615e-07\n",
      "Epoch: 1124300, elapsed: 1.13e+01, train loss: 1.16665e-06, val loss: 1.98044e-06, min loss: 9.90615e-07\n",
      "Epoch: 1124400, elapsed: 1.47e+01, train loss: 2.04295e-06, val loss: 2.55866e-06, min loss: 9.90615e-07\n",
      "Epoch: 1124500, elapsed: 1.15e+01, train loss: 1.00004e-06, val loss: 1.88656e-06, min loss: 9.90615e-07\n",
      "Epoch: 1124600, elapsed: 1.13e+01, train loss: 9.91833e-07, val loss: 1.85013e-06, min loss: 9.90615e-07\n",
      "Epoch: 1124700, elapsed: 1.14e+01, train loss: 1.00685e-06, val loss: 1.88701e-06, min loss: 9.90615e-07\n",
      "Epoch: 1124800, elapsed: 1.14e+01, train loss: 9.90022e-07, val loss: 1.85538e-06, min loss: 9.90022e-07\n",
      "Epoch: 1124900, elapsed: 1.13e+01, train loss: 1.05293e-06, val loss: 1.89217e-06, min loss: 9.90022e-07\n",
      "Epoch: 1125000, elapsed: 1.13e+01, train loss: 1.33077e-06, val loss: 2.04142e-06, min loss: 9.90022e-07\n",
      "Epoch: 1125100, elapsed: 1.34e+01, train loss: 1.58144e-06, val loss: 2.16679e-06, min loss: 9.90022e-07\n",
      "Epoch: 1125200, elapsed: 1.14e+01, train loss: 9.93975e-07, val loss: 1.87812e-06, min loss: 9.90022e-07\n",
      "Epoch: 1125300, elapsed: 1.11e+01, train loss: 9.90292e-07, val loss: 1.84459e-06, min loss: 9.90022e-07\n",
      "Epoch: 1125400, elapsed: 1.12e+01, train loss: 9.96904e-07, val loss: 1.87207e-06, min loss: 9.90022e-07\n",
      "Epoch: 1125500, elapsed: 1.13e+01, train loss: 1.22600e-06, val loss: 2.14513e-06, min loss: 9.90022e-07\n",
      "Epoch: 1125600, elapsed: 1.14e+01, train loss: 1.34085e-06, val loss: 2.32533e-06, min loss: 9.90022e-07\n",
      "Epoch: 1125700, elapsed: 1.15e+01, train loss: 1.84716e-06, val loss: 2.20358e-06, min loss: 9.90022e-07\n",
      "Epoch: 1125800, elapsed: 1.13e+01, train loss: 1.05827e-06, val loss: 1.96847e-06, min loss: 9.90022e-07\n",
      "Epoch: 1125900, elapsed: 1.14e+01, train loss: 1.10239e-06, val loss: 1.88527e-06, min loss: 9.90022e-07\n",
      "Epoch: 1126000, elapsed: 1.14e+01, train loss: 1.06598e-06, val loss: 1.95724e-06, min loss: 9.90022e-07\n",
      "Epoch: 1126100, elapsed: 1.13e+01, train loss: 1.34169e-06, val loss: 2.35490e-06, min loss: 9.90022e-07\n",
      "Epoch: 1126200, elapsed: 1.12e+01, train loss: 1.33093e-06, val loss: 2.15919e-06, min loss: 9.90022e-07\n",
      "Epoch: 1126300, elapsed: 1.13e+01, train loss: 9.86517e-07, val loss: 1.85118e-06, min loss: 9.86517e-07\n",
      "Epoch: 1126400, elapsed: 1.13e+01, train loss: 9.88050e-07, val loss: 1.85973e-06, min loss: 9.86517e-07\n",
      "Epoch: 1126500, elapsed: 1.46e+01, train loss: 1.16417e-06, val loss: 1.94196e-06, min loss: 9.86517e-07\n",
      "Epoch: 1126600, elapsed: 1.16e+01, train loss: 1.79236e-06, val loss: 3.10063e-06, min loss: 9.86517e-07\n",
      "Epoch: 1126700, elapsed: 1.15e+01, train loss: 1.02934e-06, val loss: 1.92526e-06, min loss: 9.86517e-07\n",
      "Epoch: 1126800, elapsed: 1.15e+01, train loss: 9.86720e-07, val loss: 1.84878e-06, min loss: 9.86517e-07\n",
      "Epoch: 1126900, elapsed: 1.15e+01, train loss: 9.98806e-07, val loss: 1.85957e-06, min loss: 9.86517e-07\n",
      "Epoch: 1127000, elapsed: 1.14e+01, train loss: 1.03599e-06, val loss: 1.91840e-06, min loss: 9.86517e-07\n",
      "Epoch: 1127100, elapsed: 1.13e+01, train loss: 1.52256e-06, val loss: 2.61748e-06, min loss: 9.86517e-07\n",
      "Epoch: 1127200, elapsed: 1.12e+01, train loss: 1.20499e-06, val loss: 2.13355e-06, min loss: 9.86517e-07\n",
      "Epoch: 1127300, elapsed: 1.12e+01, train loss: 1.06681e-06, val loss: 1.87337e-06, min loss: 9.86517e-07\n",
      "Epoch: 1127400, elapsed: 1.15e+01, train loss: 1.13049e-06, val loss: 1.96035e-06, min loss: 9.86517e-07\n",
      "Epoch: 1127500, elapsed: 1.13e+01, train loss: 1.31070e-06, val loss: 2.08412e-06, min loss: 9.86517e-07\n",
      "Epoch: 1127600, elapsed: 1.14e+01, train loss: 1.07765e-06, val loss: 1.97885e-06, min loss: 9.86517e-07\n",
      "Epoch: 1127700, elapsed: 1.12e+01, train loss: 1.27068e-06, val loss: 2.09391e-06, min loss: 9.86517e-07\n",
      "Epoch: 1127800, elapsed: 1.14e+01, train loss: 1.00101e-06, val loss: 1.85659e-06, min loss: 9.86517e-07\n",
      "Epoch: 1127900, elapsed: 1.13e+01, train loss: 1.15580e-06, val loss: 1.99725e-06, min loss: 9.86517e-07\n",
      "Epoch: 1128000, elapsed: 1.13e+01, train loss: 2.03466e-06, val loss: 2.89170e-06, min loss: 9.86517e-07\n",
      "Epoch: 1128100, elapsed: 1.14e+01, train loss: 1.39694e-06, val loss: 2.29221e-06, min loss: 9.86517e-07\n",
      "Epoch: 1128200, elapsed: 1.14e+01, train loss: 1.56751e-06, val loss: 2.56953e-06, min loss: 9.86517e-07\n",
      "Epoch: 1128300, elapsed: 1.14e+01, train loss: 1.07804e-06, val loss: 1.88794e-06, min loss: 9.86517e-07\n",
      "Epoch: 1128400, elapsed: 1.14e+01, train loss: 2.06851e-06, val loss: 2.88308e-06, min loss: 9.86517e-07\n",
      "Epoch: 1128500, elapsed: 1.12e+01, train loss: 1.12052e-06, val loss: 2.11705e-06, min loss: 9.86517e-07\n",
      "Epoch: 1128600, elapsed: 1.47e+01, train loss: 3.62047e-06, val loss: 4.88544e-06, min loss: 9.86517e-07\n",
      "Epoch: 1128700, elapsed: 1.15e+01, train loss: 1.14516e-06, val loss: 2.00943e-06, min loss: 9.86517e-07\n",
      "Epoch: 1128800, elapsed: 1.15e+01, train loss: 9.88723e-07, val loss: 1.84941e-06, min loss: 9.86517e-07\n",
      "Epoch: 1128900, elapsed: 1.14e+01, train loss: 9.98095e-07, val loss: 1.86551e-06, min loss: 9.86517e-07\n",
      "Epoch: 1129000, elapsed: 1.15e+01, train loss: 1.00140e-06, val loss: 1.90714e-06, min loss: 9.86517e-07\n",
      "Epoch: 1129100, elapsed: 1.15e+01, train loss: 1.24188e-06, val loss: 2.00693e-06, min loss: 9.86517e-07\n",
      "Epoch: 1129200, elapsed: 1.15e+01, train loss: 1.01073e-06, val loss: 1.86576e-06, min loss: 9.86517e-07\n",
      "Epoch: 1129300, elapsed: 1.13e+01, train loss: 9.84035e-07, val loss: 1.85188e-06, min loss: 9.84035e-07\n",
      "Epoch: 1129400, elapsed: 1.14e+01, train loss: 9.93908e-07, val loss: 1.86808e-06, min loss: 9.84035e-07\n",
      "Epoch: 1129500, elapsed: 1.15e+01, train loss: 9.87809e-07, val loss: 1.84742e-06, min loss: 9.84035e-07\n",
      "Epoch: 1129600, elapsed: 1.16e+01, train loss: 9.90946e-07, val loss: 1.84145e-06, min loss: 9.84035e-07\n",
      "Epoch: 1129700, elapsed: 1.14e+01, train loss: 9.86226e-07, val loss: 1.84237e-06, min loss: 9.84035e-07\n",
      "Epoch: 1129800, elapsed: 1.14e+01, train loss: 2.27695e-06, val loss: 2.28609e-06, min loss: 9.84035e-07\n",
      "Epoch: 1129900, elapsed: 1.13e+01, train loss: 1.00194e-06, val loss: 1.87068e-06, min loss: 9.84035e-07\n",
      "Epoch: 1130000, elapsed: 1.13e+01, train loss: 1.00056e-06, val loss: 1.87458e-06, min loss: 9.84035e-07\n",
      "Epoch: 1130100, elapsed: 1.34e+01, train loss: 1.18629e-06, val loss: 1.96931e-06, min loss: 9.84035e-07\n",
      "Epoch: 1130200, elapsed: 1.13e+01, train loss: 1.47593e-06, val loss: 2.24627e-06, min loss: 9.84035e-07\n",
      "Epoch: 1130300, elapsed: 1.14e+01, train loss: 9.82441e-07, val loss: 1.85387e-06, min loss: 9.82441e-07\n",
      "Epoch: 1130400, elapsed: 1.14e+01, train loss: 9.83058e-07, val loss: 1.85378e-06, min loss: 9.82441e-07\n",
      "Epoch: 1130500, elapsed: 1.14e+01, train loss: 1.13765e-06, val loss: 2.48238e-06, min loss: 9.82441e-07\n",
      "Epoch: 1130600, elapsed: 1.47e+01, train loss: 1.58021e-06, val loss: 2.45697e-06, min loss: 9.82441e-07\n",
      "Epoch: 1130700, elapsed: 1.14e+01, train loss: 1.03155e-06, val loss: 1.88970e-06, min loss: 9.82441e-07\n",
      "Epoch: 1130800, elapsed: 1.15e+01, train loss: 2.26710e-06, val loss: 3.18184e-06, min loss: 9.82441e-07\n",
      "Epoch: 1130900, elapsed: 1.15e+01, train loss: 1.25685e-06, val loss: 1.92985e-06, min loss: 9.82441e-07\n",
      "Epoch: 1131000, elapsed: 1.15e+01, train loss: 1.10983e-06, val loss: 2.09125e-06, min loss: 9.82441e-07\n",
      "Epoch: 1131100, elapsed: 1.15e+01, train loss: 1.02946e-06, val loss: 1.89053e-06, min loss: 9.82441e-07\n",
      "Epoch: 1131200, elapsed: 1.14e+01, train loss: 1.00484e-06, val loss: 1.87206e-06, min loss: 9.82441e-07\n",
      "Epoch: 1131300, elapsed: 1.11e+01, train loss: 1.00066e-06, val loss: 1.85682e-06, min loss: 9.82441e-07\n",
      "Epoch: 1131400, elapsed: 1.13e+01, train loss: 9.95076e-07, val loss: 1.83923e-06, min loss: 9.82441e-07\n",
      "Epoch: 1131500, elapsed: 1.15e+01, train loss: 9.88965e-07, val loss: 1.85282e-06, min loss: 9.82441e-07\n",
      "Epoch: 1131600, elapsed: 1.15e+01, train loss: 9.97114e-07, val loss: 1.85141e-06, min loss: 9.82441e-07\n",
      "Epoch: 1131700, elapsed: 1.17e+01, train loss: 9.86898e-07, val loss: 1.85161e-06, min loss: 9.82441e-07\n",
      "Epoch: 1131800, elapsed: 1.12e+01, train loss: 9.88096e-07, val loss: 1.87064e-06, min loss: 9.82441e-07\n",
      "Epoch: 1131900, elapsed: 1.13e+01, train loss: 1.17600e-06, val loss: 2.10295e-06, min loss: 9.82441e-07\n",
      "Epoch: 1132000, elapsed: 1.13e+01, train loss: 1.02082e-06, val loss: 1.99678e-06, min loss: 9.82441e-07\n",
      "Epoch: 1132100, elapsed: 1.14e+01, train loss: 1.13177e-06, val loss: 2.50461e-06, min loss: 9.82441e-07\n",
      "Epoch: 1132200, elapsed: 1.13e+01, train loss: 9.85394e-07, val loss: 1.85453e-06, min loss: 9.82441e-07\n",
      "Epoch: 1132300, elapsed: 1.14e+01, train loss: 9.83064e-07, val loss: 1.85538e-06, min loss: 9.82441e-07\n",
      "Epoch: 1132400, elapsed: 1.13e+01, train loss: 9.82325e-07, val loss: 1.85296e-06, min loss: 9.82325e-07\n",
      "Epoch: 1132500, elapsed: 1.14e+01, train loss: 9.98205e-07, val loss: 1.87267e-06, min loss: 9.82325e-07\n",
      "Epoch: 1132600, elapsed: 1.12e+01, train loss: 9.87544e-07, val loss: 1.86136e-06, min loss: 9.82325e-07\n",
      "Epoch: 1132700, elapsed: 1.45e+01, train loss: 9.83097e-07, val loss: 1.85731e-06, min loss: 9.82325e-07\n",
      "Epoch: 1132800, elapsed: 1.15e+01, train loss: 1.01013e-06, val loss: 1.90472e-06, min loss: 9.82325e-07\n",
      "Epoch: 1132900, elapsed: 1.14e+01, train loss: 1.29407e-06, val loss: 2.37301e-06, min loss: 9.82325e-07\n",
      "Epoch: 1133000, elapsed: 1.16e+01, train loss: 1.37827e-06, val loss: 2.39922e-06, min loss: 9.82325e-07\n",
      "Epoch: 1133100, elapsed: 1.15e+01, train loss: 1.02338e-06, val loss: 1.90397e-06, min loss: 9.82325e-07\n",
      "Epoch: 1133200, elapsed: 1.13e+01, train loss: 1.21080e-06, val loss: 2.07202e-06, min loss: 9.82325e-07\n",
      "Epoch: 1133300, elapsed: 1.13e+01, train loss: 1.48654e-06, val loss: 2.44383e-06, min loss: 9.82325e-07\n",
      "Epoch: 1133400, elapsed: 1.13e+01, train loss: 9.83708e-07, val loss: 1.85550e-06, min loss: 9.82325e-07\n",
      "Epoch: 1133500, elapsed: 1.14e+01, train loss: 1.33350e-06, val loss: 2.22290e-06, min loss: 9.82325e-07\n",
      "Epoch: 1133600, elapsed: 1.16e+01, train loss: 3.36853e-06, val loss: 4.79823e-06, min loss: 9.82325e-07\n",
      "Epoch: 1133700, elapsed: 1.13e+01, train loss: 9.82990e-07, val loss: 1.84552e-06, min loss: 9.82325e-07\n",
      "Epoch: 1133800, elapsed: 1.13e+01, train loss: 1.61010e-06, val loss: 2.27516e-06, min loss: 9.82325e-07\n",
      "Epoch: 1133900, elapsed: 1.13e+01, train loss: 1.56777e-06, val loss: 2.62278e-06, min loss: 9.82325e-07\n",
      "Epoch: 1134000, elapsed: 1.15e+01, train loss: 9.78796e-07, val loss: 1.84836e-06, min loss: 9.78796e-07\n",
      "Epoch: 1134100, elapsed: 1.13e+01, train loss: 1.01820e-06, val loss: 1.84417e-06, min loss: 9.78796e-07\n",
      "Epoch: 1134200, elapsed: 1.13e+01, train loss: 9.87579e-07, val loss: 1.86459e-06, min loss: 9.78796e-07\n",
      "Epoch: 1134300, elapsed: 1.12e+01, train loss: 9.93269e-07, val loss: 1.86990e-06, min loss: 9.78796e-07\n",
      "Epoch: 1134400, elapsed: 1.14e+01, train loss: 9.89052e-07, val loss: 1.86430e-06, min loss: 9.78796e-07\n",
      "Epoch: 1134500, elapsed: 1.14e+01, train loss: 1.01441e-06, val loss: 1.88998e-06, min loss: 9.78796e-07\n",
      "Epoch: 1134600, elapsed: 1.12e+01, train loss: 1.06678e-06, val loss: 2.07375e-06, min loss: 9.78796e-07\n",
      "Epoch: 1134700, elapsed: 1.13e+01, train loss: 2.30389e-06, val loss: 2.85649e-06, min loss: 9.78796e-07\n",
      "Epoch: 1134800, elapsed: 1.46e+01, train loss: 1.08927e-06, val loss: 1.90417e-06, min loss: 9.78796e-07\n",
      "Epoch: 1134900, elapsed: 1.14e+01, train loss: 1.04251e-06, val loss: 1.95220e-06, min loss: 9.78796e-07\n",
      "Epoch: 1135000, elapsed: 1.14e+01, train loss: 1.57971e-06, val loss: 2.15442e-06, min loss: 9.78796e-07\n",
      "Epoch: 1135100, elapsed: 1.35e+01, train loss: 9.88988e-07, val loss: 1.85820e-06, min loss: 9.78796e-07\n",
      "Epoch: 1135200, elapsed: 1.15e+01, train loss: 1.06368e-06, val loss: 1.93044e-06, min loss: 9.78796e-07\n",
      "Epoch: 1135300, elapsed: 1.14e+01, train loss: 1.23400e-06, val loss: 1.94274e-06, min loss: 9.78796e-07\n",
      "Epoch: 1135400, elapsed: 1.14e+01, train loss: 1.61326e-06, val loss: 2.33788e-06, min loss: 9.78796e-07\n",
      "Epoch: 1135500, elapsed: 1.15e+01, train loss: 1.68578e-06, val loss: 1.97733e-06, min loss: 9.78796e-07\n",
      "Epoch: 1135600, elapsed: 1.12e+01, train loss: 9.78871e-07, val loss: 1.85612e-06, min loss: 9.78796e-07\n",
      "Epoch: 1135700, elapsed: 1.13e+01, train loss: 9.81444e-07, val loss: 1.85170e-06, min loss: 9.78796e-07\n",
      "Epoch: 1135800, elapsed: 1.14e+01, train loss: 1.04342e-06, val loss: 1.88517e-06, min loss: 9.78796e-07\n",
      "Epoch: 1135900, elapsed: 1.12e+01, train loss: 1.83629e-06, val loss: 2.89110e-06, min loss: 9.78796e-07\n",
      "Epoch: 1136000, elapsed: 1.12e+01, train loss: 9.77385e-07, val loss: 1.84940e-06, min loss: 9.77385e-07\n",
      "Epoch: 1136100, elapsed: 1.13e+01, train loss: 1.08458e-06, val loss: 1.95435e-06, min loss: 9.77385e-07\n",
      "Epoch: 1136200, elapsed: 1.15e+01, train loss: 1.03206e-06, val loss: 1.85119e-06, min loss: 9.77385e-07\n",
      "Epoch: 1136300, elapsed: 1.13e+01, train loss: 9.85547e-07, val loss: 1.84263e-06, min loss: 9.77385e-07\n",
      "Epoch: 1136400, elapsed: 1.13e+01, train loss: 9.78619e-07, val loss: 1.85324e-06, min loss: 9.77385e-07\n",
      "Epoch: 1136500, elapsed: 1.13e+01, train loss: 9.78016e-07, val loss: 1.84340e-06, min loss: 9.77385e-07\n",
      "Epoch: 1136600, elapsed: 1.15e+01, train loss: 9.79671e-07, val loss: 1.85641e-06, min loss: 9.77385e-07\n",
      "Epoch: 1136700, elapsed: 1.14e+01, train loss: 2.35486e-06, val loss: 3.22346e-06, min loss: 9.77385e-07\n",
      "Epoch: 1136800, elapsed: 1.13e+01, train loss: 9.82412e-07, val loss: 1.84689e-06, min loss: 9.77385e-07\n",
      "Epoch: 1136900, elapsed: 1.47e+01, train loss: 1.40779e-06, val loss: 2.25964e-06, min loss: 9.77385e-07\n",
      "Epoch: 1137000, elapsed: 1.17e+01, train loss: 9.92237e-07, val loss: 1.88048e-06, min loss: 9.77385e-07\n",
      "Epoch: 1137100, elapsed: 1.14e+01, train loss: 2.82872e-06, val loss: 3.36245e-06, min loss: 9.77385e-07\n",
      "Epoch: 1137200, elapsed: 1.13e+01, train loss: 1.31156e-06, val loss: 2.34384e-06, min loss: 9.77385e-07\n",
      "Epoch: 1137300, elapsed: 1.13e+01, train loss: 2.28085e-06, val loss: 2.65858e-06, min loss: 9.77385e-07\n",
      "Epoch: 1137400, elapsed: 1.12e+01, train loss: 1.69702e-06, val loss: 2.19706e-06, min loss: 9.77385e-07\n",
      "Epoch: 1137500, elapsed: 1.12e+01, train loss: 1.19019e-06, val loss: 2.08801e-06, min loss: 9.77385e-07\n",
      "Epoch: 1137600, elapsed: 1.11e+01, train loss: 9.83052e-07, val loss: 1.84993e-06, min loss: 9.77385e-07\n",
      "Epoch: 1137700, elapsed: 1.13e+01, train loss: 1.19334e-06, val loss: 2.09274e-06, min loss: 9.77385e-07\n",
      "Epoch: 1137800, elapsed: 1.14e+01, train loss: 1.00729e-06, val loss: 1.85683e-06, min loss: 9.77385e-07\n",
      "Epoch: 1137900, elapsed: 1.12e+01, train loss: 2.36990e-06, val loss: 3.16415e-06, min loss: 9.77385e-07\n",
      "Epoch: 1138000, elapsed: 1.14e+01, train loss: 9.90890e-07, val loss: 1.84640e-06, min loss: 9.77385e-07\n",
      "Epoch: 1138100, elapsed: 1.12e+01, train loss: 1.02197e-06, val loss: 1.95311e-06, min loss: 9.77385e-07\n",
      "Epoch: 1138200, elapsed: 1.11e+01, train loss: 1.02097e-06, val loss: 1.86853e-06, min loss: 9.77385e-07\n",
      "Epoch: 1138300, elapsed: 1.13e+01, train loss: 1.09112e-06, val loss: 1.99056e-06, min loss: 9.77385e-07\n",
      "Epoch: 1138400, elapsed: 1.12e+01, train loss: 1.22209e-06, val loss: 2.10043e-06, min loss: 9.77385e-07\n",
      "Epoch: 1138500, elapsed: 1.12e+01, train loss: 1.19787e-06, val loss: 2.02165e-06, min loss: 9.77385e-07\n",
      "Epoch: 1138600, elapsed: 1.11e+01, train loss: 1.19865e-06, val loss: 2.16319e-06, min loss: 9.77385e-07\n",
      "Epoch: 1138700, elapsed: 1.12e+01, train loss: 1.10435e-06, val loss: 1.86823e-06, min loss: 9.77385e-07\n",
      "Epoch: 1138800, elapsed: 1.12e+01, train loss: 1.05235e-06, val loss: 1.93914e-06, min loss: 9.77385e-07\n",
      "Epoch: 1138900, elapsed: 1.14e+01, train loss: 1.01444e-06, val loss: 1.87956e-06, min loss: 9.77385e-07\n",
      "Epoch: 1139000, elapsed: 1.46e+01, train loss: 1.01333e-06, val loss: 1.84705e-06, min loss: 9.77385e-07\n",
      "Epoch: 1139100, elapsed: 1.17e+01, train loss: 2.03713e-06, val loss: 2.90381e-06, min loss: 9.77385e-07\n",
      "Epoch: 1139200, elapsed: 1.16e+01, train loss: 1.03543e-06, val loss: 1.93234e-06, min loss: 9.77385e-07\n",
      "Epoch: 1139300, elapsed: 1.14e+01, train loss: 1.04410e-06, val loss: 1.90829e-06, min loss: 9.77385e-07\n",
      "Epoch: 1139400, elapsed: 1.15e+01, train loss: 1.07302e-06, val loss: 1.93568e-06, min loss: 9.77385e-07\n",
      "Epoch: 1139500, elapsed: 1.14e+01, train loss: 1.33669e-06, val loss: 1.99944e-06, min loss: 9.77385e-07\n",
      "Epoch: 1139600, elapsed: 1.16e+01, train loss: 1.66291e-06, val loss: 2.08393e-06, min loss: 9.77385e-07\n",
      "Epoch: 1139700, elapsed: 1.15e+01, train loss: 5.41901e-06, val loss: 6.41339e-06, min loss: 9.77385e-07\n",
      "Epoch: 1139800, elapsed: 1.14e+01, train loss: 9.81383e-07, val loss: 1.84428e-06, min loss: 9.77385e-07\n",
      "Epoch: 1139900, elapsed: 1.14e+01, train loss: 9.77854e-07, val loss: 1.85870e-06, min loss: 9.77385e-07\n",
      "Epoch: 1140000, elapsed: 1.14e+01, train loss: 9.98634e-07, val loss: 1.84033e-06, min loss: 9.77385e-07\n",
      "Epoch: 1140100, elapsed: 1.35e+01, train loss: 1.00922e-06, val loss: 1.85935e-06, min loss: 9.77385e-07\n",
      "Epoch: 1140200, elapsed: 1.13e+01, train loss: 9.77911e-07, val loss: 1.84036e-06, min loss: 9.77385e-07\n",
      "Epoch: 1140300, elapsed: 1.13e+01, train loss: 1.00993e-06, val loss: 1.90601e-06, min loss: 9.77385e-07\n",
      "Epoch: 1140400, elapsed: 1.13e+01, train loss: 1.64362e-06, val loss: 2.46658e-06, min loss: 9.77385e-07\n",
      "Epoch: 1140500, elapsed: 1.14e+01, train loss: 1.93810e-06, val loss: 2.51631e-06, min loss: 9.77385e-07\n",
      "Epoch: 1140600, elapsed: 1.14e+01, train loss: 2.56231e-06, val loss: 3.02424e-06, min loss: 9.77385e-07\n",
      "Epoch: 1140700, elapsed: 1.13e+01, train loss: 9.74857e-07, val loss: 1.84586e-06, min loss: 9.74857e-07\n",
      "Epoch: 1140800, elapsed: 1.13e+01, train loss: 9.84346e-07, val loss: 1.84120e-06, min loss: 9.74857e-07\n",
      "Epoch: 1140900, elapsed: 1.13e+01, train loss: 9.74822e-07, val loss: 1.84951e-06, min loss: 9.74822e-07\n",
      "Epoch: 1141000, elapsed: 1.13e+01, train loss: 9.89960e-07, val loss: 1.84430e-06, min loss: 9.74822e-07\n",
      "Epoch: 1141100, elapsed: 1.46e+01, train loss: 2.76952e-06, val loss: 3.92140e-06, min loss: 9.74822e-07\n",
      "Epoch: 1141200, elapsed: 1.14e+01, train loss: 1.02899e-06, val loss: 1.94815e-06, min loss: 9.74822e-07\n",
      "Epoch: 1141300, elapsed: 1.14e+01, train loss: 9.76933e-07, val loss: 1.85523e-06, min loss: 9.74822e-07\n",
      "Epoch: 1141400, elapsed: 1.14e+01, train loss: 2.25063e-06, val loss: 3.30242e-06, min loss: 9.74822e-07\n",
      "Epoch: 1141500, elapsed: 1.15e+01, train loss: 9.78110e-07, val loss: 1.84097e-06, min loss: 9.74822e-07\n",
      "Epoch: 1141600, elapsed: 1.15e+01, train loss: 9.73095e-07, val loss: 1.84212e-06, min loss: 9.73095e-07\n",
      "Epoch: 1141700, elapsed: 1.15e+01, train loss: 9.90757e-07, val loss: 1.86000e-06, min loss: 9.73095e-07\n",
      "Epoch: 1141800, elapsed: 1.14e+01, train loss: 1.02098e-06, val loss: 1.85313e-06, min loss: 9.73095e-07\n",
      "Epoch: 1141900, elapsed: 1.16e+01, train loss: 1.00030e-06, val loss: 1.90059e-06, min loss: 9.73095e-07\n",
      "Epoch: 1142000, elapsed: 1.14e+01, train loss: 9.79956e-07, val loss: 1.83345e-06, min loss: 9.73095e-07\n",
      "Epoch: 1142100, elapsed: 1.14e+01, train loss: 9.90370e-07, val loss: 1.84223e-06, min loss: 9.73095e-07\n",
      "Epoch: 1142200, elapsed: 1.15e+01, train loss: 9.75146e-07, val loss: 1.83797e-06, min loss: 9.73095e-07\n",
      "Epoch: 1142300, elapsed: 1.14e+01, train loss: 9.85372e-07, val loss: 1.88878e-06, min loss: 9.73095e-07\n",
      "Epoch: 1142400, elapsed: 1.13e+01, train loss: 9.95315e-07, val loss: 1.90052e-06, min loss: 9.73095e-07\n",
      "Epoch: 1142500, elapsed: 1.14e+01, train loss: 9.72859e-07, val loss: 1.84778e-06, min loss: 9.72859e-07\n",
      "Epoch: 1142600, elapsed: 1.12e+01, train loss: 1.01625e-06, val loss: 1.99472e-06, min loss: 9.72859e-07\n",
      "Epoch: 1142700, elapsed: 1.14e+01, train loss: 1.04370e-06, val loss: 1.90591e-06, min loss: 9.72859e-07\n",
      "Epoch: 1142800, elapsed: 1.14e+01, train loss: 1.50801e-06, val loss: 2.42219e-06, min loss: 9.72859e-07\n",
      "Epoch: 1142900, elapsed: 1.14e+01, train loss: 1.23486e-06, val loss: 2.10833e-06, min loss: 9.72859e-07\n",
      "Epoch: 1143000, elapsed: 1.14e+01, train loss: 1.15229e-06, val loss: 2.00563e-06, min loss: 9.72859e-07\n",
      "Epoch: 1143100, elapsed: 1.14e+01, train loss: 1.00865e-06, val loss: 1.84092e-06, min loss: 9.72859e-07\n",
      "Epoch: 1143200, elapsed: 1.45e+01, train loss: 1.06105e-06, val loss: 1.94553e-06, min loss: 9.72859e-07\n",
      "Epoch: 1143300, elapsed: 1.18e+01, train loss: 9.73246e-07, val loss: 1.83538e-06, min loss: 9.72859e-07\n",
      "Epoch: 1143400, elapsed: 1.15e+01, train loss: 9.78359e-07, val loss: 1.85083e-06, min loss: 9.72859e-07\n",
      "Epoch: 1143500, elapsed: 1.14e+01, train loss: 9.88964e-07, val loss: 1.83933e-06, min loss: 9.72859e-07\n",
      "Epoch: 1143600, elapsed: 1.15e+01, train loss: 9.76643e-07, val loss: 1.83676e-06, min loss: 9.72859e-07\n",
      "Epoch: 1143700, elapsed: 1.14e+01, train loss: 1.23694e-06, val loss: 2.16526e-06, min loss: 9.72859e-07\n",
      "Epoch: 1143800, elapsed: 1.14e+01, train loss: 1.15925e-06, val loss: 2.08040e-06, min loss: 9.72859e-07\n",
      "Epoch: 1143900, elapsed: 1.14e+01, train loss: 9.82093e-07, val loss: 1.83797e-06, min loss: 9.72859e-07\n",
      "Epoch: 1144000, elapsed: 1.14e+01, train loss: 1.21540e-06, val loss: 2.06626e-06, min loss: 9.72859e-07\n",
      "Epoch: 1144100, elapsed: 1.13e+01, train loss: 3.38378e-06, val loss: 4.36479e-06, min loss: 9.72859e-07\n",
      "Epoch: 1144200, elapsed: 1.16e+01, train loss: 1.24131e-06, val loss: 2.31020e-06, min loss: 9.72859e-07\n",
      "Epoch: 1144300, elapsed: 1.15e+01, train loss: 1.01720e-06, val loss: 1.87311e-06, min loss: 9.72859e-07\n",
      "Epoch: 1144400, elapsed: 1.15e+01, train loss: 1.16995e-06, val loss: 2.04923e-06, min loss: 9.72859e-07\n",
      "Epoch: 1144500, elapsed: 1.14e+01, train loss: 1.22009e-06, val loss: 2.16656e-06, min loss: 9.72859e-07\n",
      "Epoch: 1144600, elapsed: 1.16e+01, train loss: 2.04399e-06, val loss: 2.32594e-06, min loss: 9.72859e-07\n",
      "Epoch: 1144700, elapsed: 1.12e+01, train loss: 2.06656e-06, val loss: 2.36910e-06, min loss: 9.72859e-07\n",
      "Epoch: 1144800, elapsed: 1.14e+01, train loss: 1.47777e-06, val loss: 2.21265e-06, min loss: 9.72859e-07\n",
      "Epoch: 1144900, elapsed: 1.15e+01, train loss: 1.16006e-06, val loss: 2.03703e-06, min loss: 9.72859e-07\n",
      "Epoch: 1145000, elapsed: 1.13e+01, train loss: 1.10276e-06, val loss: 1.86781e-06, min loss: 9.72859e-07\n",
      "Epoch: 1145100, elapsed: 1.35e+01, train loss: 1.02890e-06, val loss: 1.97935e-06, min loss: 9.72859e-07\n",
      "Epoch: 1145200, elapsed: 1.14e+01, train loss: 9.93414e-07, val loss: 1.88596e-06, min loss: 9.72859e-07\n",
      "Epoch: 1145300, elapsed: 1.47e+01, train loss: 1.07488e-06, val loss: 1.99486e-06, min loss: 9.72859e-07\n",
      "Epoch: 1145400, elapsed: 1.17e+01, train loss: 2.73513e-06, val loss: 2.95628e-06, min loss: 9.72859e-07\n",
      "Epoch: 1145500, elapsed: 1.16e+01, train loss: 1.25415e-06, val loss: 2.14730e-06, min loss: 9.72859e-07\n",
      "Epoch: 1145600, elapsed: 1.14e+01, train loss: 9.82248e-07, val loss: 1.84292e-06, min loss: 9.72859e-07\n",
      "Epoch: 1145700, elapsed: 1.15e+01, train loss: 9.84711e-07, val loss: 1.86824e-06, min loss: 9.72859e-07\n",
      "Epoch: 1145800, elapsed: 1.15e+01, train loss: 9.70641e-07, val loss: 1.83989e-06, min loss: 9.70641e-07\n",
      "Epoch: 1145900, elapsed: 1.13e+01, train loss: 9.81440e-07, val loss: 1.85387e-06, min loss: 9.70641e-07\n",
      "Epoch: 1146000, elapsed: 1.13e+01, train loss: 6.70646e-06, val loss: 6.72765e-06, min loss: 9.70641e-07\n",
      "Epoch: 1146100, elapsed: 1.13e+01, train loss: 9.69145e-07, val loss: 1.83819e-06, min loss: 9.69145e-07\n",
      "Epoch: 1146200, elapsed: 1.13e+01, train loss: 1.87061e-06, val loss: 2.71052e-06, min loss: 9.69145e-07\n",
      "Epoch: 1146300, elapsed: 1.12e+01, train loss: 9.71548e-07, val loss: 1.83627e-06, min loss: 9.69145e-07\n",
      "Epoch: 1146400, elapsed: 1.13e+01, train loss: 1.04670e-06, val loss: 1.92718e-06, min loss: 9.69145e-07\n",
      "Epoch: 1146500, elapsed: 1.14e+01, train loss: 1.00664e-06, val loss: 1.89998e-06, min loss: 9.69145e-07\n",
      "Epoch: 1146600, elapsed: 1.15e+01, train loss: 9.74524e-07, val loss: 1.85129e-06, min loss: 9.69145e-07\n",
      "Epoch: 1146700, elapsed: 1.13e+01, train loss: 9.70716e-07, val loss: 1.84165e-06, min loss: 9.69145e-07\n",
      "Epoch: 1146800, elapsed: 1.12e+01, train loss: 9.75864e-07, val loss: 1.83652e-06, min loss: 9.69145e-07\n",
      "Epoch: 1146900, elapsed: 1.14e+01, train loss: 9.74050e-07, val loss: 1.83558e-06, min loss: 9.69145e-07\n",
      "Epoch: 1147000, elapsed: 1.14e+01, train loss: 9.73599e-07, val loss: 1.85166e-06, min loss: 9.69145e-07\n",
      "Epoch: 1147100, elapsed: 1.13e+01, train loss: 9.72430e-07, val loss: 1.84549e-06, min loss: 9.69145e-07\n",
      "Epoch: 1147200, elapsed: 1.12e+01, train loss: 9.96206e-07, val loss: 1.86072e-06, min loss: 9.69145e-07\n",
      "Epoch: 1147300, elapsed: 1.14e+01, train loss: 1.26854e-06, val loss: 2.44381e-06, min loss: 9.69145e-07\n",
      "Epoch: 1147400, elapsed: 1.46e+01, train loss: 9.68402e-07, val loss: 1.84566e-06, min loss: 9.68402e-07\n",
      "Epoch: 1147500, elapsed: 1.18e+01, train loss: 9.85065e-07, val loss: 1.83744e-06, min loss: 9.68402e-07\n",
      "Epoch: 1147600, elapsed: 1.15e+01, train loss: 1.23789e-06, val loss: 2.08807e-06, min loss: 9.68402e-07\n",
      "Epoch: 1147700, elapsed: 1.15e+01, train loss: 1.00333e-06, val loss: 1.91655e-06, min loss: 9.68402e-07\n",
      "Epoch: 1147800, elapsed: 1.16e+01, train loss: 1.24123e-06, val loss: 2.10152e-06, min loss: 9.68402e-07\n",
      "Epoch: 1147900, elapsed: 1.15e+01, train loss: 1.06570e-06, val loss: 1.96966e-06, min loss: 9.68402e-07\n",
      "Epoch: 1148000, elapsed: 1.15e+01, train loss: 1.11151e-06, val loss: 1.90559e-06, min loss: 9.68402e-07\n",
      "Epoch: 1148100, elapsed: 1.13e+01, train loss: 1.04751e-06, val loss: 1.97817e-06, min loss: 9.68402e-07\n",
      "Epoch: 1148200, elapsed: 1.15e+01, train loss: 1.23614e-06, val loss: 2.24799e-06, min loss: 9.68402e-07\n",
      "Epoch: 1148300, elapsed: 1.14e+01, train loss: 9.67556e-07, val loss: 1.83638e-06, min loss: 9.67556e-07\n",
      "Epoch: 1148400, elapsed: 1.14e+01, train loss: 9.69287e-07, val loss: 1.83819e-06, min loss: 9.67556e-07\n",
      "Epoch: 1148500, elapsed: 1.14e+01, train loss: 4.43736e-06, val loss: 5.31412e-06, min loss: 9.67556e-07\n",
      "Epoch: 1148600, elapsed: 1.13e+01, train loss: 1.07016e-06, val loss: 1.96039e-06, min loss: 9.67556e-07\n",
      "Epoch: 1148700, elapsed: 1.12e+01, train loss: 2.35842e-06, val loss: 3.28378e-06, min loss: 9.67556e-07\n",
      "Epoch: 1148800, elapsed: 1.14e+01, train loss: 1.12718e-06, val loss: 1.98672e-06, min loss: 9.67556e-07\n",
      "Epoch: 1148900, elapsed: 1.12e+01, train loss: 1.04476e-06, val loss: 1.94197e-06, min loss: 9.67556e-07\n",
      "Epoch: 1149000, elapsed: 1.13e+01, train loss: 1.64559e-06, val loss: 2.03361e-06, min loss: 9.67556e-07\n",
      "Epoch: 1149100, elapsed: 1.14e+01, train loss: 9.96016e-07, val loss: 1.85775e-06, min loss: 9.67556e-07\n",
      "Epoch: 1149200, elapsed: 1.13e+01, train loss: 9.86736e-07, val loss: 1.84619e-06, min loss: 9.67556e-07\n",
      "Epoch: 1149300, elapsed: 1.15e+01, train loss: 1.05201e-06, val loss: 1.96351e-06, min loss: 9.67556e-07\n",
      "Epoch: 1149400, elapsed: 1.14e+01, train loss: 2.02569e-06, val loss: 2.58198e-06, min loss: 9.67556e-07\n",
      "Epoch: 1149500, elapsed: 1.14e+01, train loss: 1.04454e-06, val loss: 1.90087e-06, min loss: 9.67556e-07\n",
      "Epoch: 1149600, elapsed: 1.49e+01, train loss: 1.48515e-06, val loss: 2.33822e-06, min loss: 9.67556e-07\n",
      "Epoch: 1149700, elapsed: 1.14e+01, train loss: 1.47685e-06, val loss: 2.56996e-06, min loss: 9.67556e-07\n",
      "Epoch: 1149800, elapsed: 1.15e+01, train loss: 1.07653e-06, val loss: 2.01603e-06, min loss: 9.67556e-07\n",
      "Epoch: 1149900, elapsed: 1.15e+01, train loss: 1.08885e-06, val loss: 1.99939e-06, min loss: 9.67556e-07\n",
      "Epoch: 1150000, elapsed: 1.15e+01, train loss: 9.71797e-07, val loss: 1.85018e-06, min loss: 9.67556e-07\n",
      "Epoch: 1150100, elapsed: 1.37e+01, train loss: 9.83538e-07, val loss: 1.88917e-06, min loss: 9.67556e-07\n",
      "Epoch: 1150200, elapsed: 1.15e+01, train loss: 1.12154e-06, val loss: 2.05840e-06, min loss: 9.67556e-07\n",
      "Epoch: 1150300, elapsed: 1.14e+01, train loss: 1.07369e-06, val loss: 1.87580e-06, min loss: 9.67556e-07\n",
      "Epoch: 1150400, elapsed: 1.14e+01, train loss: 9.99865e-07, val loss: 1.84773e-06, min loss: 9.67556e-07\n",
      "Epoch: 1150500, elapsed: 1.14e+01, train loss: 1.00343e-06, val loss: 1.87583e-06, min loss: 9.67556e-07\n",
      "Epoch: 1150600, elapsed: 1.15e+01, train loss: 1.06418e-06, val loss: 1.99132e-06, min loss: 9.67556e-07\n",
      "Epoch: 1150700, elapsed: 1.16e+01, train loss: 2.13432e-06, val loss: 3.46958e-06, min loss: 9.67556e-07\n",
      "Epoch: 1150800, elapsed: 1.13e+01, train loss: 1.27517e-06, val loss: 2.26086e-06, min loss: 9.67556e-07\n",
      "Epoch: 1150900, elapsed: 1.14e+01, train loss: 9.74920e-07, val loss: 1.83876e-06, min loss: 9.67556e-07\n",
      "Epoch: 1151000, elapsed: 1.14e+01, train loss: 9.71991e-07, val loss: 1.85573e-06, min loss: 9.67556e-07\n",
      "Epoch: 1151100, elapsed: 1.15e+01, train loss: 1.05992e-06, val loss: 1.97489e-06, min loss: 9.67556e-07\n",
      "Epoch: 1151200, elapsed: 1.13e+01, train loss: 3.26162e-06, val loss: 3.83421e-06, min loss: 9.67556e-07\n",
      "Epoch: 1151300, elapsed: 1.14e+01, train loss: 5.54862e-06, val loss: 6.97279e-06, min loss: 9.67556e-07\n",
      "Epoch: 1151400, elapsed: 1.14e+01, train loss: 9.65122e-07, val loss: 1.83472e-06, min loss: 9.65122e-07\n",
      "Epoch: 1151500, elapsed: 1.12e+01, train loss: 1.11941e-06, val loss: 1.90339e-06, min loss: 9.65122e-07\n",
      "Epoch: 1151600, elapsed: 1.13e+01, train loss: 1.15739e-06, val loss: 2.09277e-06, min loss: 9.65122e-07\n",
      "Epoch: 1151700, elapsed: 1.48e+01, train loss: 1.16283e-06, val loss: 2.01406e-06, min loss: 9.65122e-07\n",
      "Epoch: 1151800, elapsed: 1.13e+01, train loss: 4.32200e-06, val loss: 4.78798e-06, min loss: 9.65122e-07\n",
      "Epoch: 1151900, elapsed: 1.15e+01, train loss: 1.01852e-06, val loss: 1.85138e-06, min loss: 9.65122e-07\n",
      "Epoch: 1152000, elapsed: 1.15e+01, train loss: 1.19519e-06, val loss: 2.07022e-06, min loss: 9.65122e-07\n",
      "Epoch: 1152100, elapsed: 1.14e+01, train loss: 1.75175e-06, val loss: 2.44848e-06, min loss: 9.65122e-07\n",
      "Epoch: 1152200, elapsed: 1.15e+01, train loss: 9.67526e-07, val loss: 1.86138e-06, min loss: 9.65122e-07\n",
      "Epoch: 1152300, elapsed: 1.13e+01, train loss: 9.74767e-07, val loss: 1.84944e-06, min loss: 9.65122e-07\n",
      "Epoch: 1152400, elapsed: 1.14e+01, train loss: 9.85363e-07, val loss: 1.86076e-06, min loss: 9.65122e-07\n",
      "Epoch: 1152500, elapsed: 1.14e+01, train loss: 1.16856e-06, val loss: 2.14585e-06, min loss: 9.65122e-07\n",
      "Epoch: 1152600, elapsed: 1.14e+01, train loss: 1.26256e-06, val loss: 2.08798e-06, min loss: 9.65122e-07\n",
      "Epoch: 1152700, elapsed: 1.11e+01, train loss: 1.60314e-06, val loss: 2.53518e-06, min loss: 9.65122e-07\n",
      "Epoch: 1152800, elapsed: 1.14e+01, train loss: 3.20409e-06, val loss: 3.92876e-06, min loss: 9.65122e-07\n",
      "Epoch: 1152900, elapsed: 1.14e+01, train loss: 5.21593e-06, val loss: 6.29311e-06, min loss: 9.65122e-07\n",
      "Epoch: 1153000, elapsed: 1.13e+01, train loss: 9.63897e-07, val loss: 1.83908e-06, min loss: 9.63897e-07\n",
      "Epoch: 1153100, elapsed: 1.14e+01, train loss: 9.65007e-07, val loss: 1.83117e-06, min loss: 9.63897e-07\n",
      "Epoch: 1153200, elapsed: 1.13e+01, train loss: 1.05452e-06, val loss: 1.92247e-06, min loss: 9.63897e-07\n",
      "Epoch: 1153300, elapsed: 1.13e+01, train loss: 1.14486e-06, val loss: 2.00958e-06, min loss: 9.63897e-07\n",
      "Epoch: 1153400, elapsed: 1.13e+01, train loss: 2.12502e-06, val loss: 3.25293e-06, min loss: 9.63897e-07\n",
      "Epoch: 1153500, elapsed: 1.14e+01, train loss: 9.64975e-07, val loss: 1.83412e-06, min loss: 9.63897e-07\n",
      "Epoch: 1153600, elapsed: 1.13e+01, train loss: 9.75223e-07, val loss: 1.84531e-06, min loss: 9.63897e-07\n",
      "Epoch: 1153700, elapsed: 1.13e+01, train loss: 9.71991e-07, val loss: 1.83762e-06, min loss: 9.63897e-07\n",
      "Epoch: 1153800, elapsed: 1.46e+01, train loss: 9.64321e-07, val loss: 1.83523e-06, min loss: 9.63897e-07\n",
      "Epoch: 1153900, elapsed: 1.14e+01, train loss: 1.04647e-06, val loss: 1.95674e-06, min loss: 9.63897e-07\n",
      "Epoch: 1154000, elapsed: 1.15e+01, train loss: 1.31992e-06, val loss: 1.96370e-06, min loss: 9.63897e-07\n",
      "Epoch: 1154100, elapsed: 1.14e+01, train loss: 9.64707e-07, val loss: 1.83560e-06, min loss: 9.63897e-07\n",
      "Epoch: 1154200, elapsed: 1.16e+01, train loss: 9.66610e-07, val loss: 1.83070e-06, min loss: 9.63897e-07\n",
      "Epoch: 1154300, elapsed: 1.13e+01, train loss: 9.62854e-07, val loss: 1.83194e-06, min loss: 9.62854e-07\n",
      "Epoch: 1154400, elapsed: 1.15e+01, train loss: 1.03315e-06, val loss: 1.91501e-06, min loss: 9.62854e-07\n",
      "Epoch: 1154500, elapsed: 1.15e+01, train loss: 1.04716e-06, val loss: 1.90705e-06, min loss: 9.62854e-07\n",
      "Epoch: 1154600, elapsed: 1.14e+01, train loss: 9.71882e-07, val loss: 1.85433e-06, min loss: 9.62854e-07\n",
      "Epoch: 1154700, elapsed: 1.14e+01, train loss: 9.88009e-07, val loss: 1.85551e-06, min loss: 9.62854e-07\n",
      "Epoch: 1154800, elapsed: 1.13e+01, train loss: 1.17609e-06, val loss: 2.09541e-06, min loss: 9.62854e-07\n",
      "Epoch: 1154900, elapsed: 1.12e+01, train loss: 9.68771e-07, val loss: 1.83111e-06, min loss: 9.62854e-07\n",
      "Epoch: 1155000, elapsed: 1.12e+01, train loss: 1.04675e-06, val loss: 1.86097e-06, min loss: 9.62854e-07\n",
      "Epoch: 1155100, elapsed: 1.33e+01, train loss: 1.42820e-06, val loss: 2.24693e-06, min loss: 9.62854e-07\n",
      "Epoch: 1155200, elapsed: 1.14e+01, train loss: 1.07012e-06, val loss: 1.98051e-06, min loss: 9.62854e-07\n",
      "Epoch: 1155300, elapsed: 1.14e+01, train loss: 9.65179e-07, val loss: 1.83032e-06, min loss: 9.62854e-07\n",
      "Epoch: 1155400, elapsed: 1.13e+01, train loss: 9.87266e-07, val loss: 1.82917e-06, min loss: 9.62854e-07\n",
      "Epoch: 1155500, elapsed: 1.13e+01, train loss: 1.28723e-06, val loss: 2.27868e-06, min loss: 9.62854e-07\n",
      "Epoch: 1155600, elapsed: 1.14e+01, train loss: 3.27284e-06, val loss: 4.36343e-06, min loss: 9.62854e-07\n",
      "Epoch: 1155700, elapsed: 1.15e+01, train loss: 1.13467e-06, val loss: 2.11940e-06, min loss: 9.62854e-07\n",
      "Epoch: 1155800, elapsed: 1.14e+01, train loss: 1.13448e-06, val loss: 1.85009e-06, min loss: 9.62854e-07\n",
      "Epoch: 1155900, elapsed: 1.47e+01, train loss: 9.90701e-07, val loss: 1.89802e-06, min loss: 9.62854e-07\n",
      "Epoch: 1156000, elapsed: 1.15e+01, train loss: 9.87238e-07, val loss: 1.86278e-06, min loss: 9.62854e-07\n",
      "Epoch: 1156100, elapsed: 1.15e+01, train loss: 9.76519e-07, val loss: 1.83076e-06, min loss: 9.62854e-07\n",
      "Epoch: 1156200, elapsed: 1.15e+01, train loss: 1.00701e-06, val loss: 1.85033e-06, min loss: 9.62854e-07\n",
      "Epoch: 1156300, elapsed: 1.16e+01, train loss: 1.08355e-06, val loss: 2.12222e-06, min loss: 9.62854e-07\n",
      "Epoch: 1156400, elapsed: 1.17e+01, train loss: 9.65462e-07, val loss: 1.85398e-06, min loss: 9.62854e-07\n",
      "Epoch: 1156500, elapsed: 1.15e+01, train loss: 9.74683e-07, val loss: 1.82784e-06, min loss: 9.62854e-07\n",
      "Epoch: 1156600, elapsed: 1.14e+01, train loss: 9.64542e-07, val loss: 1.85166e-06, min loss: 9.62854e-07\n",
      "Epoch: 1156700, elapsed: 1.14e+01, train loss: 1.06016e-06, val loss: 1.88836e-06, min loss: 9.62854e-07\n",
      "Epoch: 1156800, elapsed: 1.15e+01, train loss: 1.21886e-06, val loss: 2.15054e-06, min loss: 9.62854e-07\n",
      "Epoch: 1156900, elapsed: 1.14e+01, train loss: 1.18314e-06, val loss: 2.04030e-06, min loss: 9.62854e-07\n",
      "Epoch: 1157000, elapsed: 1.13e+01, train loss: 1.73011e-06, val loss: 2.42418e-06, min loss: 9.62854e-07\n",
      "Epoch: 1157100, elapsed: 1.13e+01, train loss: 1.03620e-06, val loss: 1.93512e-06, min loss: 9.62854e-07\n",
      "Epoch: 1157200, elapsed: 1.12e+01, train loss: 1.30581e-06, val loss: 2.16330e-06, min loss: 9.62854e-07\n",
      "Epoch: 1157300, elapsed: 1.14e+01, train loss: 1.00735e-06, val loss: 1.83688e-06, min loss: 9.62854e-07\n",
      "Epoch: 1157400, elapsed: 1.14e+01, train loss: 1.01341e-06, val loss: 1.89570e-06, min loss: 9.62854e-07\n",
      "Epoch: 1157500, elapsed: 1.14e+01, train loss: 1.42737e-06, val loss: 2.27581e-06, min loss: 9.62854e-07\n",
      "Epoch: 1157600, elapsed: 1.13e+01, train loss: 9.78269e-07, val loss: 1.84483e-06, min loss: 9.62854e-07\n",
      "Epoch: 1157700, elapsed: 1.13e+01, train loss: 9.78360e-07, val loss: 1.82876e-06, min loss: 9.62854e-07\n",
      "Epoch: 1157800, elapsed: 1.14e+01, train loss: 1.04129e-06, val loss: 1.96626e-06, min loss: 9.62854e-07\n",
      "Epoch: 1157900, elapsed: 1.13e+01, train loss: 9.60295e-07, val loss: 1.83449e-06, min loss: 9.60295e-07\n",
      "Epoch: 1158000, elapsed: 1.13e+01, train loss: 9.65733e-07, val loss: 1.82861e-06, min loss: 9.60295e-07\n",
      "Epoch: 1158100, elapsed: 1.47e+01, train loss: 9.68517e-07, val loss: 1.84414e-06, min loss: 9.60295e-07\n",
      "Epoch: 1158200, elapsed: 1.14e+01, train loss: 9.84280e-07, val loss: 1.87212e-06, min loss: 9.60295e-07\n",
      "Epoch: 1158300, elapsed: 1.15e+01, train loss: 9.65407e-07, val loss: 1.83059e-06, min loss: 9.60295e-07\n",
      "Epoch: 1158400, elapsed: 1.14e+01, train loss: 1.34349e-06, val loss: 2.12966e-06, min loss: 9.60295e-07\n",
      "Epoch: 1158500, elapsed: 1.15e+01, train loss: 9.63655e-07, val loss: 1.85681e-06, min loss: 9.60295e-07\n",
      "Epoch: 1158600, elapsed: 1.13e+01, train loss: 9.72176e-07, val loss: 1.83814e-06, min loss: 9.60295e-07\n",
      "Epoch: 1158700, elapsed: 1.14e+01, train loss: 1.17294e-06, val loss: 2.05395e-06, min loss: 9.60295e-07\n",
      "Epoch: 1158800, elapsed: 1.14e+01, train loss: 1.33669e-06, val loss: 2.18413e-06, min loss: 9.60295e-07\n",
      "Epoch: 1158900, elapsed: 1.15e+01, train loss: 1.39551e-06, val loss: 2.41447e-06, min loss: 9.60295e-07\n",
      "Epoch: 1159000, elapsed: 1.12e+01, train loss: 9.77872e-07, val loss: 1.87989e-06, min loss: 9.60295e-07\n",
      "Epoch: 1159100, elapsed: 1.15e+01, train loss: 1.01057e-06, val loss: 1.85201e-06, min loss: 9.60295e-07\n",
      "Epoch: 1159200, elapsed: 1.13e+01, train loss: 9.83042e-07, val loss: 1.87362e-06, min loss: 9.60295e-07\n",
      "Epoch: 1159300, elapsed: 1.13e+01, train loss: 9.59907e-07, val loss: 1.83342e-06, min loss: 9.59907e-07\n",
      "Epoch: 1159400, elapsed: 1.13e+01, train loss: 9.77633e-07, val loss: 1.85839e-06, min loss: 9.59907e-07\n",
      "Epoch: 1159500, elapsed: 1.12e+01, train loss: 1.22789e-06, val loss: 2.08720e-06, min loss: 9.59907e-07\n",
      "Epoch: 1159600, elapsed: 1.13e+01, train loss: 1.10097e-06, val loss: 1.86205e-06, min loss: 9.59907e-07\n",
      "Epoch: 1159700, elapsed: 1.11e+01, train loss: 1.34197e-06, val loss: 2.35443e-06, min loss: 9.59907e-07\n",
      "Epoch: 1159800, elapsed: 1.14e+01, train loss: 1.45202e-06, val loss: 1.97497e-06, min loss: 9.59907e-07\n",
      "Epoch: 1159900, elapsed: 1.14e+01, train loss: 1.19214e-06, val loss: 1.86971e-06, min loss: 9.59907e-07\n",
      "Epoch: 1160000, elapsed: 1.14e+01, train loss: 1.11501e-06, val loss: 1.99613e-06, min loss: 9.59907e-07\n",
      "Epoch: 1160100, elapsed: 1.33e+01, train loss: 1.04940e-06, val loss: 1.90954e-06, min loss: 9.59907e-07\n",
      "Epoch: 1160200, elapsed: 1.48e+01, train loss: 2.98317e-06, val loss: 2.98484e-06, min loss: 9.59907e-07\n",
      "Epoch: 1160300, elapsed: 1.13e+01, train loss: 1.13328e-06, val loss: 2.10078e-06, min loss: 9.59907e-07\n",
      "Epoch: 1160400, elapsed: 1.14e+01, train loss: 9.59912e-07, val loss: 1.83253e-06, min loss: 9.59907e-07\n",
      "Epoch: 1160500, elapsed: 1.16e+01, train loss: 9.59581e-07, val loss: 1.82775e-06, min loss: 9.59581e-07\n",
      "Epoch: 1160600, elapsed: 1.16e+01, train loss: 1.07510e-06, val loss: 1.84962e-06, min loss: 9.59581e-07\n",
      "Epoch: 1160700, elapsed: 1.14e+01, train loss: 1.09968e-06, val loss: 1.87273e-06, min loss: 9.59581e-07\n",
      "Epoch: 1160800, elapsed: 1.14e+01, train loss: 1.48498e-06, val loss: 2.39078e-06, min loss: 9.59581e-07\n",
      "Epoch: 1160900, elapsed: 1.15e+01, train loss: 1.11051e-06, val loss: 2.10898e-06, min loss: 9.59581e-07\n",
      "Epoch: 1161000, elapsed: 1.12e+01, train loss: 9.84460e-07, val loss: 1.86148e-06, min loss: 9.59581e-07\n",
      "Epoch: 1161100, elapsed: 1.12e+01, train loss: 9.66937e-07, val loss: 1.82685e-06, min loss: 9.59581e-07\n",
      "Epoch: 1161200, elapsed: 1.13e+01, train loss: 9.83866e-07, val loss: 1.88373e-06, min loss: 9.59581e-07\n",
      "Epoch: 1161300, elapsed: 1.14e+01, train loss: 9.80008e-07, val loss: 1.91475e-06, min loss: 9.59581e-07\n",
      "Epoch: 1161400, elapsed: 1.13e+01, train loss: 1.14803e-06, val loss: 2.09410e-06, min loss: 9.59581e-07\n",
      "Epoch: 1161500, elapsed: 1.14e+01, train loss: 1.28201e-06, val loss: 2.31000e-06, min loss: 9.59581e-07\n",
      "Epoch: 1161600, elapsed: 1.13e+01, train loss: 1.44781e-06, val loss: 2.37663e-06, min loss: 9.59581e-07\n",
      "Epoch: 1161700, elapsed: 1.14e+01, train loss: 1.24169e-06, val loss: 2.21054e-06, min loss: 9.59581e-07\n",
      "Epoch: 1161800, elapsed: 1.15e+01, train loss: 9.87581e-07, val loss: 1.88907e-06, min loss: 9.59581e-07\n",
      "Epoch: 1161900, elapsed: 1.14e+01, train loss: 1.00229e-06, val loss: 1.89420e-06, min loss: 9.59581e-07\n",
      "Epoch: 1162000, elapsed: 1.14e+01, train loss: 1.14226e-06, val loss: 2.10423e-06, min loss: 9.59581e-07\n",
      "Epoch: 1162100, elapsed: 1.15e+01, train loss: 9.58273e-07, val loss: 1.82450e-06, min loss: 9.58273e-07\n",
      "Epoch: 1162200, elapsed: 1.14e+01, train loss: 9.62224e-07, val loss: 1.84605e-06, min loss: 9.58273e-07\n",
      "Epoch: 1162300, elapsed: 1.46e+01, train loss: 9.72645e-07, val loss: 1.86343e-06, min loss: 9.58273e-07\n",
      "Epoch: 1162400, elapsed: 1.14e+01, train loss: 1.34356e-06, val loss: 2.46671e-06, min loss: 9.58273e-07\n",
      "Epoch: 1162500, elapsed: 1.15e+01, train loss: 9.57058e-07, val loss: 1.82475e-06, min loss: 9.57058e-07\n",
      "Epoch: 1162600, elapsed: 1.15e+01, train loss: 1.12944e-06, val loss: 2.00663e-06, min loss: 9.57058e-07\n",
      "Epoch: 1162700, elapsed: 1.16e+01, train loss: 9.56807e-07, val loss: 1.82787e-06, min loss: 9.56807e-07\n",
      "Epoch: 1162800, elapsed: 1.14e+01, train loss: 9.59987e-07, val loss: 1.82183e-06, min loss: 9.56807e-07\n",
      "Epoch: 1162900, elapsed: 1.14e+01, train loss: 9.77097e-07, val loss: 1.82611e-06, min loss: 9.56807e-07\n",
      "Epoch: 1163000, elapsed: 1.13e+01, train loss: 2.17976e-06, val loss: 2.92405e-06, min loss: 9.56807e-07\n",
      "Epoch: 1163100, elapsed: 1.15e+01, train loss: 1.04296e-06, val loss: 1.97777e-06, min loss: 9.56807e-07\n",
      "Epoch: 1163200, elapsed: 1.14e+01, train loss: 9.88111e-07, val loss: 1.89659e-06, min loss: 9.56807e-07\n",
      "Epoch: 1163300, elapsed: 1.13e+01, train loss: 1.41676e-06, val loss: 2.09093e-06, min loss: 9.56807e-07\n",
      "Epoch: 1163400, elapsed: 1.14e+01, train loss: 1.20229e-06, val loss: 1.99009e-06, min loss: 9.56807e-07\n",
      "Epoch: 1163500, elapsed: 1.14e+01, train loss: 1.15358e-06, val loss: 2.04971e-06, min loss: 9.56807e-07\n",
      "Epoch: 1163600, elapsed: 1.13e+01, train loss: 1.17176e-06, val loss: 2.11320e-06, min loss: 9.56807e-07\n",
      "Epoch: 1163700, elapsed: 1.14e+01, train loss: 9.56356e-07, val loss: 1.82979e-06, min loss: 9.56356e-07\n",
      "Epoch: 1163800, elapsed: 1.13e+01, train loss: 1.19400e-06, val loss: 2.05718e-06, min loss: 9.56356e-07\n",
      "Epoch: 1163900, elapsed: 1.14e+01, train loss: 1.13819e-06, val loss: 1.85874e-06, min loss: 9.56356e-07\n",
      "Epoch: 1164000, elapsed: 1.14e+01, train loss: 1.10434e-06, val loss: 2.04024e-06, min loss: 9.56356e-07\n",
      "Epoch: 1164100, elapsed: 1.13e+01, train loss: 9.67464e-07, val loss: 1.84020e-06, min loss: 9.56356e-07\n",
      "Epoch: 1164200, elapsed: 1.13e+01, train loss: 1.22722e-06, val loss: 2.10449e-06, min loss: 9.56356e-07\n",
      "Epoch: 1164300, elapsed: 1.15e+01, train loss: 1.03495e-06, val loss: 1.96820e-06, min loss: 9.56356e-07\n",
      "Epoch: 1164400, elapsed: 1.12e+01, train loss: 9.74514e-07, val loss: 1.82501e-06, min loss: 9.56356e-07\n",
      "Epoch: 1164500, elapsed: 1.45e+01, train loss: 9.79948e-07, val loss: 1.84976e-06, min loss: 9.56356e-07\n",
      "Epoch: 1164600, elapsed: 1.15e+01, train loss: 4.84022e-06, val loss: 4.19388e-06, min loss: 9.56356e-07\n",
      "Epoch: 1164700, elapsed: 1.16e+01, train loss: 9.55020e-07, val loss: 1.82612e-06, min loss: 9.55020e-07\n",
      "Epoch: 1164800, elapsed: 1.14e+01, train loss: 9.63516e-07, val loss: 1.84210e-06, min loss: 9.55020e-07\n",
      "Epoch: 1164900, elapsed: 1.15e+01, train loss: 1.16970e-06, val loss: 2.29736e-06, min loss: 9.55020e-07\n",
      "Epoch: 1165000, elapsed: 1.14e+01, train loss: 9.55207e-07, val loss: 1.82286e-06, min loss: 9.55020e-07\n",
      "Epoch: 1165100, elapsed: 1.35e+01, train loss: 1.04334e-06, val loss: 1.95536e-06, min loss: 9.55020e-07\n",
      "Epoch: 1165200, elapsed: 1.13e+01, train loss: 3.23711e-06, val loss: 3.84736e-06, min loss: 9.55020e-07\n",
      "Epoch: 1165300, elapsed: 1.14e+01, train loss: 1.60259e-06, val loss: 2.39254e-06, min loss: 9.55020e-07\n",
      "Epoch: 1165400, elapsed: 1.15e+01, train loss: 1.24069e-06, val loss: 2.13498e-06, min loss: 9.55020e-07\n",
      "Epoch: 1165500, elapsed: 1.14e+01, train loss: 1.01040e-06, val loss: 2.00339e-06, min loss: 9.55020e-07\n",
      "Epoch: 1165600, elapsed: 1.13e+01, train loss: 1.00113e-06, val loss: 1.89211e-06, min loss: 9.55020e-07\n",
      "Epoch: 1165700, elapsed: 1.14e+01, train loss: 9.54871e-07, val loss: 1.82957e-06, min loss: 9.54871e-07\n",
      "Epoch: 1165800, elapsed: 1.13e+01, train loss: 9.62280e-07, val loss: 1.84087e-06, min loss: 9.54871e-07\n",
      "Epoch: 1165900, elapsed: 1.13e+01, train loss: 1.05984e-06, val loss: 1.96169e-06, min loss: 9.54871e-07\n",
      "Epoch: 1166000, elapsed: 1.14e+01, train loss: 1.01321e-06, val loss: 1.87103e-06, min loss: 9.54871e-07\n",
      "Epoch: 1166100, elapsed: 1.13e+01, train loss: 1.84397e-06, val loss: 2.72139e-06, min loss: 9.54871e-07\n",
      "Epoch: 1166200, elapsed: 1.14e+01, train loss: 1.00919e-06, val loss: 1.88904e-06, min loss: 9.54871e-07\n",
      "Epoch: 1166300, elapsed: 1.15e+01, train loss: 9.75484e-07, val loss: 1.85203e-06, min loss: 9.54871e-07\n",
      "Epoch: 1166400, elapsed: 1.13e+01, train loss: 9.54621e-07, val loss: 1.82198e-06, min loss: 9.54621e-07\n",
      "Epoch: 1166500, elapsed: 1.15e+01, train loss: 9.69860e-07, val loss: 1.84377e-06, min loss: 9.54621e-07\n",
      "Epoch: 1166600, elapsed: 1.47e+01, train loss: 1.00159e-06, val loss: 1.86397e-06, min loss: 9.54621e-07\n",
      "Epoch: 1166700, elapsed: 1.16e+01, train loss: 9.95337e-07, val loss: 1.86498e-06, min loss: 9.54621e-07\n",
      "Epoch: 1166800, elapsed: 1.16e+01, train loss: 1.11105e-06, val loss: 1.97793e-06, min loss: 9.54621e-07\n",
      "Epoch: 1166900, elapsed: 1.15e+01, train loss: 1.03955e-06, val loss: 1.85342e-06, min loss: 9.54621e-07\n",
      "Epoch: 1167000, elapsed: 1.15e+01, train loss: 1.03948e-06, val loss: 1.93741e-06, min loss: 9.54621e-07\n",
      "Epoch: 1167100, elapsed: 1.14e+01, train loss: 2.47269e-06, val loss: 3.22046e-06, min loss: 9.54621e-07\n",
      "Epoch: 1167200, elapsed: 1.15e+01, train loss: 1.21879e-06, val loss: 2.10158e-06, min loss: 9.54621e-07\n",
      "Epoch: 1167300, elapsed: 1.14e+01, train loss: 1.34265e-06, val loss: 2.08595e-06, min loss: 9.54621e-07\n",
      "Epoch: 1167400, elapsed: 1.14e+01, train loss: 3.30185e-06, val loss: 4.91334e-06, min loss: 9.54621e-07\n",
      "Epoch: 1167500, elapsed: 1.15e+01, train loss: 9.53717e-07, val loss: 1.82692e-06, min loss: 9.53717e-07\n",
      "Epoch: 1167600, elapsed: 1.15e+01, train loss: 9.59467e-07, val loss: 1.82120e-06, min loss: 9.53717e-07\n",
      "Epoch: 1167700, elapsed: 1.14e+01, train loss: 1.16521e-06, val loss: 1.86847e-06, min loss: 9.53717e-07\n",
      "Epoch: 1167800, elapsed: 1.15e+01, train loss: 1.99227e-06, val loss: 2.16311e-06, min loss: 9.53717e-07\n",
      "Epoch: 1167900, elapsed: 1.12e+01, train loss: 1.27407e-06, val loss: 2.18974e-06, min loss: 9.53717e-07\n",
      "Epoch: 1168000, elapsed: 1.13e+01, train loss: 1.14553e-06, val loss: 2.15915e-06, min loss: 9.53717e-07\n",
      "Epoch: 1168100, elapsed: 1.13e+01, train loss: 9.56158e-07, val loss: 1.83488e-06, min loss: 9.53717e-07\n",
      "Epoch: 1168200, elapsed: 1.15e+01, train loss: 1.05028e-06, val loss: 1.87933e-06, min loss: 9.53717e-07\n",
      "Epoch: 1168300, elapsed: 1.12e+01, train loss: 9.62433e-07, val loss: 1.84893e-06, min loss: 9.53717e-07\n",
      "Epoch: 1168400, elapsed: 1.12e+01, train loss: 9.53332e-07, val loss: 1.81772e-06, min loss: 9.53332e-07\n",
      "Epoch: 1168500, elapsed: 1.12e+01, train loss: 9.62032e-07, val loss: 1.83984e-06, min loss: 9.53332e-07\n",
      "Epoch: 1168600, elapsed: 1.13e+01, train loss: 1.10167e-06, val loss: 1.86832e-06, min loss: 9.53332e-07\n",
      "Epoch: 1168700, elapsed: 1.13e+01, train loss: 9.52203e-07, val loss: 1.82383e-06, min loss: 9.52203e-07\n",
      "Epoch: 1168800, elapsed: 1.47e+01, train loss: 9.60757e-07, val loss: 1.82858e-06, min loss: 9.52203e-07\n",
      "Epoch: 1168900, elapsed: 1.13e+01, train loss: 1.59412e-06, val loss: 2.36487e-06, min loss: 9.52203e-07\n",
      "Epoch: 1169000, elapsed: 1.13e+01, train loss: 1.00898e-06, val loss: 1.93207e-06, min loss: 9.52203e-07\n",
      "Epoch: 1169100, elapsed: 1.13e+01, train loss: 9.52040e-07, val loss: 1.82528e-06, min loss: 9.52040e-07\n",
      "Epoch: 1169200, elapsed: 1.14e+01, train loss: 2.23678e-06, val loss: 2.64592e-06, min loss: 9.52040e-07\n",
      "Epoch: 1169300, elapsed: 1.13e+01, train loss: 1.05873e-06, val loss: 1.96365e-06, min loss: 9.52040e-07\n",
      "Epoch: 1169400, elapsed: 1.15e+01, train loss: 9.54410e-07, val loss: 1.83446e-06, min loss: 9.52040e-07\n",
      "Epoch: 1169500, elapsed: 1.13e+01, train loss: 9.58239e-07, val loss: 1.83517e-06, min loss: 9.52040e-07\n",
      "Epoch: 1169600, elapsed: 1.15e+01, train loss: 1.16510e-06, val loss: 1.97887e-06, min loss: 9.52040e-07\n",
      "Epoch: 1169700, elapsed: 1.14e+01, train loss: 9.52771e-07, val loss: 1.82353e-06, min loss: 9.52040e-07\n",
      "Epoch: 1169800, elapsed: 1.14e+01, train loss: 2.24210e-06, val loss: 3.42193e-06, min loss: 9.52040e-07\n",
      "Epoch: 1169900, elapsed: 1.12e+01, train loss: 9.51760e-07, val loss: 1.82213e-06, min loss: 9.51760e-07\n",
      "Epoch: 1170000, elapsed: 1.14e+01, train loss: 1.02547e-06, val loss: 1.83957e-06, min loss: 9.51760e-07\n",
      "Epoch: 1170100, elapsed: 1.34e+01, train loss: 1.04138e-06, val loss: 1.90751e-06, min loss: 9.51760e-07\n",
      "Epoch: 1170200, elapsed: 1.13e+01, train loss: 1.07307e-06, val loss: 1.90117e-06, min loss: 9.51760e-07\n",
      "Epoch: 1170300, elapsed: 1.14e+01, train loss: 9.77455e-07, val loss: 1.86291e-06, min loss: 9.51760e-07\n",
      "Epoch: 1170400, elapsed: 1.13e+01, train loss: 1.13774e-06, val loss: 1.90553e-06, min loss: 9.51760e-07\n",
      "Epoch: 1170500, elapsed: 1.13e+01, train loss: 1.03386e-06, val loss: 1.94627e-06, min loss: 9.51760e-07\n",
      "Epoch: 1170600, elapsed: 1.14e+01, train loss: 1.14454e-06, val loss: 2.00026e-06, min loss: 9.51760e-07\n",
      "Epoch: 1170700, elapsed: 1.12e+01, train loss: 4.92846e-06, val loss: 6.21790e-06, min loss: 9.51760e-07\n",
      "Epoch: 1170800, elapsed: 1.12e+01, train loss: 9.51477e-07, val loss: 1.82506e-06, min loss: 9.51477e-07\n",
      "Epoch: 1170900, elapsed: 1.46e+01, train loss: 9.53264e-07, val loss: 1.81735e-06, min loss: 9.51477e-07\n",
      "Epoch: 1171000, elapsed: 1.15e+01, train loss: 1.51934e-06, val loss: 2.17337e-06, min loss: 9.51477e-07\n",
      "Epoch: 1171100, elapsed: 1.14e+01, train loss: 9.93055e-07, val loss: 1.85325e-06, min loss: 9.51477e-07\n",
      "Epoch: 1171200, elapsed: 1.15e+01, train loss: 9.50782e-07, val loss: 1.82662e-06, min loss: 9.50782e-07\n",
      "Epoch: 1171300, elapsed: 1.15e+01, train loss: 1.02818e-06, val loss: 1.85689e-06, min loss: 9.50782e-07\n",
      "Epoch: 1171400, elapsed: 1.16e+01, train loss: 1.87585e-06, val loss: 2.41997e-06, min loss: 9.50782e-07\n",
      "Epoch: 1171500, elapsed: 1.15e+01, train loss: 1.33436e-06, val loss: 2.22500e-06, min loss: 9.50782e-07\n",
      "Epoch: 1171600, elapsed: 1.17e+01, train loss: 9.50230e-07, val loss: 1.82220e-06, min loss: 9.50230e-07\n",
      "Epoch: 1171700, elapsed: 1.14e+01, train loss: 9.94793e-07, val loss: 1.88659e-06, min loss: 9.50230e-07\n",
      "Epoch: 1171800, elapsed: 1.15e+01, train loss: 1.83756e-06, val loss: 2.74569e-06, min loss: 9.50230e-07\n",
      "Epoch: 1171900, elapsed: 1.15e+01, train loss: 4.26763e-06, val loss: 4.97400e-06, min loss: 9.50230e-07\n",
      "Epoch: 1172000, elapsed: 1.13e+01, train loss: 1.03673e-06, val loss: 1.89919e-06, min loss: 9.50230e-07\n",
      "Epoch: 1172100, elapsed: 1.13e+01, train loss: 1.14913e-06, val loss: 2.05497e-06, min loss: 9.50230e-07\n",
      "Epoch: 1172200, elapsed: 1.15e+01, train loss: 9.56819e-07, val loss: 1.83482e-06, min loss: 9.50230e-07\n",
      "Epoch: 1172300, elapsed: 1.13e+01, train loss: 9.81387e-07, val loss: 1.83376e-06, min loss: 9.50230e-07\n",
      "Epoch: 1172400, elapsed: 1.12e+01, train loss: 1.01741e-06, val loss: 1.89617e-06, min loss: 9.50230e-07\n",
      "Epoch: 1172500, elapsed: 1.13e+01, train loss: 1.01841e-06, val loss: 1.87984e-06, min loss: 9.50230e-07\n",
      "Epoch: 1172600, elapsed: 1.12e+01, train loss: 9.49818e-07, val loss: 1.82101e-06, min loss: 9.49818e-07\n",
      "Epoch: 1172700, elapsed: 1.12e+01, train loss: 1.07319e-06, val loss: 1.89385e-06, min loss: 9.49818e-07\n",
      "Epoch: 1172800, elapsed: 1.14e+01, train loss: 1.44027e-06, val loss: 2.23180e-06, min loss: 9.49818e-07\n",
      "Epoch: 1172900, elapsed: 1.13e+01, train loss: 1.21533e-06, val loss: 2.25524e-06, min loss: 9.49818e-07\n",
      "Epoch: 1173000, elapsed: 1.15e+01, train loss: 9.49665e-07, val loss: 1.81796e-06, min loss: 9.49665e-07\n",
      "Epoch: 1173100, elapsed: 1.46e+01, train loss: 9.66239e-07, val loss: 1.82194e-06, min loss: 9.49665e-07\n",
      "Epoch: 1173200, elapsed: 1.15e+01, train loss: 9.51879e-07, val loss: 1.81833e-06, min loss: 9.49665e-07\n",
      "Epoch: 1173300, elapsed: 1.14e+01, train loss: 9.51454e-07, val loss: 1.82540e-06, min loss: 9.49665e-07\n",
      "Epoch: 1173400, elapsed: 1.13e+01, train loss: 1.08360e-06, val loss: 1.95679e-06, min loss: 9.49665e-07\n",
      "Epoch: 1173500, elapsed: 1.14e+01, train loss: 9.78071e-07, val loss: 2.05089e-06, min loss: 9.49665e-07\n",
      "Epoch: 1173600, elapsed: 1.13e+01, train loss: 9.48795e-07, val loss: 1.82227e-06, min loss: 9.48795e-07\n",
      "Epoch: 1173700, elapsed: 1.14e+01, train loss: 9.90391e-07, val loss: 1.89078e-06, min loss: 9.48795e-07\n",
      "Epoch: 1173800, elapsed: 1.14e+01, train loss: 2.78517e-06, val loss: 4.32436e-06, min loss: 9.48795e-07\n",
      "Epoch: 1173900, elapsed: 1.13e+01, train loss: 9.48901e-07, val loss: 1.82055e-06, min loss: 9.48795e-07\n",
      "Epoch: 1174000, elapsed: 1.15e+01, train loss: 1.13397e-06, val loss: 1.97245e-06, min loss: 9.48795e-07\n",
      "Epoch: 1174100, elapsed: 1.14e+01, train loss: 1.38966e-06, val loss: 2.51445e-06, min loss: 9.48795e-07\n",
      "Epoch: 1174200, elapsed: 1.13e+01, train loss: 9.50286e-07, val loss: 1.83228e-06, min loss: 9.48795e-07\n",
      "Epoch: 1174300, elapsed: 1.15e+01, train loss: 9.61337e-07, val loss: 1.83311e-06, min loss: 9.48795e-07\n",
      "Epoch: 1174400, elapsed: 1.16e+01, train loss: 9.53910e-07, val loss: 1.83834e-06, min loss: 9.48795e-07\n",
      "Epoch: 1174500, elapsed: 1.14e+01, train loss: 1.01944e-06, val loss: 1.86647e-06, min loss: 9.48795e-07\n",
      "Epoch: 1174600, elapsed: 1.14e+01, train loss: 9.49090e-07, val loss: 1.82426e-06, min loss: 9.48795e-07\n",
      "Epoch: 1174700, elapsed: 1.13e+01, train loss: 9.53612e-07, val loss: 1.83470e-06, min loss: 9.48795e-07\n",
      "Epoch: 1174800, elapsed: 1.13e+01, train loss: 9.94251e-07, val loss: 1.82073e-06, min loss: 9.48795e-07\n",
      "Epoch: 1174900, elapsed: 1.15e+01, train loss: 1.91688e-06, val loss: 2.93790e-06, min loss: 9.48795e-07\n",
      "Epoch: 1175000, elapsed: 1.14e+01, train loss: 9.94659e-07, val loss: 1.85169e-06, min loss: 9.48795e-07\n",
      "Epoch: 1175100, elapsed: 1.34e+01, train loss: 9.57399e-07, val loss: 1.82063e-06, min loss: 9.48795e-07\n",
      "Epoch: 1175200, elapsed: 1.47e+01, train loss: 1.29987e-06, val loss: 2.06194e-06, min loss: 9.48795e-07\n",
      "Epoch: 1175300, elapsed: 1.16e+01, train loss: 3.08050e-06, val loss: 3.38877e-06, min loss: 9.48795e-07\n",
      "Epoch: 1175400, elapsed: 1.16e+01, train loss: 1.16576e-06, val loss: 2.09027e-06, min loss: 9.48795e-07\n",
      "Epoch: 1175500, elapsed: 1.14e+01, train loss: 1.24271e-06, val loss: 1.98447e-06, min loss: 9.48795e-07\n",
      "Epoch: 1175600, elapsed: 1.16e+01, train loss: 1.86114e-06, val loss: 2.76365e-06, min loss: 9.48795e-07\n",
      "Epoch: 1175700, elapsed: 1.16e+01, train loss: 5.44202e-06, val loss: 6.71251e-06, min loss: 9.48795e-07\n",
      "Epoch: 1175800, elapsed: 1.15e+01, train loss: 9.47654e-07, val loss: 1.82025e-06, min loss: 9.47654e-07\n",
      "Epoch: 1175900, elapsed: 1.14e+01, train loss: 9.67151e-07, val loss: 1.85350e-06, min loss: 9.47654e-07\n",
      "Epoch: 1176000, elapsed: 1.14e+01, train loss: 9.60894e-07, val loss: 1.82092e-06, min loss: 9.47654e-07\n",
      "Epoch: 1176100, elapsed: 1.16e+01, train loss: 1.17367e-06, val loss: 1.94830e-06, min loss: 9.47654e-07\n",
      "Epoch: 1176200, elapsed: 1.13e+01, train loss: 1.63037e-06, val loss: 2.21402e-06, min loss: 9.47654e-07\n",
      "Epoch: 1176300, elapsed: 1.13e+01, train loss: 1.05894e-06, val loss: 1.97696e-06, min loss: 9.47654e-07\n",
      "Epoch: 1176400, elapsed: 1.15e+01, train loss: 1.40718e-06, val loss: 2.32397e-06, min loss: 9.47654e-07\n",
      "Epoch: 1176500, elapsed: 1.16e+01, train loss: 1.10874e-06, val loss: 2.08393e-06, min loss: 9.47654e-07\n",
      "Epoch: 1176600, elapsed: 1.16e+01, train loss: 9.62837e-07, val loss: 1.80818e-06, min loss: 9.47654e-07\n",
      "Epoch: 1176700, elapsed: 1.17e+01, train loss: 9.60838e-07, val loss: 1.82408e-06, min loss: 9.47654e-07\n",
      "Epoch: 1176800, elapsed: 1.15e+01, train loss: 9.53712e-07, val loss: 1.82279e-06, min loss: 9.47654e-07\n",
      "Epoch: 1176900, elapsed: 1.14e+01, train loss: 3.02668e-06, val loss: 3.80255e-06, min loss: 9.47654e-07\n",
      "Epoch: 1177000, elapsed: 1.15e+01, train loss: 9.46550e-07, val loss: 1.82080e-06, min loss: 9.46550e-07\n",
      "Epoch: 1177100, elapsed: 1.14e+01, train loss: 9.71746e-07, val loss: 1.85061e-06, min loss: 9.46550e-07\n",
      "Epoch: 1177200, elapsed: 1.14e+01, train loss: 9.48463e-07, val loss: 1.82037e-06, min loss: 9.46550e-07\n",
      "Epoch: 1177300, elapsed: 1.14e+01, train loss: 9.49858e-07, val loss: 1.81432e-06, min loss: 9.46550e-07\n",
      "Epoch: 1177400, elapsed: 1.47e+01, train loss: 9.51823e-07, val loss: 1.83507e-06, min loss: 9.46550e-07\n",
      "Epoch: 1177500, elapsed: 1.16e+01, train loss: 9.56752e-07, val loss: 1.82191e-06, min loss: 9.46550e-07\n",
      "Epoch: 1177600, elapsed: 1.14e+01, train loss: 1.00379e-06, val loss: 1.85629e-06, min loss: 9.46550e-07\n",
      "Epoch: 1177700, elapsed: 1.15e+01, train loss: 9.65883e-07, val loss: 1.86633e-06, min loss: 9.46550e-07\n",
      "Epoch: 1177800, elapsed: 1.16e+01, train loss: 1.18302e-06, val loss: 2.07191e-06, min loss: 9.46550e-07\n",
      "Epoch: 1177900, elapsed: 1.14e+01, train loss: 1.22715e-06, val loss: 2.13226e-06, min loss: 9.46550e-07\n",
      "Epoch: 1178000, elapsed: 1.15e+01, train loss: 9.46262e-07, val loss: 1.81617e-06, min loss: 9.46262e-07\n",
      "Epoch: 1178100, elapsed: 1.15e+01, train loss: 1.76988e-06, val loss: 2.12625e-06, min loss: 9.46262e-07\n",
      "Epoch: 1178200, elapsed: 1.15e+01, train loss: 1.11822e-06, val loss: 2.04556e-06, min loss: 9.46262e-07\n",
      "Epoch: 1178300, elapsed: 1.16e+01, train loss: 1.19227e-06, val loss: 2.01856e-06, min loss: 9.46262e-07\n",
      "Epoch: 1178400, elapsed: 1.14e+01, train loss: 1.71612e-06, val loss: 2.66480e-06, min loss: 9.46262e-07\n",
      "Epoch: 1178500, elapsed: 1.14e+01, train loss: 1.01074e-06, val loss: 1.88216e-06, min loss: 9.46262e-07\n",
      "Epoch: 1178600, elapsed: 1.14e+01, train loss: 9.48091e-07, val loss: 1.82157e-06, min loss: 9.46262e-07\n",
      "Epoch: 1178700, elapsed: 1.15e+01, train loss: 9.53505e-07, val loss: 1.83572e-06, min loss: 9.46262e-07\n",
      "Epoch: 1178800, elapsed: 1.15e+01, train loss: 1.02494e-06, val loss: 1.96407e-06, min loss: 9.46262e-07\n",
      "Epoch: 1178900, elapsed: 1.15e+01, train loss: 9.48851e-07, val loss: 1.81282e-06, min loss: 9.46262e-07\n",
      "Epoch: 1179000, elapsed: 1.11e+01, train loss: 1.03264e-06, val loss: 1.87442e-06, min loss: 9.46262e-07\n",
      "Epoch: 1179100, elapsed: 1.12e+01, train loss: 3.09689e-06, val loss: 3.13260e-06, min loss: 9.46262e-07\n",
      "Epoch: 1179200, elapsed: 1.13e+01, train loss: 2.62268e-06, val loss: 3.81995e-06, min loss: 9.46262e-07\n",
      "Epoch: 1179300, elapsed: 1.15e+01, train loss: 9.45364e-07, val loss: 1.81914e-06, min loss: 9.45364e-07\n",
      "Epoch: 1179400, elapsed: 1.13e+01, train loss: 9.45341e-07, val loss: 1.81730e-06, min loss: 9.45341e-07\n",
      "Epoch: 1179500, elapsed: 1.12e+01, train loss: 1.15865e-06, val loss: 1.94122e-06, min loss: 9.45341e-07\n",
      "Epoch: 1179600, elapsed: 1.48e+01, train loss: 1.80087e-06, val loss: 2.22574e-06, min loss: 9.45341e-07\n",
      "Epoch: 1179700, elapsed: 1.16e+01, train loss: 9.81369e-07, val loss: 1.86469e-06, min loss: 9.45341e-07\n",
      "Epoch: 1179800, elapsed: 1.14e+01, train loss: 1.18349e-06, val loss: 1.94853e-06, min loss: 9.45341e-07\n",
      "Epoch: 1179900, elapsed: 1.15e+01, train loss: 1.15001e-06, val loss: 2.08296e-06, min loss: 9.45341e-07\n",
      "Epoch: 1180000, elapsed: 1.15e+01, train loss: 1.45878e-06, val loss: 2.13094e-06, min loss: 9.45341e-07\n",
      "Epoch: 1180100, elapsed: 1.37e+01, train loss: 1.18906e-06, val loss: 1.98673e-06, min loss: 9.45341e-07\n",
      "Epoch: 1180200, elapsed: 1.15e+01, train loss: 1.11479e-06, val loss: 2.12526e-06, min loss: 9.45341e-07\n",
      "Epoch: 1180300, elapsed: 1.14e+01, train loss: 1.15224e-06, val loss: 1.87325e-06, min loss: 9.45341e-07\n",
      "Epoch: 1180400, elapsed: 1.14e+01, train loss: 1.81890e-06, val loss: 2.75760e-06, min loss: 9.45341e-07\n",
      "Epoch: 1180500, elapsed: 1.14e+01, train loss: 1.78882e-06, val loss: 2.97543e-06, min loss: 9.45341e-07\n",
      "Epoch: 1180600, elapsed: 1.14e+01, train loss: 1.02397e-06, val loss: 1.90206e-06, min loss: 9.45341e-07\n",
      "Epoch: 1180700, elapsed: 1.13e+01, train loss: 9.50057e-07, val loss: 1.81963e-06, min loss: 9.45341e-07\n",
      "Epoch: 1180800, elapsed: 1.14e+01, train loss: 9.45404e-07, val loss: 1.81088e-06, min loss: 9.45341e-07\n",
      "Epoch: 1180900, elapsed: 1.15e+01, train loss: 9.50780e-07, val loss: 1.81796e-06, min loss: 9.45341e-07\n",
      "Epoch: 1181000, elapsed: 1.15e+01, train loss: 9.91355e-07, val loss: 1.86539e-06, min loss: 9.45341e-07\n",
      "Epoch: 1181100, elapsed: 1.13e+01, train loss: 1.65994e-06, val loss: 2.76274e-06, min loss: 9.45341e-07\n",
      "Epoch: 1181200, elapsed: 1.13e+01, train loss: 9.43975e-07, val loss: 1.81671e-06, min loss: 9.43975e-07\n",
      "Epoch: 1181300, elapsed: 1.13e+01, train loss: 1.02478e-06, val loss: 1.93822e-06, min loss: 9.43975e-07\n",
      "Epoch: 1181400, elapsed: 1.12e+01, train loss: 9.47885e-07, val loss: 1.81963e-06, min loss: 9.43975e-07\n",
      "Epoch: 1181500, elapsed: 1.13e+01, train loss: 9.44574e-07, val loss: 1.81977e-06, min loss: 9.43975e-07\n",
      "Epoch: 1181600, elapsed: 1.14e+01, train loss: 9.83624e-07, val loss: 1.81444e-06, min loss: 9.43975e-07\n",
      "Epoch: 1181700, elapsed: 1.46e+01, train loss: 1.10288e-06, val loss: 1.99889e-06, min loss: 9.43975e-07\n",
      "Epoch: 1181800, elapsed: 1.16e+01, train loss: 9.43989e-07, val loss: 1.81645e-06, min loss: 9.43975e-07\n",
      "Epoch: 1181900, elapsed: 1.14e+01, train loss: 9.46077e-07, val loss: 1.81262e-06, min loss: 9.43975e-07\n",
      "Epoch: 1182000, elapsed: 1.15e+01, train loss: 1.57887e-06, val loss: 3.65210e-06, min loss: 9.43975e-07\n",
      "Epoch: 1182100, elapsed: 1.15e+01, train loss: 9.43245e-07, val loss: 1.81779e-06, min loss: 9.43245e-07\n",
      "Epoch: 1182200, elapsed: 1.14e+01, train loss: 1.20913e-06, val loss: 2.05486e-06, min loss: 9.43245e-07\n",
      "Epoch: 1182300, elapsed: 1.15e+01, train loss: 9.43877e-07, val loss: 1.82027e-06, min loss: 9.43245e-07\n",
      "Epoch: 1182400, elapsed: 1.15e+01, train loss: 9.43273e-07, val loss: 1.81393e-06, min loss: 9.43245e-07\n",
      "Epoch: 1182500, elapsed: 1.14e+01, train loss: 1.19139e-06, val loss: 2.02342e-06, min loss: 9.43245e-07\n",
      "Epoch: 1182600, elapsed: 1.14e+01, train loss: 1.14139e-06, val loss: 1.83558e-06, min loss: 9.43245e-07\n",
      "Epoch: 1182700, elapsed: 1.14e+01, train loss: 2.60097e-06, val loss: 4.05280e-06, min loss: 9.43245e-07\n",
      "Epoch: 1182800, elapsed: 1.15e+01, train loss: 9.43004e-07, val loss: 1.81733e-06, min loss: 9.43004e-07\n",
      "Epoch: 1182900, elapsed: 1.14e+01, train loss: 9.80310e-07, val loss: 1.84451e-06, min loss: 9.43004e-07\n",
      "Epoch: 1183000, elapsed: 1.14e+01, train loss: 1.55286e-06, val loss: 2.95924e-06, min loss: 9.43004e-07\n",
      "Epoch: 1183100, elapsed: 1.16e+01, train loss: 9.42604e-07, val loss: 1.81509e-06, min loss: 9.42604e-07\n",
      "Epoch: 1183200, elapsed: 1.14e+01, train loss: 9.52825e-07, val loss: 1.81331e-06, min loss: 9.42604e-07\n",
      "Epoch: 1183300, elapsed: 1.15e+01, train loss: 9.93417e-07, val loss: 1.82103e-06, min loss: 9.42604e-07\n",
      "Epoch: 1183400, elapsed: 1.15e+01, train loss: 9.50178e-07, val loss: 1.82039e-06, min loss: 9.42604e-07\n",
      "Epoch: 1183500, elapsed: 1.14e+01, train loss: 9.45722e-07, val loss: 1.80808e-06, min loss: 9.42604e-07\n",
      "Epoch: 1183600, elapsed: 1.14e+01, train loss: 2.02890e-06, val loss: 2.71890e-06, min loss: 9.42604e-07\n",
      "Epoch: 1183700, elapsed: 1.14e+01, train loss: 1.28268e-06, val loss: 2.15934e-06, min loss: 9.42604e-07\n",
      "Epoch: 1183800, elapsed: 1.14e+01, train loss: 1.14010e-06, val loss: 1.89985e-06, min loss: 9.42604e-07\n",
      "Epoch: 1183900, elapsed: 1.47e+01, train loss: 9.49864e-07, val loss: 1.83088e-06, min loss: 9.42604e-07\n",
      "Epoch: 1184000, elapsed: 1.17e+01, train loss: 9.47445e-07, val loss: 1.82753e-06, min loss: 9.42604e-07\n",
      "Epoch: 1184100, elapsed: 1.16e+01, train loss: 9.50334e-07, val loss: 1.82621e-06, min loss: 9.42604e-07\n",
      "Epoch: 1184200, elapsed: 1.15e+01, train loss: 9.46277e-07, val loss: 1.81133e-06, min loss: 9.42604e-07\n",
      "Epoch: 1184300, elapsed: 1.14e+01, train loss: 1.03580e-06, val loss: 1.90554e-06, min loss: 9.42604e-07\n",
      "Epoch: 1184400, elapsed: 1.16e+01, train loss: 1.34028e-06, val loss: 2.58996e-06, min loss: 9.42604e-07\n",
      "Epoch: 1184500, elapsed: 1.14e+01, train loss: 1.17581e-06, val loss: 2.07817e-06, min loss: 9.42604e-07\n",
      "Epoch: 1184600, elapsed: 1.14e+01, train loss: 1.11734e-06, val loss: 2.01762e-06, min loss: 9.42604e-07\n",
      "Epoch: 1184700, elapsed: 1.14e+01, train loss: 1.33603e-06, val loss: 2.27038e-06, min loss: 9.42604e-07\n",
      "Epoch: 1184800, elapsed: 1.16e+01, train loss: 9.41405e-07, val loss: 1.81396e-06, min loss: 9.41405e-07\n",
      "Epoch: 1184900, elapsed: 1.13e+01, train loss: 9.47560e-07, val loss: 1.81104e-06, min loss: 9.41405e-07\n",
      "Epoch: 1185000, elapsed: 1.13e+01, train loss: 1.27533e-06, val loss: 2.10896e-06, min loss: 9.41405e-07\n",
      "Epoch: 1185100, elapsed: 1.34e+01, train loss: 9.60097e-07, val loss: 1.81579e-06, min loss: 9.41405e-07\n",
      "Epoch: 1185200, elapsed: 1.13e+01, train loss: 1.05055e-06, val loss: 1.88752e-06, min loss: 9.41405e-07\n",
      "Epoch: 1185300, elapsed: 1.14e+01, train loss: 1.75432e-06, val loss: 2.67205e-06, min loss: 9.41405e-07\n",
      "Epoch: 1185400, elapsed: 1.14e+01, train loss: 1.52353e-06, val loss: 2.02830e-06, min loss: 9.41405e-07\n",
      "Epoch: 1185500, elapsed: 1.13e+01, train loss: 2.07464e-06, val loss: 2.77523e-06, min loss: 9.41405e-07\n",
      "Epoch: 1185600, elapsed: 1.14e+01, train loss: 1.86642e-06, val loss: 2.85950e-06, min loss: 9.41405e-07\n",
      "Epoch: 1185700, elapsed: 1.13e+01, train loss: 9.43667e-07, val loss: 1.81658e-06, min loss: 9.41405e-07\n",
      "Epoch: 1185800, elapsed: 1.13e+01, train loss: 9.42697e-07, val loss: 1.81604e-06, min loss: 9.41405e-07\n",
      "Epoch: 1185900, elapsed: 1.11e+01, train loss: 9.47311e-07, val loss: 1.82715e-06, min loss: 9.41405e-07\n",
      "Epoch: 1186000, elapsed: 1.13e+01, train loss: 1.19396e-06, val loss: 1.97017e-06, min loss: 9.41405e-07\n",
      "Epoch: 1186100, elapsed: 1.49e+01, train loss: 9.44686e-07, val loss: 1.80894e-06, min loss: 9.41405e-07\n",
      "Epoch: 1186200, elapsed: 1.16e+01, train loss: 9.59626e-07, val loss: 1.84823e-06, min loss: 9.41405e-07\n",
      "Epoch: 1186300, elapsed: 1.15e+01, train loss: 9.59495e-07, val loss: 1.85150e-06, min loss: 9.41405e-07\n",
      "Epoch: 1186400, elapsed: 1.14e+01, train loss: 1.07084e-06, val loss: 2.01085e-06, min loss: 9.41405e-07\n",
      "Epoch: 1186500, elapsed: 1.11e+01, train loss: 1.35775e-06, val loss: 2.09146e-06, min loss: 9.41405e-07\n",
      "Epoch: 1186600, elapsed: 1.13e+01, train loss: 9.41742e-07, val loss: 1.80893e-06, min loss: 9.41405e-07\n",
      "Epoch: 1186700, elapsed: 1.13e+01, train loss: 9.47276e-07, val loss: 1.81127e-06, min loss: 9.41405e-07\n",
      "Epoch: 1186800, elapsed: 1.13e+01, train loss: 1.03043e-06, val loss: 1.87334e-06, min loss: 9.41405e-07\n",
      "Epoch: 1186900, elapsed: 1.13e+01, train loss: 4.22961e-06, val loss: 3.69930e-06, min loss: 9.41405e-07\n",
      "Epoch: 1187000, elapsed: 1.13e+01, train loss: 1.19374e-06, val loss: 2.20305e-06, min loss: 9.41405e-07\n",
      "Epoch: 1187100, elapsed: 1.14e+01, train loss: 1.22197e-06, val loss: 1.99667e-06, min loss: 9.41405e-07\n",
      "Epoch: 1187200, elapsed: 1.15e+01, train loss: 1.24914e-06, val loss: 2.19242e-06, min loss: 9.41405e-07\n",
      "Epoch: 1187300, elapsed: 1.13e+01, train loss: 1.11445e-06, val loss: 2.20763e-06, min loss: 9.41405e-07\n",
      "Epoch: 1187400, elapsed: 1.12e+01, train loss: 1.10334e-06, val loss: 1.92829e-06, min loss: 9.41405e-07\n",
      "Epoch: 1187500, elapsed: 1.14e+01, train loss: 9.93812e-07, val loss: 1.80320e-06, min loss: 9.41405e-07\n",
      "Epoch: 1187600, elapsed: 1.12e+01, train loss: 9.62190e-07, val loss: 1.81533e-06, min loss: 9.41405e-07\n",
      "Epoch: 1187700, elapsed: 1.13e+01, train loss: 9.69024e-07, val loss: 1.84463e-06, min loss: 9.41405e-07\n",
      "Epoch: 1187800, elapsed: 1.13e+01, train loss: 9.79687e-07, val loss: 1.84006e-06, min loss: 9.41405e-07\n",
      "Epoch: 1187900, elapsed: 1.13e+01, train loss: 1.21304e-06, val loss: 2.00104e-06, min loss: 9.41405e-07\n",
      "Epoch: 1188000, elapsed: 1.13e+01, train loss: 9.63794e-07, val loss: 1.86471e-06, min loss: 9.41405e-07\n",
      "Epoch: 1188100, elapsed: 1.12e+01, train loss: 9.60528e-07, val loss: 1.81079e-06, min loss: 9.41405e-07\n",
      "Epoch: 1188200, elapsed: 1.12e+01, train loss: 1.48438e-06, val loss: 2.37769e-06, min loss: 9.41405e-07\n",
      "Epoch: 1188300, elapsed: 1.49e+01, train loss: 1.14768e-06, val loss: 2.11597e-06, min loss: 9.41405e-07\n",
      "Epoch: 1188400, elapsed: 1.16e+01, train loss: 9.64938e-07, val loss: 1.87268e-06, min loss: 9.41405e-07\n",
      "Epoch: 1188500, elapsed: 1.15e+01, train loss: 1.30029e-06, val loss: 2.34975e-06, min loss: 9.41405e-07\n",
      "Epoch: 1188600, elapsed: 1.16e+01, train loss: 1.27944e-06, val loss: 2.20661e-06, min loss: 9.41405e-07\n",
      "Epoch: 1188700, elapsed: 1.14e+01, train loss: 1.06073e-06, val loss: 1.91371e-06, min loss: 9.41405e-07\n",
      "Epoch: 1188800, elapsed: 1.13e+01, train loss: 9.96505e-07, val loss: 1.83769e-06, min loss: 9.41405e-07\n",
      "Epoch: 1188900, elapsed: 1.15e+01, train loss: 9.40340e-07, val loss: 1.80986e-06, min loss: 9.40340e-07\n",
      "Epoch: 1189000, elapsed: 1.15e+01, train loss: 9.43453e-07, val loss: 1.81379e-06, min loss: 9.40340e-07\n",
      "Epoch: 1189100, elapsed: 1.16e+01, train loss: 1.27463e-06, val loss: 2.13519e-06, min loss: 9.40340e-07\n",
      "Epoch: 1189200, elapsed: 1.17e+01, train loss: 4.00823e-06, val loss: 4.58113e-06, min loss: 9.40340e-07\n",
      "Epoch: 1189300, elapsed: 1.15e+01, train loss: 3.01934e-06, val loss: 4.04497e-06, min loss: 9.40340e-07\n",
      "Epoch: 1189400, elapsed: 1.15e+01, train loss: 1.11469e-06, val loss: 1.94163e-06, min loss: 9.40340e-07\n",
      "Epoch: 1189500, elapsed: 1.14e+01, train loss: 9.39737e-07, val loss: 1.80046e-06, min loss: 9.39737e-07\n",
      "Epoch: 1189600, elapsed: 1.14e+01, train loss: 9.70619e-07, val loss: 1.82658e-06, min loss: 9.39737e-07\n",
      "Epoch: 1189700, elapsed: 1.15e+01, train loss: 9.66499e-07, val loss: 1.83382e-06, min loss: 9.39737e-07\n",
      "Epoch: 1189800, elapsed: 1.15e+01, train loss: 9.64131e-07, val loss: 1.81131e-06, min loss: 9.39737e-07\n",
      "Epoch: 1189900, elapsed: 1.14e+01, train loss: 9.53214e-07, val loss: 1.83439e-06, min loss: 9.39737e-07\n",
      "Epoch: 1190000, elapsed: 1.14e+01, train loss: 9.57125e-07, val loss: 1.81634e-06, min loss: 9.39737e-07\n",
      "Epoch: 1190100, elapsed: 1.34e+01, train loss: 9.52356e-07, val loss: 1.82865e-06, min loss: 9.39737e-07\n",
      "Epoch: 1190200, elapsed: 1.14e+01, train loss: 9.64226e-07, val loss: 1.81931e-06, min loss: 9.39737e-07\n",
      "Epoch: 1190300, elapsed: 1.15e+01, train loss: 9.44596e-07, val loss: 1.80882e-06, min loss: 9.39737e-07\n",
      "Epoch: 1190400, elapsed: 1.16e+01, train loss: 9.76549e-07, val loss: 1.86229e-06, min loss: 9.39737e-07\n",
      "Epoch: 1190500, elapsed: 1.52e+01, train loss: 9.57661e-07, val loss: 1.83872e-06, min loss: 9.39737e-07\n",
      "Epoch: 1190600, elapsed: 1.18e+01, train loss: 1.40029e-06, val loss: 2.30178e-06, min loss: 9.39737e-07\n",
      "Epoch: 1190700, elapsed: 1.16e+01, train loss: 1.14669e-06, val loss: 1.91663e-06, min loss: 9.39737e-07\n",
      "Epoch: 1190800, elapsed: 1.17e+01, train loss: 9.89036e-07, val loss: 1.89023e-06, min loss: 9.39737e-07\n",
      "Epoch: 1190900, elapsed: 1.16e+01, train loss: 1.29522e-06, val loss: 2.15674e-06, min loss: 9.39737e-07\n",
      "Epoch: 1191000, elapsed: 1.16e+01, train loss: 9.38372e-07, val loss: 1.80566e-06, min loss: 9.38372e-07\n",
      "Epoch: 1191100, elapsed: 1.15e+01, train loss: 9.79382e-07, val loss: 1.85408e-06, min loss: 9.38372e-07\n",
      "Epoch: 1191200, elapsed: 1.15e+01, train loss: 9.84905e-07, val loss: 1.85059e-06, min loss: 9.38372e-07\n",
      "Epoch: 1191300, elapsed: 1.16e+01, train loss: 9.39232e-07, val loss: 1.81139e-06, min loss: 9.38372e-07\n",
      "Epoch: 1191400, elapsed: 1.16e+01, train loss: 1.54948e-06, val loss: 2.64766e-06, min loss: 9.38372e-07\n",
      "Epoch: 1191500, elapsed: 1.17e+01, train loss: 1.12053e-06, val loss: 2.10644e-06, min loss: 9.38372e-07\n",
      "Epoch: 1191600, elapsed: 1.15e+01, train loss: 9.98523e-07, val loss: 1.86042e-06, min loss: 9.38372e-07\n",
      "Epoch: 1191700, elapsed: 1.16e+01, train loss: 9.45315e-07, val loss: 1.82866e-06, min loss: 9.38372e-07\n",
      "Epoch: 1191800, elapsed: 1.16e+01, train loss: 9.48192e-07, val loss: 1.84428e-06, min loss: 9.38372e-07\n",
      "Epoch: 1191900, elapsed: 1.15e+01, train loss: 1.41179e-06, val loss: 2.35558e-06, min loss: 9.38372e-07\n",
      "Epoch: 1192000, elapsed: 1.14e+01, train loss: 1.33872e-06, val loss: 2.26821e-06, min loss: 9.38372e-07\n",
      "Epoch: 1192100, elapsed: 1.14e+01, train loss: 1.17523e-06, val loss: 2.17315e-06, min loss: 9.38372e-07\n",
      "Epoch: 1192200, elapsed: 1.13e+01, train loss: 1.32969e-06, val loss: 2.38783e-06, min loss: 9.38372e-07\n",
      "Epoch: 1192300, elapsed: 1.16e+01, train loss: 1.07380e-06, val loss: 1.98229e-06, min loss: 9.38372e-07\n",
      "Epoch: 1192400, elapsed: 1.15e+01, train loss: 9.55830e-07, val loss: 1.86273e-06, min loss: 9.38372e-07\n",
      "Epoch: 1192500, elapsed: 1.12e+01, train loss: 2.72087e-06, val loss: 3.63427e-06, min loss: 9.38372e-07\n",
      "Epoch: 1192600, elapsed: 1.12e+01, train loss: 3.65912e-06, val loss: 4.67739e-06, min loss: 9.38372e-07\n",
      "Epoch: 1192700, elapsed: 1.49e+01, train loss: 2.02990e-06, val loss: 2.83470e-06, min loss: 9.38372e-07\n",
      "Epoch: 1192800, elapsed: 1.16e+01, train loss: 1.11364e-06, val loss: 1.89970e-06, min loss: 9.38372e-07\n",
      "Epoch: 1192900, elapsed: 1.15e+01, train loss: 9.75733e-07, val loss: 1.81625e-06, min loss: 9.38372e-07\n",
      "Epoch: 1193000, elapsed: 1.16e+01, train loss: 1.90244e-06, val loss: 2.22512e-06, min loss: 9.38372e-07\n",
      "Epoch: 1193100, elapsed: 1.14e+01, train loss: 2.83778e-06, val loss: 4.03304e-06, min loss: 9.38372e-07\n",
      "Epoch: 1193200, elapsed: 1.15e+01, train loss: 9.51576e-07, val loss: 1.84428e-06, min loss: 9.38372e-07\n",
      "Epoch: 1193300, elapsed: 1.13e+01, train loss: 9.38334e-07, val loss: 1.80804e-06, min loss: 9.38334e-07\n",
      "Epoch: 1193400, elapsed: 1.13e+01, train loss: 9.45592e-07, val loss: 1.80677e-06, min loss: 9.38334e-07\n",
      "Epoch: 1193500, elapsed: 1.14e+01, train loss: 2.24828e-06, val loss: 3.29685e-06, min loss: 9.38334e-07\n",
      "Epoch: 1193600, elapsed: 1.13e+01, train loss: 1.16531e-06, val loss: 1.91272e-06, min loss: 9.38334e-07\n",
      "Epoch: 1193700, elapsed: 1.15e+01, train loss: 9.35524e-07, val loss: 1.80615e-06, min loss: 9.35524e-07\n",
      "Epoch: 1193800, elapsed: 1.13e+01, train loss: 1.03985e-06, val loss: 1.95403e-06, min loss: 9.35524e-07\n",
      "Epoch: 1193900, elapsed: 1.14e+01, train loss: 2.18594e-06, val loss: 2.55692e-06, min loss: 9.35524e-07\n",
      "Epoch: 1194000, elapsed: 1.12e+01, train loss: 1.00561e-06, val loss: 1.84403e-06, min loss: 9.35524e-07\n",
      "Epoch: 1194100, elapsed: 1.12e+01, train loss: 3.64029e-06, val loss: 4.37347e-06, min loss: 9.35524e-07\n",
      "Epoch: 1194200, elapsed: 1.14e+01, train loss: 1.72747e-06, val loss: 2.64298e-06, min loss: 9.35524e-07\n",
      "Epoch: 1194300, elapsed: 1.13e+01, train loss: 2.10211e-06, val loss: 3.49146e-06, min loss: 9.35524e-07\n",
      "Epoch: 1194400, elapsed: 1.12e+01, train loss: 2.76213e-06, val loss: 4.12856e-06, min loss: 9.35524e-07\n",
      "Epoch: 1194500, elapsed: 1.15e+01, train loss: 9.52645e-07, val loss: 1.81437e-06, min loss: 9.35524e-07\n",
      "Epoch: 1194600, elapsed: 1.13e+01, train loss: 9.37356e-07, val loss: 1.80448e-06, min loss: 9.35524e-07\n",
      "Epoch: 1194700, elapsed: 1.13e+01, train loss: 9.39520e-07, val loss: 1.81200e-06, min loss: 9.35524e-07\n",
      "Epoch: 1194800, elapsed: 1.13e+01, train loss: 1.10421e-06, val loss: 1.96064e-06, min loss: 9.35524e-07\n",
      "Epoch: 1194900, elapsed: 1.50e+01, train loss: 1.53561e-06, val loss: 2.72465e-06, min loss: 9.35524e-07\n",
      "Epoch: 1195000, elapsed: 1.15e+01, train loss: 9.54576e-07, val loss: 1.84652e-06, min loss: 9.35524e-07\n",
      "Epoch: 1195100, elapsed: 1.34e+01, train loss: 1.78080e-06, val loss: 2.38615e-06, min loss: 9.35524e-07\n",
      "Epoch: 1195200, elapsed: 1.15e+01, train loss: 9.36169e-07, val loss: 1.80319e-06, min loss: 9.35524e-07\n",
      "Epoch: 1195300, elapsed: 1.15e+01, train loss: 9.53047e-07, val loss: 1.81258e-06, min loss: 9.35524e-07\n",
      "Epoch: 1195400, elapsed: 1.14e+01, train loss: 2.73988e-06, val loss: 3.15730e-06, min loss: 9.35524e-07\n",
      "Epoch: 1195500, elapsed: 1.13e+01, train loss: 9.57916e-07, val loss: 1.80878e-06, min loss: 9.35524e-07\n",
      "Epoch: 1195600, elapsed: 1.15e+01, train loss: 9.50270e-07, val loss: 1.81627e-06, min loss: 9.35524e-07\n",
      "Epoch: 1195700, elapsed: 1.15e+01, train loss: 9.36103e-07, val loss: 1.80009e-06, min loss: 9.35524e-07\n",
      "Epoch: 1195800, elapsed: 1.15e+01, train loss: 9.41650e-07, val loss: 1.81393e-06, min loss: 9.35524e-07\n",
      "Epoch: 1195900, elapsed: 1.15e+01, train loss: 9.39014e-07, val loss: 1.81465e-06, min loss: 9.35524e-07\n",
      "Epoch: 1196000, elapsed: 1.15e+01, train loss: 1.11325e-06, val loss: 1.87103e-06, min loss: 9.35524e-07\n",
      "Epoch: 1196100, elapsed: 1.14e+01, train loss: 9.33932e-07, val loss: 1.80665e-06, min loss: 9.33932e-07\n",
      "Epoch: 1196200, elapsed: 1.15e+01, train loss: 9.59709e-07, val loss: 1.86779e-06, min loss: 9.33932e-07\n",
      "Epoch: 1196300, elapsed: 1.16e+01, train loss: 9.34291e-07, val loss: 1.80892e-06, min loss: 9.33932e-07\n",
      "Epoch: 1196400, elapsed: 1.14e+01, train loss: 9.34195e-07, val loss: 1.80765e-06, min loss: 9.33932e-07\n",
      "Epoch: 1196500, elapsed: 1.14e+01, train loss: 1.47672e-06, val loss: 2.66749e-06, min loss: 9.33932e-07\n",
      "Epoch: 1196600, elapsed: 1.13e+01, train loss: 9.34194e-07, val loss: 1.80708e-06, min loss: 9.33932e-07\n",
      "Epoch: 1196700, elapsed: 1.15e+01, train loss: 1.02872e-06, val loss: 2.11337e-06, min loss: 9.33932e-07\n",
      "Epoch: 1196800, elapsed: 1.15e+01, train loss: 9.33649e-07, val loss: 1.80318e-06, min loss: 9.33649e-07\n",
      "Epoch: 1196900, elapsed: 1.15e+01, train loss: 9.36868e-07, val loss: 1.80006e-06, min loss: 9.33649e-07\n",
      "Epoch: 1197000, elapsed: 1.12e+01, train loss: 1.27821e-06, val loss: 2.03208e-06, min loss: 9.33649e-07\n",
      "Epoch: 1197100, elapsed: 1.49e+01, train loss: 1.18120e-06, val loss: 1.95898e-06, min loss: 9.33649e-07\n",
      "Epoch: 1197200, elapsed: 1.15e+01, train loss: 9.48849e-07, val loss: 1.80723e-06, min loss: 9.33649e-07\n",
      "Epoch: 1197300, elapsed: 1.14e+01, train loss: 9.57477e-07, val loss: 1.81853e-06, min loss: 9.33649e-07\n",
      "Epoch: 1197400, elapsed: 1.15e+01, train loss: 9.49141e-07, val loss: 1.83496e-06, min loss: 9.33649e-07\n",
      "Epoch: 1197500, elapsed: 1.16e+01, train loss: 2.12730e-06, val loss: 3.31769e-06, min loss: 9.33649e-07\n",
      "Epoch: 1197600, elapsed: 1.13e+01, train loss: 9.89242e-07, val loss: 1.95941e-06, min loss: 9.33649e-07\n",
      "Epoch: 1197700, elapsed: 1.13e+01, train loss: 9.50598e-07, val loss: 1.86208e-06, min loss: 9.33649e-07\n",
      "Epoch: 1197800, elapsed: 1.13e+01, train loss: 1.11666e-06, val loss: 1.98288e-06, min loss: 9.33649e-07\n",
      "Epoch: 1197900, elapsed: 1.15e+01, train loss: 9.44476e-07, val loss: 1.82367e-06, min loss: 9.33649e-07\n",
      "Epoch: 1198000, elapsed: 1.11e+01, train loss: 9.79135e-07, val loss: 1.85794e-06, min loss: 9.33649e-07\n",
      "Epoch: 1198100, elapsed: 1.13e+01, train loss: 9.57524e-07, val loss: 1.82481e-06, min loss: 9.33649e-07\n",
      "Epoch: 1198200, elapsed: 1.15e+01, train loss: 1.21476e-06, val loss: 2.33214e-06, min loss: 9.33649e-07\n",
      "Epoch: 1198300, elapsed: 1.14e+01, train loss: 9.32764e-07, val loss: 1.80684e-06, min loss: 9.32764e-07\n",
      "Epoch: 1198400, elapsed: 1.13e+01, train loss: 9.35697e-07, val loss: 1.79985e-06, min loss: 9.32764e-07\n",
      "Epoch: 1198500, elapsed: 1.13e+01, train loss: 1.08885e-06, val loss: 1.88932e-06, min loss: 9.32764e-07\n",
      "Epoch: 1198600, elapsed: 1.13e+01, train loss: 9.32654e-07, val loss: 1.80754e-06, min loss: 9.32654e-07\n",
      "Epoch: 1198700, elapsed: 1.13e+01, train loss: 9.36237e-07, val loss: 1.80696e-06, min loss: 9.32654e-07\n",
      "Epoch: 1198800, elapsed: 1.12e+01, train loss: 1.05078e-06, val loss: 1.90645e-06, min loss: 9.32654e-07\n",
      "Epoch: 1198900, elapsed: 1.14e+01, train loss: 1.19721e-06, val loss: 1.98994e-06, min loss: 9.32654e-07\n",
      "Epoch: 1199000, elapsed: 1.14e+01, train loss: 9.68491e-07, val loss: 1.81571e-06, min loss: 9.32654e-07\n",
      "Epoch: 1199100, elapsed: 1.16e+01, train loss: 1.18900e-06, val loss: 2.09172e-06, min loss: 9.32654e-07\n",
      "Epoch: 1199200, elapsed: 1.13e+01, train loss: 9.32902e-07, val loss: 1.80087e-06, min loss: 9.32654e-07\n",
      "Epoch: 1199300, elapsed: 1.50e+01, train loss: 9.46399e-07, val loss: 1.80948e-06, min loss: 9.32654e-07\n",
      "Epoch: 1199400, elapsed: 1.15e+01, train loss: 9.36707e-07, val loss: 1.79810e-06, min loss: 9.32654e-07\n",
      "Epoch: 1199500, elapsed: 1.16e+01, train loss: 9.39060e-07, val loss: 1.82431e-06, min loss: 9.32654e-07\n",
      "Epoch: 1199600, elapsed: 1.14e+01, train loss: 2.37245e-06, val loss: 2.94962e-06, min loss: 9.32654e-07\n",
      "Epoch: 1199700, elapsed: 1.16e+01, train loss: 9.31835e-07, val loss: 1.80252e-06, min loss: 9.31835e-07\n",
      "Epoch: 1199800, elapsed: 1.14e+01, train loss: 9.47603e-07, val loss: 1.81417e-06, min loss: 9.31835e-07\n",
      "Epoch: 1199900, elapsed: 1.14e+01, train loss: 3.08414e-06, val loss: 4.29848e-06, min loss: 9.31835e-07\n",
      "Epoch: 1200000, elapsed: 1.14e+01, train loss: 2.96784e-06, val loss: 4.07235e-06, min loss: 9.31835e-07\n",
      "Epoch: 1200100, elapsed: 1.37e+01, train loss: 1.04935e-06, val loss: 1.97863e-06, min loss: 9.31835e-07\n",
      "Epoch: 1200200, elapsed: 1.14e+01, train loss: 2.72793e-06, val loss: 3.33676e-06, min loss: 9.31835e-07\n",
      "Epoch: 1200300, elapsed: 1.13e+01, train loss: 9.85810e-07, val loss: 1.88817e-06, min loss: 9.31835e-07\n",
      "Epoch: 1200400, elapsed: 1.13e+01, train loss: 9.47324e-07, val loss: 1.81542e-06, min loss: 9.31835e-07\n",
      "Epoch: 1200500, elapsed: 1.14e+01, train loss: 9.34220e-07, val loss: 1.81073e-06, min loss: 9.31835e-07\n",
      "Epoch: 1200600, elapsed: 1.14e+01, train loss: 9.34372e-07, val loss: 1.81121e-06, min loss: 9.31835e-07\n",
      "Epoch: 1200700, elapsed: 1.12e+01, train loss: 1.05553e-06, val loss: 1.97300e-06, min loss: 9.31835e-07\n",
      "Epoch: 1200800, elapsed: 1.11e+01, train loss: 1.07401e-06, val loss: 2.01609e-06, min loss: 9.31835e-07\n",
      "Epoch: 1200900, elapsed: 1.13e+01, train loss: 1.19336e-06, val loss: 1.97283e-06, min loss: 9.31835e-07\n",
      "Epoch: 1201000, elapsed: 1.11e+01, train loss: 9.61439e-07, val loss: 1.83208e-06, min loss: 9.31835e-07\n",
      "Epoch: 1201100, elapsed: 1.12e+01, train loss: 1.25727e-06, val loss: 2.15412e-06, min loss: 9.31835e-07\n",
      "Epoch: 1201200, elapsed: 1.15e+01, train loss: 1.00244e-06, val loss: 1.92956e-06, min loss: 9.31835e-07\n",
      "Epoch: 1201300, elapsed: 1.15e+01, train loss: 1.21618e-06, val loss: 1.92780e-06, min loss: 9.31835e-07\n",
      "Epoch: 1201400, elapsed: 1.13e+01, train loss: 1.03286e-06, val loss: 1.85383e-06, min loss: 9.31835e-07\n",
      "Epoch: 1201500, elapsed: 1.49e+01, train loss: 3.70698e-06, val loss: 3.21860e-06, min loss: 9.31835e-07\n",
      "Epoch: 1201600, elapsed: 1.17e+01, train loss: 1.91068e-06, val loss: 2.74364e-06, min loss: 9.31835e-07\n",
      "Epoch: 1201700, elapsed: 1.16e+01, train loss: 9.54196e-07, val loss: 1.80851e-06, min loss: 9.31835e-07\n",
      "Epoch: 1201800, elapsed: 1.13e+01, train loss: 1.02061e-06, val loss: 1.90753e-06, min loss: 9.31835e-07\n",
      "Epoch: 1201900, elapsed: 1.13e+01, train loss: 1.04996e-06, val loss: 1.89948e-06, min loss: 9.31835e-07\n",
      "Epoch: 1202000, elapsed: 1.14e+01, train loss: 1.08463e-06, val loss: 1.91961e-06, min loss: 9.31835e-07\n",
      "Epoch: 1202100, elapsed: 1.13e+01, train loss: 9.32047e-07, val loss: 1.79831e-06, min loss: 9.31835e-07\n",
      "Epoch: 1202200, elapsed: 1.14e+01, train loss: 1.00058e-06, val loss: 1.82311e-06, min loss: 9.31835e-07\n",
      "Epoch: 1202300, elapsed: 1.12e+01, train loss: 9.30094e-07, val loss: 1.80125e-06, min loss: 9.30094e-07\n",
      "Epoch: 1202400, elapsed: 1.15e+01, train loss: 9.34404e-07, val loss: 1.80399e-06, min loss: 9.30094e-07\n",
      "Epoch: 1202500, elapsed: 1.13e+01, train loss: 9.47268e-07, val loss: 1.82364e-06, min loss: 9.30094e-07\n",
      "Epoch: 1202600, elapsed: 1.13e+01, train loss: 1.51859e-06, val loss: 2.29010e-06, min loss: 9.30094e-07\n",
      "Epoch: 1202700, elapsed: 1.14e+01, train loss: 1.09682e-06, val loss: 1.92293e-06, min loss: 9.30094e-07\n",
      "Epoch: 1202800, elapsed: 1.12e+01, train loss: 1.49930e-06, val loss: 2.26795e-06, min loss: 9.30094e-07\n",
      "Epoch: 1202900, elapsed: 1.12e+01, train loss: 9.93537e-07, val loss: 1.83811e-06, min loss: 9.30094e-07\n",
      "Epoch: 1203000, elapsed: 1.13e+01, train loss: 1.22550e-06, val loss: 2.01580e-06, min loss: 9.30094e-07\n",
      "Epoch: 1203100, elapsed: 1.13e+01, train loss: 9.30756e-07, val loss: 1.80656e-06, min loss: 9.30094e-07\n",
      "Epoch: 1203200, elapsed: 1.12e+01, train loss: 9.31455e-07, val loss: 1.80312e-06, min loss: 9.30094e-07\n",
      "Epoch: 1203300, elapsed: 1.14e+01, train loss: 1.21608e-06, val loss: 1.92392e-06, min loss: 9.30094e-07\n",
      "Epoch: 1203400, elapsed: 1.13e+01, train loss: 2.65530e-06, val loss: 3.96143e-06, min loss: 9.30094e-07\n",
      "Epoch: 1203500, elapsed: 1.14e+01, train loss: 2.07816e-06, val loss: 3.13944e-06, min loss: 9.30094e-07\n",
      "Epoch: 1203600, elapsed: 1.13e+01, train loss: 1.09382e-06, val loss: 2.05096e-06, min loss: 9.30094e-07\n",
      "Epoch: 1203700, elapsed: 1.47e+01, train loss: 9.33119e-07, val loss: 1.81068e-06, min loss: 9.30094e-07\n",
      "Epoch: 1203800, elapsed: 1.16e+01, train loss: 9.36077e-07, val loss: 1.82510e-06, min loss: 9.30094e-07\n",
      "Epoch: 1203900, elapsed: 1.13e+01, train loss: 9.52230e-07, val loss: 1.83166e-06, min loss: 9.30094e-07\n",
      "Epoch: 1204000, elapsed: 1.14e+01, train loss: 1.00881e-06, val loss: 1.87330e-06, min loss: 9.30094e-07\n",
      "Epoch: 1204100, elapsed: 1.15e+01, train loss: 1.73154e-06, val loss: 2.16306e-06, min loss: 9.30094e-07\n",
      "Epoch: 1204200, elapsed: 1.15e+01, train loss: 3.48557e-06, val loss: 4.38448e-06, min loss: 9.30094e-07\n",
      "Epoch: 1204300, elapsed: 1.13e+01, train loss: 1.22495e-06, val loss: 2.25745e-06, min loss: 9.30094e-07\n",
      "Epoch: 1204400, elapsed: 1.14e+01, train loss: 9.29893e-07, val loss: 1.80356e-06, min loss: 9.29893e-07\n",
      "Epoch: 1204500, elapsed: 1.13e+01, train loss: 9.30600e-07, val loss: 1.79666e-06, min loss: 9.29893e-07\n",
      "Epoch: 1204600, elapsed: 1.13e+01, train loss: 1.38077e-06, val loss: 1.97971e-06, min loss: 9.29893e-07\n",
      "Epoch: 1204700, elapsed: 1.16e+01, train loss: 9.65934e-07, val loss: 1.85129e-06, min loss: 9.29893e-07\n",
      "Epoch: 1204800, elapsed: 1.14e+01, train loss: 9.29674e-07, val loss: 1.79505e-06, min loss: 9.29674e-07\n",
      "Epoch: 1204900, elapsed: 1.13e+01, train loss: 9.29136e-07, val loss: 1.80594e-06, min loss: 9.29136e-07\n",
      "Epoch: 1205000, elapsed: 1.12e+01, train loss: 1.03344e-06, val loss: 1.84972e-06, min loss: 9.29136e-07\n",
      "Epoch: 1205100, elapsed: 1.35e+01, train loss: 1.00463e-06, val loss: 1.84123e-06, min loss: 9.29136e-07\n",
      "Epoch: 1205200, elapsed: 1.12e+01, train loss: 9.68498e-07, val loss: 1.81183e-06, min loss: 9.29136e-07\n",
      "Epoch: 1205300, elapsed: 1.14e+01, train loss: 9.41106e-07, val loss: 1.82048e-06, min loss: 9.29136e-07\n",
      "Epoch: 1205400, elapsed: 1.14e+01, train loss: 1.82667e-06, val loss: 2.93001e-06, min loss: 9.29136e-07\n",
      "Epoch: 1205500, elapsed: 1.13e+01, train loss: 1.06412e-06, val loss: 2.02578e-06, min loss: 9.29136e-07\n",
      "Epoch: 1205600, elapsed: 1.13e+01, train loss: 9.33230e-07, val loss: 1.79511e-06, min loss: 9.29136e-07\n",
      "Epoch: 1205700, elapsed: 1.13e+01, train loss: 9.35212e-07, val loss: 1.80713e-06, min loss: 9.29136e-07\n",
      "Epoch: 1205800, elapsed: 1.13e+01, train loss: 9.66432e-07, val loss: 1.85820e-06, min loss: 9.29136e-07\n",
      "Epoch: 1205900, elapsed: 1.50e+01, train loss: 9.31075e-07, val loss: 1.79358e-06, min loss: 9.29136e-07\n",
      "Epoch: 1206000, elapsed: 1.15e+01, train loss: 9.71006e-07, val loss: 1.80840e-06, min loss: 9.29136e-07\n",
      "Epoch: 1206100, elapsed: 1.14e+01, train loss: 1.03120e-06, val loss: 1.93055e-06, min loss: 9.29136e-07\n",
      "Epoch: 1206200, elapsed: 1.15e+01, train loss: 9.74948e-07, val loss: 1.86279e-06, min loss: 9.29136e-07\n",
      "Epoch: 1206300, elapsed: 1.14e+01, train loss: 2.05645e-06, val loss: 2.50851e-06, min loss: 9.29136e-07\n",
      "Epoch: 1206400, elapsed: 1.16e+01, train loss: 3.26342e-06, val loss: 4.64177e-06, min loss: 9.29136e-07\n",
      "Epoch: 1206500, elapsed: 1.16e+01, train loss: 1.07517e-06, val loss: 2.02251e-06, min loss: 9.29136e-07\n",
      "Epoch: 1206600, elapsed: 1.14e+01, train loss: 9.32156e-07, val loss: 1.81248e-06, min loss: 9.29136e-07\n",
      "Epoch: 1206700, elapsed: 1.14e+01, train loss: 9.32923e-07, val loss: 1.79130e-06, min loss: 9.29136e-07\n",
      "Epoch: 1206800, elapsed: 1.13e+01, train loss: 1.01795e-06, val loss: 1.88495e-06, min loss: 9.29136e-07\n",
      "Epoch: 1206900, elapsed: 1.14e+01, train loss: 9.28168e-07, val loss: 1.79739e-06, min loss: 9.28168e-07\n",
      "Epoch: 1207000, elapsed: 1.12e+01, train loss: 9.29485e-07, val loss: 1.79951e-06, min loss: 9.28168e-07\n",
      "Epoch: 1207100, elapsed: 1.14e+01, train loss: 9.36078e-07, val loss: 1.86459e-06, min loss: 9.28168e-07\n",
      "Epoch: 1207200, elapsed: 1.13e+01, train loss: 9.36873e-07, val loss: 1.82149e-06, min loss: 9.28168e-07\n",
      "Epoch: 1207300, elapsed: 1.14e+01, train loss: 9.30672e-07, val loss: 1.80250e-06, min loss: 9.28168e-07\n",
      "Epoch: 1207400, elapsed: 1.12e+01, train loss: 9.27441e-07, val loss: 1.79509e-06, min loss: 9.27441e-07\n",
      "Epoch: 1207500, elapsed: 1.14e+01, train loss: 9.48727e-07, val loss: 1.85107e-06, min loss: 9.27441e-07\n",
      "Epoch: 1207600, elapsed: 1.15e+01, train loss: 6.71262e-06, val loss: 7.19949e-06, min loss: 9.27441e-07\n",
      "Epoch: 1207700, elapsed: 1.14e+01, train loss: 9.26929e-07, val loss: 1.79550e-06, min loss: 9.26929e-07\n",
      "Epoch: 1207800, elapsed: 1.12e+01, train loss: 9.60432e-07, val loss: 1.80840e-06, min loss: 9.26929e-07\n",
      "Epoch: 1207900, elapsed: 1.14e+01, train loss: 1.49367e-06, val loss: 2.73284e-06, min loss: 9.26929e-07\n",
      "Epoch: 1208000, elapsed: 1.13e+01, train loss: 9.27411e-07, val loss: 1.79858e-06, min loss: 9.26929e-07\n",
      "Epoch: 1208100, elapsed: 1.48e+01, train loss: 9.28271e-07, val loss: 1.79266e-06, min loss: 9.26929e-07\n",
      "Epoch: 1208200, elapsed: 1.16e+01, train loss: 9.95581e-07, val loss: 1.82486e-06, min loss: 9.26929e-07\n",
      "Epoch: 1208300, elapsed: 1.15e+01, train loss: 1.28905e-06, val loss: 2.09413e-06, min loss: 9.26929e-07\n",
      "Epoch: 1208400, elapsed: 1.15e+01, train loss: 1.50865e-06, val loss: 2.58343e-06, min loss: 9.26929e-07\n",
      "Epoch: 1208500, elapsed: 1.15e+01, train loss: 2.80471e-06, val loss: 4.05131e-06, min loss: 9.26929e-07\n",
      "Epoch: 1208600, elapsed: 1.15e+01, train loss: 9.26110e-07, val loss: 1.79653e-06, min loss: 9.26110e-07\n",
      "Epoch: 1208700, elapsed: 1.15e+01, train loss: 9.41430e-07, val loss: 1.79058e-06, min loss: 9.26110e-07\n",
      "Epoch: 1208800, elapsed: 1.14e+01, train loss: 9.28365e-07, val loss: 1.81006e-06, min loss: 9.26110e-07\n",
      "Epoch: 1208900, elapsed: 1.15e+01, train loss: 9.27192e-07, val loss: 1.79150e-06, min loss: 9.26110e-07\n",
      "Epoch: 1209000, elapsed: 1.16e+01, train loss: 9.38571e-07, val loss: 1.79366e-06, min loss: 9.26110e-07\n",
      "Epoch: 1209100, elapsed: 1.12e+01, train loss: 1.49463e-06, val loss: 2.52356e-06, min loss: 9.26110e-07\n",
      "Epoch: 1209200, elapsed: 1.14e+01, train loss: 1.51582e-06, val loss: 2.56771e-06, min loss: 9.26110e-07\n",
      "Epoch: 1209300, elapsed: 1.14e+01, train loss: 2.63112e-06, val loss: 3.90700e-06, min loss: 9.26110e-07\n",
      "Epoch: 1209400, elapsed: 1.14e+01, train loss: 9.64498e-07, val loss: 1.83113e-06, min loss: 9.26110e-07\n",
      "Epoch: 1209500, elapsed: 1.14e+01, train loss: 9.28017e-07, val loss: 1.81403e-06, min loss: 9.26110e-07\n",
      "Epoch: 1209600, elapsed: 1.14e+01, train loss: 1.20629e-06, val loss: 2.10664e-06, min loss: 9.26110e-07\n",
      "Epoch: 1209700, elapsed: 1.15e+01, train loss: 1.37481e-06, val loss: 2.37937e-06, min loss: 9.26110e-07\n",
      "Epoch: 1209800, elapsed: 1.15e+01, train loss: 1.05047e-06, val loss: 1.87677e-06, min loss: 9.26110e-07\n",
      "Epoch: 1209900, elapsed: 1.14e+01, train loss: 9.31715e-07, val loss: 1.80218e-06, min loss: 9.26110e-07\n",
      "Epoch: 1210000, elapsed: 1.14e+01, train loss: 9.29881e-07, val loss: 1.79176e-06, min loss: 9.26110e-07\n",
      "Epoch: 1210100, elapsed: 1.33e+01, train loss: 9.34103e-07, val loss: 1.80784e-06, min loss: 9.26110e-07\n",
      "Epoch: 1210200, elapsed: 1.14e+01, train loss: 5.41121e-06, val loss: 5.57355e-06, min loss: 9.26110e-07\n",
      "Epoch: 1210300, elapsed: 1.51e+01, train loss: 1.15441e-06, val loss: 2.09708e-06, min loss: 9.26110e-07\n",
      "Epoch: 1210400, elapsed: 1.15e+01, train loss: 9.56313e-07, val loss: 1.80473e-06, min loss: 9.26110e-07\n",
      "Epoch: 1210500, elapsed: 1.16e+01, train loss: 1.54097e-06, val loss: 2.22452e-06, min loss: 9.26110e-07\n",
      "Epoch: 1210600, elapsed: 1.16e+01, train loss: 1.18400e-06, val loss: 2.00398e-06, min loss: 9.26110e-07\n",
      "Epoch: 1210700, elapsed: 1.16e+01, train loss: 2.03935e-06, val loss: 3.20402e-06, min loss: 9.26110e-07\n",
      "Epoch: 1210800, elapsed: 1.15e+01, train loss: 9.32374e-07, val loss: 1.79930e-06, min loss: 9.26110e-07\n",
      "Epoch: 1210900, elapsed: 1.14e+01, train loss: 9.28151e-07, val loss: 1.80129e-06, min loss: 9.26110e-07\n",
      "Epoch: 1211000, elapsed: 1.13e+01, train loss: 9.74137e-07, val loss: 1.87448e-06, min loss: 9.26110e-07\n",
      "Epoch: 1211100, elapsed: 1.13e+01, train loss: 4.15119e-06, val loss: 5.68093e-06, min loss: 9.26110e-07\n",
      "Epoch: 1211200, elapsed: 1.15e+01, train loss: 9.24597e-07, val loss: 1.79556e-06, min loss: 9.24597e-07\n",
      "Epoch: 1211300, elapsed: 1.15e+01, train loss: 9.33140e-07, val loss: 1.79884e-06, min loss: 9.24597e-07\n",
      "Epoch: 1211400, elapsed: 1.14e+01, train loss: 9.66636e-07, val loss: 1.82336e-06, min loss: 9.24597e-07\n",
      "Epoch: 1211500, elapsed: 1.15e+01, train loss: 1.01084e-06, val loss: 1.95238e-06, min loss: 9.24597e-07\n",
      "Epoch: 1211600, elapsed: 1.13e+01, train loss: 9.24627e-07, val loss: 1.79388e-06, min loss: 9.24597e-07\n",
      "Epoch: 1211700, elapsed: 1.15e+01, train loss: 9.47816e-07, val loss: 1.83104e-06, min loss: 9.24597e-07\n",
      "Epoch: 1211800, elapsed: 1.13e+01, train loss: 9.41071e-07, val loss: 1.82358e-06, min loss: 9.24597e-07\n",
      "Epoch: 1211900, elapsed: 1.14e+01, train loss: 9.29126e-07, val loss: 1.80423e-06, min loss: 9.24597e-07\n",
      "Epoch: 1212000, elapsed: 1.14e+01, train loss: 1.04023e-06, val loss: 1.92106e-06, min loss: 9.24597e-07\n",
      "Epoch: 1212100, elapsed: 1.12e+01, train loss: 3.05044e-06, val loss: 4.43162e-06, min loss: 9.24597e-07\n",
      "Epoch: 1212200, elapsed: 1.13e+01, train loss: 9.26938e-07, val loss: 1.78761e-06, min loss: 9.24597e-07\n",
      "Epoch: 1212300, elapsed: 1.13e+01, train loss: 1.04809e-06, val loss: 1.91777e-06, min loss: 9.24597e-07\n",
      "Epoch: 1212400, elapsed: 1.16e+01, train loss: 1.21862e-06, val loss: 1.89946e-06, min loss: 9.24597e-07\n",
      "Epoch: 1212500, elapsed: 1.53e+01, train loss: 9.89139e-07, val loss: 1.80778e-06, min loss: 9.24597e-07\n",
      "Epoch: 1212600, elapsed: 1.16e+01, train loss: 9.41953e-07, val loss: 1.80549e-06, min loss: 9.24597e-07\n",
      "Epoch: 1212700, elapsed: 1.15e+01, train loss: 1.58823e-06, val loss: 2.08433e-06, min loss: 9.24597e-07\n",
      "Epoch: 1212800, elapsed: 1.14e+01, train loss: 9.77918e-07, val loss: 1.82538e-06, min loss: 9.24597e-07\n",
      "Epoch: 1212900, elapsed: 1.15e+01, train loss: 2.29982e-06, val loss: 2.50771e-06, min loss: 9.24597e-07\n",
      "Epoch: 1213000, elapsed: 1.16e+01, train loss: 1.38320e-06, val loss: 2.11714e-06, min loss: 9.24597e-07\n",
      "Epoch: 1213100, elapsed: 1.15e+01, train loss: 9.64245e-07, val loss: 1.95295e-06, min loss: 9.24597e-07\n",
      "Epoch: 1213200, elapsed: 1.12e+01, train loss: 9.25689e-07, val loss: 1.79494e-06, min loss: 9.24597e-07\n",
      "Epoch: 1213300, elapsed: 1.15e+01, train loss: 9.24036e-07, val loss: 1.79674e-06, min loss: 9.24036e-07\n",
      "Epoch: 1213400, elapsed: 1.15e+01, train loss: 9.40266e-07, val loss: 1.78995e-06, min loss: 9.24036e-07\n",
      "Epoch: 1213500, elapsed: 1.12e+01, train loss: 1.18584e-06, val loss: 1.84541e-06, min loss: 9.24036e-07\n",
      "Epoch: 1213600, elapsed: 1.14e+01, train loss: 9.30590e-07, val loss: 1.79114e-06, min loss: 9.24036e-07\n",
      "Epoch: 1213700, elapsed: 1.16e+01, train loss: 9.23338e-07, val loss: 1.79188e-06, min loss: 9.23338e-07\n",
      "Epoch: 1213800, elapsed: 1.14e+01, train loss: 1.59637e-06, val loss: 2.11625e-06, min loss: 9.23338e-07\n",
      "Epoch: 1213900, elapsed: 1.13e+01, train loss: 3.89174e-06, val loss: 5.68074e-06, min loss: 9.23338e-07\n",
      "Epoch: 1214000, elapsed: 1.14e+01, train loss: 9.22922e-07, val loss: 1.79495e-06, min loss: 9.22922e-07\n",
      "Epoch: 1214100, elapsed: 1.15e+01, train loss: 9.39275e-07, val loss: 1.82593e-06, min loss: 9.22922e-07\n",
      "Epoch: 1214200, elapsed: 1.13e+01, train loss: 9.80488e-07, val loss: 1.88311e-06, min loss: 9.22922e-07\n",
      "Epoch: 1214300, elapsed: 1.14e+01, train loss: 1.09436e-06, val loss: 1.96513e-06, min loss: 9.22922e-07\n",
      "Epoch: 1214400, elapsed: 1.13e+01, train loss: 9.22794e-07, val loss: 1.79093e-06, min loss: 9.22794e-07\n",
      "Epoch: 1214500, elapsed: 1.14e+01, train loss: 9.51698e-07, val loss: 1.84022e-06, min loss: 9.22794e-07\n",
      "Epoch: 1214600, elapsed: 1.14e+01, train loss: 9.50336e-07, val loss: 1.79184e-06, min loss: 9.22794e-07\n",
      "Epoch: 1214700, elapsed: 1.14e+01, train loss: 1.04385e-06, val loss: 1.87876e-06, min loss: 9.22794e-07\n",
      "Epoch: 1214800, elapsed: 1.50e+01, train loss: 1.20511e-06, val loss: 2.09491e-06, min loss: 9.22794e-07\n",
      "Epoch: 1214900, elapsed: 1.15e+01, train loss: 3.17562e-06, val loss: 4.00544e-06, min loss: 9.22794e-07\n",
      "Epoch: 1215000, elapsed: 1.17e+01, train loss: 1.75940e-06, val loss: 2.57867e-06, min loss: 9.22794e-07\n",
      "Epoch: 1215100, elapsed: 1.37e+01, train loss: 9.96825e-07, val loss: 1.82767e-06, min loss: 9.22794e-07\n",
      "Epoch: 1215200, elapsed: 1.16e+01, train loss: 9.32541e-07, val loss: 1.77995e-06, min loss: 9.22794e-07\n",
      "Epoch: 1215300, elapsed: 1.13e+01, train loss: 9.41745e-07, val loss: 1.81112e-06, min loss: 9.22794e-07\n",
      "Epoch: 1215400, elapsed: 1.14e+01, train loss: 9.24244e-07, val loss: 1.79699e-06, min loss: 9.22794e-07\n",
      "Epoch: 1215500, elapsed: 1.15e+01, train loss: 9.31088e-07, val loss: 1.79295e-06, min loss: 9.22794e-07\n",
      "Epoch: 1215600, elapsed: 1.15e+01, train loss: 9.34831e-07, val loss: 1.78931e-06, min loss: 9.22794e-07\n",
      "Epoch: 1215700, elapsed: 1.13e+01, train loss: 9.54264e-07, val loss: 1.83649e-06, min loss: 9.22794e-07\n",
      "Epoch: 1215800, elapsed: 1.15e+01, train loss: 1.03043e-06, val loss: 1.94812e-06, min loss: 9.22794e-07\n",
      "Epoch: 1215900, elapsed: 1.15e+01, train loss: 1.04117e-06, val loss: 1.82367e-06, min loss: 9.22794e-07\n",
      "Epoch: 1216000, elapsed: 1.14e+01, train loss: 9.51351e-07, val loss: 1.84075e-06, min loss: 9.22794e-07\n",
      "Epoch: 1216100, elapsed: 1.13e+01, train loss: 1.15977e-06, val loss: 2.12820e-06, min loss: 9.22794e-07\n",
      "Epoch: 1216200, elapsed: 1.14e+01, train loss: 9.66745e-07, val loss: 1.83252e-06, min loss: 9.22794e-07\n",
      "Epoch: 1216300, elapsed: 1.13e+01, train loss: 2.34056e-06, val loss: 3.14445e-06, min loss: 9.22794e-07\n",
      "Epoch: 1216400, elapsed: 1.15e+01, train loss: 1.20968e-06, val loss: 1.93488e-06, min loss: 9.22794e-07\n",
      "Epoch: 1216500, elapsed: 1.16e+01, train loss: 9.67513e-07, val loss: 1.84596e-06, min loss: 9.22794e-07\n",
      "Epoch: 1216600, elapsed: 1.13e+01, train loss: 1.00835e-06, val loss: 1.80978e-06, min loss: 9.22794e-07\n",
      "Epoch: 1216700, elapsed: 1.15e+01, train loss: 2.37071e-06, val loss: 2.20415e-06, min loss: 9.22794e-07\n",
      "Epoch: 1216800, elapsed: 1.14e+01, train loss: 9.22310e-07, val loss: 1.79227e-06, min loss: 9.22310e-07\n",
      "Epoch: 1216900, elapsed: 1.14e+01, train loss: 2.25719e-06, val loss: 3.11274e-06, min loss: 9.22310e-07\n",
      "Epoch: 1217000, elapsed: 1.51e+01, train loss: 9.37348e-07, val loss: 1.83644e-06, min loss: 9.22310e-07\n",
      "Epoch: 1217100, elapsed: 1.19e+01, train loss: 1.06968e-06, val loss: 2.14410e-06, min loss: 9.22310e-07\n",
      "Epoch: 1217200, elapsed: 1.17e+01, train loss: 9.37591e-07, val loss: 1.81549e-06, min loss: 9.22310e-07\n",
      "Epoch: 1217300, elapsed: 1.13e+01, train loss: 9.21654e-07, val loss: 1.79188e-06, min loss: 9.21654e-07\n",
      "Epoch: 1217400, elapsed: 1.14e+01, train loss: 9.67313e-07, val loss: 1.87627e-06, min loss: 9.21654e-07\n",
      "Epoch: 1217500, elapsed: 1.17e+01, train loss: 1.34958e-06, val loss: 2.38811e-06, min loss: 9.21654e-07\n",
      "Epoch: 1217600, elapsed: 1.13e+01, train loss: 1.00730e-06, val loss: 1.91628e-06, min loss: 9.21654e-07\n",
      "Epoch: 1217700, elapsed: 1.14e+01, train loss: 9.21493e-07, val loss: 1.79298e-06, min loss: 9.21493e-07\n",
      "Epoch: 1217800, elapsed: 1.14e+01, train loss: 9.28298e-07, val loss: 1.78907e-06, min loss: 9.21493e-07\n",
      "Epoch: 1217900, elapsed: 1.14e+01, train loss: 9.42908e-07, val loss: 1.80576e-06, min loss: 9.21493e-07\n",
      "Epoch: 1218000, elapsed: 1.14e+01, train loss: 2.13099e-06, val loss: 2.39504e-06, min loss: 9.21493e-07\n",
      "Epoch: 1218100, elapsed: 1.13e+01, train loss: 1.12251e-06, val loss: 1.89089e-06, min loss: 9.21493e-07\n",
      "Epoch: 1218200, elapsed: 1.13e+01, train loss: 9.30755e-07, val loss: 1.80107e-06, min loss: 9.21493e-07\n",
      "Epoch: 1218300, elapsed: 1.14e+01, train loss: 9.22183e-07, val loss: 1.78688e-06, min loss: 9.21493e-07\n",
      "Epoch: 1218400, elapsed: 1.13e+01, train loss: 2.23102e-06, val loss: 3.74699e-06, min loss: 9.21493e-07\n",
      "Epoch: 1218500, elapsed: 1.14e+01, train loss: 9.20218e-07, val loss: 1.79069e-06, min loss: 9.20218e-07\n",
      "Epoch: 1218600, elapsed: 1.13e+01, train loss: 9.47115e-07, val loss: 1.84395e-06, min loss: 9.20218e-07\n",
      "Epoch: 1218700, elapsed: 1.14e+01, train loss: 9.29630e-07, val loss: 1.78981e-06, min loss: 9.20218e-07\n",
      "Epoch: 1218800, elapsed: 1.13e+01, train loss: 9.76184e-07, val loss: 1.91594e-06, min loss: 9.20218e-07\n",
      "Epoch: 1218900, elapsed: 1.13e+01, train loss: 1.14483e-06, val loss: 1.98388e-06, min loss: 9.20218e-07\n",
      "Epoch: 1219000, elapsed: 1.14e+01, train loss: 1.08769e-06, val loss: 1.93293e-06, min loss: 9.20218e-07\n",
      "Epoch: 1219100, elapsed: 1.14e+01, train loss: 1.52780e-06, val loss: 2.34489e-06, min loss: 9.20218e-07\n",
      "Epoch: 1219200, elapsed: 1.48e+01, train loss: 1.25193e-06, val loss: 1.90475e-06, min loss: 9.20218e-07\n",
      "Epoch: 1219300, elapsed: 1.16e+01, train loss: 1.65821e-06, val loss: 2.53554e-06, min loss: 9.20218e-07\n",
      "Epoch: 1219400, elapsed: 1.14e+01, train loss: 9.44531e-07, val loss: 1.80435e-06, min loss: 9.20218e-07\n",
      "Epoch: 1219500, elapsed: 1.15e+01, train loss: 1.39529e-06, val loss: 2.39546e-06, min loss: 9.20218e-07\n",
      "Epoch: 1219600, elapsed: 1.16e+01, train loss: 9.62152e-07, val loss: 1.79142e-06, min loss: 9.20218e-07\n",
      "Epoch: 1219700, elapsed: 1.13e+01, train loss: 2.21135e-06, val loss: 3.30717e-06, min loss: 9.20218e-07\n",
      "Epoch: 1219800, elapsed: 1.13e+01, train loss: 1.01228e-06, val loss: 1.95806e-06, min loss: 9.20218e-07\n",
      "Epoch: 1219900, elapsed: 1.12e+01, train loss: 1.17051e-06, val loss: 2.00131e-06, min loss: 9.20218e-07\n",
      "Epoch: 1220000, elapsed: 1.13e+01, train loss: 1.02093e-06, val loss: 1.87535e-06, min loss: 9.20218e-07\n",
      "Epoch: 1220100, elapsed: 1.34e+01, train loss: 5.52251e-06, val loss: 6.11512e-06, min loss: 9.20218e-07\n",
      "Epoch: 1220200, elapsed: 1.13e+01, train loss: 9.19352e-07, val loss: 1.78658e-06, min loss: 9.19352e-07\n",
      "Epoch: 1220300, elapsed: 1.13e+01, train loss: 9.21678e-07, val loss: 1.80660e-06, min loss: 9.19352e-07\n",
      "Epoch: 1220400, elapsed: 1.13e+01, train loss: 1.14494e-06, val loss: 2.02026e-06, min loss: 9.19352e-07\n",
      "Epoch: 1220500, elapsed: 1.13e+01, train loss: 9.18876e-07, val loss: 1.79216e-06, min loss: 9.18876e-07\n",
      "Epoch: 1220600, elapsed: 1.13e+01, train loss: 9.35788e-07, val loss: 1.79132e-06, min loss: 9.18876e-07\n",
      "Epoch: 1220700, elapsed: 1.13e+01, train loss: 9.88875e-07, val loss: 1.79367e-06, min loss: 9.18876e-07\n",
      "Epoch: 1220800, elapsed: 1.13e+01, train loss: 9.59264e-07, val loss: 1.85207e-06, min loss: 9.18876e-07\n",
      "Epoch: 1220900, elapsed: 1.14e+01, train loss: 9.21757e-07, val loss: 1.78493e-06, min loss: 9.18876e-07\n",
      "Epoch: 1221000, elapsed: 1.14e+01, train loss: 9.46043e-07, val loss: 1.81316e-06, min loss: 9.18876e-07\n",
      "Epoch: 1221100, elapsed: 1.12e+01, train loss: 1.25176e-06, val loss: 2.33784e-06, min loss: 9.18876e-07\n",
      "Epoch: 1221200, elapsed: 1.14e+01, train loss: 9.35273e-07, val loss: 1.82769e-06, min loss: 9.18876e-07\n",
      "Epoch: 1221300, elapsed: 1.14e+01, train loss: 9.18694e-07, val loss: 1.79134e-06, min loss: 9.18694e-07\n",
      "Epoch: 1221400, elapsed: 1.15e+01, train loss: 9.43489e-07, val loss: 1.82394e-06, min loss: 9.18694e-07\n",
      "Epoch: 1221500, elapsed: 1.52e+01, train loss: 9.26034e-07, val loss: 1.80627e-06, min loss: 9.18694e-07\n",
      "Epoch: 1221600, elapsed: 1.16e+01, train loss: 1.18815e-06, val loss: 2.08296e-06, min loss: 9.18694e-07\n",
      "Epoch: 1221700, elapsed: 1.16e+01, train loss: 1.24216e-06, val loss: 1.90698e-06, min loss: 9.18694e-07\n",
      "Epoch: 1221800, elapsed: 1.14e+01, train loss: 1.83589e-06, val loss: 2.97031e-06, min loss: 9.18694e-07\n",
      "Epoch: 1221900, elapsed: 1.14e+01, train loss: 1.71469e-06, val loss: 2.89928e-06, min loss: 9.18694e-07\n",
      "Epoch: 1222000, elapsed: 1.14e+01, train loss: 9.41615e-07, val loss: 1.81930e-06, min loss: 9.18694e-07\n",
      "Epoch: 1222100, elapsed: 1.12e+01, train loss: 9.25850e-07, val loss: 1.78307e-06, min loss: 9.18694e-07\n",
      "Epoch: 1222200, elapsed: 1.14e+01, train loss: 9.48674e-07, val loss: 1.81437e-06, min loss: 9.18694e-07\n",
      "Epoch: 1222300, elapsed: 1.15e+01, train loss: 9.83254e-07, val loss: 1.82068e-06, min loss: 9.18694e-07\n",
      "Epoch: 1222400, elapsed: 1.15e+01, train loss: 9.54933e-07, val loss: 1.87935e-06, min loss: 9.18694e-07\n",
      "Epoch: 1222500, elapsed: 1.13e+01, train loss: 1.03528e-06, val loss: 1.85895e-06, min loss: 9.18694e-07\n",
      "Epoch: 1222600, elapsed: 1.13e+01, train loss: 2.26090e-06, val loss: 3.33517e-06, min loss: 9.18694e-07\n",
      "Epoch: 1222700, elapsed: 1.13e+01, train loss: 1.05543e-06, val loss: 2.09371e-06, min loss: 9.18694e-07\n",
      "Epoch: 1222800, elapsed: 1.14e+01, train loss: 1.97169e-06, val loss: 2.39440e-06, min loss: 9.18694e-07\n",
      "Epoch: 1222900, elapsed: 1.14e+01, train loss: 1.61329e-06, val loss: 2.43085e-06, min loss: 9.18694e-07\n",
      "Epoch: 1223000, elapsed: 1.13e+01, train loss: 1.26687e-06, val loss: 2.02414e-06, min loss: 9.18694e-07\n",
      "Epoch: 1223100, elapsed: 1.12e+01, train loss: 9.49856e-07, val loss: 1.79550e-06, min loss: 9.18694e-07\n",
      "Epoch: 1223200, elapsed: 1.13e+01, train loss: 9.37213e-07, val loss: 1.82042e-06, min loss: 9.18694e-07\n",
      "Epoch: 1223300, elapsed: 1.14e+01, train loss: 2.83088e-06, val loss: 3.15563e-06, min loss: 9.18694e-07\n",
      "Epoch: 1223400, elapsed: 1.12e+01, train loss: 1.82880e-06, val loss: 2.38870e-06, min loss: 9.18694e-07\n",
      "Epoch: 1223500, elapsed: 1.12e+01, train loss: 3.40453e-06, val loss: 3.77089e-06, min loss: 9.18694e-07\n",
      "Epoch: 1223600, elapsed: 1.14e+01, train loss: 3.95207e-06, val loss: 5.45106e-06, min loss: 9.18694e-07\n",
      "Epoch: 1223700, elapsed: 1.48e+01, train loss: 1.05707e-06, val loss: 1.93372e-06, min loss: 9.18694e-07\n",
      "Epoch: 1223800, elapsed: 1.16e+01, train loss: 9.21614e-07, val loss: 1.77973e-06, min loss: 9.18694e-07\n",
      "Epoch: 1223900, elapsed: 1.15e+01, train loss: 1.00634e-06, val loss: 1.82361e-06, min loss: 9.18694e-07\n",
      "Epoch: 1224000, elapsed: 1.14e+01, train loss: 1.01845e-06, val loss: 1.88924e-06, min loss: 9.18694e-07\n",
      "Epoch: 1224100, elapsed: 1.14e+01, train loss: 1.13212e-06, val loss: 1.90281e-06, min loss: 9.18694e-07\n",
      "Epoch: 1224200, elapsed: 1.14e+01, train loss: 4.84356e-06, val loss: 5.78141e-06, min loss: 9.18694e-07\n",
      "Epoch: 1224300, elapsed: 1.15e+01, train loss: 1.62705e-06, val loss: 2.07646e-06, min loss: 9.18694e-07\n",
      "Epoch: 1224400, elapsed: 1.16e+01, train loss: 9.50525e-07, val loss: 1.90308e-06, min loss: 9.18694e-07\n",
      "Epoch: 1224500, elapsed: 1.14e+01, train loss: 9.85821e-07, val loss: 1.82921e-06, min loss: 9.18694e-07\n",
      "Epoch: 1224600, elapsed: 1.13e+01, train loss: 4.23220e-06, val loss: 5.05358e-06, min loss: 9.18694e-07\n",
      "Epoch: 1224700, elapsed: 1.12e+01, train loss: 1.35246e-06, val loss: 2.32318e-06, min loss: 9.18694e-07\n",
      "Epoch: 1224800, elapsed: 1.13e+01, train loss: 9.50483e-07, val loss: 1.84821e-06, min loss: 9.18694e-07\n",
      "Epoch: 1224900, elapsed: 1.14e+01, train loss: 1.29618e-06, val loss: 2.15996e-06, min loss: 9.18694e-07\n",
      "Epoch: 1225000, elapsed: 1.13e+01, train loss: 1.28387e-06, val loss: 2.04466e-06, min loss: 9.18694e-07\n",
      "Epoch: 1225100, elapsed: 1.33e+01, train loss: 1.05965e-06, val loss: 1.92770e-06, min loss: 9.18694e-07\n",
      "Epoch: 1225200, elapsed: 1.13e+01, train loss: 1.39089e-06, val loss: 2.26271e-06, min loss: 9.18694e-07\n",
      "Epoch: 1225300, elapsed: 1.13e+01, train loss: 1.00632e-06, val loss: 1.84610e-06, min loss: 9.18694e-07\n",
      "Epoch: 1225400, elapsed: 1.13e+01, train loss: 9.53819e-07, val loss: 1.80179e-06, min loss: 9.18694e-07\n",
      "Epoch: 1225500, elapsed: 1.12e+01, train loss: 9.21519e-07, val loss: 1.77554e-06, min loss: 9.18694e-07\n",
      "Epoch: 1225600, elapsed: 1.13e+01, train loss: 9.16153e-07, val loss: 1.78707e-06, min loss: 9.16153e-07\n",
      "Epoch: 1225700, elapsed: 1.14e+01, train loss: 9.87889e-07, val loss: 1.83846e-06, min loss: 9.16153e-07\n",
      "Epoch: 1225800, elapsed: 1.14e+01, train loss: 1.34021e-06, val loss: 2.07727e-06, min loss: 9.16153e-07\n",
      "Epoch: 1225900, elapsed: 1.49e+01, train loss: 6.46838e-06, val loss: 7.21297e-06, min loss: 9.16153e-07\n",
      "Epoch: 1226000, elapsed: 1.15e+01, train loss: 9.15518e-07, val loss: 1.78422e-06, min loss: 9.15518e-07\n",
      "Epoch: 1226100, elapsed: 1.15e+01, train loss: 9.35933e-07, val loss: 1.78868e-06, min loss: 9.15518e-07\n",
      "Epoch: 1226200, elapsed: 1.14e+01, train loss: 9.15142e-07, val loss: 1.78377e-06, min loss: 9.15142e-07\n",
      "Epoch: 1226300, elapsed: 1.15e+01, train loss: 9.17024e-07, val loss: 1.77399e-06, min loss: 9.15142e-07\n",
      "Epoch: 1226400, elapsed: 1.14e+01, train loss: 1.20141e-06, val loss: 2.00727e-06, min loss: 9.15142e-07\n",
      "Epoch: 1226500, elapsed: 1.14e+01, train loss: 1.48134e-06, val loss: 2.61044e-06, min loss: 9.15142e-07\n",
      "Epoch: 1226600, elapsed: 1.13e+01, train loss: 1.47828e-06, val loss: 2.47509e-06, min loss: 9.15142e-07\n",
      "Epoch: 1226700, elapsed: 1.14e+01, train loss: 1.40138e-06, val loss: 2.28764e-06, min loss: 9.15142e-07\n",
      "Epoch: 1226800, elapsed: 1.13e+01, train loss: 1.52744e-06, val loss: 2.41868e-06, min loss: 9.15142e-07\n",
      "Epoch: 1226900, elapsed: 1.14e+01, train loss: 2.67601e-06, val loss: 3.95927e-06, min loss: 9.15142e-07\n",
      "Epoch: 1227000, elapsed: 1.13e+01, train loss: 1.15182e-06, val loss: 2.04200e-06, min loss: 9.15142e-07\n",
      "Epoch: 1227100, elapsed: 1.14e+01, train loss: 1.06526e-06, val loss: 1.96896e-06, min loss: 9.15142e-07\n",
      "Epoch: 1227200, elapsed: 1.15e+01, train loss: 9.16337e-07, val loss: 1.78559e-06, min loss: 9.15142e-07\n",
      "Epoch: 1227300, elapsed: 1.14e+01, train loss: 1.03522e-06, val loss: 1.90126e-06, min loss: 9.15142e-07\n",
      "Epoch: 1227400, elapsed: 1.15e+01, train loss: 1.14636e-06, val loss: 2.11943e-06, min loss: 9.15142e-07\n",
      "Epoch: 1227500, elapsed: 1.14e+01, train loss: 9.55670e-07, val loss: 1.81360e-06, min loss: 9.15142e-07\n",
      "Epoch: 1227600, elapsed: 1.14e+01, train loss: 9.41600e-07, val loss: 1.79589e-06, min loss: 9.15142e-07\n",
      "Epoch: 1227700, elapsed: 1.12e+01, train loss: 9.32643e-07, val loss: 1.81389e-06, min loss: 9.15142e-07\n",
      "Epoch: 1227800, elapsed: 1.13e+01, train loss: 9.37190e-07, val loss: 1.83411e-06, min loss: 9.15142e-07\n",
      "Epoch: 1227900, elapsed: 1.13e+01, train loss: 9.16447e-07, val loss: 1.78586e-06, min loss: 9.15142e-07\n",
      "Epoch: 1228000, elapsed: 1.13e+01, train loss: 9.17384e-07, val loss: 1.79146e-06, min loss: 9.15142e-07\n",
      "Epoch: 1228100, elapsed: 1.12e+01, train loss: 9.16621e-07, val loss: 1.79129e-06, min loss: 9.15142e-07\n",
      "Epoch: 1228200, elapsed: 1.49e+01, train loss: 9.14034e-07, val loss: 1.78016e-06, min loss: 9.14034e-07\n",
      "Epoch: 1228300, elapsed: 1.14e+01, train loss: 9.16264e-07, val loss: 1.77722e-06, min loss: 9.14034e-07\n",
      "Epoch: 1228400, elapsed: 1.14e+01, train loss: 9.15318e-07, val loss: 1.78217e-06, min loss: 9.14034e-07\n",
      "Epoch: 1228500, elapsed: 1.15e+01, train loss: 9.23625e-07, val loss: 1.79108e-06, min loss: 9.14034e-07\n",
      "Epoch: 1228600, elapsed: 1.14e+01, train loss: 1.00263e-06, val loss: 1.81737e-06, min loss: 9.14034e-07\n",
      "Epoch: 1228700, elapsed: 1.14e+01, train loss: 1.02097e-06, val loss: 1.91554e-06, min loss: 9.14034e-07\n",
      "Epoch: 1228800, elapsed: 1.16e+01, train loss: 1.18584e-06, val loss: 1.91119e-06, min loss: 9.14034e-07\n",
      "Epoch: 1228900, elapsed: 1.14e+01, train loss: 2.25965e-06, val loss: 3.10360e-06, min loss: 9.14034e-07\n",
      "Epoch: 1229000, elapsed: 1.14e+01, train loss: 9.24627e-07, val loss: 1.78668e-06, min loss: 9.14034e-07\n",
      "Epoch: 1229100, elapsed: 1.15e+01, train loss: 9.70680e-07, val loss: 1.86967e-06, min loss: 9.14034e-07\n",
      "Epoch: 1229200, elapsed: 1.15e+01, train loss: 9.23603e-07, val loss: 1.79702e-06, min loss: 9.14034e-07\n",
      "Epoch: 1229300, elapsed: 1.16e+01, train loss: 9.15559e-07, val loss: 1.77813e-06, min loss: 9.14034e-07\n",
      "Epoch: 1229400, elapsed: 1.15e+01, train loss: 9.35642e-07, val loss: 1.78911e-06, min loss: 9.14034e-07\n",
      "Epoch: 1229500, elapsed: 1.17e+01, train loss: 9.85166e-07, val loss: 1.82612e-06, min loss: 9.14034e-07\n",
      "Epoch: 1229600, elapsed: 1.14e+01, train loss: 9.50633e-07, val loss: 1.78843e-06, min loss: 9.14034e-07\n",
      "Epoch: 1229700, elapsed: 1.13e+01, train loss: 9.29457e-07, val loss: 1.83139e-06, min loss: 9.14034e-07\n",
      "Epoch: 1229800, elapsed: 1.12e+01, train loss: 9.24894e-07, val loss: 1.77009e-06, min loss: 9.14034e-07\n",
      "Epoch: 1229900, elapsed: 1.13e+01, train loss: 2.41040e-06, val loss: 2.72737e-06, min loss: 9.14034e-07\n",
      "Epoch: 1230000, elapsed: 1.13e+01, train loss: 9.18669e-07, val loss: 1.79929e-06, min loss: 9.14034e-07\n",
      "Epoch: 1230100, elapsed: 1.33e+01, train loss: 9.14962e-07, val loss: 1.77580e-06, min loss: 9.14034e-07\n",
      "Epoch: 1230200, elapsed: 1.13e+01, train loss: 9.24950e-07, val loss: 1.80887e-06, min loss: 9.14034e-07\n",
      "Epoch: 1230300, elapsed: 1.14e+01, train loss: 9.36556e-07, val loss: 1.80624e-06, min loss: 9.14034e-07\n",
      "Epoch: 1230400, elapsed: 1.49e+01, train loss: 9.16477e-07, val loss: 1.78729e-06, min loss: 9.14034e-07\n",
      "Epoch: 1230500, elapsed: 1.15e+01, train loss: 9.72341e-07, val loss: 1.79734e-06, min loss: 9.14034e-07\n",
      "Epoch: 1230600, elapsed: 1.15e+01, train loss: 9.14342e-07, val loss: 1.78622e-06, min loss: 9.14034e-07\n",
      "Epoch: 1230700, elapsed: 1.16e+01, train loss: 9.25775e-07, val loss: 1.77589e-06, min loss: 9.14034e-07\n",
      "Epoch: 1230800, elapsed: 1.13e+01, train loss: 1.14888e-06, val loss: 1.98849e-06, min loss: 9.14034e-07\n",
      "Epoch: 1230900, elapsed: 1.14e+01, train loss: 1.30925e-06, val loss: 2.51682e-06, min loss: 9.14034e-07\n",
      "Epoch: 1231000, elapsed: 1.15e+01, train loss: 1.10617e-06, val loss: 2.07844e-06, min loss: 9.14034e-07\n",
      "Epoch: 1231100, elapsed: 1.14e+01, train loss: 1.05323e-06, val loss: 1.90921e-06, min loss: 9.14034e-07\n",
      "Epoch: 1231200, elapsed: 1.15e+01, train loss: 1.14216e-06, val loss: 2.05332e-06, min loss: 9.14034e-07\n",
      "Epoch: 1231300, elapsed: 1.14e+01, train loss: 9.13667e-07, val loss: 1.77312e-06, min loss: 9.13667e-07\n",
      "Epoch: 1231400, elapsed: 1.15e+01, train loss: 9.13827e-07, val loss: 1.78123e-06, min loss: 9.13667e-07\n",
      "Epoch: 1231500, elapsed: 1.16e+01, train loss: 9.75251e-07, val loss: 1.81180e-06, min loss: 9.13667e-07\n",
      "Epoch: 1231600, elapsed: 1.15e+01, train loss: 9.53580e-07, val loss: 1.81476e-06, min loss: 9.13667e-07\n",
      "Epoch: 1231700, elapsed: 1.15e+01, train loss: 1.00829e-06, val loss: 1.88985e-06, min loss: 9.13667e-07\n",
      "Epoch: 1231800, elapsed: 1.13e+01, train loss: 9.22799e-07, val loss: 1.77885e-06, min loss: 9.13667e-07\n",
      "Epoch: 1231900, elapsed: 1.13e+01, train loss: 9.17755e-07, val loss: 1.76996e-06, min loss: 9.13667e-07\n",
      "Epoch: 1232000, elapsed: 1.13e+01, train loss: 9.18625e-07, val loss: 1.77621e-06, min loss: 9.13667e-07\n",
      "Epoch: 1232100, elapsed: 1.14e+01, train loss: 9.17237e-07, val loss: 1.78570e-06, min loss: 9.13667e-07\n",
      "Epoch: 1232200, elapsed: 1.13e+01, train loss: 9.22477e-07, val loss: 1.77802e-06, min loss: 9.13667e-07\n",
      "Epoch: 1232300, elapsed: 1.12e+01, train loss: 9.18126e-07, val loss: 1.79217e-06, min loss: 9.13667e-07\n",
      "Epoch: 1232400, elapsed: 1.13e+01, train loss: 1.17905e-06, val loss: 2.33730e-06, min loss: 9.13667e-07\n",
      "Epoch: 1232500, elapsed: 1.13e+01, train loss: 9.16814e-07, val loss: 1.79149e-06, min loss: 9.13667e-07\n",
      "Epoch: 1232600, elapsed: 1.14e+01, train loss: 9.13874e-07, val loss: 1.77105e-06, min loss: 9.13667e-07\n",
      "Epoch: 1232700, elapsed: 1.50e+01, train loss: 9.64493e-07, val loss: 1.80966e-06, min loss: 9.13667e-07\n",
      "Epoch: 1232800, elapsed: 1.15e+01, train loss: 2.70511e-06, val loss: 3.63306e-06, min loss: 9.13667e-07\n",
      "Epoch: 1232900, elapsed: 1.15e+01, train loss: 1.17661e-06, val loss: 2.09780e-06, min loss: 9.13667e-07\n",
      "Epoch: 1233000, elapsed: 1.16e+01, train loss: 9.42089e-07, val loss: 1.81405e-06, min loss: 9.13667e-07\n",
      "Epoch: 1233100, elapsed: 1.15e+01, train loss: 9.58408e-07, val loss: 1.80996e-06, min loss: 9.13667e-07\n",
      "Epoch: 1233200, elapsed: 1.15e+01, train loss: 1.19927e-06, val loss: 2.05759e-06, min loss: 9.13667e-07\n",
      "Epoch: 1233300, elapsed: 1.14e+01, train loss: 1.02487e-06, val loss: 1.86851e-06, min loss: 9.13667e-07\n",
      "Epoch: 1233400, elapsed: 1.14e+01, train loss: 9.18402e-07, val loss: 1.77762e-06, min loss: 9.13667e-07\n",
      "Epoch: 1233500, elapsed: 1.14e+01, train loss: 9.27823e-07, val loss: 1.78198e-06, min loss: 9.13667e-07\n",
      "Epoch: 1233600, elapsed: 1.14e+01, train loss: 9.11846e-07, val loss: 1.78242e-06, min loss: 9.11846e-07\n",
      "Epoch: 1233700, elapsed: 1.13e+01, train loss: 9.23741e-07, val loss: 1.77934e-06, min loss: 9.11846e-07\n",
      "Epoch: 1233800, elapsed: 1.14e+01, train loss: 1.15001e-06, val loss: 2.06475e-06, min loss: 9.11846e-07\n",
      "Epoch: 1233900, elapsed: 1.13e+01, train loss: 1.25474e-06, val loss: 1.97172e-06, min loss: 9.11846e-07\n",
      "Epoch: 1234000, elapsed: 1.15e+01, train loss: 1.78155e-06, val loss: 2.72144e-06, min loss: 9.11846e-07\n",
      "Epoch: 1234100, elapsed: 1.14e+01, train loss: 3.74440e-06, val loss: 4.93248e-06, min loss: 9.11846e-07\n",
      "Epoch: 1234200, elapsed: 1.15e+01, train loss: 9.17301e-07, val loss: 1.77316e-06, min loss: 9.11846e-07\n",
      "Epoch: 1234300, elapsed: 1.14e+01, train loss: 9.11655e-07, val loss: 1.77424e-06, min loss: 9.11655e-07\n",
      "Epoch: 1234400, elapsed: 1.15e+01, train loss: 9.47162e-07, val loss: 1.79112e-06, min loss: 9.11655e-07\n",
      "Epoch: 1234500, elapsed: 1.13e+01, train loss: 3.21441e-06, val loss: 3.05040e-06, min loss: 9.11655e-07\n",
      "Epoch: 1234600, elapsed: 1.15e+01, train loss: 9.27723e-07, val loss: 1.77801e-06, min loss: 9.11655e-07\n",
      "Epoch: 1234700, elapsed: 1.11e+01, train loss: 9.12250e-07, val loss: 1.78489e-06, min loss: 9.11655e-07\n",
      "Epoch: 1234800, elapsed: 1.16e+01, train loss: 9.12853e-07, val loss: 1.78666e-06, min loss: 9.11655e-07\n",
      "Epoch: 1234900, elapsed: 1.13e+01, train loss: 1.54866e-06, val loss: 2.01830e-06, min loss: 9.11655e-07\n",
      "Epoch: 1235000, elapsed: 1.49e+01, train loss: 9.60372e-07, val loss: 1.89624e-06, min loss: 9.11655e-07\n",
      "Epoch: 1235100, elapsed: 1.35e+01, train loss: 9.33085e-07, val loss: 1.81813e-06, min loss: 9.11655e-07\n",
      "Epoch: 1235200, elapsed: 1.15e+01, train loss: 1.39568e-06, val loss: 2.03293e-06, min loss: 9.11655e-07\n",
      "Epoch: 1235300, elapsed: 1.15e+01, train loss: 9.13638e-07, val loss: 1.78049e-06, min loss: 9.11655e-07\n",
      "Epoch: 1235400, elapsed: 1.15e+01, train loss: 9.15153e-07, val loss: 1.76950e-06, min loss: 9.11655e-07\n",
      "Epoch: 1235500, elapsed: 1.14e+01, train loss: 1.28238e-06, val loss: 2.35623e-06, min loss: 9.11655e-07\n",
      "Epoch: 1235600, elapsed: 1.14e+01, train loss: 9.09491e-07, val loss: 1.77503e-06, min loss: 9.09491e-07\n",
      "Epoch: 1235700, elapsed: 1.15e+01, train loss: 9.17890e-07, val loss: 1.79782e-06, min loss: 9.09491e-07\n",
      "Epoch: 1235800, elapsed: 1.16e+01, train loss: 1.23633e-06, val loss: 2.05643e-06, min loss: 9.09491e-07\n",
      "Epoch: 1235900, elapsed: 1.14e+01, train loss: 1.00483e-06, val loss: 1.84504e-06, min loss: 9.09491e-07\n",
      "Epoch: 1236000, elapsed: 1.14e+01, train loss: 9.61537e-07, val loss: 1.82575e-06, min loss: 9.09491e-07\n",
      "Epoch: 1236100, elapsed: 1.13e+01, train loss: 1.05334e-06, val loss: 1.95681e-06, min loss: 9.09491e-07\n",
      "Epoch: 1236200, elapsed: 1.13e+01, train loss: 1.55100e-06, val loss: 2.65258e-06, min loss: 9.09491e-07\n",
      "Epoch: 1236300, elapsed: 1.13e+01, train loss: 1.07863e-06, val loss: 2.02890e-06, min loss: 9.09491e-07\n",
      "Epoch: 1236400, elapsed: 1.14e+01, train loss: 1.01204e-06, val loss: 1.83686e-06, min loss: 9.09491e-07\n",
      "Epoch: 1236500, elapsed: 1.14e+01, train loss: 1.13173e-06, val loss: 1.94723e-06, min loss: 9.09491e-07\n",
      "Epoch: 1236600, elapsed: 1.14e+01, train loss: 9.14090e-07, val loss: 1.80463e-06, min loss: 9.09491e-07\n",
      "Epoch: 1236700, elapsed: 1.12e+01, train loss: 9.32324e-07, val loss: 1.88085e-06, min loss: 9.09491e-07\n",
      "Epoch: 1236800, elapsed: 1.12e+01, train loss: 1.24550e-06, val loss: 2.15059e-06, min loss: 9.09491e-07\n",
      "Epoch: 1236900, elapsed: 1.16e+01, train loss: 9.12333e-07, val loss: 1.78692e-06, min loss: 9.09491e-07\n",
      "Epoch: 1237000, elapsed: 1.14e+01, train loss: 9.09725e-07, val loss: 1.77858e-06, min loss: 9.09491e-07\n",
      "Epoch: 1237100, elapsed: 1.15e+01, train loss: 9.13146e-07, val loss: 1.78361e-06, min loss: 9.09491e-07\n",
      "Epoch: 1237200, elapsed: 1.48e+01, train loss: 9.52227e-07, val loss: 1.86229e-06, min loss: 9.09491e-07\n",
      "Epoch: 1237300, elapsed: 1.16e+01, train loss: 9.85166e-07, val loss: 1.78338e-06, min loss: 9.09491e-07\n",
      "Epoch: 1237400, elapsed: 1.15e+01, train loss: 9.43659e-07, val loss: 1.79153e-06, min loss: 9.09491e-07\n",
      "Epoch: 1237500, elapsed: 1.13e+01, train loss: 9.11498e-07, val loss: 1.76908e-06, min loss: 9.09491e-07\n",
      "Epoch: 1237600, elapsed: 1.14e+01, train loss: 9.71031e-07, val loss: 1.83468e-06, min loss: 9.09491e-07\n",
      "Epoch: 1237700, elapsed: 1.16e+01, train loss: 9.36401e-07, val loss: 1.82335e-06, min loss: 9.09491e-07\n",
      "Epoch: 1237800, elapsed: 1.15e+01, train loss: 1.39573e-06, val loss: 2.18223e-06, min loss: 9.09491e-07\n",
      "Epoch: 1237900, elapsed: 1.13e+01, train loss: 1.40716e-06, val loss: 2.16158e-06, min loss: 9.09491e-07\n",
      "Epoch: 1238000, elapsed: 1.14e+01, train loss: 9.72542e-07, val loss: 1.84032e-06, min loss: 9.09491e-07\n",
      "Epoch: 1238100, elapsed: 1.15e+01, train loss: 1.00851e-06, val loss: 1.94648e-06, min loss: 9.09491e-07\n",
      "Epoch: 1238200, elapsed: 1.15e+01, train loss: 1.85979e-06, val loss: 3.11230e-06, min loss: 9.09491e-07\n",
      "Epoch: 1238300, elapsed: 1.16e+01, train loss: 9.37017e-07, val loss: 1.80347e-06, min loss: 9.09491e-07\n",
      "Epoch: 1238400, elapsed: 1.16e+01, train loss: 9.78074e-07, val loss: 1.84345e-06, min loss: 9.09491e-07\n",
      "Epoch: 1238500, elapsed: 1.15e+01, train loss: 2.92249e-06, val loss: 3.77565e-06, min loss: 9.09491e-07\n",
      "Epoch: 1238600, elapsed: 1.16e+01, train loss: 1.39333e-06, val loss: 1.96692e-06, min loss: 9.09491e-07\n",
      "Epoch: 1238700, elapsed: 1.15e+01, train loss: 1.07120e-06, val loss: 1.91129e-06, min loss: 9.09491e-07\n",
      "Epoch: 1238800, elapsed: 1.14e+01, train loss: 1.38870e-06, val loss: 2.11073e-06, min loss: 9.09491e-07\n",
      "Epoch: 1238900, elapsed: 1.13e+01, train loss: 5.16563e-06, val loss: 6.32697e-06, min loss: 9.09491e-07\n",
      "Epoch: 1239000, elapsed: 1.13e+01, train loss: 9.07467e-07, val loss: 1.77191e-06, min loss: 9.07467e-07\n",
      "Epoch: 1239100, elapsed: 1.13e+01, train loss: 9.33999e-07, val loss: 1.77065e-06, min loss: 9.07467e-07\n",
      "Epoch: 1239200, elapsed: 1.15e+01, train loss: 1.03565e-06, val loss: 1.90637e-06, min loss: 9.07467e-07\n",
      "Epoch: 1239300, elapsed: 1.14e+01, train loss: 2.34355e-06, val loss: 3.62096e-06, min loss: 9.07467e-07\n",
      "Epoch: 1239400, elapsed: 1.14e+01, train loss: 9.07451e-07, val loss: 1.77252e-06, min loss: 9.07451e-07\n",
      "Epoch: 1239500, elapsed: 1.49e+01, train loss: 9.15902e-07, val loss: 1.78845e-06, min loss: 9.07451e-07\n",
      "Epoch: 1239600, elapsed: 1.15e+01, train loss: 9.43264e-07, val loss: 1.81960e-06, min loss: 9.07451e-07\n",
      "Epoch: 1239700, elapsed: 1.15e+01, train loss: 2.49854e-06, val loss: 3.68378e-06, min loss: 9.07451e-07\n",
      "Epoch: 1239800, elapsed: 1.16e+01, train loss: 2.47050e-06, val loss: 2.73449e-06, min loss: 9.07451e-07\n",
      "Epoch: 1239900, elapsed: 1.15e+01, train loss: 2.60500e-06, val loss: 3.91618e-06, min loss: 9.07451e-07\n",
      "Epoch: 1240000, elapsed: 1.16e+01, train loss: 9.11003e-07, val loss: 1.76635e-06, min loss: 9.07451e-07\n",
      "Epoch: 1240100, elapsed: 1.35e+01, train loss: 9.09985e-07, val loss: 1.77476e-06, min loss: 9.07451e-07\n",
      "Epoch: 1240200, elapsed: 1.14e+01, train loss: 9.57373e-07, val loss: 1.84975e-06, min loss: 9.07451e-07\n",
      "Epoch: 1240300, elapsed: 1.13e+01, train loss: 2.18336e-06, val loss: 3.09363e-06, min loss: 9.07451e-07\n",
      "Epoch: 1240400, elapsed: 1.14e+01, train loss: 3.88968e-06, val loss: 4.13852e-06, min loss: 9.07451e-07\n",
      "Epoch: 1240500, elapsed: 1.13e+01, train loss: 1.63477e-06, val loss: 2.40251e-06, min loss: 9.07451e-07\n",
      "Epoch: 1240600, elapsed: 1.14e+01, train loss: 3.56644e-06, val loss: 4.13005e-06, min loss: 9.07451e-07\n",
      "Epoch: 1240700, elapsed: 1.14e+01, train loss: 1.14177e-06, val loss: 2.05723e-06, min loss: 9.07451e-07\n",
      "Epoch: 1240800, elapsed: 1.14e+01, train loss: 9.22239e-07, val loss: 1.77452e-06, min loss: 9.07451e-07\n",
      "Epoch: 1240900, elapsed: 1.15e+01, train loss: 9.08012e-07, val loss: 1.76648e-06, min loss: 9.07451e-07\n",
      "Epoch: 1241000, elapsed: 1.13e+01, train loss: 9.12351e-07, val loss: 1.77398e-06, min loss: 9.07451e-07\n",
      "Epoch: 1241100, elapsed: 1.12e+01, train loss: 1.11275e-06, val loss: 1.94925e-06, min loss: 9.07451e-07\n",
      "Epoch: 1241200, elapsed: 1.11e+01, train loss: 9.18517e-07, val loss: 1.77397e-06, min loss: 9.07451e-07\n",
      "Epoch: 1241300, elapsed: 1.12e+01, train loss: 9.06548e-07, val loss: 1.76837e-06, min loss: 9.06548e-07\n",
      "Epoch: 1241400, elapsed: 1.14e+01, train loss: 2.12355e-06, val loss: 3.13852e-06, min loss: 9.06548e-07\n",
      "Epoch: 1241500, elapsed: 1.16e+01, train loss: 9.11150e-07, val loss: 1.76729e-06, min loss: 9.06548e-07\n",
      "Epoch: 1241600, elapsed: 1.15e+01, train loss: 9.05817e-07, val loss: 1.76829e-06, min loss: 9.05817e-07\n",
      "Epoch: 1241700, elapsed: 1.15e+01, train loss: 1.10852e-06, val loss: 1.94380e-06, min loss: 9.05817e-07\n",
      "Epoch: 1241800, elapsed: 1.49e+01, train loss: 9.05703e-07, val loss: 1.77230e-06, min loss: 9.05703e-07\n",
      "Epoch: 1241900, elapsed: 1.15e+01, train loss: 9.10623e-07, val loss: 1.78173e-06, min loss: 9.05703e-07\n",
      "Epoch: 1242000, elapsed: 1.15e+01, train loss: 9.20807e-07, val loss: 1.80662e-06, min loss: 9.05703e-07\n",
      "Epoch: 1242100, elapsed: 1.14e+01, train loss: 1.91237e-06, val loss: 2.02232e-06, min loss: 9.05703e-07\n",
      "Epoch: 1242200, elapsed: 1.16e+01, train loss: 9.74976e-07, val loss: 1.83066e-06, min loss: 9.05703e-07\n",
      "Epoch: 1242300, elapsed: 1.15e+01, train loss: 9.12579e-07, val loss: 1.77574e-06, min loss: 9.05703e-07\n",
      "Epoch: 1242400, elapsed: 1.14e+01, train loss: 1.06278e-06, val loss: 1.90420e-06, min loss: 9.05703e-07\n",
      "Epoch: 1242500, elapsed: 1.13e+01, train loss: 1.24097e-06, val loss: 2.06766e-06, min loss: 9.05703e-07\n",
      "Epoch: 1242600, elapsed: 1.13e+01, train loss: 1.37808e-06, val loss: 2.24506e-06, min loss: 9.05703e-07\n",
      "Epoch: 1242700, elapsed: 1.14e+01, train loss: 1.09508e-06, val loss: 1.95233e-06, min loss: 9.05703e-07\n",
      "Epoch: 1242800, elapsed: 1.14e+01, train loss: 1.04797e-06, val loss: 1.81217e-06, min loss: 9.05703e-07\n",
      "Epoch: 1242900, elapsed: 1.15e+01, train loss: 9.20139e-07, val loss: 1.77799e-06, min loss: 9.05703e-07\n",
      "Epoch: 1243000, elapsed: 1.14e+01, train loss: 9.13128e-07, val loss: 1.77025e-06, min loss: 9.05703e-07\n",
      "Epoch: 1243100, elapsed: 1.14e+01, train loss: 9.17645e-07, val loss: 1.79155e-06, min loss: 9.05703e-07\n",
      "Epoch: 1243200, elapsed: 1.14e+01, train loss: 1.69860e-06, val loss: 2.33165e-06, min loss: 9.05703e-07\n",
      "Epoch: 1243300, elapsed: 1.12e+01, train loss: 1.77399e-06, val loss: 2.82908e-06, min loss: 9.05703e-07\n",
      "Epoch: 1243400, elapsed: 1.14e+01, train loss: 9.10216e-07, val loss: 1.77311e-06, min loss: 9.05703e-07\n",
      "Epoch: 1243500, elapsed: 1.14e+01, train loss: 1.18320e-06, val loss: 2.10249e-06, min loss: 9.05703e-07\n",
      "Epoch: 1243600, elapsed: 1.13e+01, train loss: 9.68862e-07, val loss: 1.92280e-06, min loss: 9.05703e-07\n",
      "Epoch: 1243700, elapsed: 1.13e+01, train loss: 1.37862e-06, val loss: 2.40263e-06, min loss: 9.05703e-07\n",
      "Epoch: 1243800, elapsed: 1.15e+01, train loss: 3.43731e-06, val loss: 4.36523e-06, min loss: 9.05703e-07\n",
      "Epoch: 1243900, elapsed: 1.14e+01, train loss: 9.09106e-07, val loss: 1.78251e-06, min loss: 9.05703e-07\n",
      "Epoch: 1244000, elapsed: 1.12e+01, train loss: 9.06733e-07, val loss: 1.76829e-06, min loss: 9.05703e-07\n",
      "Epoch: 1244100, elapsed: 1.50e+01, train loss: 9.73364e-07, val loss: 1.79602e-06, min loss: 9.05703e-07\n",
      "Epoch: 1244200, elapsed: 1.13e+01, train loss: 2.34459e-06, val loss: 3.03802e-06, min loss: 9.05703e-07\n",
      "Epoch: 1244300, elapsed: 1.14e+01, train loss: 2.66903e-06, val loss: 2.77924e-06, min loss: 9.05703e-07\n",
      "Epoch: 1244400, elapsed: 1.15e+01, train loss: 9.04138e-07, val loss: 1.76836e-06, min loss: 9.04138e-07\n",
      "Epoch: 1244500, elapsed: 1.13e+01, train loss: 9.09148e-07, val loss: 1.77856e-06, min loss: 9.04138e-07\n",
      "Epoch: 1244600, elapsed: 1.15e+01, train loss: 1.22390e-06, val loss: 2.00906e-06, min loss: 9.04138e-07\n",
      "Epoch: 1244700, elapsed: 1.14e+01, train loss: 9.03996e-07, val loss: 1.76772e-06, min loss: 9.03996e-07\n",
      "Epoch: 1244800, elapsed: 1.14e+01, train loss: 1.30499e-06, val loss: 1.97715e-06, min loss: 9.03996e-07\n",
      "Epoch: 1244900, elapsed: 1.15e+01, train loss: 1.32059e-06, val loss: 2.02626e-06, min loss: 9.03996e-07\n",
      "Epoch: 1245000, elapsed: 1.15e+01, train loss: 1.04642e-06, val loss: 1.82313e-06, min loss: 9.03996e-07\n",
      "Epoch: 1245100, elapsed: 1.35e+01, train loss: 9.22811e-07, val loss: 1.82597e-06, min loss: 9.03996e-07\n",
      "Epoch: 1245200, elapsed: 1.13e+01, train loss: 9.06740e-07, val loss: 1.77057e-06, min loss: 9.03996e-07\n",
      "Epoch: 1245300, elapsed: 1.14e+01, train loss: 9.53047e-07, val loss: 1.76724e-06, min loss: 9.03996e-07\n",
      "Epoch: 1245400, elapsed: 1.14e+01, train loss: 1.05151e-06, val loss: 1.92278e-06, min loss: 9.03996e-07\n",
      "Epoch: 1245500, elapsed: 1.15e+01, train loss: 9.39043e-07, val loss: 1.81150e-06, min loss: 9.03996e-07\n",
      "Epoch: 1245600, elapsed: 1.16e+01, train loss: 1.73059e-06, val loss: 2.80697e-06, min loss: 9.03996e-07\n",
      "Epoch: 1245700, elapsed: 1.15e+01, train loss: 9.03863e-07, val loss: 1.76362e-06, min loss: 9.03863e-07\n",
      "Epoch: 1245800, elapsed: 1.13e+01, train loss: 9.12700e-07, val loss: 1.77540e-06, min loss: 9.03863e-07\n",
      "Epoch: 1245900, elapsed: 1.14e+01, train loss: 1.16498e-06, val loss: 1.91652e-06, min loss: 9.03863e-07\n",
      "Epoch: 1246000, elapsed: 1.15e+01, train loss: 9.55283e-07, val loss: 1.83508e-06, min loss: 9.03863e-07\n",
      "Epoch: 1246100, elapsed: 1.15e+01, train loss: 9.14433e-07, val loss: 1.76739e-06, min loss: 9.03863e-07\n",
      "Epoch: 1246200, elapsed: 1.12e+01, train loss: 1.39854e-06, val loss: 2.02170e-06, min loss: 9.03863e-07\n",
      "Epoch: 1246300, elapsed: 1.49e+01, train loss: 1.02319e-06, val loss: 1.89954e-06, min loss: 9.03863e-07\n",
      "Epoch: 1246400, elapsed: 1.15e+01, train loss: 9.03977e-07, val loss: 1.76890e-06, min loss: 9.03863e-07\n",
      "Epoch: 1246500, elapsed: 1.16e+01, train loss: 9.05324e-07, val loss: 1.77777e-06, min loss: 9.03863e-07\n",
      "Epoch: 1246600, elapsed: 1.15e+01, train loss: 9.86753e-07, val loss: 1.86267e-06, min loss: 9.03863e-07\n",
      "Epoch: 1246700, elapsed: 1.15e+01, train loss: 2.21854e-06, val loss: 3.36634e-06, min loss: 9.03863e-07\n",
      "Epoch: 1246800, elapsed: 1.17e+01, train loss: 1.45385e-06, val loss: 2.38789e-06, min loss: 9.03863e-07\n",
      "Epoch: 1246900, elapsed: 1.16e+01, train loss: 9.03788e-07, val loss: 1.75867e-06, min loss: 9.03788e-07\n",
      "Epoch: 1247000, elapsed: 1.13e+01, train loss: 9.09872e-07, val loss: 1.75817e-06, min loss: 9.03788e-07\n",
      "Epoch: 1247100, elapsed: 1.14e+01, train loss: 5.46146e-06, val loss: 5.32328e-06, min loss: 9.03788e-07\n",
      "Epoch: 1247200, elapsed: 1.14e+01, train loss: 9.05855e-07, val loss: 1.76582e-06, min loss: 9.03788e-07\n",
      "Epoch: 1247300, elapsed: 1.16e+01, train loss: 9.13126e-07, val loss: 1.78797e-06, min loss: 9.03788e-07\n",
      "Epoch: 1247400, elapsed: 1.14e+01, train loss: 9.06858e-07, val loss: 1.78325e-06, min loss: 9.03788e-07\n",
      "Epoch: 1247500, elapsed: 1.14e+01, train loss: 1.32400e-06, val loss: 2.33084e-06, min loss: 9.03788e-07\n",
      "Epoch: 1247600, elapsed: 1.12e+01, train loss: 1.96943e-06, val loss: 2.94769e-06, min loss: 9.03788e-07\n",
      "Epoch: 1247700, elapsed: 1.14e+01, train loss: 1.68282e-06, val loss: 2.49326e-06, min loss: 9.03788e-07\n",
      "Epoch: 1247800, elapsed: 1.13e+01, train loss: 2.81242e-06, val loss: 2.98479e-06, min loss: 9.03788e-07\n",
      "Epoch: 1247900, elapsed: 1.14e+01, train loss: 1.07313e-06, val loss: 1.80922e-06, min loss: 9.03788e-07\n",
      "Epoch: 1248000, elapsed: 1.13e+01, train loss: 9.24404e-07, val loss: 1.87643e-06, min loss: 9.03788e-07\n",
      "Epoch: 1248100, elapsed: 1.13e+01, train loss: 1.61819e-06, val loss: 2.56475e-06, min loss: 9.03788e-07\n",
      "Epoch: 1248200, elapsed: 1.13e+01, train loss: 9.29331e-07, val loss: 1.77480e-06, min loss: 9.03788e-07\n",
      "Epoch: 1248300, elapsed: 1.14e+01, train loss: 1.46004e-06, val loss: 2.36926e-06, min loss: 9.03788e-07\n",
      "Epoch: 1248400, elapsed: 1.16e+01, train loss: 9.08310e-07, val loss: 1.76717e-06, min loss: 9.03788e-07\n",
      "Epoch: 1248500, elapsed: 1.13e+01, train loss: 9.15563e-07, val loss: 1.77683e-06, min loss: 9.03788e-07\n",
      "Epoch: 1248600, elapsed: 1.51e+01, train loss: 9.42634e-07, val loss: 1.89128e-06, min loss: 9.03788e-07\n",
      "Epoch: 1248700, elapsed: 1.17e+01, train loss: 9.32738e-07, val loss: 1.84160e-06, min loss: 9.03788e-07\n",
      "Epoch: 1248800, elapsed: 1.15e+01, train loss: 1.03720e-06, val loss: 1.96419e-06, min loss: 9.03788e-07\n",
      "Epoch: 1248900, elapsed: 1.18e+01, train loss: 1.06384e-06, val loss: 1.90130e-06, min loss: 9.03788e-07\n",
      "Epoch: 1249000, elapsed: 1.15e+01, train loss: 1.08316e-06, val loss: 2.08874e-06, min loss: 9.03788e-07\n",
      "Epoch: 1249100, elapsed: 1.15e+01, train loss: 1.07338e-06, val loss: 2.03788e-06, min loss: 9.03788e-07\n",
      "Epoch: 1249200, elapsed: 1.14e+01, train loss: 9.08744e-07, val loss: 1.76490e-06, min loss: 9.03788e-07\n",
      "Epoch: 1249300, elapsed: 1.13e+01, train loss: 1.02640e-06, val loss: 1.94419e-06, min loss: 9.03788e-07\n",
      "Epoch: 1249400, elapsed: 1.16e+01, train loss: 1.00891e-06, val loss: 1.81099e-06, min loss: 9.03788e-07\n",
      "Epoch: 1249500, elapsed: 1.15e+01, train loss: 1.10290e-06, val loss: 1.92528e-06, min loss: 9.03788e-07\n",
      "Epoch: 1249600, elapsed: 1.15e+01, train loss: 1.16625e-06, val loss: 2.08664e-06, min loss: 9.03788e-07\n",
      "Epoch: 1249700, elapsed: 1.15e+01, train loss: 9.18328e-07, val loss: 1.79757e-06, min loss: 9.03788e-07\n",
      "Epoch: 1249800, elapsed: 1.14e+01, train loss: 1.08052e-06, val loss: 1.95467e-06, min loss: 9.03788e-07\n",
      "Epoch: 1249900, elapsed: 1.14e+01, train loss: 9.69941e-07, val loss: 1.85414e-06, min loss: 9.03788e-07\n",
      "Epoch: 1250000, elapsed: 1.14e+01, train loss: 3.19047e-06, val loss: 4.49898e-06, min loss: 9.03788e-07\n",
      "Epoch: 1250100, elapsed: 1.33e+01, train loss: 1.31416e-06, val loss: 1.89913e-06, min loss: 9.03788e-07\n",
      "Epoch: 1250200, elapsed: 1.13e+01, train loss: 1.44187e-06, val loss: 1.88403e-06, min loss: 9.03788e-07\n",
      "Epoch: 1250300, elapsed: 1.15e+01, train loss: 1.16251e-06, val loss: 1.91086e-06, min loss: 9.03788e-07\n",
      "Epoch: 1250400, elapsed: 1.15e+01, train loss: 8.01066e-06, val loss: 8.15337e-06, min loss: 9.03788e-07\n",
      "Epoch: 1250500, elapsed: 1.15e+01, train loss: 9.00421e-07, val loss: 1.76298e-06, min loss: 9.00421e-07\n",
      "Epoch: 1250600, elapsed: 1.14e+01, train loss: 9.43425e-07, val loss: 1.83209e-06, min loss: 9.00421e-07\n",
      "Epoch: 1250700, elapsed: 1.16e+01, train loss: 2.89777e-06, val loss: 3.82677e-06, min loss: 9.00421e-07\n",
      "Epoch: 1250800, elapsed: 1.13e+01, train loss: 1.14491e-06, val loss: 1.96469e-06, min loss: 9.00421e-07\n",
      "Epoch: 1250900, elapsed: 1.49e+01, train loss: 9.56365e-07, val loss: 1.86351e-06, min loss: 9.00421e-07\n",
      "Epoch: 1251000, elapsed: 1.16e+01, train loss: 1.14866e-06, val loss: 2.14763e-06, min loss: 9.00421e-07\n",
      "Epoch: 1251100, elapsed: 1.15e+01, train loss: 1.11736e-06, val loss: 2.06521e-06, min loss: 9.00421e-07\n",
      "Epoch: 1251200, elapsed: 1.15e+01, train loss: 1.03825e-06, val loss: 1.87126e-06, min loss: 9.00421e-07\n",
      "Epoch: 1251300, elapsed: 1.16e+01, train loss: 1.68612e-06, val loss: 2.41796e-06, min loss: 9.00421e-07\n",
      "Epoch: 1251400, elapsed: 1.15e+01, train loss: 9.78731e-07, val loss: 1.87312e-06, min loss: 9.00421e-07\n",
      "Epoch: 1251500, elapsed: 1.14e+01, train loss: 9.42220e-07, val loss: 1.79493e-06, min loss: 9.00421e-07\n",
      "Epoch: 1251600, elapsed: 1.14e+01, train loss: 1.13623e-06, val loss: 2.09436e-06, min loss: 9.00421e-07\n",
      "Epoch: 1251700, elapsed: 1.13e+01, train loss: 1.09158e-06, val loss: 2.08213e-06, min loss: 9.00421e-07\n",
      "Epoch: 1251800, elapsed: 1.14e+01, train loss: 1.35135e-06, val loss: 2.24352e-06, min loss: 9.00421e-07\n",
      "Epoch: 1251900, elapsed: 1.14e+01, train loss: 9.65008e-07, val loss: 1.79232e-06, min loss: 9.00421e-07\n",
      "Epoch: 1252000, elapsed: 1.13e+01, train loss: 9.01518e-07, val loss: 1.75884e-06, min loss: 9.00421e-07\n",
      "Epoch: 1252100, elapsed: 1.14e+01, train loss: 9.07369e-07, val loss: 1.78851e-06, min loss: 9.00421e-07\n",
      "Epoch: 1252200, elapsed: 1.13e+01, train loss: 9.68526e-07, val loss: 1.84530e-06, min loss: 9.00421e-07\n",
      "Epoch: 1252300, elapsed: 1.14e+01, train loss: 1.00220e-06, val loss: 1.83988e-06, min loss: 9.00421e-07\n",
      "Epoch: 1252400, elapsed: 1.14e+01, train loss: 2.91487e-06, val loss: 3.13722e-06, min loss: 9.00421e-07\n",
      "Epoch: 1252500, elapsed: 1.13e+01, train loss: 9.36934e-07, val loss: 1.82970e-06, min loss: 9.00421e-07\n",
      "Epoch: 1252600, elapsed: 1.14e+01, train loss: 9.90293e-07, val loss: 1.84246e-06, min loss: 9.00421e-07\n",
      "Epoch: 1252700, elapsed: 1.13e+01, train loss: 9.04590e-07, val loss: 1.75614e-06, min loss: 9.00421e-07\n",
      "Epoch: 1252800, elapsed: 1.13e+01, train loss: 9.28249e-07, val loss: 1.78356e-06, min loss: 9.00421e-07\n",
      "Epoch: 1252900, elapsed: 1.13e+01, train loss: 9.08312e-07, val loss: 1.74901e-06, min loss: 9.00421e-07\n",
      "Epoch: 1253000, elapsed: 1.12e+01, train loss: 1.00078e-06, val loss: 1.81867e-06, min loss: 9.00421e-07\n",
      "Epoch: 1253100, elapsed: 1.14e+01, train loss: 1.57660e-06, val loss: 2.18005e-06, min loss: 9.00421e-07\n",
      "Epoch: 1253200, elapsed: 1.50e+01, train loss: 8.98860e-07, val loss: 1.76486e-06, min loss: 8.98860e-07\n",
      "Epoch: 1253300, elapsed: 1.14e+01, train loss: 1.01368e-06, val loss: 1.83517e-06, min loss: 8.98860e-07\n",
      "Epoch: 1253400, elapsed: 1.15e+01, train loss: 3.40574e-06, val loss: 4.32858e-06, min loss: 8.98860e-07\n",
      "Epoch: 1253500, elapsed: 1.14e+01, train loss: 1.56294e-06, val loss: 2.68628e-06, min loss: 8.98860e-07\n",
      "Epoch: 1253600, elapsed: 1.15e+01, train loss: 1.07061e-06, val loss: 1.95246e-06, min loss: 8.98860e-07\n",
      "Epoch: 1253700, elapsed: 1.15e+01, train loss: 8.98603e-07, val loss: 1.75503e-06, min loss: 8.98603e-07\n",
      "Epoch: 1253800, elapsed: 1.15e+01, train loss: 9.00096e-07, val loss: 1.75861e-06, min loss: 8.98603e-07\n",
      "Epoch: 1253900, elapsed: 1.15e+01, train loss: 1.41045e-06, val loss: 2.09804e-06, min loss: 8.98603e-07\n",
      "Epoch: 1254000, elapsed: 1.13e+01, train loss: 5.60308e-06, val loss: 6.05648e-06, min loss: 8.98603e-07\n",
      "Epoch: 1254100, elapsed: 1.15e+01, train loss: 8.98471e-07, val loss: 1.76182e-06, min loss: 8.98471e-07\n",
      "Epoch: 1254200, elapsed: 1.15e+01, train loss: 9.04233e-07, val loss: 1.76158e-06, min loss: 8.98471e-07\n",
      "Epoch: 1254300, elapsed: 1.14e+01, train loss: 1.05903e-06, val loss: 1.89066e-06, min loss: 8.98471e-07\n",
      "Epoch: 1254400, elapsed: 1.14e+01, train loss: 8.97932e-07, val loss: 1.75890e-06, min loss: 8.97932e-07\n",
      "Epoch: 1254500, elapsed: 1.14e+01, train loss: 9.14207e-07, val loss: 1.76118e-06, min loss: 8.97932e-07\n",
      "Epoch: 1254600, elapsed: 1.13e+01, train loss: 8.98739e-07, val loss: 1.76422e-06, min loss: 8.97932e-07\n",
      "Epoch: 1254700, elapsed: 1.13e+01, train loss: 8.99686e-07, val loss: 1.76396e-06, min loss: 8.97932e-07\n",
      "Epoch: 1254800, elapsed: 1.14e+01, train loss: 1.07314e-06, val loss: 2.05024e-06, min loss: 8.97932e-07\n",
      "Epoch: 1254900, elapsed: 1.14e+01, train loss: 8.97971e-07, val loss: 1.75474e-06, min loss: 8.97932e-07\n",
      "Epoch: 1255000, elapsed: 1.13e+01, train loss: 9.09731e-07, val loss: 1.75078e-06, min loss: 8.97932e-07\n",
      "Epoch: 1255100, elapsed: 1.34e+01, train loss: 1.40884e-06, val loss: 2.29471e-06, min loss: 8.97932e-07\n",
      "Epoch: 1255200, elapsed: 1.12e+01, train loss: 2.79326e-06, val loss: 3.69968e-06, min loss: 8.97932e-07\n",
      "Epoch: 1255300, elapsed: 1.14e+01, train loss: 9.32619e-07, val loss: 1.75873e-06, min loss: 8.97932e-07\n",
      "Epoch: 1255400, elapsed: 1.15e+01, train loss: 8.98435e-07, val loss: 1.76139e-06, min loss: 8.97932e-07\n",
      "Epoch: 1255500, elapsed: 1.50e+01, train loss: 9.74191e-07, val loss: 1.86163e-06, min loss: 8.97932e-07\n",
      "Epoch: 1255600, elapsed: 1.15e+01, train loss: 9.13903e-07, val loss: 1.82671e-06, min loss: 8.97932e-07\n",
      "Epoch: 1255700, elapsed: 1.15e+01, train loss: 5.30330e-06, val loss: 5.68174e-06, min loss: 8.97932e-07\n",
      "Epoch: 1255800, elapsed: 1.15e+01, train loss: 8.97234e-07, val loss: 1.75790e-06, min loss: 8.97234e-07\n",
      "Epoch: 1255900, elapsed: 1.14e+01, train loss: 9.05313e-07, val loss: 1.78134e-06, min loss: 8.97234e-07\n",
      "Epoch: 1256000, elapsed: 1.17e+01, train loss: 9.60856e-07, val loss: 1.80909e-06, min loss: 8.97234e-07\n",
      "Epoch: 1256100, elapsed: 1.17e+01, train loss: 2.43437e-06, val loss: 3.18449e-06, min loss: 8.97234e-07\n",
      "Epoch: 1256200, elapsed: 1.15e+01, train loss: 9.75011e-07, val loss: 1.78702e-06, min loss: 8.97234e-07\n",
      "Epoch: 1256300, elapsed: 1.15e+01, train loss: 9.00301e-07, val loss: 1.76999e-06, min loss: 8.97234e-07\n",
      "Epoch: 1256400, elapsed: 1.13e+01, train loss: 9.35874e-07, val loss: 1.76216e-06, min loss: 8.97234e-07\n",
      "Epoch: 1256500, elapsed: 1.14e+01, train loss: 1.43122e-06, val loss: 2.45955e-06, min loss: 8.97234e-07\n",
      "Epoch: 1256600, elapsed: 1.14e+01, train loss: 9.05305e-07, val loss: 1.75518e-06, min loss: 8.97234e-07\n",
      "Epoch: 1256700, elapsed: 1.14e+01, train loss: 8.99475e-07, val loss: 1.75230e-06, min loss: 8.97234e-07\n",
      "Epoch: 1256800, elapsed: 1.14e+01, train loss: 9.33133e-07, val loss: 1.76230e-06, min loss: 8.97234e-07\n",
      "Epoch: 1256900, elapsed: 1.16e+01, train loss: 2.04207e-06, val loss: 2.67669e-06, min loss: 8.97234e-07\n",
      "Epoch: 1257000, elapsed: 1.14e+01, train loss: 1.16736e-06, val loss: 2.19439e-06, min loss: 8.97234e-07\n",
      "Epoch: 1257100, elapsed: 1.13e+01, train loss: 8.97156e-07, val loss: 1.75879e-06, min loss: 8.97156e-07\n",
      "Epoch: 1257200, elapsed: 1.14e+01, train loss: 9.09391e-07, val loss: 1.76489e-06, min loss: 8.97156e-07\n",
      "Epoch: 1257300, elapsed: 1.12e+01, train loss: 9.19464e-07, val loss: 1.81752e-06, min loss: 8.97156e-07\n",
      "Epoch: 1257400, elapsed: 1.12e+01, train loss: 3.47040e-06, val loss: 2.80096e-06, min loss: 8.97156e-07\n",
      "Epoch: 1257500, elapsed: 1.14e+01, train loss: 8.96133e-07, val loss: 1.75503e-06, min loss: 8.96133e-07\n",
      "Epoch: 1257600, elapsed: 1.13e+01, train loss: 3.49779e-06, val loss: 2.62347e-06, min loss: 8.96133e-07\n",
      "Epoch: 1257700, elapsed: 1.14e+01, train loss: 8.96030e-07, val loss: 1.75678e-06, min loss: 8.96030e-07\n",
      "Epoch: 1257800, elapsed: 1.50e+01, train loss: 9.90397e-07, val loss: 1.81419e-06, min loss: 8.96030e-07\n",
      "Epoch: 1257900, elapsed: 1.17e+01, train loss: 1.36534e-06, val loss: 2.18330e-06, min loss: 8.96030e-07\n",
      "Epoch: 1258000, elapsed: 1.15e+01, train loss: 1.00713e-06, val loss: 1.90807e-06, min loss: 8.96030e-07\n",
      "Epoch: 1258100, elapsed: 1.15e+01, train loss: 9.02796e-07, val loss: 1.76876e-06, min loss: 8.96030e-07\n",
      "Epoch: 1258200, elapsed: 1.16e+01, train loss: 9.74129e-07, val loss: 1.78968e-06, min loss: 8.96030e-07\n",
      "Epoch: 1258300, elapsed: 1.14e+01, train loss: 9.19227e-07, val loss: 1.74879e-06, min loss: 8.96030e-07\n",
      "Epoch: 1258400, elapsed: 1.14e+01, train loss: 9.23147e-07, val loss: 1.76384e-06, min loss: 8.96030e-07\n",
      "Epoch: 1258500, elapsed: 1.13e+01, train loss: 9.01288e-07, val loss: 1.75546e-06, min loss: 8.96030e-07\n",
      "Epoch: 1258600, elapsed: 1.15e+01, train loss: 9.31869e-07, val loss: 1.81461e-06, min loss: 8.96030e-07\n",
      "Epoch: 1258700, elapsed: 1.13e+01, train loss: 9.01671e-07, val loss: 1.76896e-06, min loss: 8.96030e-07\n",
      "Epoch: 1258800, elapsed: 1.14e+01, train loss: 9.27028e-07, val loss: 1.79204e-06, min loss: 8.96030e-07\n",
      "Epoch: 1258900, elapsed: 1.13e+01, train loss: 9.08583e-07, val loss: 1.76125e-06, min loss: 8.96030e-07\n",
      "Epoch: 1259000, elapsed: 1.14e+01, train loss: 1.08895e-06, val loss: 1.92538e-06, min loss: 8.96030e-07\n",
      "Epoch: 1259100, elapsed: 1.15e+01, train loss: 1.87823e-06, val loss: 2.25221e-06, min loss: 8.96030e-07\n",
      "Epoch: 1259200, elapsed: 1.13e+01, train loss: 1.50863e-06, val loss: 2.12194e-06, min loss: 8.96030e-07\n",
      "Epoch: 1259300, elapsed: 1.13e+01, train loss: 2.08620e-06, val loss: 3.22428e-06, min loss: 8.96030e-07\n",
      "Epoch: 1259400, elapsed: 1.11e+01, train loss: 2.20377e-06, val loss: 2.54710e-06, min loss: 8.96030e-07\n",
      "Epoch: 1259500, elapsed: 1.12e+01, train loss: 3.34723e-06, val loss: 4.73849e-06, min loss: 8.96030e-07\n",
      "Epoch: 1259600, elapsed: 1.12e+01, train loss: 8.95422e-07, val loss: 1.75718e-06, min loss: 8.95422e-07\n",
      "Epoch: 1259700, elapsed: 1.13e+01, train loss: 9.60663e-07, val loss: 1.79386e-06, min loss: 8.95422e-07\n",
      "Epoch: 1259800, elapsed: 1.13e+01, train loss: 9.49604e-07, val loss: 1.77198e-06, min loss: 8.95422e-07\n",
      "Epoch: 1259900, elapsed: 1.14e+01, train loss: 9.49796e-07, val loss: 1.87309e-06, min loss: 8.95422e-07\n",
      "Epoch: 1260000, elapsed: 1.14e+01, train loss: 9.16973e-07, val loss: 1.81675e-06, min loss: 8.95422e-07\n",
      "Epoch: 1260100, elapsed: 1.73e+01, train loss: 1.12085e-06, val loss: 2.06325e-06, min loss: 8.95422e-07\n",
      "Epoch: 1260200, elapsed: 1.15e+01, train loss: 1.11768e-06, val loss: 1.90063e-06, min loss: 8.95422e-07\n",
      "Epoch: 1260300, elapsed: 1.15e+01, train loss: 9.12808e-07, val loss: 1.78799e-06, min loss: 8.95422e-07\n",
      "Epoch: 1260400, elapsed: 1.16e+01, train loss: 9.12076e-07, val loss: 1.78489e-06, min loss: 8.95422e-07\n",
      "Epoch: 1260500, elapsed: 1.15e+01, train loss: 1.07460e-06, val loss: 1.82332e-06, min loss: 8.95422e-07\n",
      "Epoch: 1260600, elapsed: 1.14e+01, train loss: 1.14287e-06, val loss: 2.12236e-06, min loss: 8.95422e-07\n",
      "Epoch: 1260700, elapsed: 1.14e+01, train loss: 8.96390e-07, val loss: 1.75014e-06, min loss: 8.95422e-07\n",
      "Epoch: 1260800, elapsed: 1.14e+01, train loss: 9.24276e-07, val loss: 1.79741e-06, min loss: 8.95422e-07\n",
      "Epoch: 1260900, elapsed: 1.15e+01, train loss: 9.01248e-07, val loss: 1.76587e-06, min loss: 8.95422e-07\n",
      "Epoch: 1261000, elapsed: 1.15e+01, train loss: 1.12589e-06, val loss: 1.89515e-06, min loss: 8.95422e-07\n",
      "Epoch: 1261100, elapsed: 1.14e+01, train loss: 8.93777e-07, val loss: 1.75048e-06, min loss: 8.93777e-07\n",
      "Epoch: 1261200, elapsed: 1.15e+01, train loss: 8.95420e-07, val loss: 1.75248e-06, min loss: 8.93777e-07\n",
      "Epoch: 1261300, elapsed: 1.15e+01, train loss: 1.71985e-06, val loss: 2.58130e-06, min loss: 8.93777e-07\n",
      "Epoch: 1261400, elapsed: 1.15e+01, train loss: 9.46542e-07, val loss: 1.80072e-06, min loss: 8.93777e-07\n",
      "Epoch: 1261500, elapsed: 1.14e+01, train loss: 8.95372e-07, val loss: 1.75463e-06, min loss: 8.93777e-07\n",
      "Epoch: 1261600, elapsed: 1.12e+01, train loss: 9.30902e-07, val loss: 1.79183e-06, min loss: 8.93777e-07\n",
      "Epoch: 1261700, elapsed: 1.11e+01, train loss: 9.16064e-07, val loss: 1.79225e-06, min loss: 8.93777e-07\n",
      "Epoch: 1261800, elapsed: 1.13e+01, train loss: 9.10081e-07, val loss: 1.74602e-06, min loss: 8.93777e-07\n",
      "Epoch: 1261900, elapsed: 1.14e+01, train loss: 1.06380e-06, val loss: 1.85033e-06, min loss: 8.93777e-07\n",
      "Epoch: 1262000, elapsed: 1.13e+01, train loss: 9.75296e-07, val loss: 1.84525e-06, min loss: 8.93777e-07\n",
      "Epoch: 1262100, elapsed: 1.14e+01, train loss: 9.33128e-07, val loss: 2.00819e-06, min loss: 8.93777e-07\n",
      "Epoch: 1262200, elapsed: 1.13e+01, train loss: 8.93133e-07, val loss: 1.74857e-06, min loss: 8.93133e-07\n",
      "Epoch: 1262300, elapsed: 1.13e+01, train loss: 9.06817e-07, val loss: 1.74184e-06, min loss: 8.93133e-07\n",
      "Epoch: 1262400, elapsed: 1.51e+01, train loss: 9.44913e-07, val loss: 1.79806e-06, min loss: 8.93133e-07\n",
      "Epoch: 1262500, elapsed: 1.15e+01, train loss: 1.24013e-06, val loss: 1.96923e-06, min loss: 8.93133e-07\n",
      "Epoch: 1262600, elapsed: 1.16e+01, train loss: 8.93415e-07, val loss: 1.74670e-06, min loss: 8.93133e-07\n",
      "Epoch: 1262700, elapsed: 1.15e+01, train loss: 8.94355e-07, val loss: 1.75068e-06, min loss: 8.93133e-07\n",
      "Epoch: 1262800, elapsed: 1.13e+01, train loss: 9.44193e-07, val loss: 1.82650e-06, min loss: 8.93133e-07\n",
      "Epoch: 1262900, elapsed: 1.15e+01, train loss: 8.94589e-07, val loss: 1.75565e-06, min loss: 8.93133e-07\n",
      "Epoch: 1263000, elapsed: 1.16e+01, train loss: 8.97525e-07, val loss: 1.74389e-06, min loss: 8.93133e-07\n",
      "Epoch: 1263100, elapsed: 1.14e+01, train loss: 9.62211e-07, val loss: 1.87359e-06, min loss: 8.93133e-07\n",
      "Epoch: 1263200, elapsed: 1.14e+01, train loss: 9.59103e-07, val loss: 1.86545e-06, min loss: 8.93133e-07\n",
      "Epoch: 1263300, elapsed: 1.13e+01, train loss: 1.01560e-06, val loss: 1.92701e-06, min loss: 8.93133e-07\n",
      "Epoch: 1263400, elapsed: 1.14e+01, train loss: 8.93771e-07, val loss: 1.74929e-06, min loss: 8.93133e-07\n",
      "Epoch: 1263500, elapsed: 1.13e+01, train loss: 8.96229e-07, val loss: 1.74548e-06, min loss: 8.93133e-07\n",
      "Epoch: 1263600, elapsed: 1.14e+01, train loss: 3.04756e-06, val loss: 3.93253e-06, min loss: 8.93133e-07\n",
      "Epoch: 1263700, elapsed: 1.13e+01, train loss: 1.47327e-06, val loss: 2.41747e-06, min loss: 8.93133e-07\n",
      "Epoch: 1263800, elapsed: 1.13e+01, train loss: 1.30896e-06, val loss: 1.86287e-06, min loss: 8.93133e-07\n",
      "Epoch: 1263900, elapsed: 1.12e+01, train loss: 4.84450e-06, val loss: 5.02921e-06, min loss: 8.93133e-07\n",
      "Epoch: 1264000, elapsed: 1.13e+01, train loss: 8.93462e-07, val loss: 1.74223e-06, min loss: 8.93133e-07\n",
      "Epoch: 1264100, elapsed: 1.14e+01, train loss: 8.92521e-07, val loss: 1.74625e-06, min loss: 8.92521e-07\n",
      "Epoch: 1264200, elapsed: 1.15e+01, train loss: 9.42496e-07, val loss: 1.74662e-06, min loss: 8.92521e-07\n",
      "Epoch: 1264300, elapsed: 1.14e+01, train loss: 9.55699e-07, val loss: 1.91009e-06, min loss: 8.92521e-07\n",
      "Epoch: 1264400, elapsed: 1.13e+01, train loss: 9.41566e-07, val loss: 1.80772e-06, min loss: 8.92521e-07\n",
      "Epoch: 1264500, elapsed: 1.14e+01, train loss: 1.06786e-06, val loss: 1.90302e-06, min loss: 8.92521e-07\n",
      "Epoch: 1264600, elapsed: 1.13e+01, train loss: 1.99500e-06, val loss: 2.99438e-06, min loss: 8.92521e-07\n",
      "Epoch: 1264700, elapsed: 1.51e+01, train loss: 9.11289e-07, val loss: 1.75260e-06, min loss: 8.92521e-07\n",
      "Epoch: 1264800, elapsed: 1.14e+01, train loss: 9.22386e-07, val loss: 1.74622e-06, min loss: 8.92521e-07\n",
      "Epoch: 1264900, elapsed: 1.15e+01, train loss: 9.14104e-07, val loss: 1.74413e-06, min loss: 8.92521e-07\n",
      "Epoch: 1265000, elapsed: 1.15e+01, train loss: 1.34057e-06, val loss: 2.06869e-06, min loss: 8.92521e-07\n",
      "Epoch: 1265100, elapsed: 1.35e+01, train loss: 8.92369e-07, val loss: 1.74601e-06, min loss: 8.92369e-07\n",
      "Epoch: 1265200, elapsed: 1.14e+01, train loss: 8.94493e-07, val loss: 1.75203e-06, min loss: 8.92369e-07\n",
      "Epoch: 1265300, elapsed: 1.14e+01, train loss: 9.68750e-07, val loss: 1.79291e-06, min loss: 8.92369e-07\n",
      "Epoch: 1265400, elapsed: 1.13e+01, train loss: 1.65257e-06, val loss: 2.57397e-06, min loss: 8.92369e-07\n",
      "Epoch: 1265500, elapsed: 1.14e+01, train loss: 1.92744e-06, val loss: 2.34529e-06, min loss: 8.92369e-07\n",
      "Epoch: 1265600, elapsed: 1.14e+01, train loss: 3.29277e-06, val loss: 4.35888e-06, min loss: 8.92369e-07\n",
      "Epoch: 1265700, elapsed: 1.13e+01, train loss: 8.97386e-07, val loss: 1.76273e-06, min loss: 8.92369e-07\n",
      "Epoch: 1265800, elapsed: 1.13e+01, train loss: 9.02386e-07, val loss: 1.75285e-06, min loss: 8.92369e-07\n",
      "Epoch: 1265900, elapsed: 1.13e+01, train loss: 1.33238e-06, val loss: 2.31813e-06, min loss: 8.92369e-07\n",
      "Epoch: 1266000, elapsed: 1.13e+01, train loss: 9.54714e-07, val loss: 1.87146e-06, min loss: 8.92369e-07\n",
      "Epoch: 1266100, elapsed: 1.13e+01, train loss: 9.09624e-07, val loss: 1.77859e-06, min loss: 8.92369e-07\n",
      "Epoch: 1266200, elapsed: 1.13e+01, train loss: 9.01862e-07, val loss: 1.75670e-06, min loss: 8.92369e-07\n",
      "Epoch: 1266300, elapsed: 1.13e+01, train loss: 8.92135e-07, val loss: 1.74248e-06, min loss: 8.92135e-07\n",
      "Epoch: 1266400, elapsed: 1.13e+01, train loss: 8.99625e-07, val loss: 1.75811e-06, min loss: 8.92135e-07\n",
      "Epoch: 1266500, elapsed: 1.13e+01, train loss: 8.92247e-07, val loss: 1.74341e-06, min loss: 8.92135e-07\n",
      "Epoch: 1266600, elapsed: 1.13e+01, train loss: 9.12148e-07, val loss: 1.75483e-06, min loss: 8.92135e-07\n",
      "Epoch: 1266700, elapsed: 1.13e+01, train loss: 9.46325e-07, val loss: 1.77861e-06, min loss: 8.92135e-07\n",
      "Epoch: 1266800, elapsed: 1.14e+01, train loss: 3.98636e-06, val loss: 3.81788e-06, min loss: 8.92135e-07\n",
      "Epoch: 1266900, elapsed: 1.13e+01, train loss: 8.99741e-07, val loss: 1.76934e-06, min loss: 8.92135e-07\n",
      "Epoch: 1267000, elapsed: 1.49e+01, train loss: 9.35058e-07, val loss: 1.84023e-06, min loss: 8.92135e-07\n",
      "Epoch: 1267100, elapsed: 1.14e+01, train loss: 9.07035e-07, val loss: 1.75597e-06, min loss: 8.92135e-07\n",
      "Epoch: 1267200, elapsed: 1.15e+01, train loss: 9.03899e-07, val loss: 1.75872e-06, min loss: 8.92135e-07\n",
      "Epoch: 1267300, elapsed: 1.13e+01, train loss: 9.03587e-07, val loss: 1.74740e-06, min loss: 8.92135e-07\n",
      "Epoch: 1267400, elapsed: 1.15e+01, train loss: 1.40204e-06, val loss: 2.39142e-06, min loss: 8.92135e-07\n",
      "Epoch: 1267500, elapsed: 1.14e+01, train loss: 1.22715e-06, val loss: 2.29840e-06, min loss: 8.92135e-07\n",
      "Epoch: 1267600, elapsed: 1.13e+01, train loss: 1.10389e-06, val loss: 2.01937e-06, min loss: 8.92135e-07\n",
      "Epoch: 1267700, elapsed: 1.11e+01, train loss: 1.14128e-06, val loss: 1.94863e-06, min loss: 8.92135e-07\n",
      "Epoch: 1267800, elapsed: 1.14e+01, train loss: 1.17546e-06, val loss: 1.94779e-06, min loss: 8.92135e-07\n",
      "Epoch: 1267900, elapsed: 1.13e+01, train loss: 1.51743e-06, val loss: 2.33677e-06, min loss: 8.92135e-07\n",
      "Epoch: 1268000, elapsed: 1.13e+01, train loss: 2.49318e-06, val loss: 2.76747e-06, min loss: 8.92135e-07\n",
      "Epoch: 1268100, elapsed: 1.13e+01, train loss: 2.81875e-06, val loss: 2.83800e-06, min loss: 8.92135e-07\n",
      "Epoch: 1268200, elapsed: 1.12e+01, train loss: 1.01246e-06, val loss: 1.80232e-06, min loss: 8.92135e-07\n",
      "Epoch: 1268300, elapsed: 1.11e+01, train loss: 8.93020e-07, val loss: 1.74317e-06, min loss: 8.92135e-07\n",
      "Epoch: 1268400, elapsed: 1.14e+01, train loss: 9.10140e-07, val loss: 1.74377e-06, min loss: 8.92135e-07\n",
      "Epoch: 1268500, elapsed: 1.13e+01, train loss: 9.02337e-07, val loss: 1.74723e-06, min loss: 8.92135e-07\n",
      "Epoch: 1268600, elapsed: 1.13e+01, train loss: 1.03302e-06, val loss: 1.82063e-06, min loss: 8.92135e-07\n",
      "Epoch: 1268700, elapsed: 1.13e+01, train loss: 9.42096e-07, val loss: 1.83047e-06, min loss: 8.92135e-07\n",
      "Epoch: 1268800, elapsed: 1.14e+01, train loss: 8.89177e-07, val loss: 1.74208e-06, min loss: 8.89177e-07\n",
      "Epoch: 1268900, elapsed: 1.12e+01, train loss: 8.96672e-07, val loss: 1.76059e-06, min loss: 8.89177e-07\n",
      "Epoch: 1269000, elapsed: 1.13e+01, train loss: 9.16399e-07, val loss: 1.79448e-06, min loss: 8.89177e-07\n",
      "Epoch: 1269100, elapsed: 1.13e+01, train loss: 8.88977e-07, val loss: 1.74062e-06, min loss: 8.88977e-07\n",
      "Epoch: 1269200, elapsed: 1.12e+01, train loss: 9.02322e-07, val loss: 1.77365e-06, min loss: 8.88977e-07\n",
      "Epoch: 1269300, elapsed: 1.13e+01, train loss: 9.82328e-07, val loss: 1.95693e-06, min loss: 8.88977e-07\n",
      "Epoch: 1269400, elapsed: 1.52e+01, train loss: 9.11956e-07, val loss: 1.78380e-06, min loss: 8.88977e-07\n",
      "Epoch: 1269500, elapsed: 1.15e+01, train loss: 9.05552e-07, val loss: 1.77838e-06, min loss: 8.88977e-07\n",
      "Epoch: 1269600, elapsed: 1.14e+01, train loss: 1.03515e-06, val loss: 1.84775e-06, min loss: 8.88977e-07\n",
      "Epoch: 1269700, elapsed: 1.14e+01, train loss: 8.91143e-07, val loss: 1.74156e-06, min loss: 8.88977e-07\n",
      "Epoch: 1269800, elapsed: 1.14e+01, train loss: 9.01957e-07, val loss: 1.76294e-06, min loss: 8.88977e-07\n",
      "Epoch: 1269900, elapsed: 1.16e+01, train loss: 8.93877e-07, val loss: 1.74032e-06, min loss: 8.88977e-07\n",
      "Epoch: 1270000, elapsed: 1.16e+01, train loss: 9.50961e-07, val loss: 1.85602e-06, min loss: 8.88977e-07\n",
      "Epoch: 1270100, elapsed: 1.35e+01, train loss: 9.43622e-07, val loss: 1.91213e-06, min loss: 8.88977e-07\n",
      "Epoch: 1270200, elapsed: 1.14e+01, train loss: 1.80037e-06, val loss: 2.59536e-06, min loss: 8.88977e-07\n",
      "Epoch: 1270300, elapsed: 1.14e+01, train loss: 3.78245e-06, val loss: 4.94794e-06, min loss: 8.88977e-07\n",
      "Epoch: 1270400, elapsed: 1.15e+01, train loss: 1.04053e-06, val loss: 1.89982e-06, min loss: 8.88977e-07\n",
      "Epoch: 1270500, elapsed: 1.14e+01, train loss: 3.29561e-06, val loss: 3.96517e-06, min loss: 8.88977e-07\n",
      "Epoch: 1270600, elapsed: 1.16e+01, train loss: 1.60207e-06, val loss: 2.85196e-06, min loss: 8.88977e-07\n",
      "Epoch: 1270700, elapsed: 1.13e+01, train loss: 8.88555e-07, val loss: 1.74548e-06, min loss: 8.88555e-07\n",
      "Epoch: 1270800, elapsed: 1.14e+01, train loss: 8.99251e-07, val loss: 1.73861e-06, min loss: 8.88555e-07\n",
      "Epoch: 1270900, elapsed: 1.12e+01, train loss: 1.12432e-06, val loss: 1.92022e-06, min loss: 8.88555e-07\n",
      "Epoch: 1271000, elapsed: 1.12e+01, train loss: 1.07221e-06, val loss: 2.06606e-06, min loss: 8.88555e-07\n",
      "Epoch: 1271100, elapsed: 1.14e+01, train loss: 8.87870e-07, val loss: 1.73976e-06, min loss: 8.87870e-07\n",
      "Epoch: 1271200, elapsed: 1.14e+01, train loss: 9.18878e-07, val loss: 1.74000e-06, min loss: 8.87870e-07\n",
      "Epoch: 1271300, elapsed: 1.14e+01, train loss: 9.51143e-07, val loss: 1.75140e-06, min loss: 8.87870e-07\n",
      "Epoch: 1271400, elapsed: 1.15e+01, train loss: 5.33187e-06, val loss: 5.85897e-06, min loss: 8.87870e-07\n",
      "Epoch: 1271500, elapsed: 1.14e+01, train loss: 8.87807e-07, val loss: 1.73956e-06, min loss: 8.87807e-07\n",
      "Epoch: 1271600, elapsed: 1.16e+01, train loss: 9.08026e-07, val loss: 1.74386e-06, min loss: 8.87807e-07\n",
      "Epoch: 1271700, elapsed: 1.51e+01, train loss: 8.87605e-07, val loss: 1.73489e-06, min loss: 8.87605e-07\n",
      "Epoch: 1271800, elapsed: 1.15e+01, train loss: 9.08916e-07, val loss: 1.78159e-06, min loss: 8.87605e-07\n",
      "Epoch: 1271900, elapsed: 1.14e+01, train loss: 1.03034e-06, val loss: 1.79398e-06, min loss: 8.87605e-07\n",
      "Epoch: 1272000, elapsed: 1.15e+01, train loss: 1.64289e-06, val loss: 2.64366e-06, min loss: 8.87605e-07\n",
      "Epoch: 1272100, elapsed: 1.15e+01, train loss: 9.00012e-07, val loss: 1.73866e-06, min loss: 8.87605e-07\n",
      "Epoch: 1272200, elapsed: 1.14e+01, train loss: 8.88334e-07, val loss: 1.73362e-06, min loss: 8.87605e-07\n",
      "Epoch: 1272300, elapsed: 1.14e+01, train loss: 2.03921e-06, val loss: 2.84409e-06, min loss: 8.87605e-07\n",
      "Epoch: 1272400, elapsed: 1.12e+01, train loss: 1.02431e-06, val loss: 1.85164e-06, min loss: 8.87605e-07\n",
      "Epoch: 1272500, elapsed: 1.16e+01, train loss: 1.00223e-06, val loss: 1.88061e-06, min loss: 8.87605e-07\n",
      "Epoch: 1272600, elapsed: 1.15e+01, train loss: 9.35622e-07, val loss: 1.77806e-06, min loss: 8.87605e-07\n",
      "Epoch: 1272700, elapsed: 1.12e+01, train loss: 1.28505e-06, val loss: 2.20888e-06, min loss: 8.87605e-07\n",
      "Epoch: 1272800, elapsed: 1.13e+01, train loss: 8.87783e-07, val loss: 1.74129e-06, min loss: 8.87605e-07\n",
      "Epoch: 1272900, elapsed: 1.12e+01, train loss: 8.94353e-07, val loss: 1.73019e-06, min loss: 8.87605e-07\n",
      "Epoch: 1273000, elapsed: 1.14e+01, train loss: 4.41942e-06, val loss: 5.15194e-06, min loss: 8.87605e-07\n",
      "Epoch: 1273100, elapsed: 1.13e+01, train loss: 9.87137e-07, val loss: 1.90821e-06, min loss: 8.87605e-07\n",
      "Epoch: 1273200, elapsed: 1.14e+01, train loss: 8.88622e-07, val loss: 1.73519e-06, min loss: 8.87605e-07\n",
      "Epoch: 1273300, elapsed: 1.13e+01, train loss: 8.93781e-07, val loss: 1.73318e-06, min loss: 8.87605e-07\n",
      "Epoch: 1273400, elapsed: 1.12e+01, train loss: 9.01560e-07, val loss: 1.74510e-06, min loss: 8.87605e-07\n",
      "Epoch: 1273500, elapsed: 1.13e+01, train loss: 9.57924e-07, val loss: 1.80435e-06, min loss: 8.87605e-07\n",
      "Epoch: 1273600, elapsed: 1.14e+01, train loss: 1.08288e-06, val loss: 1.90587e-06, min loss: 8.87605e-07\n",
      "Epoch: 1273700, elapsed: 1.14e+01, train loss: 1.60260e-06, val loss: 2.11962e-06, min loss: 8.87605e-07\n",
      "Epoch: 1273800, elapsed: 1.14e+01, train loss: 9.47579e-07, val loss: 1.83497e-06, min loss: 8.87605e-07\n",
      "Epoch: 1273900, elapsed: 1.12e+01, train loss: 1.41533e-06, val loss: 2.44634e-06, min loss: 8.87605e-07\n",
      "Epoch: 1274000, elapsed: 1.51e+01, train loss: 1.42898e-06, val loss: 2.14402e-06, min loss: 8.87605e-07\n",
      "Epoch: 1274100, elapsed: 1.16e+01, train loss: 1.33538e-06, val loss: 2.02880e-06, min loss: 8.87605e-07\n",
      "Epoch: 1274200, elapsed: 1.16e+01, train loss: 8.99002e-07, val loss: 1.75551e-06, min loss: 8.87605e-07\n",
      "Epoch: 1274300, elapsed: 1.16e+01, train loss: 9.05623e-07, val loss: 1.73981e-06, min loss: 8.87605e-07\n",
      "Epoch: 1274400, elapsed: 1.16e+01, train loss: 8.94589e-07, val loss: 1.75536e-06, min loss: 8.87605e-07\n",
      "Epoch: 1274500, elapsed: 1.16e+01, train loss: 8.87134e-07, val loss: 1.73857e-06, min loss: 8.87134e-07\n",
      "Epoch: 1274600, elapsed: 1.14e+01, train loss: 9.36861e-07, val loss: 1.76293e-06, min loss: 8.87134e-07\n",
      "Epoch: 1274700, elapsed: 1.14e+01, train loss: 7.30830e-06, val loss: 7.25936e-06, min loss: 8.87134e-07\n",
      "Epoch: 1274800, elapsed: 1.14e+01, train loss: 8.85543e-07, val loss: 1.73656e-06, min loss: 8.85543e-07\n",
      "Epoch: 1274900, elapsed: 1.14e+01, train loss: 9.18923e-07, val loss: 1.76886e-06, min loss: 8.85543e-07\n",
      "Epoch: 1275000, elapsed: 1.14e+01, train loss: 8.92903e-07, val loss: 1.72809e-06, min loss: 8.85543e-07\n",
      "Epoch: 1275100, elapsed: 1.35e+01, train loss: 8.88580e-07, val loss: 1.73679e-06, min loss: 8.85543e-07\n",
      "Epoch: 1275200, elapsed: 1.14e+01, train loss: 9.15173e-07, val loss: 1.74300e-06, min loss: 8.85543e-07\n",
      "Epoch: 1275300, elapsed: 1.13e+01, train loss: 9.10090e-07, val loss: 1.76702e-06, min loss: 8.85543e-07\n",
      "Epoch: 1275400, elapsed: 1.13e+01, train loss: 8.90205e-07, val loss: 1.72655e-06, min loss: 8.85543e-07\n",
      "Epoch: 1275500, elapsed: 1.12e+01, train loss: 8.89780e-07, val loss: 1.72749e-06, min loss: 8.85543e-07\n",
      "Epoch: 1275600, elapsed: 1.13e+01, train loss: 1.91272e-06, val loss: 2.43174e-06, min loss: 8.85543e-07\n",
      "Epoch: 1275700, elapsed: 1.15e+01, train loss: 8.91107e-07, val loss: 1.72702e-06, min loss: 8.85543e-07\n",
      "Epoch: 1275800, elapsed: 1.12e+01, train loss: 8.85148e-07, val loss: 1.72847e-06, min loss: 8.85148e-07\n",
      "Epoch: 1275900, elapsed: 1.15e+01, train loss: 9.20216e-07, val loss: 1.77827e-06, min loss: 8.85148e-07\n",
      "Epoch: 1276000, elapsed: 1.13e+01, train loss: 5.39753e-06, val loss: 6.24845e-06, min loss: 8.85148e-07\n",
      "Epoch: 1276100, elapsed: 1.14e+01, train loss: 8.84787e-07, val loss: 1.72916e-06, min loss: 8.84787e-07\n",
      "Epoch: 1276200, elapsed: 1.12e+01, train loss: 9.07958e-07, val loss: 1.74050e-06, min loss: 8.84787e-07\n",
      "Epoch: 1276300, elapsed: 1.51e+01, train loss: 9.55555e-07, val loss: 1.85849e-06, min loss: 8.84787e-07\n",
      "Epoch: 1276400, elapsed: 1.16e+01, train loss: 1.52538e-06, val loss: 2.57099e-06, min loss: 8.84787e-07\n",
      "Epoch: 1276500, elapsed: 1.14e+01, train loss: 9.10899e-07, val loss: 1.79077e-06, min loss: 8.84787e-07\n",
      "Epoch: 1276600, elapsed: 1.17e+01, train loss: 3.93277e-06, val loss: 4.86346e-06, min loss: 8.84787e-07\n",
      "Epoch: 1276700, elapsed: 1.16e+01, train loss: 9.63725e-07, val loss: 1.87860e-06, min loss: 8.84787e-07\n",
      "Epoch: 1276800, elapsed: 1.15e+01, train loss: 9.12013e-07, val loss: 1.75023e-06, min loss: 8.84787e-07\n",
      "Epoch: 1276900, elapsed: 1.15e+01, train loss: 8.84769e-07, val loss: 1.72495e-06, min loss: 8.84769e-07\n",
      "Epoch: 1277000, elapsed: 1.13e+01, train loss: 8.84505e-07, val loss: 1.72745e-06, min loss: 8.84505e-07\n",
      "Epoch: 1277100, elapsed: 1.13e+01, train loss: 1.04905e-06, val loss: 1.82960e-06, min loss: 8.84505e-07\n",
      "Epoch: 1277200, elapsed: 1.13e+01, train loss: 9.35822e-07, val loss: 1.78044e-06, min loss: 8.84505e-07\n",
      "Epoch: 1277300, elapsed: 1.14e+01, train loss: 1.01353e-06, val loss: 1.85207e-06, min loss: 8.84505e-07\n",
      "Epoch: 1277400, elapsed: 1.14e+01, train loss: 8.94453e-07, val loss: 1.72088e-06, min loss: 8.84505e-07\n",
      "Epoch: 1277500, elapsed: 1.13e+01, train loss: 8.84845e-07, val loss: 1.72658e-06, min loss: 8.84505e-07\n",
      "Epoch: 1277600, elapsed: 1.14e+01, train loss: 8.99094e-07, val loss: 1.73187e-06, min loss: 8.84505e-07\n",
      "Epoch: 1277700, elapsed: 1.12e+01, train loss: 9.28253e-07, val loss: 1.75905e-06, min loss: 8.84505e-07\n",
      "Epoch: 1277800, elapsed: 1.12e+01, train loss: 9.01017e-07, val loss: 1.80197e-06, min loss: 8.84505e-07\n",
      "Epoch: 1277900, elapsed: 1.13e+01, train loss: 3.46668e-06, val loss: 4.71028e-06, min loss: 8.84505e-07\n",
      "Epoch: 1278000, elapsed: 1.12e+01, train loss: 8.83813e-07, val loss: 1.72664e-06, min loss: 8.83813e-07\n",
      "Epoch: 1278100, elapsed: 1.13e+01, train loss: 8.87141e-07, val loss: 1.73556e-06, min loss: 8.83813e-07\n",
      "Epoch: 1278200, elapsed: 1.13e+01, train loss: 1.11927e-06, val loss: 2.15281e-06, min loss: 8.83813e-07\n",
      "Epoch: 1278300, elapsed: 1.14e+01, train loss: 9.51581e-07, val loss: 1.85912e-06, min loss: 8.83813e-07\n",
      "Epoch: 1278400, elapsed: 1.13e+01, train loss: 8.84680e-07, val loss: 1.73214e-06, min loss: 8.83813e-07\n",
      "Epoch: 1278500, elapsed: 1.14e+01, train loss: 1.27039e-06, val loss: 2.10567e-06, min loss: 8.83813e-07\n",
      "Epoch: 1278600, elapsed: 1.13e+01, train loss: 9.60660e-07, val loss: 1.84541e-06, min loss: 8.83813e-07\n",
      "Epoch: 1278700, elapsed: 1.52e+01, train loss: 8.86910e-07, val loss: 1.72857e-06, min loss: 8.83813e-07\n",
      "Epoch: 1278800, elapsed: 1.14e+01, train loss: 8.83793e-07, val loss: 1.72640e-06, min loss: 8.83793e-07\n",
      "Epoch: 1278900, elapsed: 1.15e+01, train loss: 8.96659e-07, val loss: 1.76003e-06, min loss: 8.83793e-07\n",
      "Epoch: 1279000, elapsed: 1.15e+01, train loss: 1.19239e-06, val loss: 1.75143e-06, min loss: 8.83793e-07\n",
      "Epoch: 1279100, elapsed: 1.15e+01, train loss: 9.42961e-07, val loss: 1.85223e-06, min loss: 8.83793e-07\n",
      "Epoch: 1279200, elapsed: 1.15e+01, train loss: 8.97509e-07, val loss: 1.72620e-06, min loss: 8.83793e-07\n",
      "Epoch: 1279300, elapsed: 1.14e+01, train loss: 8.85832e-07, val loss: 1.72088e-06, min loss: 8.83793e-07\n",
      "Epoch: 1279400, elapsed: 1.14e+01, train loss: 1.16156e-06, val loss: 2.09216e-06, min loss: 8.83793e-07\n",
      "Epoch: 1279500, elapsed: 1.14e+01, train loss: 9.51897e-07, val loss: 1.85539e-06, min loss: 8.83793e-07\n",
      "Epoch: 1279600, elapsed: 1.14e+01, train loss: 9.54238e-07, val loss: 1.76770e-06, min loss: 8.83793e-07\n",
      "Epoch: 1279700, elapsed: 1.14e+01, train loss: 9.06376e-07, val loss: 1.75810e-06, min loss: 8.83793e-07\n",
      "Epoch: 1279800, elapsed: 1.15e+01, train loss: 1.02508e-06, val loss: 1.77875e-06, min loss: 8.83793e-07\n",
      "Epoch: 1279900, elapsed: 1.15e+01, train loss: 9.48600e-07, val loss: 1.89102e-06, min loss: 8.83793e-07\n",
      "Epoch: 1280000, elapsed: 1.14e+01, train loss: 1.21367e-06, val loss: 1.92418e-06, min loss: 8.83793e-07\n",
      "Epoch: 1280100, elapsed: 1.34e+01, train loss: 8.88240e-07, val loss: 1.72366e-06, min loss: 8.83793e-07\n",
      "Epoch: 1280200, elapsed: 1.14e+01, train loss: 8.85825e-07, val loss: 1.71598e-06, min loss: 8.83793e-07\n",
      "Epoch: 1280300, elapsed: 1.12e+01, train loss: 1.24556e-06, val loss: 1.79385e-06, min loss: 8.83793e-07\n",
      "Epoch: 1280400, elapsed: 1.12e+01, train loss: 1.22148e-06, val loss: 1.96486e-06, min loss: 8.83793e-07\n",
      "Epoch: 1280500, elapsed: 1.13e+01, train loss: 3.76641e-06, val loss: 4.92825e-06, min loss: 8.83793e-07\n",
      "Epoch: 1280600, elapsed: 1.14e+01, train loss: 9.28126e-07, val loss: 1.81560e-06, min loss: 8.83793e-07\n",
      "Epoch: 1280700, elapsed: 1.13e+01, train loss: 1.47115e-06, val loss: 2.31974e-06, min loss: 8.83793e-07\n",
      "Epoch: 1280800, elapsed: 1.12e+01, train loss: 8.87757e-07, val loss: 1.73966e-06, min loss: 8.83793e-07\n",
      "Epoch: 1280900, elapsed: 1.14e+01, train loss: 9.82207e-07, val loss: 1.94761e-06, min loss: 8.83793e-07\n",
      "Epoch: 1281000, elapsed: 1.52e+01, train loss: 8.82143e-07, val loss: 1.72640e-06, min loss: 8.82143e-07\n",
      "Epoch: 1281100, elapsed: 1.13e+01, train loss: 8.83510e-07, val loss: 1.72009e-06, min loss: 8.82143e-07\n",
      "Epoch: 1281200, elapsed: 1.15e+01, train loss: 9.05576e-07, val loss: 1.73084e-06, min loss: 8.82143e-07\n",
      "Epoch: 1281300, elapsed: 1.15e+01, train loss: 1.18809e-06, val loss: 1.92407e-06, min loss: 8.82143e-07\n",
      "Epoch: 1281400, elapsed: 1.15e+01, train loss: 1.07197e-06, val loss: 2.00611e-06, min loss: 8.82143e-07\n",
      "Epoch: 1281500, elapsed: 1.13e+01, train loss: 9.41073e-07, val loss: 1.76743e-06, min loss: 8.82143e-07\n",
      "Epoch: 1281600, elapsed: 1.13e+01, train loss: 8.81661e-07, val loss: 1.72421e-06, min loss: 8.81661e-07\n",
      "Epoch: 1281700, elapsed: 1.16e+01, train loss: 8.81986e-07, val loss: 1.72262e-06, min loss: 8.81661e-07\n",
      "Epoch: 1281800, elapsed: 1.16e+01, train loss: 8.86677e-07, val loss: 1.74961e-06, min loss: 8.81661e-07\n",
      "Epoch: 1281900, elapsed: 1.14e+01, train loss: 9.21436e-07, val loss: 1.79384e-06, min loss: 8.81661e-07\n",
      "Epoch: 1282000, elapsed: 1.14e+01, train loss: 9.21493e-07, val loss: 1.77247e-06, min loss: 8.81661e-07\n",
      "Epoch: 1282100, elapsed: 1.12e+01, train loss: 8.83760e-07, val loss: 1.72466e-06, min loss: 8.81661e-07\n",
      "Epoch: 1282200, elapsed: 1.14e+01, train loss: 9.30861e-07, val loss: 1.72773e-06, min loss: 8.81661e-07\n",
      "Epoch: 1282300, elapsed: 1.13e+01, train loss: 9.09796e-07, val loss: 1.74049e-06, min loss: 8.81661e-07\n",
      "Epoch: 1282400, elapsed: 1.15e+01, train loss: 9.97199e-07, val loss: 1.77900e-06, min loss: 8.81661e-07\n",
      "Epoch: 1282500, elapsed: 1.13e+01, train loss: 8.80689e-07, val loss: 1.72421e-06, min loss: 8.80689e-07\n",
      "Epoch: 1282600, elapsed: 1.12e+01, train loss: 1.33715e-06, val loss: 2.28257e-06, min loss: 8.80689e-07\n",
      "Epoch: 1282700, elapsed: 1.12e+01, train loss: 1.24529e-06, val loss: 2.20464e-06, min loss: 8.80689e-07\n",
      "Epoch: 1282800, elapsed: 1.15e+01, train loss: 9.27061e-07, val loss: 1.77713e-06, min loss: 8.80689e-07\n",
      "Epoch: 1282900, elapsed: 1.14e+01, train loss: 1.33548e-06, val loss: 2.50016e-06, min loss: 8.80689e-07\n",
      "Epoch: 1283000, elapsed: 1.14e+01, train loss: 8.80989e-07, val loss: 1.71900e-06, min loss: 8.80689e-07\n",
      "Epoch: 1283100, elapsed: 1.11e+01, train loss: 8.81417e-07, val loss: 1.71754e-06, min loss: 8.80689e-07\n",
      "Epoch: 1283200, elapsed: 1.15e+01, train loss: 3.01942e-06, val loss: 4.51332e-06, min loss: 8.80689e-07\n",
      "Epoch: 1283300, elapsed: 1.49e+01, train loss: 8.95758e-07, val loss: 1.75924e-06, min loss: 8.80689e-07\n",
      "Epoch: 1283400, elapsed: 1.16e+01, train loss: 8.80346e-07, val loss: 1.72198e-06, min loss: 8.80346e-07\n",
      "Epoch: 1283500, elapsed: 1.15e+01, train loss: 8.94930e-07, val loss: 1.73959e-06, min loss: 8.80346e-07\n",
      "Epoch: 1283600, elapsed: 1.15e+01, train loss: 8.99805e-07, val loss: 1.78835e-06, min loss: 8.80346e-07\n",
      "Epoch: 1283700, elapsed: 1.14e+01, train loss: 1.26399e-06, val loss: 2.05339e-06, min loss: 8.80346e-07\n",
      "Epoch: 1283800, elapsed: 1.13e+01, train loss: 8.99257e-07, val loss: 1.82303e-06, min loss: 8.80346e-07\n",
      "Epoch: 1283900, elapsed: 1.13e+01, train loss: 3.41707e-06, val loss: 4.56740e-06, min loss: 8.80346e-07\n",
      "Epoch: 1284000, elapsed: 1.16e+01, train loss: 8.83499e-07, val loss: 1.73486e-06, min loss: 8.80346e-07\n",
      "Epoch: 1284100, elapsed: 1.13e+01, train loss: 8.82902e-07, val loss: 1.73031e-06, min loss: 8.80346e-07\n",
      "Epoch: 1284200, elapsed: 1.15e+01, train loss: 8.82650e-07, val loss: 1.71613e-06, min loss: 8.80346e-07\n",
      "Epoch: 1284300, elapsed: 1.13e+01, train loss: 9.74627e-07, val loss: 1.79975e-06, min loss: 8.80346e-07\n",
      "Epoch: 1284400, elapsed: 1.15e+01, train loss: 8.95181e-07, val loss: 1.71953e-06, min loss: 8.80346e-07\n",
      "Epoch: 1284500, elapsed: 1.12e+01, train loss: 1.12754e-06, val loss: 2.10291e-06, min loss: 8.80346e-07\n",
      "Epoch: 1284600, elapsed: 1.14e+01, train loss: 1.39900e-06, val loss: 2.44103e-06, min loss: 8.80346e-07\n",
      "Epoch: 1284700, elapsed: 1.14e+01, train loss: 1.75162e-06, val loss: 2.95058e-06, min loss: 8.80346e-07\n",
      "Epoch: 1284800, elapsed: 1.13e+01, train loss: 8.81476e-07, val loss: 1.72307e-06, min loss: 8.80346e-07\n",
      "Epoch: 1284900, elapsed: 1.13e+01, train loss: 8.80556e-07, val loss: 1.71589e-06, min loss: 8.80346e-07\n",
      "Epoch: 1285000, elapsed: 1.13e+01, train loss: 9.89245e-07, val loss: 1.77479e-06, min loss: 8.80346e-07\n",
      "Epoch: 1285100, elapsed: 1.34e+01, train loss: 5.10397e-06, val loss: 5.75292e-06, min loss: 8.80346e-07\n",
      "Epoch: 1285200, elapsed: 1.13e+01, train loss: 1.02151e-06, val loss: 1.85723e-06, min loss: 8.80346e-07\n",
      "Epoch: 1285300, elapsed: 1.15e+01, train loss: 8.90078e-07, val loss: 1.73339e-06, min loss: 8.80346e-07\n",
      "Epoch: 1285400, elapsed: 1.13e+01, train loss: 8.96245e-07, val loss: 1.72435e-06, min loss: 8.80346e-07\n",
      "Epoch: 1285500, elapsed: 1.12e+01, train loss: 9.04033e-07, val loss: 1.72328e-06, min loss: 8.80346e-07\n",
      "Epoch: 1285600, elapsed: 1.13e+01, train loss: 2.12286e-06, val loss: 2.72071e-06, min loss: 8.80346e-07\n",
      "Epoch: 1285700, elapsed: 1.53e+01, train loss: 8.89107e-07, val loss: 1.71608e-06, min loss: 8.80346e-07\n",
      "Epoch: 1285800, elapsed: 1.14e+01, train loss: 8.79798e-07, val loss: 1.71317e-06, min loss: 8.79798e-07\n",
      "Epoch: 1285900, elapsed: 1.15e+01, train loss: 9.13910e-07, val loss: 1.72331e-06, min loss: 8.79798e-07\n",
      "Epoch: 1286000, elapsed: 1.14e+01, train loss: 8.89560e-07, val loss: 1.74611e-06, min loss: 8.79798e-07\n",
      "Epoch: 1286100, elapsed: 1.14e+01, train loss: 1.02620e-06, val loss: 1.93583e-06, min loss: 8.79798e-07\n",
      "Epoch: 1286200, elapsed: 1.15e+01, train loss: 8.98808e-07, val loss: 1.84260e-06, min loss: 8.79798e-07\n",
      "Epoch: 1286300, elapsed: 1.15e+01, train loss: 9.86143e-07, val loss: 2.05392e-06, min loss: 8.79798e-07\n",
      "Epoch: 1286400, elapsed: 1.16e+01, train loss: 1.02036e-06, val loss: 1.85687e-06, min loss: 8.79798e-07\n",
      "Epoch: 1286500, elapsed: 1.14e+01, train loss: 8.78456e-07, val loss: 1.71633e-06, min loss: 8.78456e-07\n",
      "Epoch: 1286600, elapsed: 1.15e+01, train loss: 8.84907e-07, val loss: 1.73469e-06, min loss: 8.78456e-07\n",
      "Epoch: 1286700, elapsed: 1.15e+01, train loss: 8.89449e-07, val loss: 1.71739e-06, min loss: 8.78456e-07\n",
      "Epoch: 1286800, elapsed: 1.12e+01, train loss: 8.79495e-07, val loss: 1.73141e-06, min loss: 8.78456e-07\n",
      "Epoch: 1286900, elapsed: 1.15e+01, train loss: 9.53727e-07, val loss: 1.78198e-06, min loss: 8.78456e-07\n",
      "Epoch: 1287000, elapsed: 1.14e+01, train loss: 9.09699e-07, val loss: 1.76218e-06, min loss: 8.78456e-07\n",
      "Epoch: 1287100, elapsed: 1.14e+01, train loss: 8.84683e-07, val loss: 1.70845e-06, min loss: 8.78456e-07\n",
      "Epoch: 1287200, elapsed: 1.13e+01, train loss: 8.96367e-07, val loss: 1.72946e-06, min loss: 8.78456e-07\n",
      "Epoch: 1287300, elapsed: 1.14e+01, train loss: 1.13135e-06, val loss: 1.81007e-06, min loss: 8.78456e-07\n",
      "Epoch: 1287400, elapsed: 1.15e+01, train loss: 1.10910e-06, val loss: 1.76817e-06, min loss: 8.78456e-07\n",
      "Epoch: 1287500, elapsed: 1.12e+01, train loss: 1.33696e-06, val loss: 2.00311e-06, min loss: 8.78456e-07\n",
      "Epoch: 1287600, elapsed: 1.13e+01, train loss: 1.46389e-06, val loss: 1.96539e-06, min loss: 8.78456e-07\n",
      "Epoch: 1287700, elapsed: 1.15e+01, train loss: 3.74564e-06, val loss: 3.66203e-06, min loss: 8.78456e-07\n",
      "Epoch: 1287800, elapsed: 1.14e+01, train loss: 9.06964e-07, val loss: 1.77721e-06, min loss: 8.78456e-07\n",
      "Epoch: 1287900, elapsed: 1.12e+01, train loss: 8.78258e-07, val loss: 1.71932e-06, min loss: 8.78258e-07\n",
      "Epoch: 1288000, elapsed: 1.49e+01, train loss: 8.95656e-07, val loss: 1.76292e-06, min loss: 8.78258e-07\n",
      "Epoch: 1288100, elapsed: 1.15e+01, train loss: 1.21972e-06, val loss: 2.11684e-06, min loss: 8.78258e-07\n",
      "Epoch: 1288200, elapsed: 1.16e+01, train loss: 8.86960e-07, val loss: 1.70708e-06, min loss: 8.78258e-07\n",
      "Epoch: 1288300, elapsed: 1.14e+01, train loss: 8.96264e-07, val loss: 1.74563e-06, min loss: 8.78258e-07\n",
      "Epoch: 1288400, elapsed: 1.16e+01, train loss: 8.79785e-07, val loss: 1.71160e-06, min loss: 8.78258e-07\n",
      "Epoch: 1288500, elapsed: 1.16e+01, train loss: 8.84747e-07, val loss: 1.71995e-06, min loss: 8.78258e-07\n",
      "Epoch: 1288600, elapsed: 1.15e+01, train loss: 3.65956e-06, val loss: 4.43828e-06, min loss: 8.78258e-07\n",
      "Epoch: 1288700, elapsed: 1.14e+01, train loss: 1.61308e-06, val loss: 2.62311e-06, min loss: 8.78258e-07\n",
      "Epoch: 1288800, elapsed: 1.14e+01, train loss: 8.77017e-07, val loss: 1.71330e-06, min loss: 8.77017e-07\n",
      "Epoch: 1288900, elapsed: 1.16e+01, train loss: 8.81965e-07, val loss: 1.73112e-06, min loss: 8.77017e-07\n",
      "Epoch: 1289000, elapsed: 1.15e+01, train loss: 1.16007e-06, val loss: 1.74289e-06, min loss: 8.77017e-07\n",
      "Epoch: 1289100, elapsed: 1.16e+01, train loss: 9.31776e-07, val loss: 1.79584e-06, min loss: 8.77017e-07\n",
      "Epoch: 1289200, elapsed: 1.14e+01, train loss: 8.87377e-07, val loss: 1.70872e-06, min loss: 8.77017e-07\n",
      "Epoch: 1289300, elapsed: 1.14e+01, train loss: 1.20750e-06, val loss: 1.89968e-06, min loss: 8.77017e-07\n",
      "Epoch: 1289400, elapsed: 1.15e+01, train loss: 8.85133e-07, val loss: 1.70949e-06, min loss: 8.77017e-07\n",
      "Epoch: 1289500, elapsed: 1.14e+01, train loss: 1.33958e-06, val loss: 2.34709e-06, min loss: 8.77017e-07\n",
      "Epoch: 1289600, elapsed: 1.14e+01, train loss: 9.18329e-07, val loss: 1.71872e-06, min loss: 8.77017e-07\n",
      "Epoch: 1289700, elapsed: 1.12e+01, train loss: 9.97294e-07, val loss: 1.91218e-06, min loss: 8.77017e-07\n",
      "Epoch: 1289800, elapsed: 1.15e+01, train loss: 1.05013e-06, val loss: 2.05301e-06, min loss: 8.77017e-07\n",
      "Epoch: 1289900, elapsed: 1.15e+01, train loss: 8.86315e-07, val loss: 1.71639e-06, min loss: 8.77017e-07\n",
      "Epoch: 1290000, elapsed: 1.14e+01, train loss: 1.26356e-06, val loss: 1.97736e-06, min loss: 8.77017e-07\n",
      "Epoch: 1290100, elapsed: 1.34e+01, train loss: 8.86873e-07, val loss: 1.72245e-06, min loss: 8.77017e-07\n",
      "Epoch: 1290200, elapsed: 1.13e+01, train loss: 8.83792e-07, val loss: 1.72561e-06, min loss: 8.77017e-07\n",
      "Epoch: 1290300, elapsed: 1.16e+01, train loss: 8.77512e-07, val loss: 1.71808e-06, min loss: 8.77017e-07\n",
      "Epoch: 1290400, elapsed: 1.54e+01, train loss: 9.80808e-07, val loss: 1.79009e-06, min loss: 8.77017e-07\n",
      "Epoch: 1290500, elapsed: 1.15e+01, train loss: 8.75776e-07, val loss: 1.71212e-06, min loss: 8.75776e-07\n",
      "Epoch: 1290600, elapsed: 1.16e+01, train loss: 9.84540e-07, val loss: 1.78818e-06, min loss: 8.75776e-07\n",
      "Epoch: 1290700, elapsed: 1.16e+01, train loss: 8.75700e-07, val loss: 1.71168e-06, min loss: 8.75700e-07\n",
      "Epoch: 1290800, elapsed: 1.16e+01, train loss: 8.84089e-07, val loss: 1.71147e-06, min loss: 8.75700e-07\n",
      "Epoch: 1290900, elapsed: 1.16e+01, train loss: 1.09647e-06, val loss: 2.03008e-06, min loss: 8.75700e-07\n",
      "Epoch: 1291000, elapsed: 1.15e+01, train loss: 8.75653e-07, val loss: 1.71331e-06, min loss: 8.75653e-07\n",
      "Epoch: 1291100, elapsed: 1.14e+01, train loss: 8.76476e-07, val loss: 1.71566e-06, min loss: 8.75653e-07\n",
      "Epoch: 1291200, elapsed: 1.14e+01, train loss: 8.82313e-07, val loss: 1.73029e-06, min loss: 8.75653e-07\n",
      "Epoch: 1291300, elapsed: 1.15e+01, train loss: 1.08139e-06, val loss: 2.05811e-06, min loss: 8.75653e-07\n",
      "Epoch: 1291400, elapsed: 1.14e+01, train loss: 1.10990e-06, val loss: 2.10089e-06, min loss: 8.75653e-07\n",
      "Epoch: 1291500, elapsed: 1.13e+01, train loss: 9.40278e-07, val loss: 1.82228e-06, min loss: 8.75653e-07\n",
      "Epoch: 1291600, elapsed: 1.14e+01, train loss: 1.29426e-06, val loss: 2.20613e-06, min loss: 8.75653e-07\n",
      "Epoch: 1291700, elapsed: 1.14e+01, train loss: 8.82442e-07, val loss: 1.72891e-06, min loss: 8.75653e-07\n",
      "Epoch: 1291800, elapsed: 1.11e+01, train loss: 9.06380e-07, val loss: 1.71678e-06, min loss: 8.75653e-07\n",
      "Epoch: 1291900, elapsed: 1.13e+01, train loss: 8.94002e-07, val loss: 1.70412e-06, min loss: 8.75653e-07\n",
      "Epoch: 1292000, elapsed: 1.13e+01, train loss: 9.16715e-07, val loss: 1.76935e-06, min loss: 8.75653e-07\n",
      "Epoch: 1292100, elapsed: 1.14e+01, train loss: 1.02994e-06, val loss: 1.77038e-06, min loss: 8.75653e-07\n",
      "Epoch: 1292200, elapsed: 1.15e+01, train loss: 8.90757e-07, val loss: 1.71976e-06, min loss: 8.75653e-07\n",
      "Epoch: 1292300, elapsed: 1.12e+01, train loss: 9.56598e-07, val loss: 1.82036e-06, min loss: 8.75653e-07\n",
      "Epoch: 1292400, elapsed: 1.12e+01, train loss: 8.79375e-07, val loss: 1.72864e-06, min loss: 8.75653e-07\n",
      "Epoch: 1292500, elapsed: 1.11e+01, train loss: 1.02476e-06, val loss: 1.77486e-06, min loss: 8.75653e-07\n",
      "Epoch: 1292600, elapsed: 1.12e+01, train loss: 9.28702e-07, val loss: 1.80248e-06, min loss: 8.75653e-07\n",
      "Epoch: 1292700, elapsed: 1.12e+01, train loss: 9.07653e-07, val loss: 1.76401e-06, min loss: 8.75653e-07\n",
      "Epoch: 1292800, elapsed: 1.52e+01, train loss: 8.92342e-07, val loss: 1.70459e-06, min loss: 8.75653e-07\n",
      "Epoch: 1292900, elapsed: 1.14e+01, train loss: 9.22513e-07, val loss: 1.71635e-06, min loss: 8.75653e-07\n",
      "Epoch: 1293000, elapsed: 1.15e+01, train loss: 1.88914e-06, val loss: 3.36984e-06, min loss: 8.75653e-07\n",
      "Epoch: 1293100, elapsed: 1.14e+01, train loss: 1.91446e-06, val loss: 2.48996e-06, min loss: 8.75653e-07\n",
      "Epoch: 1293200, elapsed: 1.13e+01, train loss: 9.84629e-07, val loss: 1.87255e-06, min loss: 8.75653e-07\n",
      "Epoch: 1293300, elapsed: 1.15e+01, train loss: 1.17712e-06, val loss: 1.92879e-06, min loss: 8.75653e-07\n",
      "Epoch: 1293400, elapsed: 1.15e+01, train loss: 1.39527e-06, val loss: 2.21490e-06, min loss: 8.75653e-07\n",
      "Epoch: 1293500, elapsed: 1.14e+01, train loss: 4.31524e-06, val loss: 4.75234e-06, min loss: 8.75653e-07\n",
      "Epoch: 1293600, elapsed: 1.15e+01, train loss: 9.07750e-07, val loss: 1.71619e-06, min loss: 8.75653e-07\n",
      "Epoch: 1293700, elapsed: 1.13e+01, train loss: 1.16149e-06, val loss: 2.05495e-06, min loss: 8.75653e-07\n",
      "Epoch: 1293800, elapsed: 1.14e+01, train loss: 1.12517e-06, val loss: 2.02488e-06, min loss: 8.75653e-07\n",
      "Epoch: 1293900, elapsed: 1.15e+01, train loss: 9.54997e-07, val loss: 1.81504e-06, min loss: 8.75653e-07\n",
      "Epoch: 1294000, elapsed: 1.14e+01, train loss: 2.29600e-06, val loss: 3.11558e-06, min loss: 8.75653e-07\n",
      "Epoch: 1294100, elapsed: 1.13e+01, train loss: 1.00419e-06, val loss: 1.85301e-06, min loss: 8.75653e-07\n",
      "Epoch: 1294200, elapsed: 1.13e+01, train loss: 8.77459e-07, val loss: 1.71064e-06, min loss: 8.75653e-07\n",
      "Epoch: 1294300, elapsed: 1.13e+01, train loss: 8.80886e-07, val loss: 1.72345e-06, min loss: 8.75653e-07\n",
      "Epoch: 1294400, elapsed: 1.16e+01, train loss: 8.76960e-07, val loss: 1.71528e-06, min loss: 8.75653e-07\n",
      "Epoch: 1294500, elapsed: 1.15e+01, train loss: 8.97053e-07, val loss: 1.72639e-06, min loss: 8.75653e-07\n",
      "Epoch: 1294600, elapsed: 1.14e+01, train loss: 6.27705e-06, val loss: 5.03920e-06, min loss: 8.75653e-07\n",
      "Epoch: 1294700, elapsed: 1.14e+01, train loss: 8.73446e-07, val loss: 1.70619e-06, min loss: 8.73446e-07\n",
      "Epoch: 1294800, elapsed: 1.14e+01, train loss: 1.04548e-06, val loss: 1.77307e-06, min loss: 8.73446e-07\n",
      "Epoch: 1294900, elapsed: 1.12e+01, train loss: 8.74369e-07, val loss: 1.71348e-06, min loss: 8.73446e-07\n",
      "Epoch: 1295000, elapsed: 1.13e+01, train loss: 8.73366e-07, val loss: 1.70826e-06, min loss: 8.73366e-07\n",
      "Epoch: 1295100, elapsed: 1.72e+01, train loss: 9.36227e-07, val loss: 1.70922e-06, min loss: 8.73366e-07\n",
      "Epoch: 1295200, elapsed: 1.15e+01, train loss: 1.09256e-06, val loss: 1.91869e-06, min loss: 8.73366e-07\n",
      "Epoch: 1295300, elapsed: 1.16e+01, train loss: 1.99492e-06, val loss: 2.44918e-06, min loss: 8.73366e-07\n",
      "Epoch: 1295400, elapsed: 1.15e+01, train loss: 1.41314e-06, val loss: 2.19417e-06, min loss: 8.73366e-07\n",
      "Epoch: 1295500, elapsed: 1.15e+01, train loss: 1.00373e-06, val loss: 1.91972e-06, min loss: 8.73366e-07\n",
      "Epoch: 1295600, elapsed: 1.15e+01, train loss: 4.22822e-06, val loss: 3.78322e-06, min loss: 8.73366e-07\n",
      "Epoch: 1295700, elapsed: 1.14e+01, train loss: 8.73657e-07, val loss: 1.70924e-06, min loss: 8.73366e-07\n",
      "Epoch: 1295800, elapsed: 1.13e+01, train loss: 8.74357e-07, val loss: 1.71066e-06, min loss: 8.73366e-07\n",
      "Epoch: 1295900, elapsed: 1.13e+01, train loss: 8.93410e-07, val loss: 1.71911e-06, min loss: 8.73366e-07\n",
      "Epoch: 1296000, elapsed: 1.13e+01, train loss: 9.34228e-07, val loss: 1.74753e-06, min loss: 8.73366e-07\n",
      "Epoch: 1296100, elapsed: 1.12e+01, train loss: 1.28156e-06, val loss: 1.93905e-06, min loss: 8.73366e-07\n",
      "Epoch: 1296200, elapsed: 1.14e+01, train loss: 3.60796e-06, val loss: 4.33090e-06, min loss: 8.73366e-07\n",
      "Epoch: 1296300, elapsed: 1.13e+01, train loss: 3.19985e-06, val loss: 4.17223e-06, min loss: 8.73366e-07\n",
      "Epoch: 1296400, elapsed: 1.14e+01, train loss: 9.34749e-07, val loss: 1.74870e-06, min loss: 8.73366e-07\n",
      "Epoch: 1296500, elapsed: 1.13e+01, train loss: 8.91540e-07, val loss: 1.71284e-06, min loss: 8.73366e-07\n",
      "Epoch: 1296600, elapsed: 1.14e+01, train loss: 9.00672e-07, val loss: 1.75977e-06, min loss: 8.73366e-07\n",
      "Epoch: 1296700, elapsed: 1.14e+01, train loss: 8.94605e-07, val loss: 1.71568e-06, min loss: 8.73366e-07\n",
      "Epoch: 1296800, elapsed: 1.14e+01, train loss: 8.74926e-07, val loss: 1.70363e-06, min loss: 8.73366e-07\n",
      "Epoch: 1296900, elapsed: 1.12e+01, train loss: 8.79979e-07, val loss: 1.70661e-06, min loss: 8.73366e-07\n",
      "Epoch: 1297000, elapsed: 1.14e+01, train loss: 8.84182e-07, val loss: 1.70001e-06, min loss: 8.73366e-07\n",
      "Epoch: 1297100, elapsed: 1.13e+01, train loss: 9.72373e-07, val loss: 1.76482e-06, min loss: 8.73366e-07\n",
      "Epoch: 1297200, elapsed: 1.15e+01, train loss: 4.26604e-06, val loss: 4.05674e-06, min loss: 8.73366e-07\n",
      "Epoch: 1297300, elapsed: 1.13e+01, train loss: 2.16836e-06, val loss: 2.54894e-06, min loss: 8.73366e-07\n",
      "Epoch: 1297400, elapsed: 1.14e+01, train loss: 9.62250e-07, val loss: 1.74400e-06, min loss: 8.73366e-07\n",
      "Epoch: 1297500, elapsed: 1.52e+01, train loss: 8.74918e-07, val loss: 1.70093e-06, min loss: 8.73366e-07\n",
      "Epoch: 1297600, elapsed: 1.17e+01, train loss: 9.40945e-07, val loss: 1.73252e-06, min loss: 8.73366e-07\n",
      "Epoch: 1297700, elapsed: 1.15e+01, train loss: 1.00475e-06, val loss: 1.75461e-06, min loss: 8.73366e-07\n",
      "Epoch: 1297800, elapsed: 1.15e+01, train loss: 9.42418e-07, val loss: 1.73415e-06, min loss: 8.73366e-07\n",
      "Epoch: 1297900, elapsed: 1.16e+01, train loss: 2.81529e-06, val loss: 2.73116e-06, min loss: 8.73366e-07\n",
      "Epoch: 1298000, elapsed: 1.15e+01, train loss: 3.83646e-06, val loss: 5.53529e-06, min loss: 8.73366e-07\n",
      "Epoch: 1298100, elapsed: 1.14e+01, train loss: 8.71443e-07, val loss: 1.69857e-06, min loss: 8.71443e-07\n",
      "Epoch: 1298200, elapsed: 1.14e+01, train loss: 8.90304e-07, val loss: 1.73515e-06, min loss: 8.71443e-07\n",
      "Epoch: 1298300, elapsed: 1.13e+01, train loss: 9.87902e-07, val loss: 1.70695e-06, min loss: 8.71443e-07\n",
      "Epoch: 1298400, elapsed: 1.14e+01, train loss: 8.70895e-07, val loss: 1.70158e-06, min loss: 8.70895e-07\n",
      "Epoch: 1298500, elapsed: 1.13e+01, train loss: 8.75481e-07, val loss: 1.68788e-06, min loss: 8.70895e-07\n",
      "Epoch: 1298600, elapsed: 1.13e+01, train loss: 8.73334e-07, val loss: 1.69458e-06, min loss: 8.70895e-07\n",
      "Epoch: 1298700, elapsed: 1.12e+01, train loss: 9.18795e-07, val loss: 1.85723e-06, min loss: 8.70895e-07\n",
      "Epoch: 1298800, elapsed: 1.14e+01, train loss: 2.34667e-06, val loss: 3.37056e-06, min loss: 8.70895e-07\n",
      "Epoch: 1298900, elapsed: 1.14e+01, train loss: 1.26848e-06, val loss: 2.41723e-06, min loss: 8.70895e-07\n",
      "Epoch: 1299000, elapsed: 1.13e+01, train loss: 1.01375e-06, val loss: 1.75745e-06, min loss: 8.70895e-07\n",
      "Epoch: 1299100, elapsed: 1.13e+01, train loss: 1.28655e-06, val loss: 2.27580e-06, min loss: 8.70895e-07\n",
      "Epoch: 1299200, elapsed: 1.13e+01, train loss: 1.52520e-06, val loss: 2.21462e-06, min loss: 8.70895e-07\n",
      "Epoch: 1299300, elapsed: 1.15e+01, train loss: 8.86435e-07, val loss: 1.73886e-06, min loss: 8.70895e-07\n",
      "Epoch: 1299400, elapsed: 1.13e+01, train loss: 8.72025e-07, val loss: 1.69754e-06, min loss: 8.70895e-07\n",
      "Epoch: 1299500, elapsed: 1.13e+01, train loss: 8.75920e-07, val loss: 1.70172e-06, min loss: 8.70895e-07\n",
      "Epoch: 1299600, elapsed: 1.14e+01, train loss: 8.81115e-07, val loss: 1.69320e-06, min loss: 8.70895e-07\n",
      "Epoch: 1299700, elapsed: 1.12e+01, train loss: 8.74331e-07, val loss: 1.69655e-06, min loss: 8.70895e-07\n",
      "Epoch: 1299800, elapsed: 1.12e+01, train loss: 8.82650e-07, val loss: 1.70529e-06, min loss: 8.70895e-07\n",
      "Epoch: 1299900, elapsed: 1.54e+01, train loss: 8.73470e-07, val loss: 1.71834e-06, min loss: 8.70895e-07\n",
      "Epoch: 1300000, elapsed: 1.14e+01, train loss: 1.11920e-06, val loss: 1.97964e-06, min loss: 8.70895e-07\n",
      "Epoch: 1300100, elapsed: 1.36e+01, train loss: 1.75879e-06, val loss: 1.95323e-06, min loss: 8.70895e-07\n",
      "Epoch: 1300200, elapsed: 1.14e+01, train loss: 8.69892e-07, val loss: 1.69775e-06, min loss: 8.69892e-07\n",
      "Epoch: 1300300, elapsed: 1.13e+01, train loss: 8.91084e-07, val loss: 1.73843e-06, min loss: 8.69892e-07\n",
      "Epoch: 1300400, elapsed: 1.13e+01, train loss: 8.70313e-07, val loss: 1.70252e-06, min loss: 8.69892e-07\n",
      "Epoch: 1300500, elapsed: 1.15e+01, train loss: 1.39400e-06, val loss: 2.37549e-06, min loss: 8.69892e-07\n",
      "Epoch: 1300600, elapsed: 1.14e+01, train loss: 9.49881e-07, val loss: 1.77365e-06, min loss: 8.69892e-07\n",
      "Epoch: 1300700, elapsed: 1.16e+01, train loss: 9.30294e-07, val loss: 1.81313e-06, min loss: 8.69892e-07\n",
      "Epoch: 1300800, elapsed: 1.15e+01, train loss: 1.08500e-06, val loss: 1.99600e-06, min loss: 8.69892e-07\n",
      "Epoch: 1300900, elapsed: 1.15e+01, train loss: 9.14208e-07, val loss: 1.75915e-06, min loss: 8.69892e-07\n",
      "Epoch: 1301000, elapsed: 1.13e+01, train loss: 9.82444e-07, val loss: 1.90377e-06, min loss: 8.69892e-07\n",
      "Epoch: 1301100, elapsed: 1.13e+01, train loss: 1.07003e-06, val loss: 1.94759e-06, min loss: 8.69892e-07\n",
      "Epoch: 1301200, elapsed: 1.13e+01, train loss: 8.89818e-07, val loss: 1.69831e-06, min loss: 8.69892e-07\n",
      "Epoch: 1301300, elapsed: 1.14e+01, train loss: 8.98877e-07, val loss: 1.77225e-06, min loss: 8.69892e-07\n",
      "Epoch: 1301400, elapsed: 1.13e+01, train loss: 8.76073e-07, val loss: 1.69563e-06, min loss: 8.69892e-07\n",
      "Epoch: 1301500, elapsed: 1.15e+01, train loss: 8.80631e-07, val loss: 1.69797e-06, min loss: 8.69892e-07\n",
      "Epoch: 1301600, elapsed: 1.12e+01, train loss: 9.00440e-07, val loss: 1.72469e-06, min loss: 8.69892e-07\n",
      "Epoch: 1301700, elapsed: 1.12e+01, train loss: 9.43727e-07, val loss: 1.70006e-06, min loss: 8.69892e-07\n",
      "Epoch: 1301800, elapsed: 1.13e+01, train loss: 9.83151e-07, val loss: 1.75747e-06, min loss: 8.69892e-07\n",
      "Epoch: 1301900, elapsed: 1.12e+01, train loss: 1.00521e-06, val loss: 1.81762e-06, min loss: 8.69892e-07\n",
      "Epoch: 1302000, elapsed: 1.08e+01, train loss: 9.59545e-07, val loss: 1.73791e-06, min loss: 8.69892e-07\n",
      "Epoch: 1302100, elapsed: 1.06e+01, train loss: 9.72570e-07, val loss: 1.83919e-06, min loss: 8.69892e-07\n",
      "Epoch: 1302200, elapsed: 1.38e+01, train loss: 8.89376e-07, val loss: 1.71584e-06, min loss: 8.69892e-07\n",
      "Epoch: 1302300, elapsed: 1.08e+01, train loss: 1.26217e-06, val loss: 2.07434e-06, min loss: 8.69892e-07\n",
      "Epoch: 1302400, elapsed: 1.05e+01, train loss: 9.78943e-07, val loss: 1.77441e-06, min loss: 8.69892e-07\n",
      "Epoch: 1302500, elapsed: 1.05e+01, train loss: 9.10827e-07, val loss: 1.69564e-06, min loss: 8.69892e-07\n",
      "Epoch: 1302600, elapsed: 1.08e+01, train loss: 9.14142e-07, val loss: 1.74773e-06, min loss: 8.69892e-07\n",
      "Epoch: 1302700, elapsed: 1.08e+01, train loss: 9.04431e-07, val loss: 1.74635e-06, min loss: 8.69892e-07\n",
      "Epoch: 1302800, elapsed: 1.07e+01, train loss: 8.72940e-07, val loss: 1.69856e-06, min loss: 8.69892e-07\n",
      "Epoch: 1302900, elapsed: 1.09e+01, train loss: 8.96846e-07, val loss: 1.72934e-06, min loss: 8.69892e-07\n",
      "Epoch: 1303000, elapsed: 1.04e+01, train loss: 1.44136e-06, val loss: 1.70917e-06, min loss: 8.69892e-07\n",
      "Epoch: 1303100, elapsed: 1.05e+01, train loss: 8.68152e-07, val loss: 1.69412e-06, min loss: 8.68152e-07\n",
      "Epoch: 1303200, elapsed: 1.04e+01, train loss: 8.68883e-07, val loss: 1.69375e-06, min loss: 8.68152e-07\n",
      "Epoch: 1303300, elapsed: 1.04e+01, train loss: 8.69948e-07, val loss: 1.68572e-06, min loss: 8.68152e-07\n",
      "Epoch: 1303400, elapsed: 1.05e+01, train loss: 9.38893e-07, val loss: 1.80662e-06, min loss: 8.68152e-07\n",
      "Epoch: 1303500, elapsed: 1.04e+01, train loss: 1.29280e-06, val loss: 2.18664e-06, min loss: 8.68152e-07\n",
      "Epoch: 1303600, elapsed: 1.05e+01, train loss: 1.60384e-06, val loss: 2.58699e-06, min loss: 8.68152e-07\n",
      "Epoch: 1303700, elapsed: 1.05e+01, train loss: 9.17491e-07, val loss: 1.79715e-06, min loss: 8.68152e-07\n",
      "Epoch: 1303800, elapsed: 1.04e+01, train loss: 1.18666e-06, val loss: 2.13019e-06, min loss: 8.68152e-07\n",
      "Epoch: 1303900, elapsed: 1.06e+01, train loss: 9.79653e-07, val loss: 1.73322e-06, min loss: 8.68152e-07\n",
      "Epoch: 1304000, elapsed: 1.05e+01, train loss: 1.66344e-06, val loss: 2.50635e-06, min loss: 8.68152e-07\n",
      "Epoch: 1304100, elapsed: 1.06e+01, train loss: 9.23999e-07, val loss: 1.88339e-06, min loss: 8.68152e-07\n",
      "Epoch: 1304200, elapsed: 1.04e+01, train loss: 4.52264e-06, val loss: 4.43480e-06, min loss: 8.68152e-07\n",
      "Epoch: 1304300, elapsed: 1.04e+01, train loss: 8.68768e-07, val loss: 1.70118e-06, min loss: 8.68152e-07\n",
      "Epoch: 1304400, elapsed: 1.04e+01, train loss: 8.67664e-07, val loss: 1.69098e-06, min loss: 8.67664e-07\n",
      "Epoch: 1304500, elapsed: 1.04e+01, train loss: 8.78335e-07, val loss: 1.70910e-06, min loss: 8.67664e-07\n",
      "Epoch: 1304600, elapsed: 1.39e+01, train loss: 9.09038e-07, val loss: 1.72317e-06, min loss: 8.67664e-07\n",
      "Epoch: 1304700, elapsed: 1.07e+01, train loss: 9.09091e-07, val loss: 1.70064e-06, min loss: 8.67664e-07\n",
      "Epoch: 1304800, elapsed: 1.07e+01, train loss: 9.45608e-07, val loss: 1.70330e-06, min loss: 8.67664e-07\n",
      "Epoch: 1304900, elapsed: 1.06e+01, train loss: 3.16994e-06, val loss: 3.62354e-06, min loss: 8.67664e-07\n",
      "Epoch: 1305000, elapsed: 1.05e+01, train loss: 3.67088e-06, val loss: 4.24102e-06, min loss: 8.67664e-07\n",
      "Epoch: 1305100, elapsed: 1.24e+01, train loss: 1.04354e-06, val loss: 1.83515e-06, min loss: 8.67664e-07\n",
      "Epoch: 1305200, elapsed: 1.06e+01, train loss: 8.67637e-07, val loss: 1.68603e-06, min loss: 8.67637e-07\n",
      "Epoch: 1305300, elapsed: 1.06e+01, train loss: 8.67838e-07, val loss: 1.68550e-06, min loss: 8.67637e-07\n",
      "Epoch: 1305400, elapsed: 1.04e+01, train loss: 1.17825e-06, val loss: 1.96542e-06, min loss: 8.67637e-07\n",
      "Epoch: 1305500, elapsed: 1.05e+01, train loss: 9.55800e-07, val loss: 1.69904e-06, min loss: 8.67637e-07\n",
      "Epoch: 1305600, elapsed: 1.04e+01, train loss: 1.25394e-06, val loss: 1.83466e-06, min loss: 8.67637e-07\n",
      "Epoch: 1305700, elapsed: 1.03e+01, train loss: 1.40835e-06, val loss: 2.48783e-06, min loss: 8.67637e-07\n",
      "Epoch: 1305800, elapsed: 1.06e+01, train loss: 8.71091e-07, val loss: 1.70229e-06, min loss: 8.67637e-07\n",
      "Epoch: 1305900, elapsed: 1.05e+01, train loss: 8.66605e-07, val loss: 1.68628e-06, min loss: 8.66605e-07\n",
      "Epoch: 1306000, elapsed: 1.06e+01, train loss: 8.74512e-07, val loss: 1.69001e-06, min loss: 8.66605e-07\n",
      "Epoch: 1306100, elapsed: 1.03e+01, train loss: 9.61898e-07, val loss: 1.71236e-06, min loss: 8.66605e-07\n",
      "Epoch: 1306200, elapsed: 1.04e+01, train loss: 1.33508e-06, val loss: 2.22713e-06, min loss: 8.66605e-07\n",
      "Epoch: 1306300, elapsed: 1.04e+01, train loss: 8.74121e-07, val loss: 1.69450e-06, min loss: 8.66605e-07\n",
      "Epoch: 1306400, elapsed: 1.04e+01, train loss: 3.31085e-06, val loss: 4.05417e-06, min loss: 8.66605e-07\n",
      "Epoch: 1306500, elapsed: 1.03e+01, train loss: 8.74756e-07, val loss: 1.69463e-06, min loss: 8.66605e-07\n",
      "Epoch: 1306600, elapsed: 1.04e+01, train loss: 8.67425e-07, val loss: 1.69609e-06, min loss: 8.66605e-07\n",
      "Epoch: 1306700, elapsed: 1.06e+01, train loss: 2.30763e-06, val loss: 3.49751e-06, min loss: 8.66605e-07\n",
      "Epoch: 1306800, elapsed: 1.05e+01, train loss: 8.66468e-07, val loss: 1.68589e-06, min loss: 8.66468e-07\n",
      "Epoch: 1306900, elapsed: 1.04e+01, train loss: 8.81028e-07, val loss: 1.71268e-06, min loss: 8.66468e-07\n",
      "Epoch: 1307000, elapsed: 1.40e+01, train loss: 9.58805e-07, val loss: 1.79910e-06, min loss: 8.66468e-07\n",
      "Epoch: 1307100, elapsed: 1.07e+01, train loss: 1.18677e-06, val loss: 2.09859e-06, min loss: 8.66468e-07\n",
      "Epoch: 1307200, elapsed: 1.05e+01, train loss: 9.70547e-07, val loss: 1.80501e-06, min loss: 8.66468e-07\n",
      "Epoch: 1307300, elapsed: 1.06e+01, train loss: 9.42784e-07, val loss: 1.71191e-06, min loss: 8.66468e-07\n",
      "Epoch: 1307400, elapsed: 1.04e+01, train loss: 1.01573e-06, val loss: 1.82341e-06, min loss: 8.66468e-07\n",
      "Epoch: 1307500, elapsed: 1.06e+01, train loss: 9.08881e-07, val loss: 1.70445e-06, min loss: 8.66468e-07\n",
      "Epoch: 1307600, elapsed: 1.05e+01, train loss: 1.60261e-06, val loss: 2.07181e-06, min loss: 8.66468e-07\n",
      "Epoch: 1307700, elapsed: 1.04e+01, train loss: 9.76623e-07, val loss: 1.86405e-06, min loss: 8.66468e-07\n",
      "Epoch: 1307800, elapsed: 1.05e+01, train loss: 1.17755e-06, val loss: 1.87700e-06, min loss: 8.66468e-07\n",
      "Epoch: 1307900, elapsed: 1.06e+01, train loss: 1.13687e-06, val loss: 1.91719e-06, min loss: 8.66468e-07\n",
      "Epoch: 1308000, elapsed: 1.05e+01, train loss: 9.67277e-07, val loss: 1.77412e-06, min loss: 8.66468e-07\n",
      "Epoch: 1308100, elapsed: 1.05e+01, train loss: 3.26598e-06, val loss: 4.74983e-06, min loss: 8.66468e-07\n",
      "Epoch: 1308200, elapsed: 1.04e+01, train loss: 8.69251e-07, val loss: 1.70150e-06, min loss: 8.66468e-07\n",
      "Epoch: 1308300, elapsed: 1.05e+01, train loss: 8.77691e-07, val loss: 1.71593e-06, min loss: 8.66468e-07\n",
      "Epoch: 1308400, elapsed: 1.04e+01, train loss: 8.66365e-07, val loss: 1.68789e-06, min loss: 8.66365e-07\n",
      "Epoch: 1308500, elapsed: 1.03e+01, train loss: 8.66085e-07, val loss: 1.68554e-06, min loss: 8.66085e-07\n",
      "Epoch: 1308600, elapsed: 1.04e+01, train loss: 8.79327e-07, val loss: 1.68840e-06, min loss: 8.66085e-07\n",
      "Epoch: 1308700, elapsed: 1.04e+01, train loss: 8.87765e-07, val loss: 1.73525e-06, min loss: 8.66085e-07\n",
      "Epoch: 1308800, elapsed: 1.04e+01, train loss: 8.66538e-07, val loss: 1.68974e-06, min loss: 8.66085e-07\n",
      "Epoch: 1308900, elapsed: 1.03e+01, train loss: 8.66555e-07, val loss: 1.68817e-06, min loss: 8.66085e-07\n",
      "Epoch: 1309000, elapsed: 1.02e+01, train loss: 9.10543e-07, val loss: 1.73448e-06, min loss: 8.66085e-07\n",
      "Epoch: 1309100, elapsed: 1.06e+01, train loss: 1.36745e-06, val loss: 2.02497e-06, min loss: 8.66085e-07\n",
      "Epoch: 1309200, elapsed: 1.02e+01, train loss: 8.76991e-07, val loss: 1.67706e-06, min loss: 8.66085e-07\n",
      "Epoch: 1309300, elapsed: 1.03e+01, train loss: 9.00454e-07, val loss: 1.73281e-06, min loss: 8.66085e-07\n",
      "Epoch: 1309400, elapsed: 1.39e+01, train loss: 8.80388e-07, val loss: 1.89559e-06, min loss: 8.66085e-07\n",
      "Epoch: 1309500, elapsed: 1.05e+01, train loss: 8.64310e-07, val loss: 1.68319e-06, min loss: 8.64310e-07\n",
      "Epoch: 1309600, elapsed: 1.06e+01, train loss: 8.74913e-07, val loss: 1.68394e-06, min loss: 8.64310e-07\n",
      "Epoch: 1309700, elapsed: 1.06e+01, train loss: 1.52869e-06, val loss: 2.05903e-06, min loss: 8.64310e-07\n",
      "Epoch: 1309800, elapsed: 1.06e+01, train loss: 8.64073e-07, val loss: 1.68341e-06, min loss: 8.64073e-07\n",
      "Epoch: 1309900, elapsed: 1.05e+01, train loss: 8.70112e-07, val loss: 1.69659e-06, min loss: 8.64073e-07\n",
      "Epoch: 1310000, elapsed: 1.07e+01, train loss: 9.42899e-07, val loss: 1.72818e-06, min loss: 8.64073e-07\n",
      "Epoch: 1310100, elapsed: 1.23e+01, train loss: 1.77576e-06, val loss: 2.45874e-06, min loss: 8.64073e-07\n",
      "Epoch: 1310200, elapsed: 1.07e+01, train loss: 8.66792e-07, val loss: 1.69237e-06, min loss: 8.64073e-07\n",
      "Epoch: 1310300, elapsed: 1.06e+01, train loss: 9.00360e-07, val loss: 1.75346e-06, min loss: 8.64073e-07\n",
      "Epoch: 1310400, elapsed: 1.04e+01, train loss: 8.66964e-07, val loss: 1.68712e-06, min loss: 8.64073e-07\n",
      "Epoch: 1310500, elapsed: 1.06e+01, train loss: 8.66715e-07, val loss: 1.69032e-06, min loss: 8.64073e-07\n",
      "Epoch: 1310600, elapsed: 1.05e+01, train loss: 8.99204e-07, val loss: 1.67795e-06, min loss: 8.64073e-07\n",
      "Epoch: 1310700, elapsed: 1.06e+01, train loss: 3.66954e-06, val loss: 5.02174e-06, min loss: 8.64073e-07\n",
      "Epoch: 1310800, elapsed: 1.04e+01, train loss: 8.63866e-07, val loss: 1.68115e-06, min loss: 8.63866e-07\n",
      "Epoch: 1310900, elapsed: 1.05e+01, train loss: 9.20848e-07, val loss: 1.67520e-06, min loss: 8.63866e-07\n",
      "Epoch: 1311000, elapsed: 1.03e+01, train loss: 9.82333e-07, val loss: 1.88939e-06, min loss: 8.63866e-07\n",
      "Epoch: 1311100, elapsed: 1.05e+01, train loss: 8.63710e-07, val loss: 1.68548e-06, min loss: 8.63710e-07\n",
      "Epoch: 1311200, elapsed: 1.02e+01, train loss: 8.64775e-07, val loss: 1.68480e-06, min loss: 8.63710e-07\n",
      "Epoch: 1311300, elapsed: 1.03e+01, train loss: 9.06956e-07, val loss: 1.76334e-06, min loss: 8.63710e-07\n",
      "Epoch: 1311400, elapsed: 1.03e+01, train loss: 9.00636e-07, val loss: 1.70074e-06, min loss: 8.63710e-07\n",
      "Epoch: 1311500, elapsed: 1.04e+01, train loss: 1.07438e-06, val loss: 1.78064e-06, min loss: 8.63710e-07\n",
      "Epoch: 1311600, elapsed: 1.03e+01, train loss: 8.84401e-07, val loss: 1.73908e-06, min loss: 8.63710e-07\n",
      "Epoch: 1311700, elapsed: 1.04e+01, train loss: 8.86768e-07, val loss: 1.72050e-06, min loss: 8.63710e-07\n",
      "Epoch: 1311800, elapsed: 1.42e+01, train loss: 9.98788e-07, val loss: 1.76684e-06, min loss: 8.63710e-07\n",
      "Epoch: 1311900, elapsed: 1.09e+01, train loss: 1.03068e-06, val loss: 1.78192e-06, min loss: 8.63710e-07\n",
      "Epoch: 1312000, elapsed: 1.05e+01, train loss: 1.02128e-06, val loss: 1.78178e-06, min loss: 8.63710e-07\n",
      "Epoch: 1312100, elapsed: 1.06e+01, train loss: 9.22662e-07, val loss: 1.79965e-06, min loss: 8.63710e-07\n",
      "Epoch: 1312200, elapsed: 1.05e+01, train loss: 1.44167e-06, val loss: 2.52255e-06, min loss: 8.63710e-07\n",
      "Epoch: 1312300, elapsed: 1.05e+01, train loss: 9.74468e-07, val loss: 1.88936e-06, min loss: 8.63710e-07\n",
      "Epoch: 1312400, elapsed: 1.05e+01, train loss: 8.62905e-07, val loss: 1.67839e-06, min loss: 8.62905e-07\n",
      "Epoch: 1312500, elapsed: 1.05e+01, train loss: 8.66010e-07, val loss: 1.68174e-06, min loss: 8.62905e-07\n",
      "Epoch: 1312600, elapsed: 1.06e+01, train loss: 3.03418e-06, val loss: 3.78947e-06, min loss: 8.62905e-07\n",
      "Epoch: 1312700, elapsed: 1.05e+01, train loss: 8.62417e-07, val loss: 1.67934e-06, min loss: 8.62417e-07\n",
      "Epoch: 1312800, elapsed: 1.06e+01, train loss: 9.97178e-07, val loss: 1.77709e-06, min loss: 8.62417e-07\n",
      "Epoch: 1312900, elapsed: 1.04e+01, train loss: 8.62162e-07, val loss: 1.67920e-06, min loss: 8.62162e-07\n",
      "Epoch: 1313000, elapsed: 1.04e+01, train loss: 8.93637e-07, val loss: 1.70993e-06, min loss: 8.62162e-07\n",
      "Epoch: 1313100, elapsed: 1.04e+01, train loss: 1.29225e-06, val loss: 2.13370e-06, min loss: 8.62162e-07\n",
      "Epoch: 1313200, elapsed: 1.04e+01, train loss: 9.02544e-07, val loss: 1.69209e-06, min loss: 8.62162e-07\n",
      "Epoch: 1313300, elapsed: 1.05e+01, train loss: 1.03920e-06, val loss: 1.81891e-06, min loss: 8.62162e-07\n",
      "Epoch: 1313400, elapsed: 1.07e+01, train loss: 1.10722e-06, val loss: 1.92659e-06, min loss: 8.62162e-07\n",
      "Epoch: 1313500, elapsed: 1.04e+01, train loss: 1.02122e-06, val loss: 1.86020e-06, min loss: 8.62162e-07\n",
      "Epoch: 1313600, elapsed: 1.05e+01, train loss: 8.64681e-07, val loss: 1.68021e-06, min loss: 8.62162e-07\n",
      "Epoch: 1313700, elapsed: 1.05e+01, train loss: 8.64066e-07, val loss: 1.67772e-06, min loss: 8.62162e-07\n",
      "Epoch: 1313800, elapsed: 1.04e+01, train loss: 1.01454e-06, val loss: 1.74175e-06, min loss: 8.62162e-07\n",
      "Epoch: 1313900, elapsed: 1.04e+01, train loss: 2.15293e-06, val loss: 2.95903e-06, min loss: 8.62162e-07\n",
      "Epoch: 1314000, elapsed: 1.05e+01, train loss: 1.17305e-06, val loss: 1.99884e-06, min loss: 8.62162e-07\n",
      "Epoch: 1314100, elapsed: 1.04e+01, train loss: 9.53200e-07, val loss: 1.69005e-06, min loss: 8.62162e-07\n",
      "Epoch: 1314200, elapsed: 1.39e+01, train loss: 9.45067e-07, val loss: 1.76232e-06, min loss: 8.62162e-07\n",
      "Epoch: 1314300, elapsed: 1.06e+01, train loss: 1.22822e-06, val loss: 1.98057e-06, min loss: 8.62162e-07\n",
      "Epoch: 1314400, elapsed: 1.06e+01, train loss: 9.04552e-07, val loss: 1.70253e-06, min loss: 8.62162e-07\n",
      "Epoch: 1314500, elapsed: 1.05e+01, train loss: 8.61816e-07, val loss: 1.67382e-06, min loss: 8.61816e-07\n",
      "Epoch: 1314600, elapsed: 1.06e+01, train loss: 9.06279e-07, val loss: 1.75511e-06, min loss: 8.61816e-07\n",
      "Epoch: 1314700, elapsed: 1.05e+01, train loss: 1.65718e-06, val loss: 2.42054e-06, min loss: 8.61816e-07\n",
      "Epoch: 1314800, elapsed: 1.07e+01, train loss: 1.37559e-06, val loss: 1.88524e-06, min loss: 8.61816e-07\n",
      "Epoch: 1314900, elapsed: 1.06e+01, train loss: 9.39481e-07, val loss: 1.85792e-06, min loss: 8.61816e-07\n",
      "Epoch: 1315000, elapsed: 1.04e+01, train loss: 1.10308e-06, val loss: 1.84204e-06, min loss: 8.61816e-07\n",
      "Epoch: 1315100, elapsed: 1.23e+01, train loss: 1.52102e-06, val loss: 2.56841e-06, min loss: 8.61816e-07\n",
      "Epoch: 1315200, elapsed: 1.05e+01, train loss: 1.21561e-06, val loss: 2.11470e-06, min loss: 8.61816e-07\n",
      "Epoch: 1315300, elapsed: 1.05e+01, train loss: 8.60773e-07, val loss: 1.67424e-06, min loss: 8.60773e-07\n",
      "Epoch: 1315400, elapsed: 1.05e+01, train loss: 8.94653e-07, val loss: 1.68536e-06, min loss: 8.60773e-07\n",
      "Epoch: 1315500, elapsed: 1.04e+01, train loss: 1.05274e-06, val loss: 1.77400e-06, min loss: 8.60773e-07\n",
      "Epoch: 1315600, elapsed: 1.04e+01, train loss: 9.05793e-07, val loss: 1.71719e-06, min loss: 8.60773e-07\n",
      "Epoch: 1315700, elapsed: 1.03e+01, train loss: 8.71856e-07, val loss: 1.69935e-06, min loss: 8.60773e-07\n",
      "Epoch: 1315800, elapsed: 1.03e+01, train loss: 8.81773e-07, val loss: 1.71644e-06, min loss: 8.60773e-07\n",
      "Epoch: 1315900, elapsed: 1.04e+01, train loss: 8.73308e-07, val loss: 1.70949e-06, min loss: 8.60773e-07\n",
      "Epoch: 1316000, elapsed: 1.04e+01, train loss: 8.64149e-07, val loss: 1.68606e-06, min loss: 8.60773e-07\n",
      "Epoch: 1316100, elapsed: 1.04e+01, train loss: 1.26820e-06, val loss: 1.98247e-06, min loss: 8.60773e-07\n",
      "Epoch: 1316200, elapsed: 1.05e+01, train loss: 8.83514e-07, val loss: 1.67403e-06, min loss: 8.60773e-07\n",
      "Epoch: 1316300, elapsed: 1.04e+01, train loss: 8.85537e-07, val loss: 1.67785e-06, min loss: 8.60773e-07\n",
      "Epoch: 1316400, elapsed: 1.05e+01, train loss: 1.01857e-06, val loss: 1.88206e-06, min loss: 8.60773e-07\n",
      "Epoch: 1316500, elapsed: 1.42e+01, train loss: 8.81164e-07, val loss: 1.71467e-06, min loss: 8.60773e-07\n",
      "Epoch: 1316600, elapsed: 1.05e+01, train loss: 1.00778e-06, val loss: 1.80051e-06, min loss: 8.60773e-07\n",
      "Epoch: 1316700, elapsed: 1.05e+01, train loss: 3.38657e-06, val loss: 4.73127e-06, min loss: 8.60773e-07\n",
      "Epoch: 1316800, elapsed: 1.05e+01, train loss: 1.19700e-06, val loss: 2.25405e-06, min loss: 8.60773e-07\n",
      "Epoch: 1316900, elapsed: 1.06e+01, train loss: 2.41005e-06, val loss: 3.07985e-06, min loss: 8.60773e-07\n",
      "Epoch: 1317000, elapsed: 1.05e+01, train loss: 9.02729e-07, val loss: 1.73444e-06, min loss: 8.60773e-07\n",
      "Epoch: 1317100, elapsed: 1.05e+01, train loss: 8.79036e-07, val loss: 1.67873e-06, min loss: 8.60773e-07\n",
      "Epoch: 1317200, elapsed: 1.05e+01, train loss: 8.87358e-07, val loss: 1.68052e-06, min loss: 8.60773e-07\n",
      "Epoch: 1317300, elapsed: 1.05e+01, train loss: 9.04092e-07, val loss: 1.68398e-06, min loss: 8.60773e-07\n",
      "Epoch: 1317400, elapsed: 1.07e+01, train loss: 9.26583e-07, val loss: 1.78772e-06, min loss: 8.60773e-07\n",
      "Epoch: 1317500, elapsed: 1.05e+01, train loss: 8.63798e-07, val loss: 1.67774e-06, min loss: 8.60773e-07\n",
      "Epoch: 1317600, elapsed: 1.04e+01, train loss: 9.97474e-07, val loss: 1.78335e-06, min loss: 8.60773e-07\n",
      "Epoch: 1317700, elapsed: 1.06e+01, train loss: 8.71895e-07, val loss: 1.71412e-06, min loss: 8.60773e-07\n",
      "Epoch: 1317800, elapsed: 1.06e+01, train loss: 8.59424e-07, val loss: 1.67365e-06, min loss: 8.59424e-07\n",
      "Epoch: 1317900, elapsed: 1.05e+01, train loss: 9.74778e-07, val loss: 1.84995e-06, min loss: 8.59424e-07\n",
      "Epoch: 1318000, elapsed: 1.05e+01, train loss: 8.80893e-07, val loss: 1.68537e-06, min loss: 8.59424e-07\n",
      "Epoch: 1318100, elapsed: 1.04e+01, train loss: 9.29860e-07, val loss: 1.71001e-06, min loss: 8.59424e-07\n",
      "Epoch: 1318200, elapsed: 1.04e+01, train loss: 8.98660e-07, val loss: 1.67083e-06, min loss: 8.59424e-07\n",
      "Epoch: 1318300, elapsed: 1.03e+01, train loss: 9.19183e-07, val loss: 1.78023e-06, min loss: 8.59424e-07\n",
      "Epoch: 1318400, elapsed: 1.04e+01, train loss: 1.35805e-06, val loss: 2.20012e-06, min loss: 8.59424e-07\n",
      "Epoch: 1318500, elapsed: 1.06e+01, train loss: 8.74694e-07, val loss: 1.67667e-06, min loss: 8.59424e-07\n",
      "Epoch: 1318600, elapsed: 1.04e+01, train loss: 8.67079e-07, val loss: 1.66707e-06, min loss: 8.59424e-07\n",
      "Epoch: 1318700, elapsed: 1.06e+01, train loss: 8.96605e-07, val loss: 1.70780e-06, min loss: 8.59424e-07\n",
      "Epoch: 1318800, elapsed: 1.05e+01, train loss: 8.93325e-07, val loss: 1.68428e-06, min loss: 8.59424e-07\n",
      "Epoch: 1318900, elapsed: 1.05e+01, train loss: 8.62187e-07, val loss: 1.68132e-06, min loss: 8.59424e-07\n",
      "Epoch: 1319000, elapsed: 1.40e+01, train loss: 8.58866e-07, val loss: 1.66698e-06, min loss: 8.58866e-07\n",
      "Epoch: 1319100, elapsed: 1.06e+01, train loss: 8.69216e-07, val loss: 1.66463e-06, min loss: 8.58866e-07\n",
      "Epoch: 1319200, elapsed: 1.06e+01, train loss: 1.53801e-06, val loss: 2.26269e-06, min loss: 8.58866e-07\n",
      "Epoch: 1319300, elapsed: 1.05e+01, train loss: 1.59080e-06, val loss: 2.29279e-06, min loss: 8.58866e-07\n",
      "Epoch: 1319400, elapsed: 1.04e+01, train loss: 2.01429e-06, val loss: 3.27671e-06, min loss: 8.58866e-07\n",
      "Epoch: 1319500, elapsed: 1.06e+01, train loss: 9.21342e-07, val loss: 1.77200e-06, min loss: 8.58866e-07\n",
      "Epoch: 1319600, elapsed: 1.06e+01, train loss: 8.58185e-07, val loss: 1.66724e-06, min loss: 8.58185e-07\n",
      "Epoch: 1319700, elapsed: 1.05e+01, train loss: 8.62632e-07, val loss: 1.68710e-06, min loss: 8.58185e-07\n",
      "Epoch: 1319800, elapsed: 1.05e+01, train loss: 9.56206e-07, val loss: 1.73893e-06, min loss: 8.58185e-07\n",
      "Epoch: 1319900, elapsed: 1.05e+01, train loss: 8.74550e-07, val loss: 1.67311e-06, min loss: 8.58185e-07\n",
      "Epoch: 1320000, elapsed: 1.06e+01, train loss: 8.68713e-07, val loss: 1.66450e-06, min loss: 8.58185e-07\n",
      "Epoch: 1320100, elapsed: 1.23e+01, train loss: 8.62757e-07, val loss: 1.66086e-06, min loss: 8.58185e-07\n",
      "Epoch: 1320200, elapsed: 1.04e+01, train loss: 8.90875e-07, val loss: 1.67010e-06, min loss: 8.58185e-07\n",
      "Epoch: 1320300, elapsed: 1.03e+01, train loss: 8.59390e-07, val loss: 1.66074e-06, min loss: 8.58185e-07\n",
      "Epoch: 1320400, elapsed: 1.06e+01, train loss: 9.23318e-07, val loss: 1.71252e-06, min loss: 8.58185e-07\n",
      "Epoch: 1320500, elapsed: 1.03e+01, train loss: 1.36191e-06, val loss: 2.04474e-06, min loss: 8.58185e-07\n",
      "Epoch: 1320600, elapsed: 1.04e+01, train loss: 9.13336e-07, val loss: 1.70552e-06, min loss: 8.58185e-07\n",
      "Epoch: 1320700, elapsed: 1.04e+01, train loss: 1.02593e-06, val loss: 1.76484e-06, min loss: 8.58185e-07\n",
      "Epoch: 1320800, elapsed: 1.06e+01, train loss: 8.69917e-07, val loss: 1.67175e-06, min loss: 8.58185e-07\n",
      "Epoch: 1320900, elapsed: 1.05e+01, train loss: 3.06458e-06, val loss: 3.23356e-06, min loss: 8.58185e-07\n",
      "Epoch: 1321000, elapsed: 1.03e+01, train loss: 9.62605e-07, val loss: 1.81869e-06, min loss: 8.58185e-07\n",
      "Epoch: 1321100, elapsed: 1.03e+01, train loss: 8.70582e-07, val loss: 1.72820e-06, min loss: 8.58185e-07\n",
      "Epoch: 1321200, elapsed: 1.02e+01, train loss: 1.08808e-06, val loss: 2.51896e-06, min loss: 8.58185e-07\n",
      "Epoch: 1321300, elapsed: 1.05e+01, train loss: 8.57136e-07, val loss: 1.66667e-06, min loss: 8.57136e-07\n",
      "Epoch: 1321400, elapsed: 1.42e+01, train loss: 9.24451e-07, val loss: 1.72140e-06, min loss: 8.57136e-07\n",
      "Epoch: 1321500, elapsed: 1.06e+01, train loss: 8.56963e-07, val loss: 1.66658e-06, min loss: 8.56963e-07\n",
      "Epoch: 1321600, elapsed: 1.06e+01, train loss: 8.57393e-07, val loss: 1.66767e-06, min loss: 8.56963e-07\n",
      "Epoch: 1321700, elapsed: 1.05e+01, train loss: 8.72067e-07, val loss: 1.66769e-06, min loss: 8.56963e-07\n",
      "Epoch: 1321800, elapsed: 1.04e+01, train loss: 1.11191e-06, val loss: 1.95803e-06, min loss: 8.56963e-07\n",
      "Epoch: 1321900, elapsed: 1.04e+01, train loss: 9.73218e-07, val loss: 1.76153e-06, min loss: 8.56963e-07\n",
      "Epoch: 1322000, elapsed: 1.04e+01, train loss: 9.38086e-07, val loss: 1.74103e-06, min loss: 8.56963e-07\n",
      "Epoch: 1322100, elapsed: 1.05e+01, train loss: 4.06435e-06, val loss: 4.41342e-06, min loss: 8.56963e-07\n",
      "Epoch: 1322200, elapsed: 1.06e+01, train loss: 8.87027e-07, val loss: 1.67791e-06, min loss: 8.56963e-07\n",
      "Epoch: 1322300, elapsed: 1.06e+01, train loss: 8.56933e-07, val loss: 1.66456e-06, min loss: 8.56933e-07\n",
      "Epoch: 1322400, elapsed: 1.04e+01, train loss: 8.60373e-07, val loss: 1.65462e-06, min loss: 8.56933e-07\n",
      "Epoch: 1322500, elapsed: 1.05e+01, train loss: 9.07149e-07, val loss: 1.85697e-06, min loss: 8.56933e-07\n",
      "Epoch: 1322600, elapsed: 1.04e+01, train loss: 1.18558e-06, val loss: 2.12538e-06, min loss: 8.56933e-07\n",
      "Epoch: 1322700, elapsed: 1.05e+01, train loss: 3.15697e-06, val loss: 4.20713e-06, min loss: 8.56933e-07\n",
      "Epoch: 1322800, elapsed: 1.05e+01, train loss: 1.17559e-06, val loss: 1.90861e-06, min loss: 8.56933e-07\n",
      "Epoch: 1322900, elapsed: 1.05e+01, train loss: 9.80286e-07, val loss: 1.70921e-06, min loss: 8.56933e-07\n",
      "Epoch: 1323000, elapsed: 1.05e+01, train loss: 1.07630e-06, val loss: 1.81025e-06, min loss: 8.56933e-07\n",
      "Epoch: 1323100, elapsed: 1.07e+01, train loss: 2.73982e-06, val loss: 2.79977e-06, min loss: 8.56933e-07\n",
      "Epoch: 1323200, elapsed: 1.04e+01, train loss: 1.43386e-06, val loss: 1.96050e-06, min loss: 8.56933e-07\n",
      "Epoch: 1323300, elapsed: 1.05e+01, train loss: 8.58861e-07, val loss: 1.67456e-06, min loss: 8.56933e-07\n",
      "Epoch: 1323400, elapsed: 1.05e+01, train loss: 8.68546e-07, val loss: 1.68371e-06, min loss: 8.56933e-07\n",
      "Epoch: 1323500, elapsed: 1.04e+01, train loss: 8.87564e-07, val loss: 1.67049e-06, min loss: 8.56933e-07\n",
      "Epoch: 1323600, elapsed: 1.04e+01, train loss: 8.79822e-07, val loss: 1.66791e-06, min loss: 8.56933e-07\n",
      "Epoch: 1323700, elapsed: 1.07e+01, train loss: 9.05150e-07, val loss: 1.70259e-06, min loss: 8.56933e-07\n",
      "Epoch: 1323800, elapsed: 1.41e+01, train loss: 8.92457e-07, val loss: 1.65489e-06, min loss: 8.56933e-07\n",
      "Epoch: 1323900, elapsed: 1.07e+01, train loss: 8.64768e-07, val loss: 1.65472e-06, min loss: 8.56933e-07\n",
      "Epoch: 1324000, elapsed: 1.06e+01, train loss: 8.97561e-07, val loss: 1.71436e-06, min loss: 8.56933e-07\n",
      "Epoch: 1324100, elapsed: 1.06e+01, train loss: 8.77707e-07, val loss: 1.69106e-06, min loss: 8.56933e-07\n",
      "Epoch: 1324200, elapsed: 1.07e+01, train loss: 1.01124e-06, val loss: 1.87917e-06, min loss: 8.56933e-07\n",
      "Epoch: 1324300, elapsed: 1.05e+01, train loss: 1.68250e-06, val loss: 2.11399e-06, min loss: 8.56933e-07\n",
      "Epoch: 1324400, elapsed: 1.05e+01, train loss: 9.11315e-07, val loss: 1.77787e-06, min loss: 8.56933e-07\n",
      "Epoch: 1324500, elapsed: 1.05e+01, train loss: 8.59806e-07, val loss: 1.66744e-06, min loss: 8.56933e-07\n",
      "Epoch: 1324600, elapsed: 1.07e+01, train loss: 9.51132e-07, val loss: 1.72451e-06, min loss: 8.56933e-07\n",
      "Epoch: 1324700, elapsed: 1.03e+01, train loss: 1.09879e-06, val loss: 1.99939e-06, min loss: 8.56933e-07\n",
      "Epoch: 1324800, elapsed: 1.05e+01, train loss: 8.55521e-07, val loss: 1.65727e-06, min loss: 8.55521e-07\n",
      "Epoch: 1324900, elapsed: 1.06e+01, train loss: 8.59329e-07, val loss: 1.66834e-06, min loss: 8.55521e-07\n",
      "Epoch: 1325000, elapsed: 1.07e+01, train loss: 1.70798e-06, val loss: 2.62158e-06, min loss: 8.55521e-07\n",
      "Epoch: 1325100, elapsed: 1.29e+01, train loss: 8.59574e-07, val loss: 1.66262e-06, min loss: 8.55521e-07\n",
      "Epoch: 1325200, elapsed: 1.09e+01, train loss: 8.94552e-07, val loss: 1.68445e-06, min loss: 8.55521e-07\n",
      "Epoch: 1325300, elapsed: 1.08e+01, train loss: 1.08535e-06, val loss: 2.26426e-06, min loss: 8.55521e-07\n",
      "Epoch: 1325400, elapsed: 1.08e+01, train loss: 8.54530e-07, val loss: 1.65864e-06, min loss: 8.54530e-07\n",
      "Epoch: 1325500, elapsed: 1.08e+01, train loss: 8.72116e-07, val loss: 1.65690e-06, min loss: 8.54530e-07\n",
      "Epoch: 1325600, elapsed: 1.10e+01, train loss: 4.48968e-06, val loss: 4.20026e-06, min loss: 8.54530e-07\n",
      "Epoch: 1325700, elapsed: 1.09e+01, train loss: 8.54561e-07, val loss: 1.65581e-06, min loss: 8.54530e-07\n",
      "Epoch: 1325800, elapsed: 1.09e+01, train loss: 8.58432e-07, val loss: 1.65233e-06, min loss: 8.54530e-07\n",
      "Epoch: 1325900, elapsed: 1.09e+01, train loss: 1.00517e-06, val loss: 1.66176e-06, min loss: 8.54530e-07\n",
      "Epoch: 1326000, elapsed: 1.07e+01, train loss: 1.82487e-06, val loss: 2.57913e-06, min loss: 8.54530e-07\n",
      "Epoch: 1326100, elapsed: 1.08e+01, train loss: 9.15317e-07, val loss: 1.70099e-06, min loss: 8.54530e-07\n",
      "Epoch: 1326200, elapsed: 1.45e+01, train loss: 8.55073e-07, val loss: 1.66314e-06, min loss: 8.54530e-07\n",
      "Epoch: 1326300, elapsed: 1.10e+01, train loss: 8.59689e-07, val loss: 1.64671e-06, min loss: 8.54530e-07\n",
      "Epoch: 1326400, elapsed: 1.10e+01, train loss: 8.98602e-07, val loss: 1.68824e-06, min loss: 8.54530e-07\n",
      "Epoch: 1326500, elapsed: 1.08e+01, train loss: 9.32593e-07, val loss: 1.73340e-06, min loss: 8.54530e-07\n",
      "Epoch: 1326600, elapsed: 1.10e+01, train loss: 8.54532e-07, val loss: 1.66062e-06, min loss: 8.54530e-07\n",
      "Epoch: 1326700, elapsed: 1.10e+01, train loss: 1.02452e-06, val loss: 1.75381e-06, min loss: 8.54530e-07\n",
      "Epoch: 1326800, elapsed: 1.10e+01, train loss: 9.87910e-07, val loss: 1.82469e-06, min loss: 8.54530e-07\n",
      "Epoch: 1326900, elapsed: 1.09e+01, train loss: 1.52136e-06, val loss: 2.04832e-06, min loss: 8.54530e-07\n",
      "Epoch: 1327000, elapsed: 1.09e+01, train loss: 1.39727e-06, val loss: 2.42786e-06, min loss: 8.54530e-07\n",
      "Epoch: 1327100, elapsed: 1.09e+01, train loss: 1.33495e-06, val loss: 2.25067e-06, min loss: 8.54530e-07\n",
      "Epoch: 1327200, elapsed: 1.10e+01, train loss: 1.74794e-06, val loss: 2.76976e-06, min loss: 8.54530e-07\n",
      "Epoch: 1327300, elapsed: 1.10e+01, train loss: 1.26246e-06, val loss: 2.27897e-06, min loss: 8.54530e-07\n",
      "Epoch: 1327400, elapsed: 1.10e+01, train loss: 9.80967e-07, val loss: 1.81206e-06, min loss: 8.54530e-07\n",
      "Epoch: 1327500, elapsed: 1.10e+01, train loss: 1.05435e-06, val loss: 1.86622e-06, min loss: 8.54530e-07\n",
      "Epoch: 1327600, elapsed: 1.10e+01, train loss: 8.53648e-07, val loss: 1.65480e-06, min loss: 8.53648e-07\n",
      "Epoch: 1327700, elapsed: 1.07e+01, train loss: 9.22339e-07, val loss: 1.66866e-06, min loss: 8.53648e-07\n",
      "Epoch: 1327800, elapsed: 1.09e+01, train loss: 8.85946e-07, val loss: 1.65443e-06, min loss: 8.53648e-07\n",
      "Epoch: 1327900, elapsed: 1.09e+01, train loss: 8.95538e-07, val loss: 1.71497e-06, min loss: 8.53648e-07\n",
      "Epoch: 1328000, elapsed: 1.09e+01, train loss: 3.78089e-06, val loss: 3.90698e-06, min loss: 8.53648e-07\n",
      "Epoch: 1328100, elapsed: 1.08e+01, train loss: 1.03555e-06, val loss: 1.83849e-06, min loss: 8.53648e-07\n",
      "Epoch: 1328200, elapsed: 1.10e+01, train loss: 8.67969e-07, val loss: 1.68582e-06, min loss: 8.53648e-07\n",
      "Epoch: 1328300, elapsed: 1.08e+01, train loss: 8.53932e-07, val loss: 1.65599e-06, min loss: 8.53648e-07\n",
      "Epoch: 1328400, elapsed: 1.08e+01, train loss: 8.56523e-07, val loss: 1.65087e-06, min loss: 8.53648e-07\n",
      "Epoch: 1328500, elapsed: 1.08e+01, train loss: 1.08175e-06, val loss: 1.81054e-06, min loss: 8.53648e-07\n",
      "Epoch: 1328600, elapsed: 1.46e+01, train loss: 1.17390e-06, val loss: 1.93979e-06, min loss: 8.53648e-07\n",
      "Epoch: 1328700, elapsed: 1.11e+01, train loss: 8.71418e-07, val loss: 1.71069e-06, min loss: 8.53648e-07\n",
      "Epoch: 1328800, elapsed: 1.11e+01, train loss: 1.12524e-06, val loss: 1.92854e-06, min loss: 8.53648e-07\n",
      "Epoch: 1328900, elapsed: 1.10e+01, train loss: 8.60767e-07, val loss: 1.67699e-06, min loss: 8.53648e-07\n",
      "Epoch: 1329000, elapsed: 1.10e+01, train loss: 8.75040e-07, val loss: 1.68195e-06, min loss: 8.53648e-07\n",
      "Epoch: 1329100, elapsed: 1.10e+01, train loss: 9.88904e-07, val loss: 1.78361e-06, min loss: 8.53648e-07\n",
      "Epoch: 1329200, elapsed: 1.12e+01, train loss: 1.06073e-06, val loss: 1.81692e-06, min loss: 8.53648e-07\n",
      "Epoch: 1329300, elapsed: 1.10e+01, train loss: 1.33140e-06, val loss: 2.15408e-06, min loss: 8.53648e-07\n",
      "Epoch: 1329400, elapsed: 1.08e+01, train loss: 8.91277e-07, val loss: 1.67209e-06, min loss: 8.53648e-07\n",
      "Epoch: 1329500, elapsed: 1.09e+01, train loss: 9.23879e-07, val loss: 1.73565e-06, min loss: 8.53648e-07\n",
      "Epoch: 1329600, elapsed: 1.09e+01, train loss: 9.03135e-07, val loss: 1.65545e-06, min loss: 8.53648e-07\n",
      "Epoch: 1329700, elapsed: 1.09e+01, train loss: 8.70156e-07, val loss: 1.65246e-06, min loss: 8.53648e-07\n",
      "Epoch: 1329800, elapsed: 1.10e+01, train loss: 8.57084e-07, val loss: 1.65979e-06, min loss: 8.53648e-07\n",
      "Epoch: 1329900, elapsed: 1.09e+01, train loss: 8.58838e-07, val loss: 1.64723e-06, min loss: 8.53648e-07\n",
      "Epoch: 1330000, elapsed: 1.08e+01, train loss: 8.69717e-07, val loss: 1.67273e-06, min loss: 8.53648e-07\n",
      "Epoch: 1330100, elapsed: 1.31e+01, train loss: 1.67614e-06, val loss: 2.84857e-06, min loss: 8.53648e-07\n",
      "Epoch: 1330200, elapsed: 1.09e+01, train loss: 8.69349e-07, val loss: 1.68263e-06, min loss: 8.53648e-07\n",
      "Epoch: 1330300, elapsed: 1.09e+01, train loss: 9.61394e-07, val loss: 1.71904e-06, min loss: 8.53648e-07\n",
      "Epoch: 1330400, elapsed: 1.08e+01, train loss: 1.85108e-06, val loss: 3.10087e-06, min loss: 8.53648e-07\n",
      "Epoch: 1330500, elapsed: 1.08e+01, train loss: 8.51477e-07, val loss: 1.65370e-06, min loss: 8.51477e-07\n",
      "Epoch: 1330600, elapsed: 1.09e+01, train loss: 8.60111e-07, val loss: 1.64099e-06, min loss: 8.51477e-07\n",
      "Epoch: 1330700, elapsed: 1.09e+01, train loss: 9.85178e-07, val loss: 1.71978e-06, min loss: 8.51477e-07\n",
      "Epoch: 1330800, elapsed: 1.09e+01, train loss: 8.51608e-07, val loss: 1.65282e-06, min loss: 8.51477e-07\n",
      "Epoch: 1330900, elapsed: 1.06e+01, train loss: 8.56772e-07, val loss: 1.64402e-06, min loss: 8.51477e-07\n",
      "Epoch: 1331000, elapsed: 1.45e+01, train loss: 8.77933e-07, val loss: 1.66581e-06, min loss: 8.51477e-07\n",
      "Epoch: 1331100, elapsed: 1.13e+01, train loss: 9.12355e-07, val loss: 1.67926e-06, min loss: 8.51477e-07\n",
      "Epoch: 1331200, elapsed: 1.10e+01, train loss: 9.18577e-07, val loss: 1.80639e-06, min loss: 8.51477e-07\n",
      "Epoch: 1331300, elapsed: 1.10e+01, train loss: 9.30588e-07, val loss: 1.78778e-06, min loss: 8.51477e-07\n",
      "Epoch: 1331400, elapsed: 1.09e+01, train loss: 8.51983e-07, val loss: 1.65322e-06, min loss: 8.51477e-07\n",
      "Epoch: 1331500, elapsed: 1.09e+01, train loss: 9.35551e-07, val loss: 1.66595e-06, min loss: 8.51477e-07\n",
      "Epoch: 1331600, elapsed: 1.09e+01, train loss: 1.02771e-06, val loss: 1.79286e-06, min loss: 8.51477e-07\n",
      "Epoch: 1331700, elapsed: 1.09e+01, train loss: 1.00273e-06, val loss: 1.77314e-06, min loss: 8.51477e-07\n",
      "Epoch: 1331800, elapsed: 1.08e+01, train loss: 2.98005e-06, val loss: 2.83722e-06, min loss: 8.51477e-07\n",
      "Epoch: 1331900, elapsed: 1.07e+01, train loss: 1.13889e-06, val loss: 2.12560e-06, min loss: 8.51477e-07\n",
      "Epoch: 1332000, elapsed: 1.06e+01, train loss: 8.51103e-07, val loss: 1.64781e-06, min loss: 8.51103e-07\n",
      "Epoch: 1332100, elapsed: 1.04e+01, train loss: 8.73033e-07, val loss: 1.64446e-06, min loss: 8.51103e-07\n",
      "Epoch: 1332200, elapsed: 1.04e+01, train loss: 8.68141e-07, val loss: 1.65622e-06, min loss: 8.51103e-07\n",
      "Epoch: 1332300, elapsed: 1.05e+01, train loss: 8.62147e-07, val loss: 1.66072e-06, min loss: 8.51103e-07\n",
      "Epoch: 1332400, elapsed: 1.04e+01, train loss: 9.07030e-07, val loss: 1.72453e-06, min loss: 8.51103e-07\n",
      "Epoch: 1332500, elapsed: 1.06e+01, train loss: 1.38627e-06, val loss: 1.95387e-06, min loss: 8.51103e-07\n",
      "Epoch: 1332600, elapsed: 1.04e+01, train loss: 2.84457e-06, val loss: 3.28178e-06, min loss: 8.51103e-07\n",
      "Epoch: 1332700, elapsed: 1.04e+01, train loss: 1.63167e-06, val loss: 1.69589e-06, min loss: 8.51103e-07\n",
      "Epoch: 1332800, elapsed: 1.04e+01, train loss: 9.19013e-07, val loss: 1.71252e-06, min loss: 8.51103e-07\n",
      "Epoch: 1332900, elapsed: 1.05e+01, train loss: 8.57970e-07, val loss: 1.65385e-06, min loss: 8.51103e-07\n",
      "Epoch: 1333000, elapsed: 1.02e+01, train loss: 9.58877e-07, val loss: 1.83585e-06, min loss: 8.51103e-07\n",
      "Epoch: 1333100, elapsed: 1.03e+01, train loss: 9.51921e-07, val loss: 1.70334e-06, min loss: 8.51103e-07\n",
      "Epoch: 1333200, elapsed: 1.03e+01, train loss: 2.16280e-06, val loss: 2.83615e-06, min loss: 8.51103e-07\n",
      "Epoch: 1333300, elapsed: 1.04e+01, train loss: 8.51723e-07, val loss: 1.64482e-06, min loss: 8.51103e-07\n",
      "Epoch: 1333400, elapsed: 1.03e+01, train loss: 8.51348e-07, val loss: 1.65210e-06, min loss: 8.51103e-07\n",
      "Epoch: 1333500, elapsed: 1.41e+01, train loss: 9.22735e-07, val loss: 1.77893e-06, min loss: 8.51103e-07\n",
      "Epoch: 1333600, elapsed: 1.05e+01, train loss: 1.72389e-06, val loss: 2.48632e-06, min loss: 8.51103e-07\n",
      "Epoch: 1333700, elapsed: 1.05e+01, train loss: 8.60266e-07, val loss: 1.65242e-06, min loss: 8.51103e-07\n",
      "Epoch: 1333800, elapsed: 1.06e+01, train loss: 8.70409e-07, val loss: 1.67164e-06, min loss: 8.51103e-07\n",
      "Epoch: 1333900, elapsed: 1.05e+01, train loss: 9.78615e-07, val loss: 1.87474e-06, min loss: 8.51103e-07\n",
      "Epoch: 1334000, elapsed: 1.04e+01, train loss: 8.50011e-07, val loss: 1.64340e-06, min loss: 8.50011e-07\n",
      "Epoch: 1334100, elapsed: 1.07e+01, train loss: 8.49931e-07, val loss: 1.65189e-06, min loss: 8.49931e-07\n",
      "Epoch: 1334200, elapsed: 1.05e+01, train loss: 1.32741e-06, val loss: 2.14561e-06, min loss: 8.49931e-07\n",
      "Epoch: 1334300, elapsed: 1.05e+01, train loss: 1.53636e-06, val loss: 2.67183e-06, min loss: 8.49931e-07\n",
      "Epoch: 1334400, elapsed: 1.06e+01, train loss: 1.13174e-06, val loss: 1.91293e-06, min loss: 8.49931e-07\n",
      "Epoch: 1334500, elapsed: 1.04e+01, train loss: 9.14596e-07, val loss: 1.76665e-06, min loss: 8.49931e-07\n",
      "Epoch: 1334600, elapsed: 1.05e+01, train loss: 1.01960e-06, val loss: 1.94296e-06, min loss: 8.49931e-07\n",
      "Epoch: 1334700, elapsed: 1.05e+01, train loss: 1.42177e-06, val loss: 2.46858e-06, min loss: 8.49931e-07\n",
      "Epoch: 1334800, elapsed: 1.05e+01, train loss: 2.30015e-06, val loss: 3.53516e-06, min loss: 8.49931e-07\n",
      "Epoch: 1334900, elapsed: 1.05e+01, train loss: 1.89895e-06, val loss: 2.34354e-06, min loss: 8.49931e-07\n",
      "Epoch: 1335000, elapsed: 1.05e+01, train loss: 8.93158e-07, val loss: 1.65039e-06, min loss: 8.49931e-07\n",
      "Epoch: 1335100, elapsed: 1.23e+01, train loss: 1.23657e-06, val loss: 1.87634e-06, min loss: 8.49931e-07\n",
      "Epoch: 1335200, elapsed: 1.07e+01, train loss: 2.58905e-06, val loss: 4.01891e-06, min loss: 8.49931e-07\n",
      "Epoch: 1335300, elapsed: 1.07e+01, train loss: 8.52348e-07, val loss: 1.64595e-06, min loss: 8.49931e-07\n",
      "Epoch: 1335400, elapsed: 1.05e+01, train loss: 8.50587e-07, val loss: 1.65492e-06, min loss: 8.49931e-07\n",
      "Epoch: 1335500, elapsed: 1.08e+01, train loss: 8.89084e-07, val loss: 1.66316e-06, min loss: 8.49931e-07\n",
      "Epoch: 1335600, elapsed: 1.07e+01, train loss: 8.49238e-07, val loss: 1.64315e-06, min loss: 8.49238e-07\n",
      "Epoch: 1335700, elapsed: 1.05e+01, train loss: 8.53284e-07, val loss: 1.64893e-06, min loss: 8.49238e-07\n",
      "Epoch: 1335800, elapsed: 1.08e+01, train loss: 2.90340e-06, val loss: 3.54871e-06, min loss: 8.49238e-07\n",
      "Epoch: 1335900, elapsed: 1.45e+01, train loss: 8.48269e-07, val loss: 1.64241e-06, min loss: 8.48269e-07\n",
      "Epoch: 1336000, elapsed: 1.07e+01, train loss: 1.53205e-06, val loss: 2.24135e-06, min loss: 8.48269e-07\n",
      "Epoch: 1336100, elapsed: 1.07e+01, train loss: 8.48162e-07, val loss: 1.64385e-06, min loss: 8.48162e-07\n",
      "Epoch: 1336200, elapsed: 1.07e+01, train loss: 9.08529e-07, val loss: 1.65465e-06, min loss: 8.48162e-07\n",
      "Epoch: 1336300, elapsed: 1.06e+01, train loss: 8.49717e-07, val loss: 1.64006e-06, min loss: 8.48162e-07\n",
      "Epoch: 1336400, elapsed: 1.09e+01, train loss: 8.49215e-07, val loss: 1.64473e-06, min loss: 8.48162e-07\n",
      "Epoch: 1336500, elapsed: 1.09e+01, train loss: 8.48156e-07, val loss: 1.63459e-06, min loss: 8.48156e-07\n",
      "Epoch: 1336600, elapsed: 1.06e+01, train loss: 8.72859e-07, val loss: 1.65106e-06, min loss: 8.48156e-07\n",
      "Epoch: 1336700, elapsed: 1.05e+01, train loss: 1.04696e-06, val loss: 1.82377e-06, min loss: 8.48156e-07\n",
      "Epoch: 1336800, elapsed: 1.06e+01, train loss: 1.06526e-06, val loss: 1.84200e-06, min loss: 8.48156e-07\n",
      "Epoch: 1336900, elapsed: 1.06e+01, train loss: 2.63356e-06, val loss: 3.55515e-06, min loss: 8.48156e-07\n",
      "Epoch: 1337000, elapsed: 1.06e+01, train loss: 1.17078e-06, val loss: 1.86677e-06, min loss: 8.48156e-07\n",
      "Epoch: 1337100, elapsed: 1.09e+01, train loss: 8.86587e-07, val loss: 1.69856e-06, min loss: 8.48156e-07\n",
      "Epoch: 1337200, elapsed: 1.06e+01, train loss: 1.73459e-06, val loss: 2.97036e-06, min loss: 8.48156e-07\n",
      "Epoch: 1337300, elapsed: 1.09e+01, train loss: 8.47290e-07, val loss: 1.63996e-06, min loss: 8.47290e-07\n",
      "Epoch: 1337400, elapsed: 1.07e+01, train loss: 9.07654e-07, val loss: 1.71206e-06, min loss: 8.47290e-07\n",
      "Epoch: 1337500, elapsed: 1.11e+01, train loss: 1.32152e-06, val loss: 1.86504e-06, min loss: 8.47290e-07\n",
      "Epoch: 1337600, elapsed: 1.10e+01, train loss: 1.01807e-06, val loss: 1.77146e-06, min loss: 8.47290e-07\n",
      "Epoch: 1337700, elapsed: 1.08e+01, train loss: 1.08852e-06, val loss: 1.77654e-06, min loss: 8.47290e-07\n",
      "Epoch: 1337800, elapsed: 1.08e+01, train loss: 8.50763e-07, val loss: 1.63487e-06, min loss: 8.47290e-07\n",
      "Epoch: 1337900, elapsed: 1.09e+01, train loss: 1.00082e-06, val loss: 1.71216e-06, min loss: 8.47290e-07\n",
      "Epoch: 1338000, elapsed: 1.11e+01, train loss: 1.57030e-06, val loss: 2.23679e-06, min loss: 8.47290e-07\n",
      "Epoch: 1338100, elapsed: 1.09e+01, train loss: 1.91545e-06, val loss: 2.79774e-06, min loss: 8.47290e-07\n",
      "Epoch: 1338200, elapsed: 1.08e+01, train loss: 1.37989e-06, val loss: 2.42574e-06, min loss: 8.47290e-07\n",
      "Epoch: 1338300, elapsed: 1.43e+01, train loss: 8.62108e-07, val loss: 1.65270e-06, min loss: 8.47290e-07\n",
      "Epoch: 1338400, elapsed: 1.11e+01, train loss: 8.46484e-07, val loss: 1.63919e-06, min loss: 8.46484e-07\n",
      "Epoch: 1338500, elapsed: 1.11e+01, train loss: 8.75815e-07, val loss: 1.66882e-06, min loss: 8.46484e-07\n",
      "Epoch: 1338600, elapsed: 1.11e+01, train loss: 6.25257e-06, val loss: 5.67973e-06, min loss: 8.46484e-07\n",
      "Epoch: 1338700, elapsed: 1.10e+01, train loss: 8.46401e-07, val loss: 1.63818e-06, min loss: 8.46401e-07\n",
      "Epoch: 1338800, elapsed: 1.11e+01, train loss: 9.81875e-07, val loss: 1.65416e-06, min loss: 8.46401e-07\n",
      "Epoch: 1338900, elapsed: 1.12e+01, train loss: 8.46388e-07, val loss: 1.63690e-06, min loss: 8.46388e-07\n",
      "Epoch: 1339000, elapsed: 1.10e+01, train loss: 8.46864e-07, val loss: 1.64079e-06, min loss: 8.46388e-07\n",
      "Epoch: 1339100, elapsed: 1.10e+01, train loss: 9.33627e-07, val loss: 1.72497e-06, min loss: 8.46388e-07\n",
      "Epoch: 1339200, elapsed: 1.09e+01, train loss: 1.81407e-06, val loss: 2.43797e-06, min loss: 8.46388e-07\n",
      "Epoch: 1339300, elapsed: 1.09e+01, train loss: 1.10183e-06, val loss: 2.06553e-06, min loss: 8.46388e-07\n",
      "Epoch: 1339400, elapsed: 1.11e+01, train loss: 9.60792e-07, val loss: 1.75396e-06, min loss: 8.46388e-07\n",
      "Epoch: 1339500, elapsed: 1.11e+01, train loss: 8.51423e-07, val loss: 1.64158e-06, min loss: 8.46388e-07\n",
      "Epoch: 1339600, elapsed: 1.09e+01, train loss: 9.08553e-07, val loss: 1.72964e-06, min loss: 8.46388e-07\n",
      "Epoch: 1339700, elapsed: 1.10e+01, train loss: 1.89852e-06, val loss: 2.28497e-06, min loss: 8.46388e-07\n",
      "Epoch: 1339800, elapsed: 1.09e+01, train loss: 1.25620e-06, val loss: 2.05468e-06, min loss: 8.46388e-07\n",
      "Epoch: 1339900, elapsed: 1.10e+01, train loss: 7.22291e-06, val loss: 6.67016e-06, min loss: 8.46388e-07\n",
      "Epoch: 1340000, elapsed: 1.09e+01, train loss: 8.84131e-07, val loss: 1.65852e-06, min loss: 8.46388e-07\n",
      "Epoch: 1340100, elapsed: 1.29e+01, train loss: 1.28837e-06, val loss: 1.88815e-06, min loss: 8.46388e-07\n",
      "Epoch: 1340200, elapsed: 1.08e+01, train loss: 1.37988e-06, val loss: 2.19761e-06, min loss: 8.46388e-07\n",
      "Epoch: 1340300, elapsed: 1.07e+01, train loss: 8.52344e-07, val loss: 1.64185e-06, min loss: 8.46388e-07\n",
      "Epoch: 1340400, elapsed: 1.09e+01, train loss: 8.57779e-07, val loss: 1.65040e-06, min loss: 8.46388e-07\n",
      "Epoch: 1340500, elapsed: 1.10e+01, train loss: 1.00517e-06, val loss: 1.90077e-06, min loss: 8.46388e-07\n",
      "Epoch: 1340600, elapsed: 1.08e+01, train loss: 1.13926e-06, val loss: 1.82665e-06, min loss: 8.46388e-07\n",
      "Epoch: 1340700, elapsed: 1.08e+01, train loss: 1.12546e-06, val loss: 1.82566e-06, min loss: 8.46388e-07\n",
      "Epoch: 1340800, elapsed: 1.48e+01, train loss: 1.55333e-06, val loss: 2.30355e-06, min loss: 8.46388e-07\n",
      "Epoch: 1340900, elapsed: 1.10e+01, train loss: 1.93957e-06, val loss: 2.50781e-06, min loss: 8.46388e-07\n",
      "Epoch: 1341000, elapsed: 1.12e+01, train loss: 1.27125e-06, val loss: 1.80523e-06, min loss: 8.46388e-07\n",
      "Epoch: 1341100, elapsed: 1.13e+01, train loss: 8.88967e-07, val loss: 1.75480e-06, min loss: 8.46388e-07\n",
      "Epoch: 1341200, elapsed: 1.11e+01, train loss: 9.29715e-07, val loss: 1.71371e-06, min loss: 8.46388e-07\n",
      "Epoch: 1341300, elapsed: 1.12e+01, train loss: 1.01687e-06, val loss: 1.77804e-06, min loss: 8.46388e-07\n",
      "Epoch: 1341400, elapsed: 1.11e+01, train loss: 2.34502e-06, val loss: 3.35568e-06, min loss: 8.46388e-07\n",
      "Epoch: 1341500, elapsed: 1.11e+01, train loss: 1.00817e-06, val loss: 1.90045e-06, min loss: 8.46388e-07\n",
      "Epoch: 1341600, elapsed: 1.09e+01, train loss: 8.45619e-07, val loss: 1.63182e-06, min loss: 8.45619e-07\n",
      "Epoch: 1341700, elapsed: 1.09e+01, train loss: 8.77875e-07, val loss: 1.69189e-06, min loss: 8.45619e-07\n",
      "Epoch: 1341800, elapsed: 1.10e+01, train loss: 9.71973e-07, val loss: 1.86014e-06, min loss: 8.45619e-07\n",
      "Epoch: 1341900, elapsed: 1.10e+01, train loss: 8.73723e-07, val loss: 1.68495e-06, min loss: 8.45619e-07\n",
      "Epoch: 1342000, elapsed: 1.10e+01, train loss: 9.00515e-07, val loss: 1.78499e-06, min loss: 8.45619e-07\n",
      "Epoch: 1342100, elapsed: 1.10e+01, train loss: 9.15331e-07, val loss: 1.65785e-06, min loss: 8.45619e-07\n",
      "Epoch: 1342200, elapsed: 1.11e+01, train loss: 1.80632e-06, val loss: 2.48426e-06, min loss: 8.45619e-07\n",
      "Epoch: 1342300, elapsed: 1.10e+01, train loss: 1.06760e-06, val loss: 1.66735e-06, min loss: 8.45619e-07\n",
      "Epoch: 1342400, elapsed: 1.09e+01, train loss: 9.62905e-07, val loss: 1.64870e-06, min loss: 8.45619e-07\n",
      "Epoch: 1342500, elapsed: 1.11e+01, train loss: 8.83462e-07, val loss: 1.68155e-06, min loss: 8.45619e-07\n",
      "Epoch: 1342600, elapsed: 1.07e+01, train loss: 1.15839e-06, val loss: 1.83763e-06, min loss: 8.45619e-07\n",
      "Epoch: 1342700, elapsed: 1.09e+01, train loss: 8.76810e-07, val loss: 1.68449e-06, min loss: 8.45619e-07\n",
      "Epoch: 1342800, elapsed: 1.12e+01, train loss: 8.49431e-07, val loss: 1.65452e-06, min loss: 8.45619e-07\n",
      "Epoch: 1342900, elapsed: 1.11e+01, train loss: 8.75851e-07, val loss: 1.66644e-06, min loss: 8.45619e-07\n",
      "Epoch: 1343000, elapsed: 1.11e+01, train loss: 1.06519e-06, val loss: 1.79882e-06, min loss: 8.45619e-07\n",
      "Epoch: 1343100, elapsed: 1.08e+01, train loss: 1.06228e-06, val loss: 1.69083e-06, min loss: 8.45619e-07\n",
      "Epoch: 1343200, elapsed: 1.47e+01, train loss: 1.54709e-06, val loss: 2.20707e-06, min loss: 8.45619e-07\n",
      "Epoch: 1343300, elapsed: 1.12e+01, train loss: 1.09493e-06, val loss: 1.65241e-06, min loss: 8.45619e-07\n",
      "Epoch: 1343400, elapsed: 1.10e+01, train loss: 9.58927e-07, val loss: 1.75050e-06, min loss: 8.45619e-07\n",
      "Epoch: 1343500, elapsed: 1.09e+01, train loss: 8.71610e-07, val loss: 1.67916e-06, min loss: 8.45619e-07\n",
      "Epoch: 1343600, elapsed: 1.08e+01, train loss: 1.07068e-06, val loss: 1.71513e-06, min loss: 8.45619e-07\n",
      "Epoch: 1343700, elapsed: 1.10e+01, train loss: 8.48008e-07, val loss: 1.64909e-06, min loss: 8.45619e-07\n",
      "Epoch: 1343800, elapsed: 1.10e+01, train loss: 8.45507e-07, val loss: 1.62710e-06, min loss: 8.45507e-07\n",
      "Epoch: 1343900, elapsed: 1.12e+01, train loss: 1.27884e-06, val loss: 1.90342e-06, min loss: 8.45507e-07\n",
      "Epoch: 1344000, elapsed: 1.09e+01, train loss: 1.03451e-06, val loss: 1.82450e-06, min loss: 8.45507e-07\n",
      "Epoch: 1344100, elapsed: 1.06e+01, train loss: 2.33562e-06, val loss: 3.23342e-06, min loss: 8.45507e-07\n",
      "Epoch: 1344200, elapsed: 1.07e+01, train loss: 1.51388e-06, val loss: 2.33242e-06, min loss: 8.45507e-07\n",
      "Epoch: 1344300, elapsed: 1.07e+01, train loss: 8.43304e-07, val loss: 1.62688e-06, min loss: 8.43304e-07\n",
      "Epoch: 1344400, elapsed: 1.03e+01, train loss: 8.51196e-07, val loss: 1.61861e-06, min loss: 8.43304e-07\n",
      "Epoch: 1344500, elapsed: 1.09e+01, train loss: 8.85291e-07, val loss: 1.65834e-06, min loss: 8.43304e-07\n",
      "Epoch: 1344600, elapsed: 1.06e+01, train loss: 8.46344e-07, val loss: 1.62178e-06, min loss: 8.43304e-07\n",
      "Epoch: 1344700, elapsed: 1.09e+01, train loss: 8.44124e-07, val loss: 1.62888e-06, min loss: 8.43304e-07\n",
      "Epoch: 1344800, elapsed: 1.08e+01, train loss: 8.61129e-07, val loss: 1.62607e-06, min loss: 8.43304e-07\n",
      "Epoch: 1344900, elapsed: 1.10e+01, train loss: 8.48698e-07, val loss: 1.63715e-06, min loss: 8.43304e-07\n",
      "Epoch: 1345000, elapsed: 1.09e+01, train loss: 1.09404e-06, val loss: 1.98449e-06, min loss: 8.43304e-07\n",
      "Epoch: 1345100, elapsed: 1.30e+01, train loss: 1.31217e-06, val loss: 1.89509e-06, min loss: 8.43304e-07\n",
      "Epoch: 1345200, elapsed: 1.09e+01, train loss: 1.10765e-06, val loss: 1.84124e-06, min loss: 8.43304e-07\n",
      "Epoch: 1345300, elapsed: 1.08e+01, train loss: 8.68014e-07, val loss: 1.64230e-06, min loss: 8.43304e-07\n",
      "Epoch: 1345400, elapsed: 1.07e+01, train loss: 8.56987e-07, val loss: 1.65068e-06, min loss: 8.43304e-07\n",
      "Epoch: 1345500, elapsed: 1.08e+01, train loss: 1.84051e-06, val loss: 2.66577e-06, min loss: 8.43304e-07\n",
      "Epoch: 1345600, elapsed: 1.09e+01, train loss: 3.35437e-06, val loss: 4.11919e-06, min loss: 8.43304e-07\n",
      "Epoch: 1345700, elapsed: 1.47e+01, train loss: 2.16351e-06, val loss: 2.80176e-06, min loss: 8.43304e-07\n",
      "Epoch: 1345800, elapsed: 1.12e+01, train loss: 1.63960e-06, val loss: 2.79900e-06, min loss: 8.43304e-07\n",
      "Epoch: 1345900, elapsed: 1.11e+01, train loss: 9.53626e-07, val loss: 1.78646e-06, min loss: 8.43304e-07\n",
      "Epoch: 1346000, elapsed: 1.09e+01, train loss: 8.46824e-07, val loss: 1.64697e-06, min loss: 8.43304e-07\n",
      "Epoch: 1346100, elapsed: 1.11e+01, train loss: 8.45973e-07, val loss: 1.63857e-06, min loss: 8.43304e-07\n",
      "Epoch: 1346200, elapsed: 1.11e+01, train loss: 8.57158e-07, val loss: 1.65733e-06, min loss: 8.43304e-07\n",
      "Epoch: 1346300, elapsed: 1.11e+01, train loss: 1.04350e-06, val loss: 1.66460e-06, min loss: 8.43304e-07\n",
      "Epoch: 1346400, elapsed: 1.11e+01, train loss: 9.04902e-07, val loss: 1.70804e-06, min loss: 8.43304e-07\n",
      "Epoch: 1346500, elapsed: 1.11e+01, train loss: 8.88632e-07, val loss: 1.68723e-06, min loss: 8.43304e-07\n",
      "Epoch: 1346600, elapsed: 1.10e+01, train loss: 1.00566e-06, val loss: 1.78212e-06, min loss: 8.43304e-07\n",
      "Epoch: 1346700, elapsed: 1.12e+01, train loss: 8.42354e-07, val loss: 1.63311e-06, min loss: 8.42354e-07\n",
      "Epoch: 1346800, elapsed: 1.12e+01, train loss: 8.43592e-07, val loss: 1.62483e-06, min loss: 8.42354e-07\n",
      "Epoch: 1346900, elapsed: 1.10e+01, train loss: 8.51399e-07, val loss: 1.62651e-06, min loss: 8.42354e-07\n",
      "Epoch: 1347000, elapsed: 1.10e+01, train loss: 9.59098e-07, val loss: 1.66808e-06, min loss: 8.42354e-07\n",
      "Epoch: 1347100, elapsed: 1.09e+01, train loss: 9.77904e-07, val loss: 1.72247e-06, min loss: 8.42354e-07\n",
      "Epoch: 1347200, elapsed: 1.10e+01, train loss: 8.42252e-07, val loss: 1.63142e-06, min loss: 8.42252e-07\n",
      "Epoch: 1347300, elapsed: 1.11e+01, train loss: 8.42136e-07, val loss: 1.62814e-06, min loss: 8.42136e-07\n",
      "Epoch: 1347400, elapsed: 1.11e+01, train loss: 8.55109e-07, val loss: 1.61877e-06, min loss: 8.42136e-07\n",
      "Epoch: 1347500, elapsed: 1.09e+01, train loss: 9.17349e-07, val loss: 1.69748e-06, min loss: 8.42136e-07\n",
      "Epoch: 1347600, elapsed: 1.10e+01, train loss: 1.76879e-06, val loss: 2.49828e-06, min loss: 8.42136e-07\n",
      "Epoch: 1347700, elapsed: 1.10e+01, train loss: 1.00898e-06, val loss: 1.82581e-06, min loss: 8.42136e-07\n",
      "Epoch: 1347800, elapsed: 1.10e+01, train loss: 8.83803e-07, val loss: 1.67096e-06, min loss: 8.42136e-07\n",
      "Epoch: 1347900, elapsed: 1.09e+01, train loss: 8.63071e-07, val loss: 1.62283e-06, min loss: 8.42136e-07\n",
      "Epoch: 1348000, elapsed: 1.10e+01, train loss: 1.20968e-06, val loss: 2.03134e-06, min loss: 8.42136e-07\n",
      "Epoch: 1348100, elapsed: 1.44e+01, train loss: 1.01528e-06, val loss: 1.80846e-06, min loss: 8.42136e-07\n",
      "Epoch: 1348200, elapsed: 1.14e+01, train loss: 8.52863e-07, val loss: 1.61948e-06, min loss: 8.42136e-07\n",
      "Epoch: 1348300, elapsed: 1.10e+01, train loss: 8.62724e-07, val loss: 1.64364e-06, min loss: 8.42136e-07\n",
      "Epoch: 1348400, elapsed: 1.09e+01, train loss: 8.45104e-07, val loss: 1.62502e-06, min loss: 8.42136e-07\n",
      "Epoch: 1348500, elapsed: 1.10e+01, train loss: 2.86388e-06, val loss: 4.03350e-06, min loss: 8.42136e-07\n",
      "Epoch: 1348600, elapsed: 1.07e+01, train loss: 8.40565e-07, val loss: 1.62555e-06, min loss: 8.40565e-07\n",
      "Epoch: 1348700, elapsed: 1.08e+01, train loss: 8.72818e-07, val loss: 1.63725e-06, min loss: 8.40565e-07\n",
      "Epoch: 1348800, elapsed: 1.04e+01, train loss: 1.26409e-06, val loss: 2.09284e-06, min loss: 8.40565e-07\n",
      "Epoch: 1348900, elapsed: 1.05e+01, train loss: 9.04547e-07, val loss: 1.71222e-06, min loss: 8.40565e-07\n",
      "Epoch: 1349000, elapsed: 1.05e+01, train loss: 1.01571e-06, val loss: 1.77956e-06, min loss: 8.40565e-07\n",
      "Epoch: 1349100, elapsed: 1.06e+01, train loss: 8.93210e-07, val loss: 1.67610e-06, min loss: 8.40565e-07\n",
      "Epoch: 1349200, elapsed: 1.05e+01, train loss: 8.57884e-07, val loss: 1.65269e-06, min loss: 8.40565e-07\n",
      "Epoch: 1349300, elapsed: 1.06e+01, train loss: 5.69669e-06, val loss: 6.42436e-06, min loss: 8.40565e-07\n",
      "Epoch: 1349400, elapsed: 1.06e+01, train loss: 8.39953e-07, val loss: 1.62243e-06, min loss: 8.39953e-07\n",
      "Epoch: 1349500, elapsed: 1.04e+01, train loss: 8.76464e-07, val loss: 1.62902e-06, min loss: 8.39953e-07\n",
      "Epoch: 1349600, elapsed: 1.05e+01, train loss: 8.39830e-07, val loss: 1.62039e-06, min loss: 8.39830e-07\n",
      "Epoch: 1349700, elapsed: 1.04e+01, train loss: 8.40903e-07, val loss: 1.61999e-06, min loss: 8.39830e-07\n",
      "Epoch: 1349800, elapsed: 1.05e+01, train loss: 9.69833e-07, val loss: 1.68839e-06, min loss: 8.39830e-07\n",
      "Epoch: 1349900, elapsed: 1.06e+01, train loss: 1.01258e-06, val loss: 1.63411e-06, min loss: 8.39830e-07\n",
      "Epoch: 1350000, elapsed: 1.06e+01, train loss: 1.13019e-06, val loss: 2.03154e-06, min loss: 8.39830e-07\n",
      "Epoch: 1350100, elapsed: 1.23e+01, train loss: 9.11029e-07, val loss: 1.69830e-06, min loss: 8.39830e-07\n",
      "Epoch: 1350200, elapsed: 1.08e+01, train loss: 1.85795e-06, val loss: 2.67913e-06, min loss: 8.39830e-07\n",
      "Epoch: 1350300, elapsed: 1.06e+01, train loss: 9.42561e-07, val loss: 1.65548e-06, min loss: 8.39830e-07\n",
      "Epoch: 1350400, elapsed: 1.09e+01, train loss: 9.33757e-07, val loss: 1.75449e-06, min loss: 8.39830e-07\n",
      "Epoch: 1350500, elapsed: 1.07e+01, train loss: 8.52984e-07, val loss: 1.63117e-06, min loss: 8.39830e-07\n",
      "Epoch: 1350600, elapsed: 1.46e+01, train loss: 8.66208e-07, val loss: 1.67316e-06, min loss: 8.39830e-07\n",
      "Epoch: 1350700, elapsed: 1.10e+01, train loss: 8.41899e-07, val loss: 1.63254e-06, min loss: 8.39830e-07\n",
      "Epoch: 1350800, elapsed: 1.09e+01, train loss: 8.50566e-07, val loss: 1.63351e-06, min loss: 8.39830e-07\n",
      "Epoch: 1350900, elapsed: 1.11e+01, train loss: 9.71934e-07, val loss: 1.71962e-06, min loss: 8.39830e-07\n",
      "Epoch: 1351000, elapsed: 1.12e+01, train loss: 9.21209e-07, val loss: 1.63515e-06, min loss: 8.39830e-07\n",
      "Epoch: 1351100, elapsed: 1.12e+01, train loss: 8.41911e-07, val loss: 1.62106e-06, min loss: 8.39830e-07\n",
      "Epoch: 1351200, elapsed: 1.12e+01, train loss: 8.44187e-07, val loss: 1.61857e-06, min loss: 8.39830e-07\n",
      "Epoch: 1351300, elapsed: 1.10e+01, train loss: 8.39805e-07, val loss: 1.62163e-06, min loss: 8.39805e-07\n",
      "Epoch: 1351400, elapsed: 1.10e+01, train loss: 8.47710e-07, val loss: 1.62080e-06, min loss: 8.39805e-07\n",
      "Epoch: 1351500, elapsed: 1.10e+01, train loss: 8.59415e-07, val loss: 1.64936e-06, min loss: 8.39805e-07\n",
      "Epoch: 1351600, elapsed: 1.07e+01, train loss: 8.39097e-07, val loss: 1.61444e-06, min loss: 8.39097e-07\n",
      "Epoch: 1351700, elapsed: 1.06e+01, train loss: 8.50510e-07, val loss: 1.64132e-06, min loss: 8.39097e-07\n",
      "Epoch: 1351800, elapsed: 1.05e+01, train loss: 1.08273e-06, val loss: 2.00726e-06, min loss: 8.39097e-07\n",
      "Epoch: 1351900, elapsed: 1.04e+01, train loss: 8.38296e-07, val loss: 1.61980e-06, min loss: 8.38296e-07\n",
      "Epoch: 1352000, elapsed: 1.05e+01, train loss: 8.41286e-07, val loss: 1.62718e-06, min loss: 8.38296e-07\n",
      "Epoch: 1352100, elapsed: 1.06e+01, train loss: 8.81847e-07, val loss: 1.66378e-06, min loss: 8.38296e-07\n",
      "Epoch: 1352200, elapsed: 1.05e+01, train loss: 9.17554e-07, val loss: 1.69438e-06, min loss: 8.38296e-07\n",
      "Epoch: 1352300, elapsed: 1.05e+01, train loss: 8.49866e-07, val loss: 1.60538e-06, min loss: 8.38296e-07\n",
      "Epoch: 1352400, elapsed: 1.06e+01, train loss: 8.39725e-07, val loss: 1.61935e-06, min loss: 8.38296e-07\n",
      "Epoch: 1352500, elapsed: 1.04e+01, train loss: 8.42326e-07, val loss: 1.61223e-06, min loss: 8.38296e-07\n",
      "Epoch: 1352600, elapsed: 1.02e+01, train loss: 8.59548e-07, val loss: 1.67424e-06, min loss: 8.38296e-07\n",
      "Epoch: 1352700, elapsed: 1.05e+01, train loss: 3.96919e-06, val loss: 5.87614e-06, min loss: 8.38296e-07\n",
      "Epoch: 1352800, elapsed: 1.05e+01, train loss: 8.37999e-07, val loss: 1.61923e-06, min loss: 8.37999e-07\n",
      "Epoch: 1352900, elapsed: 1.05e+01, train loss: 8.92594e-07, val loss: 1.62687e-06, min loss: 8.37999e-07\n",
      "Epoch: 1353000, elapsed: 1.39e+01, train loss: 9.21487e-07, val loss: 1.69676e-06, min loss: 8.37999e-07\n",
      "Epoch: 1353100, elapsed: 1.08e+01, train loss: 9.22128e-07, val loss: 1.65505e-06, min loss: 8.37999e-07\n",
      "Epoch: 1353200, elapsed: 1.09e+01, train loss: 9.28774e-07, val loss: 1.65339e-06, min loss: 8.37999e-07\n",
      "Epoch: 1353300, elapsed: 1.07e+01, train loss: 8.45371e-07, val loss: 1.61206e-06, min loss: 8.37999e-07\n",
      "Epoch: 1353400, elapsed: 1.10e+01, train loss: 8.51927e-07, val loss: 1.64631e-06, min loss: 8.37999e-07\n",
      "Epoch: 1353500, elapsed: 1.10e+01, train loss: 8.38324e-07, val loss: 1.61849e-06, min loss: 8.37999e-07\n",
      "Epoch: 1353600, elapsed: 1.11e+01, train loss: 8.40726e-07, val loss: 1.62260e-06, min loss: 8.37999e-07\n",
      "Epoch: 1353700, elapsed: 1.11e+01, train loss: 1.30214e-06, val loss: 2.26975e-06, min loss: 8.37999e-07\n",
      "Epoch: 1353800, elapsed: 1.12e+01, train loss: 8.96047e-07, val loss: 1.77098e-06, min loss: 8.37999e-07\n",
      "Epoch: 1353900, elapsed: 1.10e+01, train loss: 2.40342e-06, val loss: 3.31262e-06, min loss: 8.37999e-07\n",
      "Epoch: 1354000, elapsed: 1.09e+01, train loss: 8.48012e-07, val loss: 1.61246e-06, min loss: 8.37999e-07\n",
      "Epoch: 1354100, elapsed: 1.09e+01, train loss: 8.37171e-07, val loss: 1.61675e-06, min loss: 8.37171e-07\n",
      "Epoch: 1354200, elapsed: 1.08e+01, train loss: 8.48959e-07, val loss: 1.60976e-06, min loss: 8.37171e-07\n",
      "Epoch: 1354300, elapsed: 1.08e+01, train loss: 8.55673e-07, val loss: 1.65194e-06, min loss: 8.37171e-07\n",
      "Epoch: 1354400, elapsed: 1.07e+01, train loss: 9.17765e-07, val loss: 1.73575e-06, min loss: 8.37171e-07\n",
      "Epoch: 1354500, elapsed: 1.06e+01, train loss: 2.18426e-06, val loss: 2.49026e-06, min loss: 8.37171e-07\n",
      "Epoch: 1354600, elapsed: 1.08e+01, train loss: 1.85274e-06, val loss: 2.16111e-06, min loss: 8.37171e-07\n",
      "Epoch: 1354700, elapsed: 1.05e+01, train loss: 1.02091e-06, val loss: 1.73626e-06, min loss: 8.37171e-07\n",
      "Epoch: 1354800, elapsed: 1.07e+01, train loss: 8.48508e-07, val loss: 1.63533e-06, min loss: 8.37171e-07\n",
      "Epoch: 1354900, elapsed: 1.05e+01, train loss: 1.36809e-06, val loss: 2.20188e-06, min loss: 8.37171e-07\n",
      "Epoch: 1355000, elapsed: 1.06e+01, train loss: 8.39251e-07, val loss: 1.60663e-06, min loss: 8.37171e-07\n",
      "Epoch: 1355100, elapsed: 1.25e+01, train loss: 8.41315e-07, val loss: 1.61673e-06, min loss: 8.37171e-07\n",
      "Epoch: 1355200, elapsed: 1.07e+01, train loss: 8.36856e-07, val loss: 1.61192e-06, min loss: 8.36856e-07\n",
      "Epoch: 1355300, elapsed: 1.07e+01, train loss: 1.34849e-06, val loss: 1.75149e-06, min loss: 8.36856e-07\n",
      "Epoch: 1355400, elapsed: 1.07e+01, train loss: 9.82623e-07, val loss: 1.76017e-06, min loss: 8.36856e-07\n",
      "Epoch: 1355500, elapsed: 1.43e+01, train loss: 1.13484e-06, val loss: 1.88683e-06, min loss: 8.36856e-07\n",
      "Epoch: 1355600, elapsed: 1.07e+01, train loss: 2.01317e-06, val loss: 2.39744e-06, min loss: 8.36856e-07\n",
      "Epoch: 1355700, elapsed: 1.09e+01, train loss: 2.12860e-06, val loss: 2.22731e-06, min loss: 8.36856e-07\n",
      "Epoch: 1355800, elapsed: 1.08e+01, train loss: 8.35999e-07, val loss: 1.61104e-06, min loss: 8.35999e-07\n",
      "Epoch: 1355900, elapsed: 1.06e+01, train loss: 8.37066e-07, val loss: 1.61037e-06, min loss: 8.35999e-07\n",
      "Epoch: 1356000, elapsed: 1.07e+01, train loss: 8.53296e-07, val loss: 1.64120e-06, min loss: 8.35999e-07\n",
      "Epoch: 1356100, elapsed: 1.08e+01, train loss: 9.27286e-07, val loss: 2.16761e-06, min loss: 8.35999e-07\n",
      "Epoch: 1356200, elapsed: 1.06e+01, train loss: 1.38888e-06, val loss: 2.36861e-06, min loss: 8.35999e-07\n",
      "Epoch: 1356300, elapsed: 1.07e+01, train loss: 9.20514e-07, val loss: 1.75103e-06, min loss: 8.35999e-07\n",
      "Epoch: 1356400, elapsed: 1.07e+01, train loss: 8.35878e-07, val loss: 1.61056e-06, min loss: 8.35878e-07\n",
      "Epoch: 1356500, elapsed: 1.08e+01, train loss: 1.46849e-06, val loss: 2.46768e-06, min loss: 8.35878e-07\n",
      "Epoch: 1356600, elapsed: 1.06e+01, train loss: 4.17541e-06, val loss: 4.51673e-06, min loss: 8.35878e-07\n",
      "Epoch: 1356700, elapsed: 1.07e+01, train loss: 8.48362e-07, val loss: 1.62295e-06, min loss: 8.35878e-07\n",
      "Epoch: 1356800, elapsed: 1.08e+01, train loss: 8.36646e-07, val loss: 1.61636e-06, min loss: 8.35878e-07\n",
      "Epoch: 1356900, elapsed: 1.06e+01, train loss: 8.64989e-07, val loss: 1.60917e-06, min loss: 8.35878e-07\n",
      "Epoch: 1357000, elapsed: 1.08e+01, train loss: 9.02993e-07, val loss: 1.61486e-06, min loss: 8.35878e-07\n",
      "Epoch: 1357100, elapsed: 1.07e+01, train loss: 1.44775e-06, val loss: 2.58540e-06, min loss: 8.35878e-07\n",
      "Epoch: 1357200, elapsed: 1.06e+01, train loss: 8.88347e-07, val loss: 1.72459e-06, min loss: 8.35878e-07\n",
      "Epoch: 1357300, elapsed: 1.04e+01, train loss: 9.37820e-07, val loss: 1.61712e-06, min loss: 8.35878e-07\n",
      "Epoch: 1357400, elapsed: 1.06e+01, train loss: 1.35574e-06, val loss: 2.22678e-06, min loss: 8.35878e-07\n",
      "Epoch: 1357500, elapsed: 1.07e+01, train loss: 8.35618e-07, val loss: 1.60636e-06, min loss: 8.35618e-07\n",
      "Epoch: 1357600, elapsed: 1.06e+01, train loss: 8.36638e-07, val loss: 1.60784e-06, min loss: 8.35618e-07\n",
      "Epoch: 1357700, elapsed: 1.08e+01, train loss: 8.91345e-07, val loss: 1.63750e-06, min loss: 8.35618e-07\n",
      "Epoch: 1357800, elapsed: 1.08e+01, train loss: 9.05895e-07, val loss: 1.71592e-06, min loss: 8.35618e-07\n",
      "Epoch: 1357900, elapsed: 1.05e+01, train loss: 2.51203e-06, val loss: 3.02329e-06, min loss: 8.35618e-07\n",
      "Epoch: 1358000, elapsed: 1.49e+01, train loss: 1.47180e-06, val loss: 2.51733e-06, min loss: 8.35618e-07\n",
      "Epoch: 1358100, elapsed: 1.11e+01, train loss: 8.75230e-07, val loss: 1.68208e-06, min loss: 8.35618e-07\n",
      "Epoch: 1358200, elapsed: 1.11e+01, train loss: 8.69878e-07, val loss: 1.68602e-06, min loss: 8.35618e-07\n",
      "Epoch: 1358300, elapsed: 1.11e+01, train loss: 9.70814e-07, val loss: 1.74254e-06, min loss: 8.35618e-07\n",
      "Epoch: 1358400, elapsed: 1.13e+01, train loss: 8.48267e-07, val loss: 1.60452e-06, min loss: 8.35618e-07\n",
      "Epoch: 1358500, elapsed: 1.10e+01, train loss: 8.51191e-07, val loss: 1.63160e-06, min loss: 8.35618e-07\n",
      "Epoch: 1358600, elapsed: 1.10e+01, train loss: 8.47600e-07, val loss: 1.60170e-06, min loss: 8.35618e-07\n",
      "Epoch: 1358700, elapsed: 1.11e+01, train loss: 8.46542e-07, val loss: 1.63296e-06, min loss: 8.35618e-07\n",
      "Epoch: 1358800, elapsed: 1.07e+01, train loss: 8.56943e-07, val loss: 1.65583e-06, min loss: 8.35618e-07\n",
      "Epoch: 1358900, elapsed: 1.07e+01, train loss: 9.64479e-07, val loss: 1.71559e-06, min loss: 8.35618e-07\n",
      "Epoch: 1359000, elapsed: 1.08e+01, train loss: 8.65407e-07, val loss: 1.65076e-06, min loss: 8.35618e-07\n",
      "Epoch: 1359100, elapsed: 1.06e+01, train loss: 8.71349e-07, val loss: 1.67887e-06, min loss: 8.35618e-07\n",
      "Epoch: 1359200, elapsed: 1.07e+01, train loss: 1.81777e-06, val loss: 2.80038e-06, min loss: 8.35618e-07\n",
      "Epoch: 1359300, elapsed: 1.10e+01, train loss: 8.37879e-07, val loss: 1.60290e-06, min loss: 8.35618e-07\n",
      "Epoch: 1359400, elapsed: 1.09e+01, train loss: 8.38788e-07, val loss: 1.60606e-06, min loss: 8.35618e-07\n",
      "Epoch: 1359500, elapsed: 1.07e+01, train loss: 9.13304e-07, val loss: 1.73787e-06, min loss: 8.35618e-07\n",
      "Epoch: 1359600, elapsed: 1.10e+01, train loss: 8.41276e-07, val loss: 1.60610e-06, min loss: 8.35618e-07\n",
      "Epoch: 1359700, elapsed: 1.08e+01, train loss: 3.99059e-06, val loss: 3.72729e-06, min loss: 8.35618e-07\n",
      "Epoch: 1359800, elapsed: 1.10e+01, train loss: 8.36871e-07, val loss: 1.60922e-06, min loss: 8.35618e-07\n",
      "Epoch: 1359900, elapsed: 1.09e+01, train loss: 8.34437e-07, val loss: 1.60928e-06, min loss: 8.34437e-07\n",
      "Epoch: 1360000, elapsed: 1.07e+01, train loss: 8.39862e-07, val loss: 1.62061e-06, min loss: 8.34437e-07\n",
      "Epoch: 1360100, elapsed: 1.29e+01, train loss: 8.97292e-07, val loss: 1.68742e-06, min loss: 8.34437e-07\n",
      "Epoch: 1360200, elapsed: 1.08e+01, train loss: 1.32542e-06, val loss: 1.84511e-06, min loss: 8.34437e-07\n",
      "Epoch: 1360300, elapsed: 1.08e+01, train loss: 1.12681e-06, val loss: 2.07861e-06, min loss: 8.34437e-07\n",
      "Epoch: 1360400, elapsed: 1.50e+01, train loss: 8.33899e-07, val loss: 1.60536e-06, min loss: 8.33899e-07\n",
      "Epoch: 1360500, elapsed: 1.10e+01, train loss: 8.72966e-07, val loss: 1.60408e-06, min loss: 8.33899e-07\n",
      "Epoch: 1360600, elapsed: 1.11e+01, train loss: 8.96247e-07, val loss: 1.65706e-06, min loss: 8.33899e-07\n",
      "Epoch: 1360700, elapsed: 1.09e+01, train loss: 2.10150e-06, val loss: 2.57123e-06, min loss: 8.33899e-07\n",
      "Epoch: 1360800, elapsed: 1.07e+01, train loss: 8.46187e-07, val loss: 1.61353e-06, min loss: 8.33899e-07\n",
      "Epoch: 1360900, elapsed: 1.07e+01, train loss: 8.34342e-07, val loss: 1.60842e-06, min loss: 8.33899e-07\n",
      "Epoch: 1361000, elapsed: 1.05e+01, train loss: 8.48601e-07, val loss: 1.64679e-06, min loss: 8.33899e-07\n",
      "Epoch: 1361100, elapsed: 1.06e+01, train loss: 8.38280e-07, val loss: 1.60434e-06, min loss: 8.33899e-07\n",
      "Epoch: 1361200, elapsed: 1.10e+01, train loss: 9.31755e-07, val loss: 1.75579e-06, min loss: 8.33899e-07\n",
      "Epoch: 1361300, elapsed: 1.08e+01, train loss: 2.06052e-06, val loss: 2.62855e-06, min loss: 8.33899e-07\n",
      "Epoch: 1361400, elapsed: 1.11e+01, train loss: 8.34214e-07, val loss: 1.61482e-06, min loss: 8.33899e-07\n",
      "Epoch: 1361500, elapsed: 1.10e+01, train loss: 8.34470e-07, val loss: 1.60790e-06, min loss: 8.33899e-07\n",
      "Epoch: 1361600, elapsed: 1.10e+01, train loss: 8.58529e-07, val loss: 1.61463e-06, min loss: 8.33899e-07\n",
      "Epoch: 1361700, elapsed: 1.11e+01, train loss: 1.33897e-06, val loss: 2.17831e-06, min loss: 8.33899e-07\n",
      "Epoch: 1361800, elapsed: 1.11e+01, train loss: 9.07388e-07, val loss: 1.65530e-06, min loss: 8.33899e-07\n",
      "Epoch: 1361900, elapsed: 1.09e+01, train loss: 8.33245e-07, val loss: 1.61091e-06, min loss: 8.33245e-07\n",
      "Epoch: 1362000, elapsed: 1.11e+01, train loss: 9.28387e-07, val loss: 1.72551e-06, min loss: 8.33245e-07\n",
      "Epoch: 1362100, elapsed: 1.09e+01, train loss: 8.59117e-07, val loss: 1.61228e-06, min loss: 8.33245e-07\n",
      "Epoch: 1362200, elapsed: 1.11e+01, train loss: 8.32388e-07, val loss: 1.60100e-06, min loss: 8.32388e-07\n",
      "Epoch: 1362300, elapsed: 1.10e+01, train loss: 8.35648e-07, val loss: 1.60886e-06, min loss: 8.32388e-07\n",
      "Epoch: 1362400, elapsed: 1.10e+01, train loss: 8.35431e-07, val loss: 1.60002e-06, min loss: 8.32388e-07\n",
      "Epoch: 1362500, elapsed: 1.10e+01, train loss: 8.97383e-07, val loss: 1.65367e-06, min loss: 8.32388e-07\n",
      "Epoch: 1362600, elapsed: 1.10e+01, train loss: 1.46440e-06, val loss: 2.08910e-06, min loss: 8.32388e-07\n",
      "Epoch: 1362700, elapsed: 1.10e+01, train loss: 1.45515e-06, val loss: 1.77838e-06, min loss: 8.32388e-07\n",
      "Epoch: 1362800, elapsed: 1.11e+01, train loss: 1.31639e-06, val loss: 2.02465e-06, min loss: 8.32388e-07\n",
      "Epoch: 1362900, elapsed: 1.47e+01, train loss: 1.22648e-06, val loss: 1.87055e-06, min loss: 8.32388e-07\n",
      "Epoch: 1363000, elapsed: 1.11e+01, train loss: 8.64634e-07, val loss: 1.61014e-06, min loss: 8.32388e-07\n",
      "Epoch: 1363100, elapsed: 1.12e+01, train loss: 2.53954e-06, val loss: 3.29413e-06, min loss: 8.32388e-07\n",
      "Epoch: 1363200, elapsed: 1.11e+01, train loss: 2.06547e-06, val loss: 3.07732e-06, min loss: 8.32388e-07\n",
      "Epoch: 1363300, elapsed: 1.11e+01, train loss: 8.37670e-07, val loss: 1.59385e-06, min loss: 8.32388e-07\n",
      "Epoch: 1363400, elapsed: 1.12e+01, train loss: 8.31824e-07, val loss: 1.60464e-06, min loss: 8.31824e-07\n",
      "Epoch: 1363500, elapsed: 1.10e+01, train loss: 9.03702e-07, val loss: 1.61096e-06, min loss: 8.31824e-07\n",
      "Epoch: 1363600, elapsed: 1.11e+01, train loss: 9.48983e-07, val loss: 1.92177e-06, min loss: 8.31824e-07\n",
      "Epoch: 1363700, elapsed: 1.13e+01, train loss: 9.31761e-07, val loss: 1.72866e-06, min loss: 8.31824e-07\n",
      "Epoch: 1363800, elapsed: 1.09e+01, train loss: 8.35128e-07, val loss: 1.60531e-06, min loss: 8.31824e-07\n",
      "Epoch: 1363900, elapsed: 1.10e+01, train loss: 8.36547e-07, val loss: 1.62293e-06, min loss: 8.31824e-07\n",
      "Epoch: 1364000, elapsed: 1.11e+01, train loss: 2.93518e-06, val loss: 2.53631e-06, min loss: 8.31824e-07\n",
      "Epoch: 1364100, elapsed: 1.11e+01, train loss: 8.46869e-07, val loss: 1.61219e-06, min loss: 8.31824e-07\n",
      "Epoch: 1364200, elapsed: 1.09e+01, train loss: 8.31906e-07, val loss: 1.60284e-06, min loss: 8.31824e-07\n",
      "Epoch: 1364300, elapsed: 1.11e+01, train loss: 8.31166e-07, val loss: 1.60437e-06, min loss: 8.31166e-07\n",
      "Epoch: 1364400, elapsed: 1.09e+01, train loss: 8.92479e-07, val loss: 1.61346e-06, min loss: 8.31166e-07\n",
      "Epoch: 1364500, elapsed: 1.10e+01, train loss: 2.61019e-06, val loss: 2.97293e-06, min loss: 8.31166e-07\n",
      "Epoch: 1364600, elapsed: 1.12e+01, train loss: 8.30996e-07, val loss: 1.60341e-06, min loss: 8.30996e-07\n",
      "Epoch: 1364700, elapsed: 1.09e+01, train loss: 9.18591e-07, val loss: 1.74962e-06, min loss: 8.30996e-07\n",
      "Epoch: 1364800, elapsed: 1.09e+01, train loss: 1.12278e-06, val loss: 2.00208e-06, min loss: 8.30996e-07\n",
      "Epoch: 1364900, elapsed: 1.12e+01, train loss: 2.15834e-06, val loss: 2.30680e-06, min loss: 8.30996e-07\n",
      "Epoch: 1365000, elapsed: 1.11e+01, train loss: 1.11345e-06, val loss: 1.89427e-06, min loss: 8.30996e-07\n",
      "Epoch: 1365100, elapsed: 1.30e+01, train loss: 8.51198e-07, val loss: 1.63589e-06, min loss: 8.30996e-07\n",
      "Epoch: 1365200, elapsed: 1.09e+01, train loss: 9.10615e-07, val loss: 1.72085e-06, min loss: 8.30996e-07\n",
      "Epoch: 1365300, elapsed: 1.11e+01, train loss: 1.05575e-06, val loss: 1.81573e-06, min loss: 8.30996e-07\n",
      "Epoch: 1365400, elapsed: 1.49e+01, train loss: 9.44000e-07, val loss: 1.74461e-06, min loss: 8.30996e-07\n",
      "Epoch: 1365500, elapsed: 1.12e+01, train loss: 1.69070e-06, val loss: 2.72925e-06, min loss: 8.30996e-07\n",
      "Epoch: 1365600, elapsed: 1.13e+01, train loss: 1.21846e-06, val loss: 1.66281e-06, min loss: 8.30996e-07\n",
      "Epoch: 1365700, elapsed: 1.12e+01, train loss: 1.00555e-06, val loss: 1.64511e-06, min loss: 8.30996e-07\n",
      "Epoch: 1365800, elapsed: 1.11e+01, train loss: 1.64575e-06, val loss: 2.46425e-06, min loss: 8.30996e-07\n",
      "Epoch: 1365900, elapsed: 1.10e+01, train loss: 9.01449e-07, val loss: 1.62331e-06, min loss: 8.30996e-07\n",
      "Epoch: 1366000, elapsed: 1.12e+01, train loss: 9.54768e-07, val loss: 1.64992e-06, min loss: 8.30996e-07\n",
      "Epoch: 1366100, elapsed: 1.10e+01, train loss: 8.91670e-07, val loss: 1.60855e-06, min loss: 8.30996e-07\n",
      "Epoch: 1366200, elapsed: 1.12e+01, train loss: 8.53832e-07, val loss: 1.64585e-06, min loss: 8.30996e-07\n",
      "Epoch: 1366300, elapsed: 1.10e+01, train loss: 8.72328e-07, val loss: 1.65929e-06, min loss: 8.30996e-07\n",
      "Epoch: 1366400, elapsed: 1.09e+01, train loss: 1.11686e-06, val loss: 1.71178e-06, min loss: 8.30996e-07\n",
      "Epoch: 1366500, elapsed: 1.11e+01, train loss: 8.30936e-07, val loss: 1.60499e-06, min loss: 8.30936e-07\n",
      "Epoch: 1366600, elapsed: 1.10e+01, train loss: 8.31500e-07, val loss: 1.60314e-06, min loss: 8.30936e-07\n",
      "Epoch: 1366700, elapsed: 1.09e+01, train loss: 8.39480e-07, val loss: 1.59531e-06, min loss: 8.30936e-07\n",
      "Epoch: 1366800, elapsed: 1.11e+01, train loss: 8.43263e-07, val loss: 1.61844e-06, min loss: 8.30936e-07\n",
      "Epoch: 1366900, elapsed: 1.10e+01, train loss: 8.38369e-07, val loss: 1.61415e-06, min loss: 8.30936e-07\n",
      "Epoch: 1367000, elapsed: 1.11e+01, train loss: 8.98756e-07, val loss: 1.65342e-06, min loss: 8.30936e-07\n",
      "Epoch: 1367100, elapsed: 1.11e+01, train loss: 1.15447e-06, val loss: 1.96210e-06, min loss: 8.30936e-07\n",
      "Epoch: 1367200, elapsed: 1.10e+01, train loss: 8.28899e-07, val loss: 1.59525e-06, min loss: 8.28899e-07\n",
      "Epoch: 1367300, elapsed: 1.12e+01, train loss: 8.38007e-07, val loss: 1.57951e-06, min loss: 8.28899e-07\n",
      "Epoch: 1367400, elapsed: 1.12e+01, train loss: 8.78075e-07, val loss: 1.64963e-06, min loss: 8.28899e-07\n",
      "Epoch: 1367500, elapsed: 1.12e+01, train loss: 1.13592e-06, val loss: 1.84163e-06, min loss: 8.28899e-07\n",
      "Epoch: 1367600, elapsed: 1.11e+01, train loss: 8.67345e-07, val loss: 1.64771e-06, min loss: 8.28899e-07\n",
      "Epoch: 1367700, elapsed: 1.10e+01, train loss: 8.32836e-07, val loss: 1.59136e-06, min loss: 8.28899e-07\n",
      "Epoch: 1367800, elapsed: 1.10e+01, train loss: 8.33132e-07, val loss: 1.60193e-06, min loss: 8.28899e-07\n",
      "Epoch: 1367900, elapsed: 1.50e+01, train loss: 9.63617e-07, val loss: 1.83548e-06, min loss: 8.28899e-07\n",
      "Epoch: 1368000, elapsed: 1.13e+01, train loss: 8.34343e-07, val loss: 1.61498e-06, min loss: 8.28899e-07\n",
      "Epoch: 1368100, elapsed: 1.11e+01, train loss: 8.97131e-07, val loss: 1.71236e-06, min loss: 8.28899e-07\n",
      "Epoch: 1368200, elapsed: 1.11e+01, train loss: 1.00512e-06, val loss: 1.81307e-06, min loss: 8.28899e-07\n",
      "Epoch: 1368300, elapsed: 1.10e+01, train loss: 9.35422e-07, val loss: 1.73678e-06, min loss: 8.28899e-07\n",
      "Epoch: 1368400, elapsed: 1.10e+01, train loss: 8.28413e-07, val loss: 1.59515e-06, min loss: 8.28413e-07\n",
      "Epoch: 1368500, elapsed: 1.10e+01, train loss: 8.35259e-07, val loss: 1.59367e-06, min loss: 8.28413e-07\n",
      "Epoch: 1368600, elapsed: 1.11e+01, train loss: 1.15698e-06, val loss: 1.86559e-06, min loss: 8.28413e-07\n",
      "Epoch: 1368700, elapsed: 1.10e+01, train loss: 9.01263e-07, val loss: 1.61341e-06, min loss: 8.28413e-07\n",
      "Epoch: 1368800, elapsed: 1.10e+01, train loss: 8.31573e-07, val loss: 1.60614e-06, min loss: 8.28413e-07\n",
      "Epoch: 1368900, elapsed: 1.11e+01, train loss: 8.52408e-07, val loss: 1.64623e-06, min loss: 8.28413e-07\n",
      "Epoch: 1369000, elapsed: 1.10e+01, train loss: 1.97443e-06, val loss: 2.99012e-06, min loss: 8.28413e-07\n",
      "Epoch: 1369100, elapsed: 1.11e+01, train loss: 8.27945e-07, val loss: 1.59452e-06, min loss: 8.27945e-07\n",
      "Epoch: 1369200, elapsed: 1.12e+01, train loss: 1.14588e-06, val loss: 1.78643e-06, min loss: 8.27945e-07\n",
      "Epoch: 1369300, elapsed: 1.10e+01, train loss: 1.14191e-06, val loss: 2.10261e-06, min loss: 8.27945e-07\n",
      "Epoch: 1369400, elapsed: 1.11e+01, train loss: 8.28298e-07, val loss: 1.59757e-06, min loss: 8.27945e-07\n",
      "Epoch: 1369500, elapsed: 1.08e+01, train loss: 9.59140e-07, val loss: 1.80352e-06, min loss: 8.27945e-07\n",
      "Epoch: 1369600, elapsed: 1.10e+01, train loss: 1.70265e-06, val loss: 2.60053e-06, min loss: 8.27945e-07\n",
      "Epoch: 1369700, elapsed: 1.10e+01, train loss: 4.43358e-06, val loss: 5.33979e-06, min loss: 8.27945e-07\n",
      "Epoch: 1369800, elapsed: 1.10e+01, train loss: 1.56712e-06, val loss: 2.64283e-06, min loss: 8.27945e-07\n",
      "Epoch: 1369900, elapsed: 1.09e+01, train loss: 8.35098e-07, val loss: 1.60284e-06, min loss: 8.27945e-07\n",
      "Epoch: 1370000, elapsed: 1.09e+01, train loss: 9.47537e-07, val loss: 1.70315e-06, min loss: 8.27945e-07\n",
      "Epoch: 1370100, elapsed: 1.36e+01, train loss: 8.43334e-07, val loss: 1.58742e-06, min loss: 8.27945e-07\n",
      "Epoch: 1370200, elapsed: 1.10e+01, train loss: 1.02143e-06, val loss: 1.78234e-06, min loss: 8.27945e-07\n",
      "Epoch: 1370300, elapsed: 1.09e+01, train loss: 8.95848e-07, val loss: 1.63903e-06, min loss: 8.27945e-07\n",
      "Epoch: 1370400, elapsed: 1.48e+01, train loss: 8.53436e-07, val loss: 1.62758e-06, min loss: 8.27945e-07\n",
      "Epoch: 1370500, elapsed: 1.11e+01, train loss: 8.53400e-07, val loss: 1.63086e-06, min loss: 8.27945e-07\n",
      "Epoch: 1370600, elapsed: 1.13e+01, train loss: 8.66001e-07, val loss: 1.60245e-06, min loss: 8.27945e-07\n",
      "Epoch: 1370700, elapsed: 1.11e+01, train loss: 1.12886e-06, val loss: 1.82964e-06, min loss: 8.27945e-07\n",
      "Epoch: 1370800, elapsed: 1.09e+01, train loss: 8.65545e-07, val loss: 1.80513e-06, min loss: 8.27945e-07\n",
      "Epoch: 1370900, elapsed: 1.11e+01, train loss: 2.44468e-06, val loss: 3.44169e-06, min loss: 8.27945e-07\n",
      "Epoch: 1371000, elapsed: 1.10e+01, train loss: 8.27256e-07, val loss: 1.59634e-06, min loss: 8.27256e-07\n",
      "Epoch: 1371100, elapsed: 1.11e+01, train loss: 8.26468e-07, val loss: 1.58802e-06, min loss: 8.26468e-07\n",
      "Epoch: 1371200, elapsed: 1.11e+01, train loss: 1.56758e-06, val loss: 2.52896e-06, min loss: 8.26468e-07\n",
      "Epoch: 1371300, elapsed: 1.10e+01, train loss: 8.44503e-07, val loss: 1.58310e-06, min loss: 8.26468e-07\n",
      "Epoch: 1371400, elapsed: 1.09e+01, train loss: 8.26497e-07, val loss: 1.59146e-06, min loss: 8.26468e-07\n",
      "Epoch: 1371500, elapsed: 1.11e+01, train loss: 8.33349e-07, val loss: 1.59638e-06, min loss: 8.26468e-07\n",
      "Epoch: 1371600, elapsed: 1.09e+01, train loss: 8.56193e-07, val loss: 1.59914e-06, min loss: 8.26468e-07\n",
      "Epoch: 1371700, elapsed: 1.10e+01, train loss: 8.43778e-07, val loss: 1.58252e-06, min loss: 8.26468e-07\n",
      "Epoch: 1371800, elapsed: 1.09e+01, train loss: 1.39914e-06, val loss: 2.04688e-06, min loss: 8.26468e-07\n",
      "Epoch: 1371900, elapsed: 1.12e+01, train loss: 1.54389e-06, val loss: 1.85466e-06, min loss: 8.26468e-07\n",
      "Epoch: 1372000, elapsed: 1.09e+01, train loss: 1.03901e-06, val loss: 1.79497e-06, min loss: 8.26468e-07\n",
      "Epoch: 1372100, elapsed: 1.09e+01, train loss: 1.25235e-06, val loss: 2.08293e-06, min loss: 8.26468e-07\n",
      "Epoch: 1372200, elapsed: 1.09e+01, train loss: 1.02237e-06, val loss: 1.66614e-06, min loss: 8.26468e-07\n",
      "Epoch: 1372300, elapsed: 1.10e+01, train loss: 8.52688e-07, val loss: 1.66815e-06, min loss: 8.26468e-07\n",
      "Epoch: 1372400, elapsed: 1.08e+01, train loss: 2.32010e-06, val loss: 3.58423e-06, min loss: 8.26468e-07\n",
      "Epoch: 1372500, elapsed: 1.08e+01, train loss: 8.35497e-07, val loss: 1.61901e-06, min loss: 8.26468e-07\n",
      "Epoch: 1372600, elapsed: 1.10e+01, train loss: 8.28700e-07, val loss: 1.58502e-06, min loss: 8.26468e-07\n",
      "Epoch: 1372700, elapsed: 1.09e+01, train loss: 8.81339e-07, val loss: 1.61395e-06, min loss: 8.26468e-07\n",
      "Epoch: 1372800, elapsed: 1.09e+01, train loss: 2.09330e-06, val loss: 2.89804e-06, min loss: 8.26468e-07\n",
      "Epoch: 1372900, elapsed: 1.46e+01, train loss: 8.72572e-07, val loss: 1.61176e-06, min loss: 8.26468e-07\n",
      "Epoch: 1373000, elapsed: 1.12e+01, train loss: 8.26699e-07, val loss: 1.59574e-06, min loss: 8.26468e-07\n",
      "Epoch: 1373100, elapsed: 1.13e+01, train loss: 8.50362e-07, val loss: 1.59539e-06, min loss: 8.26468e-07\n",
      "Epoch: 1373200, elapsed: 1.10e+01, train loss: 1.13265e-06, val loss: 1.80427e-06, min loss: 8.26468e-07\n",
      "Epoch: 1373300, elapsed: 1.12e+01, train loss: 8.29351e-07, val loss: 1.59339e-06, min loss: 8.26468e-07\n",
      "Epoch: 1373400, elapsed: 1.11e+01, train loss: 8.57330e-07, val loss: 1.60411e-06, min loss: 8.26468e-07\n",
      "Epoch: 1373500, elapsed: 1.09e+01, train loss: 9.28712e-07, val loss: 1.62977e-06, min loss: 8.26468e-07\n",
      "Epoch: 1373600, elapsed: 1.11e+01, train loss: 3.13493e-06, val loss: 2.74124e-06, min loss: 8.26468e-07\n",
      "Epoch: 1373700, elapsed: 1.09e+01, train loss: 2.02015e-06, val loss: 3.01940e-06, min loss: 8.26468e-07\n",
      "Epoch: 1373800, elapsed: 1.10e+01, train loss: 2.62076e-06, val loss: 2.61146e-06, min loss: 8.26468e-07\n",
      "Epoch: 1373900, elapsed: 1.12e+01, train loss: 2.39293e-06, val loss: 3.05341e-06, min loss: 8.26468e-07\n",
      "Epoch: 1374000, elapsed: 1.10e+01, train loss: 8.38730e-07, val loss: 1.60223e-06, min loss: 8.26468e-07\n",
      "Epoch: 1374100, elapsed: 1.09e+01, train loss: 8.71236e-07, val loss: 1.60452e-06, min loss: 8.26468e-07\n",
      "Epoch: 1374200, elapsed: 1.09e+01, train loss: 1.06415e-06, val loss: 1.83190e-06, min loss: 8.26468e-07\n",
      "Epoch: 1374300, elapsed: 1.09e+01, train loss: 9.63653e-07, val loss: 1.68260e-06, min loss: 8.26468e-07\n",
      "Epoch: 1374400, elapsed: 1.11e+01, train loss: 2.16281e-06, val loss: 2.45868e-06, min loss: 8.26468e-07\n",
      "Epoch: 1374500, elapsed: 1.10e+01, train loss: 1.65754e-06, val loss: 2.25984e-06, min loss: 8.26468e-07\n",
      "Epoch: 1374600, elapsed: 1.09e+01, train loss: 1.16314e-06, val loss: 1.80453e-06, min loss: 8.26468e-07\n",
      "Epoch: 1374700, elapsed: 1.08e+01, train loss: 9.26273e-07, val loss: 1.74869e-06, min loss: 8.26468e-07\n",
      "Epoch: 1374800, elapsed: 1.10e+01, train loss: 8.26128e-07, val loss: 1.58599e-06, min loss: 8.26128e-07\n",
      "Epoch: 1374900, elapsed: 1.10e+01, train loss: 8.32614e-07, val loss: 1.61111e-06, min loss: 8.26128e-07\n",
      "Epoch: 1375000, elapsed: 1.08e+01, train loss: 8.69363e-07, val loss: 1.60574e-06, min loss: 8.26128e-07\n",
      "Epoch: 1375100, elapsed: 1.30e+01, train loss: 2.43470e-06, val loss: 2.72973e-06, min loss: 8.26128e-07\n",
      "Epoch: 1375200, elapsed: 1.09e+01, train loss: 8.98549e-07, val loss: 1.69410e-06, min loss: 8.26128e-07\n",
      "Epoch: 1375300, elapsed: 1.10e+01, train loss: 8.65767e-07, val loss: 1.63619e-06, min loss: 8.26128e-07\n",
      "Epoch: 1375400, elapsed: 1.47e+01, train loss: 8.23945e-07, val loss: 1.57941e-06, min loss: 8.23945e-07\n",
      "Epoch: 1375500, elapsed: 1.11e+01, train loss: 1.04346e-06, val loss: 1.79129e-06, min loss: 8.23945e-07\n",
      "Epoch: 1375600, elapsed: 1.12e+01, train loss: 8.27445e-07, val loss: 1.58293e-06, min loss: 8.23945e-07\n",
      "Epoch: 1375700, elapsed: 1.10e+01, train loss: 8.25790e-07, val loss: 1.58175e-06, min loss: 8.23945e-07\n",
      "Epoch: 1375800, elapsed: 1.11e+01, train loss: 9.28051e-07, val loss: 1.68889e-06, min loss: 8.23945e-07\n",
      "Epoch: 1375900, elapsed: 1.12e+01, train loss: 9.16999e-07, val loss: 1.73342e-06, min loss: 8.23945e-07\n",
      "Epoch: 1376000, elapsed: 1.10e+01, train loss: 1.98086e-06, val loss: 2.40627e-06, min loss: 8.23945e-07\n",
      "Epoch: 1376100, elapsed: 1.10e+01, train loss: 9.08285e-07, val loss: 1.72548e-06, min loss: 8.23945e-07\n",
      "Epoch: 1376200, elapsed: 1.12e+01, train loss: 1.11573e-06, val loss: 1.86068e-06, min loss: 8.23945e-07\n",
      "Epoch: 1376300, elapsed: 1.10e+01, train loss: 9.27617e-07, val loss: 1.75429e-06, min loss: 8.23945e-07\n",
      "Epoch: 1376400, elapsed: 1.10e+01, train loss: 1.02844e-06, val loss: 1.91503e-06, min loss: 8.23945e-07\n",
      "Epoch: 1376500, elapsed: 1.09e+01, train loss: 1.10586e-06, val loss: 1.97639e-06, min loss: 8.23945e-07\n",
      "Epoch: 1376600, elapsed: 1.10e+01, train loss: 8.43132e-07, val loss: 1.60314e-06, min loss: 8.23945e-07\n",
      "Epoch: 1376700, elapsed: 1.10e+01, train loss: 8.97730e-07, val loss: 1.63854e-06, min loss: 8.23945e-07\n",
      "Epoch: 1376800, elapsed: 1.11e+01, train loss: 8.26035e-07, val loss: 1.57844e-06, min loss: 8.23945e-07\n",
      "Epoch: 1376900, elapsed: 1.10e+01, train loss: 8.25182e-07, val loss: 1.57622e-06, min loss: 8.23945e-07\n",
      "Epoch: 1377000, elapsed: 1.12e+01, train loss: 1.20409e-06, val loss: 2.15014e-06, min loss: 8.23945e-07\n",
      "Epoch: 1377100, elapsed: 1.10e+01, train loss: 8.22700e-07, val loss: 1.58235e-06, min loss: 8.22700e-07\n",
      "Epoch: 1377200, elapsed: 1.11e+01, train loss: 9.82324e-07, val loss: 1.61898e-06, min loss: 8.22700e-07\n",
      "Epoch: 1377300, elapsed: 1.12e+01, train loss: 8.38418e-07, val loss: 1.57731e-06, min loss: 8.22700e-07\n",
      "Epoch: 1377400, elapsed: 1.09e+01, train loss: 8.52106e-07, val loss: 1.63751e-06, min loss: 8.22700e-07\n",
      "Epoch: 1377500, elapsed: 1.08e+01, train loss: 8.28187e-07, val loss: 1.58175e-06, min loss: 8.22700e-07\n",
      "Epoch: 1377600, elapsed: 1.10e+01, train loss: 9.12683e-07, val loss: 1.65179e-06, min loss: 8.22700e-07\n",
      "Epoch: 1377700, elapsed: 1.10e+01, train loss: 8.22604e-07, val loss: 1.58427e-06, min loss: 8.22604e-07\n",
      "Epoch: 1377800, elapsed: 1.10e+01, train loss: 8.34249e-07, val loss: 1.58689e-06, min loss: 8.22604e-07\n",
      "Epoch: 1377900, elapsed: 1.50e+01, train loss: 9.33746e-07, val loss: 1.70980e-06, min loss: 8.22604e-07\n",
      "Epoch: 1378000, elapsed: 1.10e+01, train loss: 8.58343e-07, val loss: 1.64770e-06, min loss: 8.22604e-07\n",
      "Epoch: 1378100, elapsed: 1.11e+01, train loss: 1.04221e-06, val loss: 1.81465e-06, min loss: 8.22604e-07\n",
      "Epoch: 1378200, elapsed: 1.09e+01, train loss: 8.36501e-07, val loss: 1.60266e-06, min loss: 8.22604e-07\n",
      "Epoch: 1378300, elapsed: 1.08e+01, train loss: 8.24094e-07, val loss: 1.58654e-06, min loss: 8.22604e-07\n",
      "Epoch: 1378400, elapsed: 1.10e+01, train loss: 8.27590e-07, val loss: 1.57999e-06, min loss: 8.22604e-07\n",
      "Epoch: 1378500, elapsed: 1.12e+01, train loss: 8.78797e-07, val loss: 1.60210e-06, min loss: 8.22604e-07\n",
      "Epoch: 1378600, elapsed: 1.10e+01, train loss: 2.88227e-06, val loss: 3.99528e-06, min loss: 8.22604e-07\n",
      "Epoch: 1378700, elapsed: 1.09e+01, train loss: 9.63791e-07, val loss: 1.76067e-06, min loss: 8.22604e-07\n",
      "Epoch: 1378800, elapsed: 1.10e+01, train loss: 9.34269e-07, val loss: 1.63460e-06, min loss: 8.22604e-07\n",
      "Epoch: 1378900, elapsed: 1.10e+01, train loss: 1.03728e-06, val loss: 1.90705e-06, min loss: 8.22604e-07\n",
      "Epoch: 1379000, elapsed: 1.10e+01, train loss: 9.63748e-07, val loss: 1.72708e-06, min loss: 8.22604e-07\n",
      "Epoch: 1379100, elapsed: 1.12e+01, train loss: 8.39979e-07, val loss: 1.58958e-06, min loss: 8.22604e-07\n",
      "Epoch: 1379200, elapsed: 1.10e+01, train loss: 1.52773e-06, val loss: 1.95522e-06, min loss: 8.22604e-07\n",
      "Epoch: 1379300, elapsed: 1.10e+01, train loss: 1.02297e-06, val loss: 1.77445e-06, min loss: 8.22604e-07\n",
      "Epoch: 1379400, elapsed: 1.08e+01, train loss: 8.42689e-07, val loss: 1.56898e-06, min loss: 8.22604e-07\n",
      "Epoch: 1379500, elapsed: 1.11e+01, train loss: 8.56732e-07, val loss: 1.59549e-06, min loss: 8.22604e-07\n",
      "Epoch: 1379600, elapsed: 1.09e+01, train loss: 1.28716e-06, val loss: 2.11533e-06, min loss: 8.22604e-07\n",
      "Epoch: 1379700, elapsed: 1.10e+01, train loss: 1.14342e-06, val loss: 2.05896e-06, min loss: 8.22604e-07\n",
      "Epoch: 1379800, elapsed: 1.09e+01, train loss: 9.76947e-07, val loss: 1.80543e-06, min loss: 8.22604e-07\n",
      "Epoch: 1379900, elapsed: 1.09e+01, train loss: 8.21214e-07, val loss: 1.57500e-06, min loss: 8.21214e-07\n",
      "Epoch: 1380000, elapsed: 1.09e+01, train loss: 8.22556e-07, val loss: 1.57612e-06, min loss: 8.21214e-07\n",
      "Epoch: 1380100, elapsed: 1.28e+01, train loss: 8.30882e-07, val loss: 1.59111e-06, min loss: 8.21214e-07\n",
      "Epoch: 1380200, elapsed: 1.08e+01, train loss: 8.59047e-07, val loss: 1.64597e-06, min loss: 8.21214e-07\n",
      "Epoch: 1380300, elapsed: 1.09e+01, train loss: 1.33337e-06, val loss: 1.96930e-06, min loss: 8.21214e-07\n",
      "Epoch: 1380400, elapsed: 1.48e+01, train loss: 1.00289e-06, val loss: 1.72526e-06, min loss: 8.21214e-07\n",
      "Epoch: 1380500, elapsed: 1.09e+01, train loss: 8.68638e-07, val loss: 1.63468e-06, min loss: 8.21214e-07\n",
      "Epoch: 1380600, elapsed: 1.11e+01, train loss: 8.30100e-07, val loss: 1.59297e-06, min loss: 8.21214e-07\n",
      "Epoch: 1380700, elapsed: 1.10e+01, train loss: 1.53973e-06, val loss: 2.21288e-06, min loss: 8.21214e-07\n",
      "Epoch: 1380800, elapsed: 1.10e+01, train loss: 8.37360e-07, val loss: 1.61330e-06, min loss: 8.21214e-07\n",
      "Epoch: 1380900, elapsed: 1.11e+01, train loss: 8.69534e-07, val loss: 1.64613e-06, min loss: 8.21214e-07\n",
      "Epoch: 1381000, elapsed: 1.09e+01, train loss: 8.32123e-07, val loss: 1.58484e-06, min loss: 8.21214e-07\n",
      "Epoch: 1381100, elapsed: 1.10e+01, train loss: 8.27968e-07, val loss: 1.59274e-06, min loss: 8.21214e-07\n",
      "Epoch: 1381200, elapsed: 1.11e+01, train loss: 8.37069e-07, val loss: 1.57234e-06, min loss: 8.21214e-07\n",
      "Epoch: 1381300, elapsed: 1.12e+01, train loss: 8.47057e-07, val loss: 1.62879e-06, min loss: 8.21214e-07\n",
      "Epoch: 1381400, elapsed: 1.12e+01, train loss: 8.23236e-07, val loss: 1.57267e-06, min loss: 8.21214e-07\n",
      "Epoch: 1381500, elapsed: 1.09e+01, train loss: 8.21433e-07, val loss: 1.57950e-06, min loss: 8.21214e-07\n",
      "Epoch: 1381600, elapsed: 1.11e+01, train loss: 9.77847e-07, val loss: 1.68526e-06, min loss: 8.21214e-07\n",
      "Epoch: 1381700, elapsed: 1.09e+01, train loss: 1.52673e-06, val loss: 2.57259e-06, min loss: 8.21214e-07\n",
      "Epoch: 1381800, elapsed: 1.09e+01, train loss: 1.03550e-06, val loss: 1.75944e-06, min loss: 8.21214e-07\n",
      "Epoch: 1381900, elapsed: 1.09e+01, train loss: 8.20586e-07, val loss: 1.56996e-06, min loss: 8.20586e-07\n",
      "Epoch: 1382000, elapsed: 1.12e+01, train loss: 2.01440e-06, val loss: 3.42634e-06, min loss: 8.20586e-07\n",
      "Epoch: 1382100, elapsed: 1.12e+01, train loss: 8.19613e-07, val loss: 1.57609e-06, min loss: 8.19613e-07\n",
      "Epoch: 1382200, elapsed: 1.10e+01, train loss: 8.31576e-07, val loss: 1.57920e-06, min loss: 8.19613e-07\n",
      "Epoch: 1382300, elapsed: 1.09e+01, train loss: 1.42214e-06, val loss: 2.03768e-06, min loss: 8.19613e-07\n",
      "Epoch: 1382400, elapsed: 1.11e+01, train loss: 1.05690e-06, val loss: 1.72946e-06, min loss: 8.19613e-07\n",
      "Epoch: 1382500, elapsed: 1.09e+01, train loss: 1.13464e-06, val loss: 1.90285e-06, min loss: 8.19613e-07\n",
      "Epoch: 1382600, elapsed: 1.08e+01, train loss: 1.47652e-06, val loss: 2.07032e-06, min loss: 8.19613e-07\n",
      "Epoch: 1382700, elapsed: 1.10e+01, train loss: 8.92498e-07, val loss: 1.61576e-06, min loss: 8.19613e-07\n",
      "Epoch: 1382800, elapsed: 1.10e+01, train loss: 8.30562e-07, val loss: 1.59405e-06, min loss: 8.19613e-07\n",
      "Epoch: 1382900, elapsed: 1.47e+01, train loss: 8.31355e-07, val loss: 1.57296e-06, min loss: 8.19613e-07\n",
      "Epoch: 1383000, elapsed: 1.11e+01, train loss: 9.62908e-07, val loss: 1.82392e-06, min loss: 8.19613e-07\n",
      "Epoch: 1383100, elapsed: 1.12e+01, train loss: 9.88070e-07, val loss: 2.02426e-06, min loss: 8.19613e-07\n",
      "Epoch: 1383200, elapsed: 1.10e+01, train loss: 2.22346e-06, val loss: 3.17861e-06, min loss: 8.19613e-07\n",
      "Epoch: 1383300, elapsed: 1.12e+01, train loss: 8.34292e-07, val loss: 1.57199e-06, min loss: 8.19613e-07\n",
      "Epoch: 1383400, elapsed: 1.12e+01, train loss: 8.20572e-07, val loss: 1.58431e-06, min loss: 8.19613e-07\n",
      "Epoch: 1383500, elapsed: 1.10e+01, train loss: 8.21207e-07, val loss: 1.57761e-06, min loss: 8.19613e-07\n",
      "Epoch: 1383600, elapsed: 1.11e+01, train loss: 8.87413e-07, val loss: 1.64162e-06, min loss: 8.19613e-07\n",
      "Epoch: 1383700, elapsed: 1.12e+01, train loss: 3.05444e-06, val loss: 2.72845e-06, min loss: 8.19613e-07\n",
      "Epoch: 1383800, elapsed: 1.12e+01, train loss: 1.07521e-06, val loss: 1.96239e-06, min loss: 8.19613e-07\n",
      "Epoch: 1383900, elapsed: 1.11e+01, train loss: 8.22597e-07, val loss: 1.57656e-06, min loss: 8.19613e-07\n",
      "Epoch: 1384000, elapsed: 1.11e+01, train loss: 8.20982e-07, val loss: 1.57522e-06, min loss: 8.19613e-07\n",
      "Epoch: 1384100, elapsed: 1.10e+01, train loss: 8.69184e-07, val loss: 1.63125e-06, min loss: 8.19613e-07\n",
      "Epoch: 1384200, elapsed: 1.09e+01, train loss: 4.82486e-06, val loss: 5.70130e-06, min loss: 8.19613e-07\n",
      "Epoch: 1384300, elapsed: 1.08e+01, train loss: 8.21578e-07, val loss: 1.58504e-06, min loss: 8.19613e-07\n",
      "Epoch: 1384400, elapsed: 1.08e+01, train loss: 8.19335e-07, val loss: 1.57737e-06, min loss: 8.19335e-07\n",
      "Epoch: 1384500, elapsed: 1.10e+01, train loss: 9.37604e-07, val loss: 1.62533e-06, min loss: 8.19335e-07\n",
      "Epoch: 1384600, elapsed: 1.09e+01, train loss: 1.80295e-06, val loss: 2.70234e-06, min loss: 8.19335e-07\n",
      "Epoch: 1384700, elapsed: 1.09e+01, train loss: 8.30052e-07, val loss: 1.61881e-06, min loss: 8.19335e-07\n",
      "Epoch: 1384800, elapsed: 1.10e+01, train loss: 8.60403e-07, val loss: 1.58373e-06, min loss: 8.19335e-07\n",
      "Epoch: 1384900, elapsed: 1.09e+01, train loss: 9.15531e-07, val loss: 1.56414e-06, min loss: 8.19335e-07\n",
      "Epoch: 1385000, elapsed: 1.08e+01, train loss: 8.62531e-07, val loss: 1.65572e-06, min loss: 8.19335e-07\n",
      "Epoch: 1385100, elapsed: 1.28e+01, train loss: 1.79454e-06, val loss: 2.70986e-06, min loss: 8.19335e-07\n",
      "Epoch: 1385200, elapsed: 1.10e+01, train loss: 8.18397e-07, val loss: 1.57055e-06, min loss: 8.18397e-07\n",
      "Epoch: 1385300, elapsed: 1.09e+01, train loss: 8.18730e-07, val loss: 1.57176e-06, min loss: 8.18397e-07\n",
      "Epoch: 1385400, elapsed: 1.48e+01, train loss: 1.34062e-06, val loss: 2.14475e-06, min loss: 8.18397e-07\n",
      "Epoch: 1385500, elapsed: 1.09e+01, train loss: 8.17415e-07, val loss: 1.57049e-06, min loss: 8.17415e-07\n",
      "Epoch: 1385600, elapsed: 1.11e+01, train loss: 8.71234e-07, val loss: 1.64678e-06, min loss: 8.17415e-07\n",
      "Epoch: 1385700, elapsed: 1.10e+01, train loss: 2.21595e-06, val loss: 3.04754e-06, min loss: 8.17415e-07\n",
      "Epoch: 1385800, elapsed: 1.12e+01, train loss: 8.47384e-07, val loss: 1.61450e-06, min loss: 8.17415e-07\n",
      "Epoch: 1385900, elapsed: 1.11e+01, train loss: 8.49480e-07, val loss: 1.61581e-06, min loss: 8.17415e-07\n",
      "Epoch: 1386000, elapsed: 1.11e+01, train loss: 8.20581e-07, val loss: 1.57433e-06, min loss: 8.17415e-07\n",
      "Epoch: 1386100, elapsed: 1.11e+01, train loss: 9.14308e-07, val loss: 1.63981e-06, min loss: 8.17415e-07\n",
      "Epoch: 1386200, elapsed: 1.12e+01, train loss: 8.34705e-07, val loss: 1.61525e-06, min loss: 8.17415e-07\n",
      "Epoch: 1386300, elapsed: 1.13e+01, train loss: 9.00413e-07, val loss: 1.63735e-06, min loss: 8.17415e-07\n",
      "Epoch: 1386400, elapsed: 1.11e+01, train loss: 9.19236e-07, val loss: 1.69783e-06, min loss: 8.17415e-07\n",
      "Epoch: 1386500, elapsed: 1.11e+01, train loss: 1.29556e-06, val loss: 2.05099e-06, min loss: 8.17415e-07\n",
      "Epoch: 1386600, elapsed: 1.10e+01, train loss: 9.70924e-07, val loss: 1.80067e-06, min loss: 8.17415e-07\n",
      "Epoch: 1386700, elapsed: 1.11e+01, train loss: 8.19569e-07, val loss: 1.58110e-06, min loss: 8.17415e-07\n",
      "Epoch: 1386800, elapsed: 1.10e+01, train loss: 8.21151e-07, val loss: 1.57111e-06, min loss: 8.17415e-07\n",
      "Epoch: 1386900, elapsed: 1.11e+01, train loss: 3.44830e-06, val loss: 3.95715e-06, min loss: 8.17415e-07\n",
      "Epoch: 1387000, elapsed: 1.11e+01, train loss: 8.23401e-07, val loss: 1.58249e-06, min loss: 8.17415e-07\n",
      "Epoch: 1387100, elapsed: 1.10e+01, train loss: 8.37271e-07, val loss: 1.56027e-06, min loss: 8.17415e-07\n",
      "Epoch: 1387200, elapsed: 1.09e+01, train loss: 8.54392e-07, val loss: 1.64542e-06, min loss: 8.17415e-07\n",
      "Epoch: 1387300, elapsed: 1.11e+01, train loss: 1.58904e-06, val loss: 2.38157e-06, min loss: 8.17415e-07\n",
      "Epoch: 1387400, elapsed: 1.08e+01, train loss: 8.16215e-07, val loss: 1.56694e-06, min loss: 8.16215e-07\n",
      "Epoch: 1387500, elapsed: 1.07e+01, train loss: 8.24553e-07, val loss: 1.56598e-06, min loss: 8.16215e-07\n",
      "Epoch: 1387600, elapsed: 1.10e+01, train loss: 1.02525e-06, val loss: 1.75159e-06, min loss: 8.16215e-07\n",
      "Epoch: 1387700, elapsed: 1.09e+01, train loss: 1.92100e-06, val loss: 2.49373e-06, min loss: 8.16215e-07\n",
      "Epoch: 1387800, elapsed: 1.10e+01, train loss: 8.24832e-07, val loss: 1.57535e-06, min loss: 8.16215e-07\n",
      "Epoch: 1387900, elapsed: 1.46e+01, train loss: 8.34883e-07, val loss: 1.57881e-06, min loss: 8.16215e-07\n",
      "Epoch: 1388000, elapsed: 1.13e+01, train loss: 8.37186e-07, val loss: 1.61607e-06, min loss: 8.16215e-07\n",
      "Epoch: 1388100, elapsed: 1.11e+01, train loss: 8.22187e-07, val loss: 1.58026e-06, min loss: 8.16215e-07\n",
      "Epoch: 1388200, elapsed: 1.10e+01, train loss: 8.38781e-07, val loss: 1.61975e-06, min loss: 8.16215e-07\n",
      "Epoch: 1388300, elapsed: 1.12e+01, train loss: 8.17732e-07, val loss: 1.56818e-06, min loss: 8.16215e-07\n",
      "Epoch: 1388400, elapsed: 1.11e+01, train loss: 8.81983e-07, val loss: 1.68284e-06, min loss: 8.16215e-07\n",
      "Epoch: 1388500, elapsed: 1.11e+01, train loss: 3.24978e-06, val loss: 4.70688e-06, min loss: 8.16215e-07\n",
      "Epoch: 1388600, elapsed: 1.10e+01, train loss: 8.15941e-07, val loss: 1.56480e-06, min loss: 8.15941e-07\n",
      "Epoch: 1388700, elapsed: 1.09e+01, train loss: 8.16842e-07, val loss: 1.57698e-06, min loss: 8.15941e-07\n",
      "Epoch: 1388800, elapsed: 1.12e+01, train loss: 8.55461e-07, val loss: 1.62544e-06, min loss: 8.15941e-07\n",
      "Epoch: 1388900, elapsed: 1.11e+01, train loss: 1.03408e-06, val loss: 1.91414e-06, min loss: 8.15941e-07\n",
      "Epoch: 1389000, elapsed: 1.10e+01, train loss: 1.22978e-06, val loss: 1.95521e-06, min loss: 8.15941e-07\n",
      "Epoch: 1389100, elapsed: 1.11e+01, train loss: 8.17193e-07, val loss: 1.56984e-06, min loss: 8.15941e-07\n",
      "Epoch: 1389200, elapsed: 1.11e+01, train loss: 8.66207e-07, val loss: 1.56847e-06, min loss: 8.15941e-07\n",
      "Epoch: 1389300, elapsed: 1.10e+01, train loss: 1.43700e-06, val loss: 2.49755e-06, min loss: 8.15941e-07\n",
      "Epoch: 1389400, elapsed: 1.11e+01, train loss: 1.39105e-06, val loss: 2.10675e-06, min loss: 8.15941e-07\n",
      "Epoch: 1389500, elapsed: 1.09e+01, train loss: 8.75850e-07, val loss: 1.66016e-06, min loss: 8.15941e-07\n",
      "Epoch: 1389600, elapsed: 1.10e+01, train loss: 8.36993e-07, val loss: 1.60283e-06, min loss: 8.15941e-07\n",
      "Epoch: 1389700, elapsed: 1.08e+01, train loss: 8.50065e-07, val loss: 1.59260e-06, min loss: 8.15941e-07\n",
      "Epoch: 1389800, elapsed: 1.12e+01, train loss: 9.39052e-07, val loss: 1.69205e-06, min loss: 8.15941e-07\n",
      "Epoch: 1389900, elapsed: 1.11e+01, train loss: 9.37790e-07, val loss: 1.87626e-06, min loss: 8.15941e-07\n",
      "Epoch: 1390000, elapsed: 1.09e+01, train loss: 1.18958e-06, val loss: 1.95827e-06, min loss: 8.15941e-07\n",
      "Epoch: 1390100, elapsed: 1.29e+01, train loss: 8.68689e-07, val loss: 1.66431e-06, min loss: 8.15941e-07\n",
      "Epoch: 1390200, elapsed: 1.09e+01, train loss: 8.17634e-07, val loss: 1.56911e-06, min loss: 8.15941e-07\n",
      "Epoch: 1390300, elapsed: 1.11e+01, train loss: 8.15252e-07, val loss: 1.57014e-06, min loss: 8.15252e-07\n",
      "Epoch: 1390400, elapsed: 1.49e+01, train loss: 8.23934e-07, val loss: 1.57976e-06, min loss: 8.15252e-07\n",
      "Epoch: 1390500, elapsed: 1.11e+01, train loss: 8.25509e-07, val loss: 1.60509e-06, min loss: 8.15252e-07\n",
      "Epoch: 1390600, elapsed: 1.13e+01, train loss: 8.49092e-07, val loss: 1.57040e-06, min loss: 8.15252e-07\n",
      "Epoch: 1390700, elapsed: 1.12e+01, train loss: 1.00193e-06, val loss: 1.63604e-06, min loss: 8.15252e-07\n",
      "Epoch: 1390800, elapsed: 1.11e+01, train loss: 8.54321e-07, val loss: 1.62941e-06, min loss: 8.15252e-07\n",
      "Epoch: 1390900, elapsed: 1.11e+01, train loss: 8.46853e-07, val loss: 1.56691e-06, min loss: 8.15252e-07\n",
      "Epoch: 1391000, elapsed: 1.10e+01, train loss: 9.26000e-07, val loss: 1.70955e-06, min loss: 8.15252e-07\n",
      "Epoch: 1391100, elapsed: 1.10e+01, train loss: 8.95968e-07, val loss: 1.72756e-06, min loss: 8.15252e-07\n",
      "Epoch: 1391200, elapsed: 1.10e+01, train loss: 8.52372e-07, val loss: 1.64689e-06, min loss: 8.15252e-07\n",
      "Epoch: 1391300, elapsed: 1.10e+01, train loss: 8.24298e-07, val loss: 1.57781e-06, min loss: 8.15252e-07\n",
      "Epoch: 1391400, elapsed: 1.09e+01, train loss: 2.74363e-06, val loss: 2.44329e-06, min loss: 8.15252e-07\n",
      "Epoch: 1391500, elapsed: 1.09e+01, train loss: 1.42739e-06, val loss: 1.99296e-06, min loss: 8.15252e-07\n",
      "Epoch: 1391600, elapsed: 1.10e+01, train loss: 4.94487e-06, val loss: 4.28917e-06, min loss: 8.15252e-07\n",
      "Epoch: 1391700, elapsed: 1.09e+01, train loss: 8.79188e-07, val loss: 1.58560e-06, min loss: 8.15252e-07\n",
      "Epoch: 1391800, elapsed: 1.11e+01, train loss: 9.11003e-07, val loss: 1.67058e-06, min loss: 8.15252e-07\n",
      "Epoch: 1391900, elapsed: 1.11e+01, train loss: 2.68531e-06, val loss: 3.50239e-06, min loss: 8.15252e-07\n",
      "Epoch: 1392000, elapsed: 1.10e+01, train loss: 8.58411e-07, val loss: 1.58235e-06, min loss: 8.15252e-07\n",
      "Epoch: 1392100, elapsed: 1.10e+01, train loss: 8.25503e-07, val loss: 1.58118e-06, min loss: 8.15252e-07\n",
      "Epoch: 1392200, elapsed: 1.10e+01, train loss: 8.19337e-07, val loss: 1.56200e-06, min loss: 8.15252e-07\n",
      "Epoch: 1392300, elapsed: 1.11e+01, train loss: 8.33709e-07, val loss: 1.59231e-06, min loss: 8.15252e-07\n",
      "Epoch: 1392400, elapsed: 1.09e+01, train loss: 9.15743e-07, val loss: 1.69279e-06, min loss: 8.15252e-07\n",
      "Epoch: 1392500, elapsed: 1.11e+01, train loss: 8.24973e-07, val loss: 1.59511e-06, min loss: 8.15252e-07\n",
      "Epoch: 1392600, elapsed: 1.10e+01, train loss: 8.19335e-07, val loss: 1.55544e-06, min loss: 8.15252e-07\n",
      "Epoch: 1392700, elapsed: 1.10e+01, train loss: 8.31523e-07, val loss: 1.55669e-06, min loss: 8.15252e-07\n",
      "Epoch: 1392800, elapsed: 1.10e+01, train loss: 5.35380e-06, val loss: 5.97706e-06, min loss: 8.15252e-07\n",
      "Epoch: 1392900, elapsed: 1.10e+01, train loss: 9.16106e-07, val loss: 1.74544e-06, min loss: 8.15252e-07\n",
      "Epoch: 1393000, elapsed: 1.51e+01, train loss: 8.51375e-07, val loss: 1.60893e-06, min loss: 8.15252e-07\n",
      "Epoch: 1393100, elapsed: 1.13e+01, train loss: 2.25485e-06, val loss: 2.82761e-06, min loss: 8.15252e-07\n",
      "Epoch: 1393200, elapsed: 1.12e+01, train loss: 1.00755e-06, val loss: 1.80536e-06, min loss: 8.15252e-07\n",
      "Epoch: 1393300, elapsed: 1.11e+01, train loss: 1.09014e-06, val loss: 1.85790e-06, min loss: 8.15252e-07\n",
      "Epoch: 1393400, elapsed: 1.10e+01, train loss: 1.09193e-06, val loss: 1.78755e-06, min loss: 8.15252e-07\n",
      "Epoch: 1393500, elapsed: 1.11e+01, train loss: 8.20937e-07, val loss: 1.56364e-06, min loss: 8.15252e-07\n",
      "Epoch: 1393600, elapsed: 1.09e+01, train loss: 1.56290e-06, val loss: 2.50152e-06, min loss: 8.15252e-07\n",
      "Epoch: 1393700, elapsed: 1.09e+01, train loss: 9.41889e-07, val loss: 1.64805e-06, min loss: 8.15252e-07\n",
      "Epoch: 1393800, elapsed: 1.12e+01, train loss: 1.14697e-06, val loss: 1.81366e-06, min loss: 8.15252e-07\n",
      "Epoch: 1393900, elapsed: 1.12e+01, train loss: 1.40039e-06, val loss: 2.25162e-06, min loss: 8.15252e-07\n",
      "Epoch: 1394000, elapsed: 1.08e+01, train loss: 9.83528e-07, val loss: 1.73285e-06, min loss: 8.15252e-07\n",
      "Epoch: 1394100, elapsed: 1.10e+01, train loss: 8.20662e-07, val loss: 1.59650e-06, min loss: 8.15252e-07\n",
      "Epoch: 1394200, elapsed: 1.11e+01, train loss: 9.10619e-07, val loss: 1.66714e-06, min loss: 8.15252e-07\n",
      "Epoch: 1394300, elapsed: 1.09e+01, train loss: 9.13222e-07, val loss: 1.64694e-06, min loss: 8.15252e-07\n",
      "Epoch: 1394400, elapsed: 1.11e+01, train loss: 1.26004e-06, val loss: 2.23830e-06, min loss: 8.15252e-07\n",
      "Epoch: 1394500, elapsed: 1.11e+01, train loss: 1.33817e-06, val loss: 1.95817e-06, min loss: 8.15252e-07\n",
      "Epoch: 1394600, elapsed: 1.10e+01, train loss: 9.05883e-07, val loss: 1.72636e-06, min loss: 8.15252e-07\n",
      "Epoch: 1394700, elapsed: 1.10e+01, train loss: 8.29635e-07, val loss: 1.56047e-06, min loss: 8.15252e-07\n",
      "Epoch: 1394800, elapsed: 1.12e+01, train loss: 8.46194e-07, val loss: 1.61578e-06, min loss: 8.15252e-07\n",
      "Epoch: 1394900, elapsed: 1.09e+01, train loss: 8.12525e-07, val loss: 1.55211e-06, min loss: 8.12525e-07\n",
      "Epoch: 1395000, elapsed: 1.11e+01, train loss: 8.11803e-07, val loss: 1.55262e-06, min loss: 8.11803e-07\n",
      "Epoch: 1395100, elapsed: 1.29e+01, train loss: 9.14721e-07, val loss: 1.80879e-06, min loss: 8.11803e-07\n",
      "Epoch: 1395200, elapsed: 1.09e+01, train loss: 8.17156e-07, val loss: 1.55365e-06, min loss: 8.11803e-07\n",
      "Epoch: 1395300, elapsed: 1.09e+01, train loss: 8.11766e-07, val loss: 1.56033e-06, min loss: 8.11766e-07\n",
      "Epoch: 1395400, elapsed: 1.08e+01, train loss: 9.61260e-07, val loss: 1.61522e-06, min loss: 8.11766e-07\n",
      "Epoch: 1395500, elapsed: 1.51e+01, train loss: 8.45619e-07, val loss: 1.56870e-06, min loss: 8.11766e-07\n",
      "Epoch: 1395600, elapsed: 1.14e+01, train loss: 8.27380e-07, val loss: 1.55367e-06, min loss: 8.11766e-07\n",
      "Epoch: 1395700, elapsed: 1.12e+01, train loss: 1.21778e-06, val loss: 2.12814e-06, min loss: 8.11766e-07\n",
      "Epoch: 1395800, elapsed: 1.11e+01, train loss: 1.01932e-06, val loss: 1.70822e-06, min loss: 8.11766e-07\n",
      "Epoch: 1395900, elapsed: 1.11e+01, train loss: 1.00442e-06, val loss: 1.63697e-06, min loss: 8.11766e-07\n",
      "Epoch: 1396000, elapsed: 1.10e+01, train loss: 8.97024e-07, val loss: 1.77617e-06, min loss: 8.11766e-07\n",
      "Epoch: 1396100, elapsed: 1.10e+01, train loss: 1.01548e-06, val loss: 1.92297e-06, min loss: 8.11766e-07\n",
      "Epoch: 1396200, elapsed: 1.10e+01, train loss: 8.15024e-07, val loss: 1.57216e-06, min loss: 8.11766e-07\n",
      "Epoch: 1396300, elapsed: 1.11e+01, train loss: 8.11894e-07, val loss: 1.55150e-06, min loss: 8.11766e-07\n",
      "Epoch: 1396400, elapsed: 1.12e+01, train loss: 8.19200e-07, val loss: 1.55051e-06, min loss: 8.11766e-07\n",
      "Epoch: 1396500, elapsed: 1.09e+01, train loss: 8.35517e-07, val loss: 1.59132e-06, min loss: 8.11766e-07\n",
      "Epoch: 1396600, elapsed: 1.08e+01, train loss: 8.13873e-07, val loss: 1.56262e-06, min loss: 8.11766e-07\n",
      "Epoch: 1396700, elapsed: 1.10e+01, train loss: 8.12773e-07, val loss: 1.55436e-06, min loss: 8.11766e-07\n",
      "Epoch: 1396800, elapsed: 1.09e+01, train loss: 2.31588e-06, val loss: 3.23005e-06, min loss: 8.11766e-07\n",
      "Epoch: 1396900, elapsed: 1.12e+01, train loss: 9.06107e-07, val loss: 1.62243e-06, min loss: 8.11766e-07\n",
      "Epoch: 1397000, elapsed: 1.10e+01, train loss: 9.79589e-07, val loss: 1.65037e-06, min loss: 8.11766e-07\n",
      "Epoch: 1397100, elapsed: 1.09e+01, train loss: 8.28105e-07, val loss: 1.58622e-06, min loss: 8.11766e-07\n",
      "Epoch: 1397200, elapsed: 1.09e+01, train loss: 8.58272e-07, val loss: 1.62576e-06, min loss: 8.11766e-07\n",
      "Epoch: 1397300, elapsed: 1.10e+01, train loss: 1.71174e-06, val loss: 2.29742e-06, min loss: 8.11766e-07\n",
      "Epoch: 1397400, elapsed: 1.11e+01, train loss: 1.13039e-06, val loss: 1.88863e-06, min loss: 8.11766e-07\n",
      "Epoch: 1397500, elapsed: 1.09e+01, train loss: 8.95277e-07, val loss: 1.69861e-06, min loss: 8.11766e-07\n",
      "Epoch: 1397600, elapsed: 1.08e+01, train loss: 8.09769e-07, val loss: 1.55347e-06, min loss: 8.09769e-07\n",
      "Epoch: 1397700, elapsed: 1.08e+01, train loss: 8.10807e-07, val loss: 1.55710e-06, min loss: 8.09769e-07\n",
      "Epoch: 1397800, elapsed: 1.09e+01, train loss: 8.15163e-07, val loss: 1.56952e-06, min loss: 8.09769e-07\n",
      "Epoch: 1397900, elapsed: 1.08e+01, train loss: 8.09978e-07, val loss: 1.55021e-06, min loss: 8.09769e-07\n",
      "Epoch: 1398000, elapsed: 1.10e+01, train loss: 8.14415e-07, val loss: 1.54478e-06, min loss: 8.09769e-07\n",
      "Epoch: 1398100, elapsed: 1.51e+01, train loss: 8.95007e-07, val loss: 1.57096e-06, min loss: 8.09769e-07\n",
      "Epoch: 1398200, elapsed: 1.11e+01, train loss: 8.53717e-07, val loss: 1.63615e-06, min loss: 8.09769e-07\n",
      "Epoch: 1398300, elapsed: 1.10e+01, train loss: 2.42808e-06, val loss: 2.70574e-06, min loss: 8.09769e-07\n",
      "Epoch: 1398400, elapsed: 1.09e+01, train loss: 1.01969e-06, val loss: 1.89097e-06, min loss: 8.09769e-07\n",
      "Epoch: 1398500, elapsed: 1.10e+01, train loss: 3.57753e-06, val loss: 4.02788e-06, min loss: 8.09769e-07\n",
      "Epoch: 1398600, elapsed: 1.12e+01, train loss: 1.60994e-06, val loss: 2.69663e-06, min loss: 8.09769e-07\n",
      "Epoch: 1398700, elapsed: 1.11e+01, train loss: 8.09634e-07, val loss: 1.54914e-06, min loss: 8.09634e-07\n",
      "Epoch: 1398800, elapsed: 1.12e+01, train loss: 8.15504e-07, val loss: 1.55137e-06, min loss: 8.09634e-07\n",
      "Epoch: 1398900, elapsed: 1.10e+01, train loss: 8.30197e-07, val loss: 1.55930e-06, min loss: 8.09634e-07\n",
      "Epoch: 1399000, elapsed: 1.11e+01, train loss: 8.09087e-07, val loss: 1.55289e-06, min loss: 8.09087e-07\n",
      "Epoch: 1399100, elapsed: 1.09e+01, train loss: 2.73483e-06, val loss: 4.07149e-06, min loss: 8.09087e-07\n",
      "Epoch: 1399200, elapsed: 1.09e+01, train loss: 8.14029e-07, val loss: 1.57387e-06, min loss: 8.09087e-07\n",
      "Epoch: 1399300, elapsed: 1.09e+01, train loss: 8.83859e-07, val loss: 1.68151e-06, min loss: 8.09087e-07\n",
      "Epoch: 1399400, elapsed: 1.11e+01, train loss: 8.62113e-07, val loss: 1.58180e-06, min loss: 8.09087e-07\n",
      "Epoch: 1399500, elapsed: 1.10e+01, train loss: 1.26452e-06, val loss: 2.05662e-06, min loss: 8.09087e-07\n",
      "Epoch: 1399600, elapsed: 1.09e+01, train loss: 1.05813e-06, val loss: 1.79905e-06, min loss: 8.09087e-07\n",
      "Epoch: 1399700, elapsed: 1.09e+01, train loss: 9.18358e-07, val loss: 1.73799e-06, min loss: 8.09087e-07\n",
      "Epoch: 1399800, elapsed: 1.08e+01, train loss: 1.03004e-06, val loss: 1.85616e-06, min loss: 8.09087e-07\n",
      "Epoch: 1399900, elapsed: 1.10e+01, train loss: 8.10271e-07, val loss: 1.54633e-06, min loss: 8.09087e-07\n",
      "Epoch: 1400000, elapsed: 1.06e+01, train loss: 8.48587e-07, val loss: 1.57728e-06, min loss: 8.09087e-07\n",
      "Epoch: 1400100, elapsed: 1.28e+01, train loss: 8.26915e-07, val loss: 1.58399e-06, min loss: 8.09087e-07\n",
      "Epoch: 1400200, elapsed: 1.09e+01, train loss: 8.30034e-07, val loss: 1.58392e-06, min loss: 8.09087e-07\n",
      "Epoch: 1400300, elapsed: 1.08e+01, train loss: 9.92515e-07, val loss: 1.69685e-06, min loss: 8.09087e-07\n",
      "Epoch: 1400400, elapsed: 1.09e+01, train loss: 2.48139e-06, val loss: 3.16601e-06, min loss: 8.09087e-07\n",
      "Epoch: 1400500, elapsed: 1.08e+01, train loss: 8.75640e-07, val loss: 1.57352e-06, min loss: 8.09087e-07\n",
      "Epoch: 1400600, elapsed: 1.51e+01, train loss: 1.11760e-06, val loss: 1.95210e-06, min loss: 8.09087e-07\n",
      "Epoch: 1400700, elapsed: 1.11e+01, train loss: 8.08267e-07, val loss: 1.55196e-06, min loss: 8.08267e-07\n",
      "Epoch: 1400800, elapsed: 1.11e+01, train loss: 8.09199e-07, val loss: 1.54365e-06, min loss: 8.08267e-07\n",
      "Epoch: 1400900, elapsed: 1.10e+01, train loss: 1.26097e-06, val loss: 2.14079e-06, min loss: 8.08267e-07\n",
      "Epoch: 1401000, elapsed: 1.12e+01, train loss: 2.60071e-06, val loss: 3.78935e-06, min loss: 8.08267e-07\n",
      "Epoch: 1401100, elapsed: 1.11e+01, train loss: 1.05011e-06, val loss: 1.96919e-06, min loss: 8.08267e-07\n",
      "Epoch: 1401200, elapsed: 1.09e+01, train loss: 8.60386e-07, val loss: 1.57495e-06, min loss: 8.08267e-07\n",
      "Epoch: 1401300, elapsed: 1.11e+01, train loss: 8.10882e-07, val loss: 1.54221e-06, min loss: 8.08267e-07\n",
      "Epoch: 1401400, elapsed: 1.11e+01, train loss: 9.36236e-07, val loss: 1.69364e-06, min loss: 8.08267e-07\n",
      "Epoch: 1401500, elapsed: 1.11e+01, train loss: 8.39169e-07, val loss: 1.59802e-06, min loss: 8.08267e-07\n",
      "Epoch: 1401600, elapsed: 1.08e+01, train loss: 8.20616e-07, val loss: 1.54997e-06, min loss: 8.08267e-07\n",
      "Epoch: 1401700, elapsed: 1.14e+01, train loss: 8.25672e-07, val loss: 1.55005e-06, min loss: 8.08267e-07\n",
      "Epoch: 1401800, elapsed: 1.09e+01, train loss: 2.27676e-06, val loss: 2.26055e-06, min loss: 8.08267e-07\n",
      "Epoch: 1401900, elapsed: 1.11e+01, train loss: 3.58777e-06, val loss: 4.04787e-06, min loss: 8.08267e-07\n",
      "Epoch: 1402000, elapsed: 1.11e+01, train loss: 1.62720e-06, val loss: 2.28403e-06, min loss: 8.08267e-07\n",
      "Epoch: 1402100, elapsed: 1.09e+01, train loss: 8.41323e-07, val loss: 1.59928e-06, min loss: 8.08267e-07\n",
      "Epoch: 1402200, elapsed: 1.12e+01, train loss: 2.28628e-06, val loss: 3.24662e-06, min loss: 8.08267e-07\n",
      "Epoch: 1402300, elapsed: 1.10e+01, train loss: 8.10466e-07, val loss: 1.55830e-06, min loss: 8.08267e-07\n",
      "Epoch: 1402400, elapsed: 1.11e+01, train loss: 8.09769e-07, val loss: 1.54497e-06, min loss: 8.08267e-07\n",
      "Epoch: 1402500, elapsed: 1.10e+01, train loss: 8.11487e-07, val loss: 1.56606e-06, min loss: 8.08267e-07\n",
      "Epoch: 1402600, elapsed: 1.09e+01, train loss: 1.02750e-06, val loss: 1.66451e-06, min loss: 8.08267e-07\n",
      "Epoch: 1402700, elapsed: 1.10e+01, train loss: 8.29280e-07, val loss: 1.60106e-06, min loss: 8.08267e-07\n",
      "Epoch: 1402800, elapsed: 1.10e+01, train loss: 8.09573e-07, val loss: 1.55106e-06, min loss: 8.08267e-07\n",
      "Epoch: 1402900, elapsed: 1.10e+01, train loss: 8.16699e-07, val loss: 1.55095e-06, min loss: 8.08267e-07\n",
      "Epoch: 1403000, elapsed: 1.10e+01, train loss: 8.24851e-07, val loss: 1.57266e-06, min loss: 8.08267e-07\n",
      "Epoch: 1403100, elapsed: 1.48e+01, train loss: 1.36103e-06, val loss: 1.97771e-06, min loss: 8.08267e-07\n",
      "Epoch: 1403200, elapsed: 1.11e+01, train loss: 1.07490e-06, val loss: 1.90420e-06, min loss: 8.08267e-07\n",
      "Epoch: 1403300, elapsed: 1.12e+01, train loss: 8.65968e-07, val loss: 1.65932e-06, min loss: 8.08267e-07\n",
      "Epoch: 1403400, elapsed: 1.11e+01, train loss: 8.06623e-07, val loss: 1.54853e-06, min loss: 8.06623e-07\n",
      "Epoch: 1403500, elapsed: 1.13e+01, train loss: 8.17535e-07, val loss: 1.53880e-06, min loss: 8.06623e-07\n",
      "Epoch: 1403600, elapsed: 1.12e+01, train loss: 8.25430e-07, val loss: 1.58165e-06, min loss: 8.06623e-07\n",
      "Epoch: 1403700, elapsed: 1.09e+01, train loss: 8.27732e-07, val loss: 1.57823e-06, min loss: 8.06623e-07\n",
      "Epoch: 1403800, elapsed: 1.12e+01, train loss: 8.14266e-07, val loss: 1.59745e-06, min loss: 8.06623e-07\n",
      "Epoch: 1403900, elapsed: 1.13e+01, train loss: 9.44268e-07, val loss: 1.65390e-06, min loss: 8.06623e-07\n",
      "Epoch: 1404000, elapsed: 1.12e+01, train loss: 1.20865e-06, val loss: 1.77272e-06, min loss: 8.06623e-07\n",
      "Epoch: 1404100, elapsed: 1.12e+01, train loss: 8.15910e-07, val loss: 1.55075e-06, min loss: 8.06623e-07\n",
      "Epoch: 1404200, elapsed: 1.11e+01, train loss: 8.69559e-07, val loss: 1.59157e-06, min loss: 8.06623e-07\n",
      "Epoch: 1404300, elapsed: 1.09e+01, train loss: 8.36522e-07, val loss: 1.60136e-06, min loss: 8.06623e-07\n",
      "Epoch: 1404400, elapsed: 1.10e+01, train loss: 9.64370e-07, val loss: 1.72932e-06, min loss: 8.06623e-07\n",
      "Epoch: 1404500, elapsed: 1.12e+01, train loss: 9.72128e-07, val loss: 1.58819e-06, min loss: 8.06623e-07\n",
      "Epoch: 1404600, elapsed: 1.12e+01, train loss: 1.78073e-06, val loss: 3.02318e-06, min loss: 8.06623e-07\n",
      "Epoch: 1404700, elapsed: 1.12e+01, train loss: 8.05458e-07, val loss: 1.54448e-06, min loss: 8.05458e-07\n",
      "Epoch: 1404800, elapsed: 1.11e+01, train loss: 8.08291e-07, val loss: 1.55619e-06, min loss: 8.05458e-07\n",
      "Epoch: 1404900, elapsed: 1.09e+01, train loss: 8.11854e-07, val loss: 1.53895e-06, min loss: 8.05458e-07\n",
      "Epoch: 1405000, elapsed: 1.09e+01, train loss: 8.19679e-07, val loss: 1.57012e-06, min loss: 8.05458e-07\n",
      "Epoch: 1405100, elapsed: 1.30e+01, train loss: 9.22582e-07, val loss: 1.61334e-06, min loss: 8.05458e-07\n",
      "Epoch: 1405200, elapsed: 1.09e+01, train loss: 1.00842e-06, val loss: 1.77868e-06, min loss: 8.05458e-07\n",
      "Epoch: 1405300, elapsed: 1.10e+01, train loss: 1.41292e-06, val loss: 2.26861e-06, min loss: 8.05458e-07\n",
      "Epoch: 1405400, elapsed: 1.10e+01, train loss: 9.53030e-07, val loss: 1.75155e-06, min loss: 8.05458e-07\n",
      "Epoch: 1405500, elapsed: 1.08e+01, train loss: 8.92907e-07, val loss: 1.63903e-06, min loss: 8.05458e-07\n",
      "Epoch: 1405600, elapsed: 1.10e+01, train loss: 1.60405e-06, val loss: 2.39281e-06, min loss: 8.05458e-07\n",
      "Epoch: 1405700, elapsed: 1.50e+01, train loss: 9.00970e-07, val loss: 1.64157e-06, min loss: 8.05458e-07\n",
      "Epoch: 1405800, elapsed: 1.13e+01, train loss: 8.66945e-07, val loss: 1.66866e-06, min loss: 8.05458e-07\n",
      "Epoch: 1405900, elapsed: 1.10e+01, train loss: 8.08013e-07, val loss: 1.55206e-06, min loss: 8.05458e-07\n",
      "Epoch: 1406000, elapsed: 1.09e+01, train loss: 8.04507e-07, val loss: 1.54307e-06, min loss: 8.04507e-07\n",
      "Epoch: 1406100, elapsed: 1.11e+01, train loss: 8.10591e-07, val loss: 1.54207e-06, min loss: 8.04507e-07\n",
      "Epoch: 1406200, elapsed: 1.10e+01, train loss: 2.99375e-06, val loss: 3.61672e-06, min loss: 8.04507e-07\n",
      "Epoch: 1406300, elapsed: 1.11e+01, train loss: 2.70751e-06, val loss: 2.59059e-06, min loss: 8.04507e-07\n",
      "Epoch: 1406400, elapsed: 1.09e+01, train loss: 9.15572e-07, val loss: 1.68274e-06, min loss: 8.04507e-07\n",
      "Epoch: 1406500, elapsed: 1.12e+01, train loss: 8.04242e-07, val loss: 1.53992e-06, min loss: 8.04242e-07\n",
      "Epoch: 1406600, elapsed: 1.11e+01, train loss: 8.14893e-07, val loss: 1.53124e-06, min loss: 8.04242e-07\n",
      "Epoch: 1406700, elapsed: 1.10e+01, train loss: 2.17398e-06, val loss: 2.20389e-06, min loss: 8.04242e-07\n",
      "Epoch: 1406800, elapsed: 1.13e+01, train loss: 8.04170e-07, val loss: 1.54200e-06, min loss: 8.04170e-07\n",
      "Epoch: 1406900, elapsed: 1.10e+01, train loss: 8.06471e-07, val loss: 1.53920e-06, min loss: 8.04170e-07\n",
      "Epoch: 1407000, elapsed: 1.11e+01, train loss: 8.32107e-07, val loss: 1.55740e-06, min loss: 8.04170e-07\n",
      "Epoch: 1407100, elapsed: 1.10e+01, train loss: 1.01750e-06, val loss: 1.88604e-06, min loss: 8.04170e-07\n",
      "Epoch: 1407200, elapsed: 1.09e+01, train loss: 8.12309e-07, val loss: 1.54478e-06, min loss: 8.04170e-07\n",
      "Epoch: 1407300, elapsed: 1.12e+01, train loss: 8.25769e-07, val loss: 1.57695e-06, min loss: 8.04170e-07\n",
      "Epoch: 1407400, elapsed: 1.11e+01, train loss: 8.90594e-07, val loss: 1.56810e-06, min loss: 8.04170e-07\n",
      "Epoch: 1407500, elapsed: 1.10e+01, train loss: 8.15988e-07, val loss: 1.56509e-06, min loss: 8.04170e-07\n",
      "Epoch: 1407600, elapsed: 1.10e+01, train loss: 8.07815e-07, val loss: 1.56399e-06, min loss: 8.04170e-07\n",
      "Epoch: 1407700, elapsed: 1.10e+01, train loss: 8.03670e-07, val loss: 1.54052e-06, min loss: 8.03670e-07\n",
      "Epoch: 1407800, elapsed: 1.09e+01, train loss: 9.93278e-07, val loss: 1.62000e-06, min loss: 8.03670e-07\n",
      "Epoch: 1407900, elapsed: 1.07e+01, train loss: 8.03089e-07, val loss: 1.53941e-06, min loss: 8.03089e-07\n",
      "Epoch: 1408000, elapsed: 1.09e+01, train loss: 8.23187e-07, val loss: 1.55076e-06, min loss: 8.03089e-07\n",
      "Epoch: 1408100, elapsed: 1.09e+01, train loss: 8.02958e-07, val loss: 1.53929e-06, min loss: 8.02958e-07\n",
      "Epoch: 1408200, elapsed: 1.09e+01, train loss: 8.04927e-07, val loss: 1.54349e-06, min loss: 8.02958e-07\n",
      "Epoch: 1408300, elapsed: 1.52e+01, train loss: 8.16709e-07, val loss: 1.54723e-06, min loss: 8.02958e-07\n",
      "Epoch: 1408400, elapsed: 1.12e+01, train loss: 1.08788e-05, val loss: 8.91130e-06, min loss: 8.02958e-07\n",
      "Epoch: 1408500, elapsed: 1.11e+01, train loss: 2.02920e-06, val loss: 2.49789e-06, min loss: 8.02958e-07\n",
      "Epoch: 1408600, elapsed: 1.12e+01, train loss: 1.62913e-06, val loss: 2.17979e-06, min loss: 8.02958e-07\n",
      "Epoch: 1408700, elapsed: 1.12e+01, train loss: 1.43142e-06, val loss: 2.02902e-06, min loss: 8.02958e-07\n",
      "Epoch: 1408800, elapsed: 1.11e+01, train loss: 1.29652e-06, val loss: 1.92166e-06, min loss: 8.02958e-07\n",
      "Epoch: 1408900, elapsed: 1.13e+01, train loss: 1.88954e-06, val loss: 3.18230e-06, min loss: 8.02958e-07\n",
      "Epoch: 1409000, elapsed: 1.11e+01, train loss: 1.12227e-06, val loss: 1.75687e-06, min loss: 8.02958e-07\n",
      "Epoch: 1409100, elapsed: 1.12e+01, train loss: 1.07351e-06, val loss: 1.71762e-06, min loss: 8.02958e-07\n",
      "Epoch: 1409200, elapsed: 1.12e+01, train loss: 1.03508e-06, val loss: 1.67371e-06, min loss: 8.02958e-07\n",
      "Epoch: 1409300, elapsed: 1.09e+01, train loss: 1.00616e-06, val loss: 1.64658e-06, min loss: 8.02958e-07\n",
      "Epoch: 1409400, elapsed: 1.10e+01, train loss: 9.83815e-07, val loss: 1.62672e-06, min loss: 8.02958e-07\n",
      "Epoch: 1409500, elapsed: 1.12e+01, train loss: 1.10558e-06, val loss: 1.72624e-06, min loss: 8.02958e-07\n",
      "Epoch: 1409600, elapsed: 1.11e+01, train loss: 1.29021e-06, val loss: 1.60101e-06, min loss: 8.02958e-07\n",
      "Epoch: 1409700, elapsed: 1.11e+01, train loss: 9.44662e-07, val loss: 1.59327e-06, min loss: 8.02958e-07\n",
      "Epoch: 1409800, elapsed: 1.12e+01, train loss: 9.32785e-07, val loss: 1.58230e-06, min loss: 8.02958e-07\n",
      "Epoch: 1409900, elapsed: 1.11e+01, train loss: 2.39549e-06, val loss: 2.31470e-06, min loss: 8.02958e-07\n",
      "Epoch: 1410000, elapsed: 1.12e+01, train loss: 2.68247e-06, val loss: 2.86727e-06, min loss: 8.02958e-07\n",
      "Epoch: 1410100, elapsed: 1.31e+01, train loss: 1.13467e-06, val loss: 1.60534e-06, min loss: 8.02958e-07\n",
      "Epoch: 1410200, elapsed: 1.11e+01, train loss: 1.08852e-06, val loss: 1.77804e-06, min loss: 8.02958e-07\n",
      "Epoch: 1410300, elapsed: 1.11e+01, train loss: 9.15954e-07, val loss: 1.57429e-06, min loss: 8.02958e-07\n",
      "Epoch: 1410400, elapsed: 1.10e+01, train loss: 4.74018e-06, val loss: 4.59866e-06, min loss: 8.02958e-07\n",
      "Epoch: 1410500, elapsed: 1.10e+01, train loss: 8.82361e-07, val loss: 1.54322e-06, min loss: 8.02958e-07\n",
      "Epoch: 1410600, elapsed: 1.10e+01, train loss: 8.81246e-07, val loss: 1.54288e-06, min loss: 8.02958e-07\n",
      "Epoch: 1410700, elapsed: 1.11e+01, train loss: 8.72802e-07, val loss: 1.54607e-06, min loss: 8.02958e-07\n",
      "Epoch: 1410800, elapsed: 1.51e+01, train loss: 9.01561e-07, val loss: 1.55580e-06, min loss: 8.02958e-07\n",
      "Epoch: 1410900, elapsed: 1.11e+01, train loss: 9.12128e-07, val loss: 1.57113e-06, min loss: 8.02958e-07\n",
      "Epoch: 1411000, elapsed: 1.12e+01, train loss: 8.72117e-07, val loss: 1.54456e-06, min loss: 8.02958e-07\n",
      "Epoch: 1411100, elapsed: 1.12e+01, train loss: 1.53812e-06, val loss: 2.32506e-06, min loss: 8.02958e-07\n",
      "Epoch: 1411200, elapsed: 1.12e+01, train loss: 1.12008e-06, val loss: 1.95172e-06, min loss: 8.02958e-07\n",
      "Epoch: 1411300, elapsed: 1.13e+01, train loss: 9.92563e-07, val loss: 1.59093e-06, min loss: 8.02958e-07\n",
      "Epoch: 1411400, elapsed: 1.12e+01, train loss: 9.26591e-07, val loss: 1.61376e-06, min loss: 8.02958e-07\n",
      "Epoch: 1411500, elapsed: 1.12e+01, train loss: 1.42754e-06, val loss: 2.08377e-06, min loss: 8.02958e-07\n",
      "Epoch: 1411600, elapsed: 1.10e+01, train loss: 8.75348e-07, val loss: 1.55766e-06, min loss: 8.02958e-07\n",
      "Epoch: 1411700, elapsed: 1.12e+01, train loss: 1.51147e-06, val loss: 2.10885e-06, min loss: 8.02958e-07\n",
      "Epoch: 1411800, elapsed: 1.11e+01, train loss: 9.45730e-07, val loss: 1.66973e-06, min loss: 8.02958e-07\n",
      "Epoch: 1411900, elapsed: 1.12e+01, train loss: 1.01728e-06, val loss: 1.70719e-06, min loss: 8.02958e-07\n",
      "Epoch: 1412000, elapsed: 1.11e+01, train loss: 8.41606e-07, val loss: 1.53324e-06, min loss: 8.02958e-07\n",
      "Epoch: 1412100, elapsed: 1.12e+01, train loss: 1.60420e-06, val loss: 2.56340e-06, min loss: 8.02958e-07\n",
      "Epoch: 1412200, elapsed: 1.11e+01, train loss: 9.07064e-07, val loss: 1.72717e-06, min loss: 8.02958e-07\n",
      "Epoch: 1412300, elapsed: 1.10e+01, train loss: 9.90672e-07, val loss: 1.62296e-06, min loss: 8.02958e-07\n",
      "Epoch: 1412400, elapsed: 1.10e+01, train loss: 1.80658e-06, val loss: 2.42457e-06, min loss: 8.02958e-07\n",
      "Epoch: 1412500, elapsed: 1.08e+01, train loss: 8.33667e-07, val loss: 1.53161e-06, min loss: 8.02958e-07\n",
      "Epoch: 1412600, elapsed: 1.09e+01, train loss: 1.62252e-06, val loss: 2.44370e-06, min loss: 8.02958e-07\n",
      "Epoch: 1412700, elapsed: 1.10e+01, train loss: 9.79614e-07, val loss: 1.65355e-06, min loss: 8.02958e-07\n",
      "Epoch: 1412800, elapsed: 1.10e+01, train loss: 8.41308e-07, val loss: 1.53050e-06, min loss: 8.02958e-07\n",
      "Epoch: 1412900, elapsed: 1.10e+01, train loss: 8.72057e-07, val loss: 1.57601e-06, min loss: 8.02958e-07\n",
      "Epoch: 1413000, elapsed: 1.07e+01, train loss: 8.37564e-07, val loss: 1.53456e-06, min loss: 8.02958e-07\n",
      "Epoch: 1413100, elapsed: 1.10e+01, train loss: 8.29095e-07, val loss: 1.53687e-06, min loss: 8.02958e-07\n",
      "Epoch: 1413200, elapsed: 1.10e+01, train loss: 8.32939e-07, val loss: 1.53657e-06, min loss: 8.02958e-07\n",
      "Epoch: 1413300, elapsed: 1.11e+01, train loss: 8.43593e-07, val loss: 1.56347e-06, min loss: 8.02958e-07\n",
      "Epoch: 1413400, elapsed: 1.53e+01, train loss: 8.33118e-07, val loss: 1.52277e-06, min loss: 8.02958e-07\n",
      "Epoch: 1413500, elapsed: 1.10e+01, train loss: 8.27291e-07, val loss: 1.52844e-06, min loss: 8.02958e-07\n",
      "Epoch: 1413600, elapsed: 1.10e+01, train loss: 1.95958e-06, val loss: 2.82780e-06, min loss: 8.02958e-07\n",
      "Epoch: 1413700, elapsed: 1.12e+01, train loss: 9.14731e-07, val loss: 1.62132e-06, min loss: 8.02958e-07\n",
      "Epoch: 1413800, elapsed: 1.12e+01, train loss: 8.64171e-07, val loss: 1.59007e-06, min loss: 8.02958e-07\n",
      "Epoch: 1413900, elapsed: 1.12e+01, train loss: 9.59151e-07, val loss: 1.70043e-06, min loss: 8.02958e-07\n",
      "Epoch: 1414000, elapsed: 1.13e+01, train loss: 8.26590e-07, val loss: 1.51782e-06, min loss: 8.02958e-07\n",
      "Epoch: 1414100, elapsed: 1.13e+01, train loss: 2.10819e-06, val loss: 3.07329e-06, min loss: 8.02958e-07\n",
      "Epoch: 1414200, elapsed: 1.11e+01, train loss: 9.76500e-07, val loss: 1.64495e-06, min loss: 8.02958e-07\n",
      "Epoch: 1414300, elapsed: 1.13e+01, train loss: 8.72636e-07, val loss: 1.51256e-06, min loss: 8.02958e-07\n",
      "Epoch: 1414400, elapsed: 1.10e+01, train loss: 1.35748e-06, val loss: 2.09858e-06, min loss: 8.02958e-07\n",
      "Epoch: 1414500, elapsed: 1.12e+01, train loss: 1.55827e-06, val loss: 2.27960e-06, min loss: 8.02958e-07\n",
      "Epoch: 1414600, elapsed: 1.10e+01, train loss: 1.51461e-06, val loss: 2.10900e-06, min loss: 8.02958e-07\n",
      "Epoch: 1414700, elapsed: 1.12e+01, train loss: 8.52367e-07, val loss: 1.57836e-06, min loss: 8.02958e-07\n",
      "Epoch: 1414800, elapsed: 1.11e+01, train loss: 1.44834e-06, val loss: 1.64466e-06, min loss: 8.02958e-07\n",
      "Epoch: 1414900, elapsed: 1.10e+01, train loss: 8.17397e-07, val loss: 1.51785e-06, min loss: 8.02958e-07\n",
      "Epoch: 1415000, elapsed: 1.10e+01, train loss: 8.25437e-07, val loss: 1.51606e-06, min loss: 8.02958e-07\n",
      "Epoch: 1415100, elapsed: 1.31e+01, train loss: 8.29166e-07, val loss: 1.53911e-06, min loss: 8.02958e-07\n",
      "Epoch: 1415200, elapsed: 1.11e+01, train loss: 9.66612e-07, val loss: 1.60866e-06, min loss: 8.02958e-07\n",
      "Epoch: 1415300, elapsed: 1.09e+01, train loss: 8.32997e-07, val loss: 1.54240e-06, min loss: 8.02958e-07\n",
      "Epoch: 1415400, elapsed: 1.09e+01, train loss: 9.31772e-07, val loss: 1.63041e-06, min loss: 8.02958e-07\n",
      "Epoch: 1415500, elapsed: 1.11e+01, train loss: 8.33843e-07, val loss: 1.55062e-06, min loss: 8.02958e-07\n",
      "Epoch: 1415600, elapsed: 1.09e+01, train loss: 8.34408e-07, val loss: 1.53687e-06, min loss: 8.02958e-07\n",
      "Epoch: 1415700, elapsed: 1.09e+01, train loss: 9.05575e-07, val loss: 1.67479e-06, min loss: 8.02958e-07\n",
      "Epoch: 1415800, elapsed: 1.08e+01, train loss: 8.30979e-07, val loss: 1.51444e-06, min loss: 8.02958e-07\n",
      "Epoch: 1415900, elapsed: 1.49e+01, train loss: 8.26228e-07, val loss: 1.51739e-06, min loss: 8.02958e-07\n",
      "Epoch: 1416000, elapsed: 1.12e+01, train loss: 8.88531e-07, val loss: 1.64882e-06, min loss: 8.02958e-07\n",
      "Epoch: 1416100, elapsed: 1.11e+01, train loss: 1.05399e-06, val loss: 1.71644e-06, min loss: 8.02958e-07\n",
      "Epoch: 1416200, elapsed: 1.15e+01, train loss: 8.23024e-07, val loss: 1.52037e-06, min loss: 8.02958e-07\n",
      "Epoch: 1416300, elapsed: 1.12e+01, train loss: 8.17194e-07, val loss: 1.52354e-06, min loss: 8.02958e-07\n",
      "Epoch: 1416400, elapsed: 1.11e+01, train loss: 1.98201e-06, val loss: 2.55254e-06, min loss: 8.02958e-07\n",
      "Epoch: 1416500, elapsed: 1.13e+01, train loss: 8.52701e-07, val loss: 1.55035e-06, min loss: 8.02958e-07\n",
      "Epoch: 1416600, elapsed: 1.10e+01, train loss: 8.12152e-07, val loss: 1.50465e-06, min loss: 8.02958e-07\n",
      "Epoch: 1416700, elapsed: 1.10e+01, train loss: 8.19163e-07, val loss: 1.51429e-06, min loss: 8.02958e-07\n",
      "Epoch: 1416800, elapsed: 1.10e+01, train loss: 8.14953e-07, val loss: 1.50962e-06, min loss: 8.02958e-07\n",
      "Epoch: 1416900, elapsed: 1.10e+01, train loss: 8.22873e-07, val loss: 1.51562e-06, min loss: 8.02958e-07\n",
      "Epoch: 1417000, elapsed: 1.10e+01, train loss: 1.21296e-06, val loss: 2.05236e-06, min loss: 8.02958e-07\n",
      "Epoch: 1417100, elapsed: 1.12e+01, train loss: 8.32807e-07, val loss: 1.55782e-06, min loss: 8.02958e-07\n",
      "Epoch: 1417200, elapsed: 1.11e+01, train loss: 9.26235e-07, val loss: 1.61262e-06, min loss: 8.02958e-07\n",
      "Epoch: 1417300, elapsed: 1.09e+01, train loss: 1.93551e-06, val loss: 2.83846e-06, min loss: 8.02958e-07\n",
      "Epoch: 1417400, elapsed: 1.11e+01, train loss: 1.25080e-06, val loss: 1.66153e-06, min loss: 8.02958e-07\n",
      "Epoch: 1417500, elapsed: 1.10e+01, train loss: 8.70166e-07, val loss: 1.59242e-06, min loss: 8.02958e-07\n",
      "Epoch: 1417600, elapsed: 1.07e+01, train loss: 8.09820e-07, val loss: 1.50157e-06, min loss: 8.02958e-07\n",
      "Epoch: 1417700, elapsed: 1.10e+01, train loss: 8.08938e-07, val loss: 1.50453e-06, min loss: 8.02958e-07\n",
      "Epoch: 1417800, elapsed: 1.11e+01, train loss: 8.10047e-07, val loss: 1.49730e-06, min loss: 8.02958e-07\n",
      "Epoch: 1417900, elapsed: 1.10e+01, train loss: 1.44217e-06, val loss: 2.26660e-06, min loss: 8.02958e-07\n",
      "Epoch: 1418000, elapsed: 1.09e+01, train loss: 8.08599e-07, val loss: 1.50433e-06, min loss: 8.02958e-07\n",
      "Epoch: 1418100, elapsed: 1.10e+01, train loss: 8.10786e-07, val loss: 1.49862e-06, min loss: 8.02958e-07\n",
      "Epoch: 1418200, elapsed: 1.10e+01, train loss: 9.39356e-07, val loss: 1.54950e-06, min loss: 8.02958e-07\n",
      "Epoch: 1418300, elapsed: 1.09e+01, train loss: 8.18014e-07, val loss: 1.53050e-06, min loss: 8.02958e-07\n",
      "Epoch: 1418400, elapsed: 1.09e+01, train loss: 1.81537e-06, val loss: 2.61152e-06, min loss: 8.02958e-07\n",
      "Epoch: 1418500, elapsed: 1.51e+01, train loss: 8.07831e-07, val loss: 1.49515e-06, min loss: 8.02958e-07\n",
      "Epoch: 1418600, elapsed: 1.13e+01, train loss: 8.09308e-07, val loss: 1.49271e-06, min loss: 8.02958e-07\n",
      "Epoch: 1418700, elapsed: 1.10e+01, train loss: 8.61994e-07, val loss: 1.55556e-06, min loss: 8.02958e-07\n",
      "Epoch: 1418800, elapsed: 1.12e+01, train loss: 1.19834e-06, val loss: 1.92422e-06, min loss: 8.02958e-07\n",
      "Epoch: 1418900, elapsed: 1.11e+01, train loss: 8.42464e-07, val loss: 1.58210e-06, min loss: 8.02958e-07\n",
      "Epoch: 1419000, elapsed: 1.13e+01, train loss: 1.11037e-06, val loss: 1.75000e-06, min loss: 8.02958e-07\n",
      "Epoch: 1419100, elapsed: 1.08e+01, train loss: 8.89446e-07, val loss: 1.62031e-06, min loss: 8.02958e-07\n",
      "Epoch: 1419200, elapsed: 1.07e+01, train loss: 1.15609e-06, val loss: 1.96116e-06, min loss: 8.02958e-07\n",
      "Epoch: 1419300, elapsed: 1.09e+01, train loss: 3.41457e-06, val loss: 4.98181e-06, min loss: 8.02958e-07\n",
      "Epoch: 1419400, elapsed: 1.10e+01, train loss: 8.05837e-07, val loss: 1.49215e-06, min loss: 8.02958e-07\n",
      "Epoch: 1419500, elapsed: 1.11e+01, train loss: 8.07246e-07, val loss: 1.49114e-06, min loss: 8.02958e-07\n",
      "Epoch: 1419600, elapsed: 1.11e+01, train loss: 1.07157e-06, val loss: 1.71960e-06, min loss: 8.02958e-07\n",
      "Epoch: 1419700, elapsed: 1.09e+01, train loss: 9.78996e-07, val loss: 2.14766e-06, min loss: 8.02958e-07\n",
      "Epoch: 1419800, elapsed: 1.10e+01, train loss: 9.59053e-07, val loss: 1.78572e-06, min loss: 8.02958e-07\n",
      "Epoch: 1419900, elapsed: 1.10e+01, train loss: 9.00191e-07, val loss: 1.50724e-06, min loss: 8.02958e-07\n",
      "Epoch: 1420000, elapsed: 1.11e+01, train loss: 8.43177e-07, val loss: 1.55810e-06, min loss: 8.02958e-07\n",
      "Epoch: 1420100, elapsed: 1.29e+01, train loss: 1.01187e-06, val loss: 1.82827e-06, min loss: 8.02958e-07\n",
      "Epoch: 1420200, elapsed: 1.10e+01, train loss: 1.33942e-06, val loss: 1.93728e-06, min loss: 8.02958e-07\n",
      "Epoch: 1420300, elapsed: 1.11e+01, train loss: 2.25855e-06, val loss: 3.13287e-06, min loss: 8.02958e-07\n",
      "Epoch: 1420400, elapsed: 1.10e+01, train loss: 9.33073e-07, val loss: 1.58076e-06, min loss: 8.02958e-07\n",
      "Epoch: 1420500, elapsed: 1.09e+01, train loss: 8.45893e-07, val loss: 1.50897e-06, min loss: 8.02958e-07\n",
      "Epoch: 1420600, elapsed: 1.11e+01, train loss: 8.14887e-07, val loss: 1.49562e-06, min loss: 8.02958e-07\n",
      "Epoch: 1420700, elapsed: 1.08e+01, train loss: 8.05571e-07, val loss: 1.48602e-06, min loss: 8.02958e-07\n",
      "Epoch: 1420800, elapsed: 1.09e+01, train loss: 8.16147e-07, val loss: 1.50936e-06, min loss: 8.02958e-07\n",
      "Epoch: 1420900, elapsed: 1.09e+01, train loss: 8.78394e-07, val loss: 1.66930e-06, min loss: 8.02958e-07\n",
      "Epoch: 1421000, elapsed: 1.09e+01, train loss: 4.02250e-06, val loss: 5.01023e-06, min loss: 8.02958e-07\n",
      "Epoch: 1421100, elapsed: 1.51e+01, train loss: 8.75185e-07, val loss: 1.61374e-06, min loss: 8.02958e-07\n",
      "Epoch: 1421200, elapsed: 1.12e+01, train loss: 2.90190e-06, val loss: 3.89149e-06, min loss: 8.02958e-07\n",
      "Epoch: 1421300, elapsed: 1.10e+01, train loss: 1.11067e-06, val loss: 1.79198e-06, min loss: 8.02958e-07\n",
      "Epoch: 1421400, elapsed: 1.11e+01, train loss: 9.24468e-07, val loss: 1.64087e-06, min loss: 8.02958e-07\n",
      "Epoch: 1421500, elapsed: 1.12e+01, train loss: 8.02907e-07, val loss: 1.48560e-06, min loss: 8.02907e-07\n",
      "Epoch: 1421600, elapsed: 1.12e+01, train loss: 8.27816e-07, val loss: 1.51049e-06, min loss: 8.02907e-07\n",
      "Epoch: 1421700, elapsed: 1.10e+01, train loss: 9.48343e-07, val loss: 1.53969e-06, min loss: 8.02907e-07\n",
      "Epoch: 1421800, elapsed: 1.10e+01, train loss: 7.44080e-06, val loss: 6.83742e-06, min loss: 8.02907e-07\n",
      "Epoch: 1421900, elapsed: 1.10e+01, train loss: 8.01563e-07, val loss: 1.48517e-06, min loss: 8.01563e-07\n",
      "Epoch: 1422000, elapsed: 1.10e+01, train loss: 2.01254e-06, val loss: 2.30873e-06, min loss: 8.01563e-07\n",
      "Epoch: 1422100, elapsed: 1.10e+01, train loss: 1.33928e-06, val loss: 1.77036e-06, min loss: 8.01563e-07\n",
      "Epoch: 1422200, elapsed: 1.11e+01, train loss: 1.27355e-06, val loss: 1.92701e-06, min loss: 8.01563e-07\n",
      "Epoch: 1422300, elapsed: 1.12e+01, train loss: 8.83604e-07, val loss: 1.59774e-06, min loss: 8.01563e-07\n",
      "Epoch: 1422400, elapsed: 1.11e+01, train loss: 8.52949e-07, val loss: 1.50361e-06, min loss: 8.01563e-07\n",
      "Epoch: 1422500, elapsed: 1.10e+01, train loss: 1.81114e-06, val loss: 2.69048e-06, min loss: 8.01563e-07\n",
      "Epoch: 1422600, elapsed: 1.10e+01, train loss: 1.13813e-06, val loss: 2.13435e-06, min loss: 8.01563e-07\n",
      "Epoch: 1422700, elapsed: 1.10e+01, train loss: 2.10608e-06, val loss: 2.53252e-06, min loss: 8.01563e-07\n",
      "Epoch: 1422800, elapsed: 1.07e+01, train loss: 8.71548e-07, val loss: 1.54233e-06, min loss: 8.01563e-07\n",
      "Epoch: 1422900, elapsed: 1.09e+01, train loss: 1.14818e-06, val loss: 1.93968e-06, min loss: 8.01563e-07\n",
      "Epoch: 1423000, elapsed: 1.09e+01, train loss: 8.40241e-07, val loss: 1.51520e-06, min loss: 8.01563e-07\n",
      "Epoch: 1423100, elapsed: 1.09e+01, train loss: 1.52979e-06, val loss: 2.03738e-06, min loss: 8.01563e-07\n",
      "Epoch: 1423200, elapsed: 1.09e+01, train loss: 8.53492e-07, val loss: 1.50990e-06, min loss: 8.01563e-07\n",
      "Epoch: 1423300, elapsed: 1.08e+01, train loss: 9.65667e-07, val loss: 1.63848e-06, min loss: 8.01563e-07\n",
      "Epoch: 1423400, elapsed: 1.10e+01, train loss: 1.22113e-06, val loss: 1.98828e-06, min loss: 8.01563e-07\n",
      "Epoch: 1423500, elapsed: 1.10e+01, train loss: 8.06310e-07, val loss: 1.49657e-06, min loss: 8.01563e-07\n",
      "Epoch: 1423600, elapsed: 1.09e+01, train loss: 8.75606e-07, val loss: 1.80101e-06, min loss: 8.01563e-07\n",
      "Epoch: 1423700, elapsed: 1.49e+01, train loss: 7.98895e-07, val loss: 1.48336e-06, min loss: 7.98895e-07\n",
      "Epoch: 1423800, elapsed: 1.11e+01, train loss: 8.04609e-07, val loss: 1.49548e-06, min loss: 7.98895e-07\n",
      "Epoch: 1423900, elapsed: 1.12e+01, train loss: 7.99077e-07, val loss: 1.47849e-06, min loss: 7.98895e-07\n",
      "Epoch: 1424000, elapsed: 1.11e+01, train loss: 8.10961e-07, val loss: 1.50232e-06, min loss: 7.98895e-07\n",
      "Epoch: 1424100, elapsed: 1.11e+01, train loss: 8.41197e-07, val loss: 1.52530e-06, min loss: 7.98895e-07\n",
      "Epoch: 1424200, elapsed: 1.11e+01, train loss: 8.52447e-07, val loss: 1.49359e-06, min loss: 7.98895e-07\n",
      "Epoch: 1424300, elapsed: 1.09e+01, train loss: 8.14093e-07, val loss: 1.47826e-06, min loss: 7.98895e-07\n",
      "Epoch: 1424400, elapsed: 1.12e+01, train loss: 8.26042e-07, val loss: 1.53833e-06, min loss: 7.98895e-07\n",
      "Epoch: 1424500, elapsed: 1.09e+01, train loss: 8.76642e-07, val loss: 1.54814e-06, min loss: 7.98895e-07\n",
      "Epoch: 1424600, elapsed: 1.11e+01, train loss: 1.90120e-06, val loss: 2.26035e-06, min loss: 7.98895e-07\n",
      "Epoch: 1424700, elapsed: 1.12e+01, train loss: 7.99305e-07, val loss: 1.47393e-06, min loss: 7.98895e-07\n",
      "Epoch: 1424800, elapsed: 1.12e+01, train loss: 7.98766e-07, val loss: 1.47775e-06, min loss: 7.98766e-07\n",
      "Epoch: 1424900, elapsed: 1.12e+01, train loss: 9.60083e-07, val loss: 1.57282e-06, min loss: 7.98766e-07\n",
      "Epoch: 1425000, elapsed: 1.14e+01, train loss: 1.04370e-06, val loss: 1.86120e-06, min loss: 7.98766e-07\n",
      "Epoch: 1425100, elapsed: 1.31e+01, train loss: 8.41277e-07, val loss: 1.49068e-06, min loss: 7.98766e-07\n",
      "Epoch: 1425200, elapsed: 1.11e+01, train loss: 9.36572e-07, val loss: 1.54044e-06, min loss: 7.98766e-07\n",
      "Epoch: 1425300, elapsed: 1.10e+01, train loss: 1.64420e-06, val loss: 1.64214e-06, min loss: 7.98766e-07\n",
      "Epoch: 1425400, elapsed: 1.09e+01, train loss: 8.01792e-07, val loss: 1.49407e-06, min loss: 7.98766e-07\n",
      "Epoch: 1425500, elapsed: 1.11e+01, train loss: 8.18588e-07, val loss: 1.49388e-06, min loss: 7.98766e-07\n",
      "Epoch: 1425600, elapsed: 1.10e+01, train loss: 8.33721e-07, val loss: 1.62142e-06, min loss: 7.98766e-07\n",
      "Epoch: 1425700, elapsed: 1.08e+01, train loss: 2.67082e-06, val loss: 3.55111e-06, min loss: 7.98766e-07\n",
      "Epoch: 1425800, elapsed: 1.09e+01, train loss: 2.22674e-06, val loss: 3.39890e-06, min loss: 7.98766e-07\n",
      "Epoch: 1425900, elapsed: 1.09e+01, train loss: 9.12843e-07, val loss: 1.67314e-06, min loss: 7.98766e-07\n",
      "Epoch: 1426000, elapsed: 1.11e+01, train loss: 2.03425e-06, val loss: 2.64370e-06, min loss: 7.98766e-07\n",
      "Epoch: 1426100, elapsed: 1.08e+01, train loss: 2.90856e-06, val loss: 2.61659e-06, min loss: 7.98766e-07\n",
      "Epoch: 1426200, elapsed: 1.48e+01, train loss: 8.33520e-07, val loss: 1.47741e-06, min loss: 7.98766e-07\n",
      "Epoch: 1426300, elapsed: 1.13e+01, train loss: 1.03735e-06, val loss: 1.84711e-06, min loss: 7.98766e-07\n",
      "Epoch: 1426400, elapsed: 1.10e+01, train loss: 8.25425e-07, val loss: 1.47301e-06, min loss: 7.98766e-07\n",
      "Epoch: 1426500, elapsed: 1.12e+01, train loss: 7.96140e-07, val loss: 1.47468e-06, min loss: 7.96140e-07\n",
      "Epoch: 1426600, elapsed: 1.10e+01, train loss: 8.01469e-07, val loss: 1.47811e-06, min loss: 7.96140e-07\n",
      "Epoch: 1426700, elapsed: 1.10e+01, train loss: 1.04539e-06, val loss: 1.68804e-06, min loss: 7.96140e-07\n",
      "Epoch: 1426800, elapsed: 1.10e+01, train loss: 1.58761e-06, val loss: 2.55060e-06, min loss: 7.96140e-07\n",
      "Epoch: 1426900, elapsed: 1.09e+01, train loss: 1.24682e-06, val loss: 1.89850e-06, min loss: 7.96140e-07\n",
      "Epoch: 1427000, elapsed: 1.11e+01, train loss: 8.63484e-07, val loss: 1.64182e-06, min loss: 7.96140e-07\n",
      "Epoch: 1427100, elapsed: 1.10e+01, train loss: 1.20564e-06, val loss: 1.83544e-06, min loss: 7.96140e-07\n",
      "Epoch: 1427200, elapsed: 1.11e+01, train loss: 1.03406e-06, val loss: 1.64028e-06, min loss: 7.96140e-07\n",
      "Epoch: 1427300, elapsed: 1.09e+01, train loss: 8.00140e-07, val loss: 1.49112e-06, min loss: 7.96140e-07\n",
      "Epoch: 1427400, elapsed: 1.10e+01, train loss: 7.97858e-07, val loss: 1.47248e-06, min loss: 7.96140e-07\n",
      "Epoch: 1427500, elapsed: 1.11e+01, train loss: 7.94860e-07, val loss: 1.47652e-06, min loss: 7.94860e-07\n",
      "Epoch: 1427600, elapsed: 1.11e+01, train loss: 8.02090e-07, val loss: 1.48347e-06, min loss: 7.94860e-07\n",
      "Epoch: 1427700, elapsed: 1.10e+01, train loss: 8.04928e-07, val loss: 1.46206e-06, min loss: 7.94860e-07\n",
      "Epoch: 1427800, elapsed: 1.10e+01, train loss: 2.56811e-06, val loss: 4.02930e-06, min loss: 7.94860e-07\n",
      "Epoch: 1427900, elapsed: 1.10e+01, train loss: 7.94163e-07, val loss: 1.47358e-06, min loss: 7.94163e-07\n",
      "Epoch: 1428000, elapsed: 1.08e+01, train loss: 8.13352e-07, val loss: 1.48684e-06, min loss: 7.94163e-07\n",
      "Epoch: 1428100, elapsed: 1.10e+01, train loss: 1.71872e-06, val loss: 2.67927e-06, min loss: 7.94163e-07\n",
      "Epoch: 1428200, elapsed: 1.09e+01, train loss: 8.08603e-07, val loss: 1.49034e-06, min loss: 7.94163e-07\n",
      "Epoch: 1428300, elapsed: 1.09e+01, train loss: 1.04983e-06, val loss: 1.67272e-06, min loss: 7.94163e-07\n",
      "Epoch: 1428400, elapsed: 1.10e+01, train loss: 9.34785e-07, val loss: 1.54953e-06, min loss: 7.94163e-07\n",
      "Epoch: 1428500, elapsed: 1.08e+01, train loss: 9.38533e-07, val loss: 1.53000e-06, min loss: 7.94163e-07\n",
      "Epoch: 1428600, elapsed: 1.08e+01, train loss: 8.85131e-07, val loss: 1.56632e-06, min loss: 7.94163e-07\n",
      "Epoch: 1428700, elapsed: 1.10e+01, train loss: 9.00177e-07, val loss: 1.53437e-06, min loss: 7.94163e-07\n",
      "Epoch: 1428800, elapsed: 1.49e+01, train loss: 9.84024e-07, val loss: 1.63775e-06, min loss: 7.94163e-07\n",
      "Epoch: 1428900, elapsed: 1.11e+01, train loss: 9.75435e-07, val loss: 1.53301e-06, min loss: 7.94163e-07\n",
      "Epoch: 1429000, elapsed: 1.10e+01, train loss: 2.18249e-06, val loss: 2.64840e-06, min loss: 7.94163e-07\n",
      "Epoch: 1429100, elapsed: 1.11e+01, train loss: 9.34879e-07, val loss: 1.68770e-06, min loss: 7.94163e-07\n",
      "Epoch: 1429200, elapsed: 1.10e+01, train loss: 8.43968e-07, val loss: 1.54539e-06, min loss: 7.94163e-07\n",
      "Epoch: 1429300, elapsed: 1.11e+01, train loss: 8.26674e-07, val loss: 1.52396e-06, min loss: 7.94163e-07\n",
      "Epoch: 1429400, elapsed: 1.11e+01, train loss: 7.93124e-07, val loss: 1.47443e-06, min loss: 7.93124e-07\n",
      "Epoch: 1429500, elapsed: 1.08e+01, train loss: 8.50496e-07, val loss: 1.47916e-06, min loss: 7.93124e-07\n",
      "Epoch: 1429600, elapsed: 1.09e+01, train loss: 1.09367e-06, val loss: 1.85423e-06, min loss: 7.93124e-07\n",
      "Epoch: 1429700, elapsed: 1.10e+01, train loss: 1.39518e-06, val loss: 1.69885e-06, min loss: 7.93124e-07\n",
      "Epoch: 1429800, elapsed: 1.10e+01, train loss: 4.24920e-06, val loss: 4.17423e-06, min loss: 7.93124e-07\n",
      "Epoch: 1429900, elapsed: 1.09e+01, train loss: 1.47351e-06, val loss: 2.58257e-06, min loss: 7.93124e-07\n",
      "Epoch: 1430000, elapsed: 1.07e+01, train loss: 8.63938e-07, val loss: 1.47697e-06, min loss: 7.93124e-07\n",
      "Epoch: 1430100, elapsed: 1.28e+01, train loss: 8.31424e-07, val loss: 1.52768e-06, min loss: 7.93124e-07\n",
      "Epoch: 1430200, elapsed: 1.10e+01, train loss: 8.71267e-07, val loss: 1.58828e-06, min loss: 7.93124e-07\n",
      "Epoch: 1430300, elapsed: 1.08e+01, train loss: 8.06297e-07, val loss: 1.48145e-06, min loss: 7.93124e-07\n",
      "Epoch: 1430400, elapsed: 1.09e+01, train loss: 7.93992e-07, val loss: 1.47821e-06, min loss: 7.93124e-07\n",
      "Epoch: 1430500, elapsed: 1.09e+01, train loss: 8.52436e-07, val loss: 1.50323e-06, min loss: 7.93124e-07\n",
      "Epoch: 1430600, elapsed: 1.09e+01, train loss: 1.39600e-06, val loss: 2.25116e-06, min loss: 7.93124e-07\n",
      "Epoch: 1430700, elapsed: 1.09e+01, train loss: 1.07147e-06, val loss: 1.68710e-06, min loss: 7.93124e-07\n",
      "Epoch: 1430800, elapsed: 1.08e+01, train loss: 5.13074e-06, val loss: 5.41252e-06, min loss: 7.93124e-07\n",
      "Epoch: 1430900, elapsed: 1.08e+01, train loss: 7.92424e-07, val loss: 1.47868e-06, min loss: 7.92424e-07\n",
      "Epoch: 1431000, elapsed: 1.08e+01, train loss: 7.92877e-07, val loss: 1.46252e-06, min loss: 7.92424e-07\n",
      "Epoch: 1431100, elapsed: 1.06e+01, train loss: 7.96077e-07, val loss: 1.46223e-06, min loss: 7.92424e-07\n",
      "Epoch: 1431200, elapsed: 1.08e+01, train loss: 7.99668e-07, val loss: 1.48727e-06, min loss: 7.92424e-07\n",
      "Epoch: 1431300, elapsed: 1.09e+01, train loss: 8.02079e-07, val loss: 1.46701e-06, min loss: 7.92424e-07\n",
      "Epoch: 1431400, elapsed: 1.48e+01, train loss: 2.00373e-06, val loss: 2.77698e-06, min loss: 7.92424e-07\n",
      "Epoch: 1431500, elapsed: 1.10e+01, train loss: 8.43156e-07, val loss: 1.53665e-06, min loss: 7.92424e-07\n",
      "Epoch: 1431600, elapsed: 1.08e+01, train loss: 7.94306e-07, val loss: 1.46819e-06, min loss: 7.92424e-07\n",
      "Epoch: 1431700, elapsed: 1.10e+01, train loss: 8.02523e-07, val loss: 1.50055e-06, min loss: 7.92424e-07\n",
      "Epoch: 1431800, elapsed: 1.09e+01, train loss: 7.98811e-07, val loss: 1.45273e-06, min loss: 7.92424e-07\n",
      "Epoch: 1431900, elapsed: 1.08e+01, train loss: 7.93975e-07, val loss: 1.47721e-06, min loss: 7.92424e-07\n",
      "Epoch: 1432000, elapsed: 1.09e+01, train loss: 8.48341e-07, val loss: 1.57354e-06, min loss: 7.92424e-07\n",
      "Epoch: 1432100, elapsed: 1.08e+01, train loss: 2.07284e-06, val loss: 3.18943e-06, min loss: 7.92424e-07\n",
      "Epoch: 1432200, elapsed: 1.10e+01, train loss: 8.01659e-07, val loss: 1.51484e-06, min loss: 7.92424e-07\n",
      "Epoch: 1432300, elapsed: 1.10e+01, train loss: 1.08100e-06, val loss: 1.90916e-06, min loss: 7.92424e-07\n",
      "Epoch: 1432400, elapsed: 1.07e+01, train loss: 1.00481e-06, val loss: 1.63722e-06, min loss: 7.92424e-07\n",
      "Epoch: 1432500, elapsed: 1.08e+01, train loss: 9.49477e-07, val loss: 1.73248e-06, min loss: 7.92424e-07\n",
      "Epoch: 1432600, elapsed: 1.09e+01, train loss: 1.40494e-06, val loss: 1.84367e-06, min loss: 7.92424e-07\n",
      "Epoch: 1432700, elapsed: 1.08e+01, train loss: 3.77337e-06, val loss: 3.33048e-06, min loss: 7.92424e-07\n",
      "Epoch: 1432800, elapsed: 1.07e+01, train loss: 7.90912e-07, val loss: 1.46303e-06, min loss: 7.90912e-07\n",
      "Epoch: 1432900, elapsed: 1.07e+01, train loss: 7.90308e-07, val loss: 1.47056e-06, min loss: 7.90308e-07\n",
      "Epoch: 1433000, elapsed: 1.08e+01, train loss: 8.14470e-07, val loss: 1.50126e-06, min loss: 7.90308e-07\n",
      "Epoch: 1433100, elapsed: 1.09e+01, train loss: 7.90178e-07, val loss: 1.46891e-06, min loss: 7.90178e-07\n",
      "Epoch: 1433200, elapsed: 1.09e+01, train loss: 7.93941e-07, val loss: 1.46150e-06, min loss: 7.90178e-07\n",
      "Epoch: 1433300, elapsed: 1.06e+01, train loss: 7.88924e-07, val loss: 1.46675e-06, min loss: 7.88924e-07\n",
      "Epoch: 1433400, elapsed: 1.08e+01, train loss: 7.91677e-07, val loss: 1.46619e-06, min loss: 7.88924e-07\n",
      "Epoch: 1433500, elapsed: 1.06e+01, train loss: 8.27729e-07, val loss: 1.48172e-06, min loss: 7.88924e-07\n",
      "Epoch: 1433600, elapsed: 1.06e+01, train loss: 9.76202e-07, val loss: 1.81593e-06, min loss: 7.88924e-07\n",
      "Epoch: 1433700, elapsed: 1.09e+01, train loss: 7.88724e-07, val loss: 1.46833e-06, min loss: 7.88724e-07\n",
      "Epoch: 1433800, elapsed: 1.09e+01, train loss: 2.14635e-06, val loss: 2.49276e-06, min loss: 7.88724e-07\n",
      "Epoch: 1433900, elapsed: 1.07e+01, train loss: 2.14478e-06, val loss: 2.72736e-06, min loss: 7.88724e-07\n",
      "Epoch: 1434000, elapsed: 1.47e+01, train loss: 1.81599e-06, val loss: 1.82148e-06, min loss: 7.88724e-07\n",
      "Epoch: 1434100, elapsed: 1.12e+01, train loss: 1.19526e-06, val loss: 1.76896e-06, min loss: 7.88724e-07\n",
      "Epoch: 1434200, elapsed: 1.11e+01, train loss: 7.91813e-07, val loss: 1.47723e-06, min loss: 7.88724e-07\n",
      "Epoch: 1434300, elapsed: 1.09e+01, train loss: 8.05154e-07, val loss: 1.48797e-06, min loss: 7.88724e-07\n",
      "Epoch: 1434400, elapsed: 1.10e+01, train loss: 8.60634e-07, val loss: 1.54974e-06, min loss: 7.88724e-07\n",
      "Epoch: 1434500, elapsed: 1.10e+01, train loss: 8.22943e-07, val loss: 1.53824e-06, min loss: 7.88724e-07\n",
      "Epoch: 1434600, elapsed: 1.10e+01, train loss: 1.45014e-06, val loss: 2.07233e-06, min loss: 7.88724e-07\n",
      "Epoch: 1434700, elapsed: 1.09e+01, train loss: 1.30067e-06, val loss: 1.78405e-06, min loss: 7.88724e-07\n",
      "Epoch: 1434800, elapsed: 1.09e+01, train loss: 1.33304e-06, val loss: 1.99092e-06, min loss: 7.88724e-07\n",
      "Epoch: 1434900, elapsed: 1.09e+01, train loss: 9.86875e-07, val loss: 1.59548e-06, min loss: 7.88724e-07\n",
      "Epoch: 1435000, elapsed: 1.09e+01, train loss: 9.42331e-07, val loss: 1.54832e-06, min loss: 7.88724e-07\n",
      "Epoch: 1435100, elapsed: 1.27e+01, train loss: 8.47258e-07, val loss: 1.49343e-06, min loss: 7.88724e-07\n",
      "Epoch: 1435200, elapsed: 1.09e+01, train loss: 5.80874e-06, val loss: 4.91321e-06, min loss: 7.88724e-07\n",
      "Epoch: 1435300, elapsed: 1.08e+01, train loss: 8.06228e-07, val loss: 1.53781e-06, min loss: 7.88724e-07\n",
      "Epoch: 1435400, elapsed: 1.09e+01, train loss: 8.03467e-07, val loss: 1.48551e-06, min loss: 7.88724e-07\n",
      "Epoch: 1435500, elapsed: 1.08e+01, train loss: 1.12447e-06, val loss: 1.91909e-06, min loss: 7.88724e-07\n",
      "Epoch: 1435600, elapsed: 1.09e+01, train loss: 7.90589e-07, val loss: 1.45632e-06, min loss: 7.88724e-07\n",
      "Epoch: 1435700, elapsed: 1.07e+01, train loss: 7.87411e-07, val loss: 1.45958e-06, min loss: 7.87411e-07\n",
      "Epoch: 1435800, elapsed: 1.09e+01, train loss: 8.01284e-07, val loss: 1.45248e-06, min loss: 7.87411e-07\n",
      "Epoch: 1435900, elapsed: 1.07e+01, train loss: 8.53876e-07, val loss: 1.49728e-06, min loss: 7.87411e-07\n",
      "Epoch: 1436000, elapsed: 1.07e+01, train loss: 8.14134e-07, val loss: 1.46672e-06, min loss: 7.87411e-07\n",
      "Epoch: 1436100, elapsed: 1.08e+01, train loss: 7.99336e-07, val loss: 1.45867e-06, min loss: 7.87411e-07\n",
      "Epoch: 1436200, elapsed: 1.08e+01, train loss: 8.22185e-07, val loss: 1.52002e-06, min loss: 7.87411e-07\n",
      "Epoch: 1436300, elapsed: 1.09e+01, train loss: 7.93652e-07, val loss: 1.47132e-06, min loss: 7.87411e-07\n",
      "Epoch: 1436400, elapsed: 1.08e+01, train loss: 8.32850e-07, val loss: 1.55903e-06, min loss: 7.87411e-07\n",
      "Epoch: 1436500, elapsed: 1.07e+01, train loss: 8.97364e-07, val loss: 1.68004e-06, min loss: 7.87411e-07\n",
      "Epoch: 1436600, elapsed: 1.49e+01, train loss: 8.71892e-07, val loss: 1.59948e-06, min loss: 7.87411e-07\n",
      "Epoch: 1436700, elapsed: 1.12e+01, train loss: 7.95979e-07, val loss: 1.49658e-06, min loss: 7.87411e-07\n",
      "Epoch: 1436800, elapsed: 1.10e+01, train loss: 8.06719e-07, val loss: 1.50364e-06, min loss: 7.87411e-07\n",
      "Epoch: 1436900, elapsed: 1.10e+01, train loss: 9.83145e-07, val loss: 1.78549e-06, min loss: 7.87411e-07\n",
      "Epoch: 1437000, elapsed: 1.10e+01, train loss: 7.90059e-07, val loss: 1.46199e-06, min loss: 7.87411e-07\n",
      "Epoch: 1437100, elapsed: 1.10e+01, train loss: 7.92549e-07, val loss: 1.46602e-06, min loss: 7.87411e-07\n",
      "Epoch: 1437200, elapsed: 1.10e+01, train loss: 9.72018e-07, val loss: 1.60485e-06, min loss: 7.87411e-07\n",
      "Epoch: 1437300, elapsed: 1.08e+01, train loss: 8.74938e-07, val loss: 1.49247e-06, min loss: 7.87411e-07\n",
      "Epoch: 1437400, elapsed: 1.10e+01, train loss: 9.01301e-07, val loss: 1.62284e-06, min loss: 7.87411e-07\n",
      "Epoch: 1437500, elapsed: 1.08e+01, train loss: 7.86578e-07, val loss: 1.46594e-06, min loss: 7.86578e-07\n",
      "Epoch: 1437600, elapsed: 1.10e+01, train loss: 9.47669e-07, val loss: 1.46178e-06, min loss: 7.86578e-07\n",
      "Epoch: 1437700, elapsed: 1.09e+01, train loss: 8.76585e-07, val loss: 1.57377e-06, min loss: 7.86578e-07\n",
      "Epoch: 1437800, elapsed: 1.09e+01, train loss: 8.03775e-07, val loss: 1.48266e-06, min loss: 7.86578e-07\n",
      "Epoch: 1437900, elapsed: 1.09e+01, train loss: 8.04134e-07, val loss: 1.50955e-06, min loss: 7.86578e-07\n",
      "Epoch: 1438000, elapsed: 1.09e+01, train loss: 7.89532e-07, val loss: 1.47567e-06, min loss: 7.86578e-07\n",
      "Epoch: 1438100, elapsed: 1.08e+01, train loss: 8.15440e-07, val loss: 1.50012e-06, min loss: 7.86578e-07\n",
      "Epoch: 1438200, elapsed: 1.10e+01, train loss: 8.07126e-07, val loss: 1.48383e-06, min loss: 7.86578e-07\n",
      "Epoch: 1438300, elapsed: 1.08e+01, train loss: 1.25764e-06, val loss: 1.89452e-06, min loss: 7.86578e-07\n",
      "Epoch: 1438400, elapsed: 1.07e+01, train loss: 7.89679e-07, val loss: 1.49067e-06, min loss: 7.86578e-07\n",
      "Epoch: 1438500, elapsed: 1.06e+01, train loss: 7.84985e-07, val loss: 1.46363e-06, min loss: 7.84985e-07\n",
      "Epoch: 1438600, elapsed: 1.06e+01, train loss: 7.93977e-07, val loss: 1.46231e-06, min loss: 7.84985e-07\n",
      "Epoch: 1438700, elapsed: 1.07e+01, train loss: 8.23665e-07, val loss: 1.49744e-06, min loss: 7.84985e-07\n",
      "Epoch: 1438800, elapsed: 1.09e+01, train loss: 3.01721e-06, val loss: 3.77336e-06, min loss: 7.84985e-07\n",
      "Epoch: 1438900, elapsed: 1.06e+01, train loss: 9.09758e-07, val loss: 1.67046e-06, min loss: 7.84985e-07\n",
      "Epoch: 1439000, elapsed: 1.08e+01, train loss: 9.08359e-07, val loss: 1.56285e-06, min loss: 7.84985e-07\n",
      "Epoch: 1439100, elapsed: 1.08e+01, train loss: 7.85966e-07, val loss: 1.46615e-06, min loss: 7.84985e-07\n",
      "Epoch: 1439200, elapsed: 1.48e+01, train loss: 7.88244e-07, val loss: 1.45804e-06, min loss: 7.84985e-07\n",
      "Epoch: 1439300, elapsed: 1.10e+01, train loss: 7.95514e-07, val loss: 1.47318e-06, min loss: 7.84985e-07\n",
      "Epoch: 1439400, elapsed: 1.09e+01, train loss: 1.36998e-06, val loss: 1.99419e-06, min loss: 7.84985e-07\n",
      "Epoch: 1439500, elapsed: 1.09e+01, train loss: 7.95424e-07, val loss: 1.46060e-06, min loss: 7.84985e-07\n",
      "Epoch: 1439600, elapsed: 1.10e+01, train loss: 8.32906e-07, val loss: 1.56983e-06, min loss: 7.84985e-07\n",
      "Epoch: 1439700, elapsed: 1.10e+01, train loss: 7.84740e-07, val loss: 1.46025e-06, min loss: 7.84740e-07\n",
      "Epoch: 1439800, elapsed: 1.10e+01, train loss: 8.22596e-07, val loss: 1.50880e-06, min loss: 7.84740e-07\n",
      "Epoch: 1439900, elapsed: 1.09e+01, train loss: 9.47952e-07, val loss: 1.50731e-06, min loss: 7.84740e-07\n",
      "Epoch: 1440000, elapsed: 1.08e+01, train loss: 7.98443e-07, val loss: 1.49123e-06, min loss: 7.84740e-07\n",
      "Epoch: 1440100, elapsed: 1.28e+01, train loss: 7.87794e-07, val loss: 1.47220e-06, min loss: 7.84740e-07\n",
      "Epoch: 1440200, elapsed: 1.08e+01, train loss: 5.64053e-06, val loss: 4.57419e-06, min loss: 7.84740e-07\n",
      "Epoch: 1440300, elapsed: 1.08e+01, train loss: 7.83565e-07, val loss: 1.46358e-06, min loss: 7.83565e-07\n",
      "Epoch: 1440400, elapsed: 1.08e+01, train loss: 9.15408e-07, val loss: 1.66674e-06, min loss: 7.83565e-07\n",
      "Epoch: 1440500, elapsed: 1.08e+01, train loss: 7.83323e-07, val loss: 1.46216e-06, min loss: 7.83323e-07\n",
      "Epoch: 1440600, elapsed: 1.09e+01, train loss: 9.26907e-07, val loss: 1.60321e-06, min loss: 7.83323e-07\n",
      "Epoch: 1440700, elapsed: 1.08e+01, train loss: 7.83140e-07, val loss: 1.46233e-06, min loss: 7.83140e-07\n",
      "Epoch: 1440800, elapsed: 1.08e+01, train loss: 1.14967e-06, val loss: 1.99853e-06, min loss: 7.83140e-07\n",
      "Epoch: 1440900, elapsed: 1.09e+01, train loss: 7.83112e-07, val loss: 1.46438e-06, min loss: 7.83112e-07\n",
      "Epoch: 1441000, elapsed: 1.09e+01, train loss: 7.89259e-07, val loss: 1.46637e-06, min loss: 7.83112e-07\n",
      "Epoch: 1441100, elapsed: 1.09e+01, train loss: 1.27841e-06, val loss: 1.74917e-06, min loss: 7.83112e-07\n",
      "Epoch: 1441200, elapsed: 1.06e+01, train loss: 9.47652e-07, val loss: 1.58180e-06, min loss: 7.83112e-07\n",
      "Epoch: 1441300, elapsed: 1.07e+01, train loss: 2.36396e-06, val loss: 3.13938e-06, min loss: 7.83112e-07\n",
      "Epoch: 1441400, elapsed: 1.08e+01, train loss: 1.05058e-06, val loss: 1.60689e-06, min loss: 7.83112e-07\n",
      "Epoch: 1441500, elapsed: 1.10e+01, train loss: 8.29187e-07, val loss: 1.46518e-06, min loss: 7.83112e-07\n",
      "Epoch: 1441600, elapsed: 1.08e+01, train loss: 7.95433e-07, val loss: 1.48039e-06, min loss: 7.83112e-07\n",
      "Epoch: 1441700, elapsed: 1.09e+01, train loss: 7.89290e-07, val loss: 1.45584e-06, min loss: 7.83112e-07\n",
      "Epoch: 1441800, elapsed: 1.50e+01, train loss: 8.41167e-07, val loss: 1.48738e-06, min loss: 7.83112e-07\n",
      "Epoch: 1441900, elapsed: 1.11e+01, train loss: 1.14498e-06, val loss: 1.90961e-06, min loss: 7.83112e-07\n",
      "Epoch: 1442000, elapsed: 1.08e+01, train loss: 8.66490e-07, val loss: 1.50490e-06, min loss: 7.83112e-07\n",
      "Epoch: 1442100, elapsed: 1.10e+01, train loss: 1.92869e-06, val loss: 1.99711e-06, min loss: 7.83112e-07\n",
      "Epoch: 1442200, elapsed: 1.11e+01, train loss: 1.66156e-06, val loss: 2.99480e-06, min loss: 7.83112e-07\n",
      "Epoch: 1442300, elapsed: 1.09e+01, train loss: 7.82166e-07, val loss: 1.46204e-06, min loss: 7.82166e-07\n",
      "Epoch: 1442400, elapsed: 1.09e+01, train loss: 7.91884e-07, val loss: 1.49977e-06, min loss: 7.82166e-07\n",
      "Epoch: 1442500, elapsed: 1.10e+01, train loss: 1.10093e-06, val loss: 1.68898e-06, min loss: 7.82166e-07\n",
      "Epoch: 1442600, elapsed: 1.08e+01, train loss: 8.62573e-07, val loss: 1.53326e-06, min loss: 7.82166e-07\n",
      "Epoch: 1442700, elapsed: 1.08e+01, train loss: 7.96117e-07, val loss: 1.46010e-06, min loss: 7.82166e-07\n",
      "Epoch: 1442800, elapsed: 1.09e+01, train loss: 7.82196e-07, val loss: 1.45829e-06, min loss: 7.82166e-07\n",
      "Epoch: 1442900, elapsed: 1.09e+01, train loss: 7.84020e-07, val loss: 1.46613e-06, min loss: 7.82166e-07\n",
      "Epoch: 1443000, elapsed: 1.09e+01, train loss: 9.61513e-07, val loss: 1.52548e-06, min loss: 7.82166e-07\n",
      "Epoch: 1443100, elapsed: 1.09e+01, train loss: 3.52963e-06, val loss: 4.99963e-06, min loss: 7.82166e-07\n",
      "Epoch: 1443200, elapsed: 1.09e+01, train loss: 7.81431e-07, val loss: 1.45972e-06, min loss: 7.81431e-07\n",
      "Epoch: 1443300, elapsed: 1.09e+01, train loss: 7.96417e-07, val loss: 1.46117e-06, min loss: 7.81431e-07\n",
      "Epoch: 1443400, elapsed: 1.07e+01, train loss: 8.37418e-07, val loss: 1.51244e-06, min loss: 7.81431e-07\n",
      "Epoch: 1443500, elapsed: 1.08e+01, train loss: 7.81994e-07, val loss: 1.46969e-06, min loss: 7.81431e-07\n",
      "Epoch: 1443600, elapsed: 1.09e+01, train loss: 7.81984e-07, val loss: 1.45572e-06, min loss: 7.81431e-07\n",
      "Epoch: 1443700, elapsed: 1.06e+01, train loss: 9.12987e-07, val loss: 2.06229e-06, min loss: 7.81431e-07\n",
      "Epoch: 1443800, elapsed: 1.09e+01, train loss: 9.10729e-07, val loss: 1.66049e-06, min loss: 7.81431e-07\n",
      "Epoch: 1443900, elapsed: 1.06e+01, train loss: 8.54153e-07, val loss: 1.47741e-06, min loss: 7.81431e-07\n",
      "Epoch: 1444000, elapsed: 1.07e+01, train loss: 1.27558e-06, val loss: 1.90541e-06, min loss: 7.81431e-07\n",
      "Epoch: 1444100, elapsed: 1.07e+01, train loss: 1.19886e-06, val loss: 1.71496e-06, min loss: 7.81431e-07\n",
      "Epoch: 1444200, elapsed: 1.09e+01, train loss: 8.20214e-07, val loss: 1.49520e-06, min loss: 7.81431e-07\n",
      "Epoch: 1444300, elapsed: 1.10e+01, train loss: 7.89755e-07, val loss: 1.47078e-06, min loss: 7.81431e-07\n",
      "Epoch: 1444400, elapsed: 1.08e+01, train loss: 7.99217e-07, val loss: 1.45457e-06, min loss: 7.81431e-07\n",
      "Epoch: 1444500, elapsed: 1.50e+01, train loss: 8.04690e-07, val loss: 1.46724e-06, min loss: 7.81431e-07\n",
      "Epoch: 1444600, elapsed: 1.09e+01, train loss: 1.28374e-06, val loss: 1.97160e-06, min loss: 7.81431e-07\n",
      "Epoch: 1444700, elapsed: 1.08e+01, train loss: 2.23919e-06, val loss: 2.96609e-06, min loss: 7.81431e-07\n",
      "Epoch: 1444800, elapsed: 1.09e+01, train loss: 8.74009e-07, val loss: 1.50730e-06, min loss: 7.81431e-07\n",
      "Epoch: 1444900, elapsed: 1.10e+01, train loss: 8.39293e-07, val loss: 1.52269e-06, min loss: 7.81431e-07\n",
      "Epoch: 1445000, elapsed: 1.09e+01, train loss: 8.27328e-07, val loss: 1.57876e-06, min loss: 7.81431e-07\n",
      "Epoch: 1445100, elapsed: 1.29e+01, train loss: 8.15732e-07, val loss: 1.54986e-06, min loss: 7.81431e-07\n",
      "Epoch: 1445200, elapsed: 1.08e+01, train loss: 2.11481e-06, val loss: 2.51506e-06, min loss: 7.81431e-07\n",
      "Epoch: 1445300, elapsed: 1.06e+01, train loss: 9.32144e-07, val loss: 1.66304e-06, min loss: 7.81431e-07\n",
      "Epoch: 1445400, elapsed: 1.09e+01, train loss: 2.00978e-06, val loss: 2.76771e-06, min loss: 7.81431e-07\n",
      "Epoch: 1445500, elapsed: 1.08e+01, train loss: 7.81056e-07, val loss: 1.45495e-06, min loss: 7.81056e-07\n",
      "Epoch: 1445600, elapsed: 1.09e+01, train loss: 7.81612e-07, val loss: 1.46351e-06, min loss: 7.81056e-07\n",
      "Epoch: 1445700, elapsed: 1.08e+01, train loss: 9.16539e-07, val loss: 1.62209e-06, min loss: 7.81056e-07\n",
      "Epoch: 1445800, elapsed: 1.09e+01, train loss: 5.30490e-06, val loss: 5.67775e-06, min loss: 7.81056e-07\n",
      "Epoch: 1445900, elapsed: 1.09e+01, train loss: 1.10601e-06, val loss: 1.86314e-06, min loss: 7.81056e-07\n",
      "Epoch: 1446000, elapsed: 1.09e+01, train loss: 7.82755e-07, val loss: 1.48578e-06, min loss: 7.81056e-07\n",
      "Epoch: 1446100, elapsed: 1.10e+01, train loss: 2.97998e-06, val loss: 3.14091e-06, min loss: 7.81056e-07\n",
      "Epoch: 1446200, elapsed: 1.09e+01, train loss: 7.82904e-07, val loss: 1.45732e-06, min loss: 7.81056e-07\n",
      "Epoch: 1446300, elapsed: 1.08e+01, train loss: 7.79223e-07, val loss: 1.45999e-06, min loss: 7.79223e-07\n",
      "Epoch: 1446400, elapsed: 1.07e+01, train loss: 1.64090e-06, val loss: 2.73047e-06, min loss: 7.79223e-07\n",
      "Epoch: 1446500, elapsed: 1.05e+01, train loss: 8.50702e-07, val loss: 1.59227e-06, min loss: 7.79223e-07\n",
      "Epoch: 1446600, elapsed: 1.07e+01, train loss: 7.78937e-07, val loss: 1.45722e-06, min loss: 7.78937e-07\n",
      "Epoch: 1446700, elapsed: 1.07e+01, train loss: 7.92459e-07, val loss: 1.48034e-06, min loss: 7.78937e-07\n",
      "Epoch: 1446800, elapsed: 1.08e+01, train loss: 8.93311e-07, val loss: 1.69837e-06, min loss: 7.78937e-07\n",
      "Epoch: 1446900, elapsed: 1.09e+01, train loss: 7.78529e-07, val loss: 1.45740e-06, min loss: 7.78529e-07\n",
      "Epoch: 1447000, elapsed: 1.07e+01, train loss: 8.83865e-07, val loss: 1.60417e-06, min loss: 7.78529e-07\n",
      "Epoch: 1447100, elapsed: 1.50e+01, train loss: 1.70919e-06, val loss: 1.90174e-06, min loss: 7.78529e-07\n",
      "Epoch: 1447200, elapsed: 1.09e+01, train loss: 2.20368e-06, val loss: 3.19138e-06, min loss: 7.78529e-07\n",
      "Epoch: 1447300, elapsed: 1.09e+01, train loss: 1.28072e-06, val loss: 1.63418e-06, min loss: 7.78529e-07\n",
      "Epoch: 1447400, elapsed: 1.05e+01, train loss: 8.75659e-07, val loss: 1.60178e-06, min loss: 7.78529e-07\n",
      "Epoch: 1447500, elapsed: 1.10e+01, train loss: 7.78346e-07, val loss: 1.45659e-06, min loss: 7.78346e-07\n",
      "Epoch: 1447600, elapsed: 1.10e+01, train loss: 7.85634e-07, val loss: 1.46359e-06, min loss: 7.78346e-07\n",
      "Epoch: 1447700, elapsed: 1.08e+01, train loss: 1.35023e-06, val loss: 2.21750e-06, min loss: 7.78346e-07\n",
      "Epoch: 1447800, elapsed: 1.11e+01, train loss: 7.97885e-07, val loss: 1.45852e-06, min loss: 7.78346e-07\n",
      "Epoch: 1447900, elapsed: 1.09e+01, train loss: 8.12414e-07, val loss: 1.52769e-06, min loss: 7.78346e-07\n",
      "Epoch: 1448000, elapsed: 1.09e+01, train loss: 8.33379e-07, val loss: 1.48745e-06, min loss: 7.78346e-07\n",
      "Epoch: 1448100, elapsed: 1.09e+01, train loss: 1.51870e-06, val loss: 2.33004e-06, min loss: 7.78346e-07\n",
      "Epoch: 1448200, elapsed: 1.08e+01, train loss: 7.94034e-07, val loss: 1.45777e-06, min loss: 7.78346e-07\n",
      "Epoch: 1448300, elapsed: 1.10e+01, train loss: 8.03992e-07, val loss: 1.46701e-06, min loss: 7.78346e-07\n",
      "Epoch: 1448400, elapsed: 1.07e+01, train loss: 7.87235e-07, val loss: 1.46086e-06, min loss: 7.78346e-07\n",
      "Epoch: 1448500, elapsed: 1.11e+01, train loss: 8.07925e-07, val loss: 1.47245e-06, min loss: 7.78346e-07\n",
      "Epoch: 1448600, elapsed: 1.08e+01, train loss: 7.79711e-07, val loss: 1.47067e-06, min loss: 7.78346e-07\n",
      "Epoch: 1448700, elapsed: 1.08e+01, train loss: 7.79082e-07, val loss: 1.46148e-06, min loss: 7.78346e-07\n",
      "Epoch: 1448800, elapsed: 1.07e+01, train loss: 7.92029e-07, val loss: 1.45942e-06, min loss: 7.78346e-07\n",
      "Epoch: 1448900, elapsed: 1.08e+01, train loss: 1.07625e-06, val loss: 1.84056e-06, min loss: 7.78346e-07\n",
      "Epoch: 1449000, elapsed: 1.08e+01, train loss: 7.94861e-07, val loss: 1.44645e-06, min loss: 7.78346e-07\n",
      "Epoch: 1449100, elapsed: 1.06e+01, train loss: 7.77079e-07, val loss: 1.46068e-06, min loss: 7.77079e-07\n",
      "Epoch: 1449200, elapsed: 1.07e+01, train loss: 8.06945e-07, val loss: 1.47419e-06, min loss: 7.77079e-07\n",
      "Epoch: 1449300, elapsed: 1.09e+01, train loss: 8.56827e-07, val loss: 1.59615e-06, min loss: 7.77079e-07\n",
      "Epoch: 1449400, elapsed: 1.08e+01, train loss: 1.19003e-06, val loss: 1.79994e-06, min loss: 7.77079e-07\n",
      "Epoch: 1449500, elapsed: 1.08e+01, train loss: 7.79408e-07, val loss: 1.45763e-06, min loss: 7.77079e-07\n",
      "Epoch: 1449600, elapsed: 1.08e+01, train loss: 8.39550e-07, val loss: 1.48476e-06, min loss: 7.77079e-07\n",
      "Epoch: 1449700, elapsed: 1.47e+01, train loss: 7.76609e-07, val loss: 1.46084e-06, min loss: 7.76609e-07\n",
      "Epoch: 1449800, elapsed: 1.11e+01, train loss: 7.77056e-07, val loss: 1.45282e-06, min loss: 7.76609e-07\n",
      "Epoch: 1449900, elapsed: 1.10e+01, train loss: 8.14405e-07, val loss: 1.49295e-06, min loss: 7.76609e-07\n",
      "Epoch: 1450000, elapsed: 1.11e+01, train loss: 8.05531e-07, val loss: 1.47700e-06, min loss: 7.76609e-07\n",
      "Epoch: 1450100, elapsed: 1.29e+01, train loss: 1.11375e-06, val loss: 1.86687e-06, min loss: 7.76609e-07\n",
      "Epoch: 1450200, elapsed: 1.09e+01, train loss: 2.63847e-06, val loss: 2.36524e-06, min loss: 7.76609e-07\n",
      "Epoch: 1450300, elapsed: 1.09e+01, train loss: 8.79316e-07, val loss: 1.52530e-06, min loss: 7.76609e-07\n",
      "Epoch: 1450400, elapsed: 1.10e+01, train loss: 8.83135e-07, val loss: 1.63924e-06, min loss: 7.76609e-07\n",
      "Epoch: 1450500, elapsed: 1.08e+01, train loss: 1.01890e-06, val loss: 1.63320e-06, min loss: 7.76609e-07\n",
      "Epoch: 1450600, elapsed: 1.07e+01, train loss: 7.93738e-07, val loss: 1.47753e-06, min loss: 7.76609e-07\n",
      "Epoch: 1450700, elapsed: 1.10e+01, train loss: 9.24359e-07, val loss: 1.70345e-06, min loss: 7.76609e-07\n",
      "Epoch: 1450800, elapsed: 1.09e+01, train loss: 2.84419e-06, val loss: 3.59507e-06, min loss: 7.76609e-07\n",
      "Epoch: 1450900, elapsed: 1.09e+01, train loss: 1.15727e-06, val loss: 1.66710e-06, min loss: 7.76609e-07\n",
      "Epoch: 1451000, elapsed: 1.08e+01, train loss: 7.82696e-07, val loss: 1.44853e-06, min loss: 7.76609e-07\n",
      "Epoch: 1451100, elapsed: 1.08e+01, train loss: 9.73443e-07, val loss: 1.59318e-06, min loss: 7.76609e-07\n",
      "Epoch: 1451200, elapsed: 1.10e+01, train loss: 7.82222e-07, val loss: 1.45507e-06, min loss: 7.76609e-07\n",
      "Epoch: 1451300, elapsed: 1.09e+01, train loss: 8.12394e-07, val loss: 1.49278e-06, min loss: 7.76609e-07\n",
      "Epoch: 1451400, elapsed: 1.09e+01, train loss: 7.78816e-07, val loss: 1.45045e-06, min loss: 7.76609e-07\n",
      "Epoch: 1451500, elapsed: 1.07e+01, train loss: 8.89597e-07, val loss: 1.47906e-06, min loss: 7.76609e-07\n",
      "Epoch: 1451600, elapsed: 1.08e+01, train loss: 1.44538e-06, val loss: 2.33501e-06, min loss: 7.76609e-07\n",
      "Epoch: 1451700, elapsed: 1.06e+01, train loss: 8.70434e-07, val loss: 1.56341e-06, min loss: 7.76609e-07\n",
      "Epoch: 1451800, elapsed: 1.07e+01, train loss: 7.76404e-07, val loss: 1.46036e-06, min loss: 7.76404e-07\n",
      "Epoch: 1451900, elapsed: 1.08e+01, train loss: 8.75819e-07, val loss: 1.64186e-06, min loss: 7.76404e-07\n",
      "Epoch: 1452000, elapsed: 1.07e+01, train loss: 7.79553e-07, val loss: 1.46025e-06, min loss: 7.76404e-07\n",
      "Epoch: 1452100, elapsed: 1.08e+01, train loss: 8.93336e-07, val loss: 1.53264e-06, min loss: 7.76404e-07\n",
      "Epoch: 1452200, elapsed: 1.08e+01, train loss: 8.99049e-07, val loss: 1.68160e-06, min loss: 7.76404e-07\n",
      "Epoch: 1452300, elapsed: 1.47e+01, train loss: 7.75320e-07, val loss: 1.45724e-06, min loss: 7.75320e-07\n",
      "Epoch: 1452400, elapsed: 1.09e+01, train loss: 8.07748e-07, val loss: 1.45416e-06, min loss: 7.75320e-07\n",
      "Epoch: 1452500, elapsed: 1.08e+01, train loss: 7.84580e-07, val loss: 1.48374e-06, min loss: 7.75320e-07\n",
      "Epoch: 1452600, elapsed: 1.09e+01, train loss: 7.76654e-07, val loss: 1.46206e-06, min loss: 7.75320e-07\n",
      "Epoch: 1452700, elapsed: 1.10e+01, train loss: 7.78043e-07, val loss: 1.47183e-06, min loss: 7.75320e-07\n",
      "Epoch: 1452800, elapsed: 1.08e+01, train loss: 8.15835e-07, val loss: 1.52879e-06, min loss: 7.75320e-07\n",
      "Epoch: 1452900, elapsed: 1.10e+01, train loss: 7.97301e-07, val loss: 1.45652e-06, min loss: 7.75320e-07\n",
      "Epoch: 1453000, elapsed: 1.10e+01, train loss: 8.00767e-07, val loss: 1.49789e-06, min loss: 7.75320e-07\n",
      "Epoch: 1453100, elapsed: 1.09e+01, train loss: 7.83364e-07, val loss: 1.44524e-06, min loss: 7.75320e-07\n",
      "Epoch: 1453200, elapsed: 1.10e+01, train loss: 1.12868e-06, val loss: 1.61771e-06, min loss: 7.75320e-07\n",
      "Epoch: 1453300, elapsed: 1.09e+01, train loss: 8.20052e-07, val loss: 1.46370e-06, min loss: 7.75320e-07\n",
      "Epoch: 1453400, elapsed: 1.08e+01, train loss: 7.84286e-07, val loss: 1.45214e-06, min loss: 7.75320e-07\n",
      "Epoch: 1453500, elapsed: 1.09e+01, train loss: 8.91824e-07, val loss: 1.49177e-06, min loss: 7.75320e-07\n",
      "Epoch: 1453600, elapsed: 1.09e+01, train loss: 1.80932e-06, val loss: 3.01569e-06, min loss: 7.75320e-07\n",
      "Epoch: 1453700, elapsed: 1.06e+01, train loss: 7.74468e-07, val loss: 1.45820e-06, min loss: 7.74468e-07\n",
      "Epoch: 1453800, elapsed: 1.08e+01, train loss: 8.86498e-07, val loss: 1.56178e-06, min loss: 7.74468e-07\n",
      "Epoch: 1453900, elapsed: 1.09e+01, train loss: 8.26703e-07, val loss: 1.47502e-06, min loss: 7.74468e-07\n",
      "Epoch: 1454000, elapsed: 1.10e+01, train loss: 8.47681e-07, val loss: 1.49034e-06, min loss: 7.74468e-07\n",
      "Epoch: 1454100, elapsed: 1.09e+01, train loss: 1.67498e-06, val loss: 2.34275e-06, min loss: 7.74468e-07\n",
      "Epoch: 1454200, elapsed: 1.07e+01, train loss: 9.61558e-07, val loss: 1.58898e-06, min loss: 7.74468e-07\n",
      "Epoch: 1454300, elapsed: 1.06e+01, train loss: 1.75982e-06, val loss: 2.33749e-06, min loss: 7.74468e-07\n",
      "Epoch: 1454400, elapsed: 1.08e+01, train loss: 8.92219e-07, val loss: 1.50343e-06, min loss: 7.74468e-07\n",
      "Epoch: 1454500, elapsed: 1.08e+01, train loss: 8.71438e-07, val loss: 1.56316e-06, min loss: 7.74468e-07\n",
      "Epoch: 1454600, elapsed: 1.08e+01, train loss: 1.81425e-06, val loss: 2.83644e-06, min loss: 7.74468e-07\n",
      "Epoch: 1454700, elapsed: 1.08e+01, train loss: 1.05906e-06, val loss: 1.70769e-06, min loss: 7.74468e-07\n",
      "Epoch: 1454800, elapsed: 1.07e+01, train loss: 7.85372e-07, val loss: 1.47193e-06, min loss: 7.74468e-07\n",
      "Epoch: 1454900, elapsed: 1.08e+01, train loss: 9.02084e-07, val loss: 1.56862e-06, min loss: 7.74468e-07\n",
      "Epoch: 1455000, elapsed: 1.50e+01, train loss: 9.71847e-07, val loss: 1.70188e-06, min loss: 7.74468e-07\n",
      "Epoch: 1455100, elapsed: 1.29e+01, train loss: 7.76086e-07, val loss: 1.46319e-06, min loss: 7.74468e-07\n",
      "Epoch: 1455200, elapsed: 1.09e+01, train loss: 7.79589e-07, val loss: 1.45646e-06, min loss: 7.74468e-07\n",
      "Epoch: 1455300, elapsed: 1.08e+01, train loss: 7.75257e-07, val loss: 1.44407e-06, min loss: 7.74468e-07\n",
      "Epoch: 1455400, elapsed: 1.10e+01, train loss: 7.73426e-07, val loss: 1.44853e-06, min loss: 7.73426e-07\n",
      "Epoch: 1455500, elapsed: 1.10e+01, train loss: 9.58223e-07, val loss: 1.50182e-06, min loss: 7.73426e-07\n",
      "Epoch: 1455600, elapsed: 1.08e+01, train loss: 8.84377e-07, val loss: 1.54055e-06, min loss: 7.73426e-07\n",
      "Epoch: 1455700, elapsed: 1.09e+01, train loss: 1.32400e-06, val loss: 1.59017e-06, min loss: 7.73426e-07\n",
      "Epoch: 1455800, elapsed: 1.11e+01, train loss: 7.73517e-07, val loss: 1.44916e-06, min loss: 7.73426e-07\n",
      "Epoch: 1455900, elapsed: 1.10e+01, train loss: 8.16034e-07, val loss: 1.54195e-06, min loss: 7.73426e-07\n",
      "Epoch: 1456000, elapsed: 1.09e+01, train loss: 8.34671e-07, val loss: 1.51027e-06, min loss: 7.73426e-07\n",
      "Epoch: 1456100, elapsed: 1.08e+01, train loss: 1.05855e-06, val loss: 1.63898e-06, min loss: 7.73426e-07\n",
      "Epoch: 1456200, elapsed: 1.09e+01, train loss: 1.68200e-06, val loss: 1.61904e-06, min loss: 7.73426e-07\n",
      "Epoch: 1456300, elapsed: 1.09e+01, train loss: 1.48688e-06, val loss: 1.93795e-06, min loss: 7.73426e-07\n",
      "Epoch: 1456400, elapsed: 1.09e+01, train loss: 8.10722e-07, val loss: 1.46957e-06, min loss: 7.73426e-07\n",
      "Epoch: 1456500, elapsed: 1.08e+01, train loss: 8.28587e-07, val loss: 1.46212e-06, min loss: 7.73426e-07\n",
      "Epoch: 1456600, elapsed: 1.08e+01, train loss: 1.84413e-06, val loss: 2.35161e-06, min loss: 7.73426e-07\n",
      "Epoch: 1456700, elapsed: 1.10e+01, train loss: 9.32433e-07, val loss: 1.47514e-06, min loss: 7.73426e-07\n",
      "Epoch: 1456800, elapsed: 1.08e+01, train loss: 4.35560e-06, val loss: 4.48203e-06, min loss: 7.73426e-07\n",
      "Epoch: 1456900, elapsed: 1.06e+01, train loss: 7.71528e-07, val loss: 1.45586e-06, min loss: 7.71528e-07\n",
      "Epoch: 1457000, elapsed: 1.07e+01, train loss: 7.71385e-07, val loss: 1.45210e-06, min loss: 7.71385e-07\n",
      "Epoch: 1457100, elapsed: 1.08e+01, train loss: 1.56313e-06, val loss: 2.04701e-06, min loss: 7.71385e-07\n",
      "Epoch: 1457200, elapsed: 1.08e+01, train loss: 1.49536e-06, val loss: 1.84634e-06, min loss: 7.71385e-07\n",
      "Epoch: 1457300, elapsed: 1.10e+01, train loss: 7.71753e-07, val loss: 1.44742e-06, min loss: 7.71385e-07\n",
      "Epoch: 1457400, elapsed: 1.08e+01, train loss: 1.18563e-06, val loss: 1.79699e-06, min loss: 7.71385e-07\n",
      "Epoch: 1457500, elapsed: 1.09e+01, train loss: 7.70541e-07, val loss: 1.45058e-06, min loss: 7.70541e-07\n",
      "Epoch: 1457600, elapsed: 1.48e+01, train loss: 3.83509e-06, val loss: 4.00976e-06, min loss: 7.70541e-07\n",
      "Epoch: 1457700, elapsed: 1.11e+01, train loss: 7.78956e-07, val loss: 1.47328e-06, min loss: 7.70541e-07\n",
      "Epoch: 1457800, elapsed: 1.10e+01, train loss: 7.70810e-07, val loss: 1.45038e-06, min loss: 7.70541e-07\n",
      "Epoch: 1457900, elapsed: 1.10e+01, train loss: 7.78350e-07, val loss: 1.45513e-06, min loss: 7.70541e-07\n",
      "Epoch: 1458000, elapsed: 1.10e+01, train loss: 7.88706e-07, val loss: 1.50250e-06, min loss: 7.70541e-07\n",
      "Epoch: 1458100, elapsed: 1.10e+01, train loss: 7.75772e-07, val loss: 1.45626e-06, min loss: 7.70541e-07\n",
      "Epoch: 1458200, elapsed: 1.08e+01, train loss: 1.64782e-06, val loss: 2.28616e-06, min loss: 7.70541e-07\n",
      "Epoch: 1458300, elapsed: 1.08e+01, train loss: 7.85581e-07, val loss: 1.47786e-06, min loss: 7.70541e-07\n",
      "Epoch: 1458400, elapsed: 1.08e+01, train loss: 7.80786e-07, val loss: 1.45066e-06, min loss: 7.70541e-07\n",
      "Epoch: 1458500, elapsed: 1.07e+01, train loss: 1.16487e-06, val loss: 1.94537e-06, min loss: 7.70541e-07\n",
      "Epoch: 1458600, elapsed: 1.09e+01, train loss: 8.21821e-07, val loss: 1.46887e-06, min loss: 7.70541e-07\n",
      "Epoch: 1458700, elapsed: 1.08e+01, train loss: 8.99597e-07, val loss: 1.52627e-06, min loss: 7.70541e-07\n",
      "Epoch: 1458800, elapsed: 1.10e+01, train loss: 8.52861e-07, val loss: 1.58254e-06, min loss: 7.70541e-07\n",
      "Epoch: 1458900, elapsed: 1.08e+01, train loss: 9.03309e-07, val loss: 1.53645e-06, min loss: 7.70541e-07\n",
      "Epoch: 1459000, elapsed: 1.09e+01, train loss: 1.90076e-06, val loss: 2.25954e-06, min loss: 7.70541e-07\n",
      "Epoch: 1459100, elapsed: 1.08e+01, train loss: 7.81792e-07, val loss: 1.48199e-06, min loss: 7.70541e-07\n",
      "Epoch: 1459200, elapsed: 1.07e+01, train loss: 7.84112e-07, val loss: 1.47888e-06, min loss: 7.70541e-07\n",
      "Epoch: 1459300, elapsed: 1.08e+01, train loss: 8.11894e-07, val loss: 1.50555e-06, min loss: 7.70541e-07\n",
      "Epoch: 1459400, elapsed: 1.10e+01, train loss: 1.22775e-06, val loss: 1.90960e-06, min loss: 7.70541e-07\n",
      "Epoch: 1459500, elapsed: 1.07e+01, train loss: 1.33564e-06, val loss: 2.29539e-06, min loss: 7.70541e-07\n",
      "Epoch: 1459600, elapsed: 1.07e+01, train loss: 7.98609e-07, val loss: 1.50611e-06, min loss: 7.70541e-07\n",
      "Epoch: 1459700, elapsed: 1.08e+01, train loss: 8.24522e-07, val loss: 1.46995e-06, min loss: 7.70541e-07\n",
      "Epoch: 1459800, elapsed: 1.10e+01, train loss: 7.74572e-07, val loss: 1.44381e-06, min loss: 7.70541e-07\n",
      "Epoch: 1459900, elapsed: 1.08e+01, train loss: 1.10136e-06, val loss: 1.99951e-06, min loss: 7.70541e-07\n",
      "Epoch: 1460000, elapsed: 1.09e+01, train loss: 8.26309e-07, val loss: 1.53047e-06, min loss: 7.70541e-07\n",
      "Epoch: 1460100, elapsed: 1.29e+01, train loss: 8.14192e-07, val loss: 1.50283e-06, min loss: 7.70541e-07\n",
      "Epoch: 1460200, elapsed: 1.48e+01, train loss: 8.25369e-07, val loss: 1.51815e-06, min loss: 7.70541e-07\n",
      "Epoch: 1460300, elapsed: 1.10e+01, train loss: 7.73569e-07, val loss: 1.44541e-06, min loss: 7.70541e-07\n",
      "Epoch: 1460400, elapsed: 1.09e+01, train loss: 1.89440e-06, val loss: 3.51904e-06, min loss: 7.70541e-07\n",
      "Epoch: 1460500, elapsed: 1.09e+01, train loss: 7.68387e-07, val loss: 1.44830e-06, min loss: 7.68387e-07\n",
      "Epoch: 1460600, elapsed: 1.08e+01, train loss: 4.86499e-06, val loss: 5.92051e-06, min loss: 7.68387e-07\n",
      "Epoch: 1460700, elapsed: 1.10e+01, train loss: 7.68129e-07, val loss: 1.44700e-06, min loss: 7.68129e-07\n",
      "Epoch: 1460800, elapsed: 1.10e+01, train loss: 8.35081e-07, val loss: 1.47644e-06, min loss: 7.68129e-07\n",
      "Epoch: 1460900, elapsed: 1.09e+01, train loss: 1.16159e-06, val loss: 1.55872e-06, min loss: 7.68129e-07\n",
      "Epoch: 1461000, elapsed: 1.10e+01, train loss: 8.79206e-07, val loss: 1.60092e-06, min loss: 7.68129e-07\n",
      "Epoch: 1461100, elapsed: 1.09e+01, train loss: 1.24691e-06, val loss: 2.13524e-06, min loss: 7.68129e-07\n",
      "Epoch: 1461200, elapsed: 1.10e+01, train loss: 1.01394e-06, val loss: 1.65828e-06, min loss: 7.68129e-07\n",
      "Epoch: 1461300, elapsed: 1.10e+01, train loss: 7.75309e-07, val loss: 1.44781e-06, min loss: 7.68129e-07\n",
      "Epoch: 1461400, elapsed: 1.09e+01, train loss: 8.31330e-07, val loss: 1.47625e-06, min loss: 7.68129e-07\n",
      "Epoch: 1461500, elapsed: 1.09e+01, train loss: 7.76767e-07, val loss: 1.42839e-06, min loss: 7.68129e-07\n",
      "Epoch: 1461600, elapsed: 1.07e+01, train loss: 7.69374e-07, val loss: 1.45964e-06, min loss: 7.68129e-07\n",
      "Epoch: 1461700, elapsed: 1.08e+01, train loss: 1.02739e-06, val loss: 1.80885e-06, min loss: 7.68129e-07\n",
      "Epoch: 1461800, elapsed: 1.08e+01, train loss: 1.75447e-06, val loss: 2.99637e-06, min loss: 7.68129e-07\n",
      "Epoch: 1461900, elapsed: 1.08e+01, train loss: 8.84685e-07, val loss: 1.64570e-06, min loss: 7.68129e-07\n",
      "Epoch: 1462000, elapsed: 1.09e+01, train loss: 7.83403e-07, val loss: 1.44406e-06, min loss: 7.68129e-07\n",
      "Epoch: 1462100, elapsed: 1.08e+01, train loss: 2.03347e-06, val loss: 2.81930e-06, min loss: 7.68129e-07\n",
      "Epoch: 1462200, elapsed: 1.09e+01, train loss: 8.78712e-07, val loss: 1.60341e-06, min loss: 7.68129e-07\n",
      "Epoch: 1462300, elapsed: 1.08e+01, train loss: 7.75542e-07, val loss: 1.46784e-06, min loss: 7.68129e-07\n",
      "Epoch: 1462400, elapsed: 1.07e+01, train loss: 7.69940e-07, val loss: 1.45013e-06, min loss: 7.68129e-07\n",
      "Epoch: 1462500, elapsed: 1.07e+01, train loss: 1.20063e-06, val loss: 2.08161e-06, min loss: 7.68129e-07\n",
      "Epoch: 1462600, elapsed: 1.08e+01, train loss: 1.44324e-06, val loss: 2.40517e-06, min loss: 7.68129e-07\n",
      "Epoch: 1462700, elapsed: 1.07e+01, train loss: 7.90401e-07, val loss: 1.47753e-06, min loss: 7.68129e-07\n",
      "Epoch: 1462800, elapsed: 1.06e+01, train loss: 7.68287e-07, val loss: 1.44747e-06, min loss: 7.68129e-07\n",
      "Epoch: 1462900, elapsed: 1.50e+01, train loss: 7.68685e-07, val loss: 1.44166e-06, min loss: 7.68129e-07\n",
      "Epoch: 1463000, elapsed: 1.11e+01, train loss: 1.73931e-06, val loss: 2.68942e-06, min loss: 7.68129e-07\n",
      "Epoch: 1463100, elapsed: 1.07e+01, train loss: 7.66325e-07, val loss: 1.44586e-06, min loss: 7.66325e-07\n",
      "Epoch: 1463200, elapsed: 1.09e+01, train loss: 9.41189e-07, val loss: 1.50154e-06, min loss: 7.66325e-07\n",
      "Epoch: 1463300, elapsed: 1.09e+01, train loss: 7.66432e-07, val loss: 1.43970e-06, min loss: 7.66325e-07\n",
      "Epoch: 1463400, elapsed: 1.08e+01, train loss: 8.01313e-07, val loss: 1.45752e-06, min loss: 7.66325e-07\n",
      "Epoch: 1463500, elapsed: 1.08e+01, train loss: 8.21150e-07, val loss: 1.51346e-06, min loss: 7.66325e-07\n",
      "Epoch: 1463600, elapsed: 1.09e+01, train loss: 7.67981e-07, val loss: 1.43591e-06, min loss: 7.66325e-07\n",
      "Epoch: 1463700, elapsed: 1.09e+01, train loss: 7.68308e-07, val loss: 1.43740e-06, min loss: 7.66325e-07\n",
      "Epoch: 1463800, elapsed: 1.07e+01, train loss: 7.68612e-07, val loss: 1.46525e-06, min loss: 7.66325e-07\n",
      "Epoch: 1463900, elapsed: 1.07e+01, train loss: 7.74678e-07, val loss: 1.46603e-06, min loss: 7.66325e-07\n",
      "Epoch: 1464000, elapsed: 1.09e+01, train loss: 1.47451e-06, val loss: 2.54276e-06, min loss: 7.66325e-07\n",
      "Epoch: 1464100, elapsed: 1.09e+01, train loss: 2.79062e-06, val loss: 3.82019e-06, min loss: 7.66325e-07\n",
      "Epoch: 1464200, elapsed: 1.08e+01, train loss: 1.22206e-06, val loss: 1.62173e-06, min loss: 7.66325e-07\n",
      "Epoch: 1464300, elapsed: 1.08e+01, train loss: 2.06522e-06, val loss: 2.04334e-06, min loss: 7.66325e-07\n",
      "Epoch: 1464400, elapsed: 1.07e+01, train loss: 3.09927e-06, val loss: 3.60332e-06, min loss: 7.66325e-07\n",
      "Epoch: 1464500, elapsed: 1.07e+01, train loss: 1.14971e-06, val loss: 1.66747e-06, min loss: 7.66325e-07\n",
      "Epoch: 1464600, elapsed: 1.07e+01, train loss: 7.66607e-07, val loss: 1.44089e-06, min loss: 7.66325e-07\n",
      "Epoch: 1464700, elapsed: 1.07e+01, train loss: 7.67000e-07, val loss: 1.44892e-06, min loss: 7.66325e-07\n",
      "Epoch: 1464800, elapsed: 1.07e+01, train loss: 7.90743e-07, val loss: 1.46146e-06, min loss: 7.66325e-07\n",
      "Epoch: 1464900, elapsed: 1.09e+01, train loss: 8.22914e-07, val loss: 1.44690e-06, min loss: 7.66325e-07\n",
      "Epoch: 1465000, elapsed: 1.08e+01, train loss: 8.64970e-07, val loss: 1.46305e-06, min loss: 7.66325e-07\n",
      "Epoch: 1465100, elapsed: 1.28e+01, train loss: 7.70809e-07, val loss: 1.44347e-06, min loss: 7.66325e-07\n",
      "Epoch: 1465200, elapsed: 1.08e+01, train loss: 8.03231e-07, val loss: 1.47743e-06, min loss: 7.66325e-07\n",
      "Epoch: 1465300, elapsed: 1.08e+01, train loss: 9.01986e-07, val loss: 1.53052e-06, min loss: 7.66325e-07\n",
      "Epoch: 1465400, elapsed: 1.09e+01, train loss: 7.79359e-07, val loss: 1.50639e-06, min loss: 7.66325e-07\n",
      "Epoch: 1465500, elapsed: 1.49e+01, train loss: 3.10916e-06, val loss: 2.74951e-06, min loss: 7.66325e-07\n",
      "Epoch: 1465600, elapsed: 1.09e+01, train loss: 9.13867e-07, val loss: 1.53070e-06, min loss: 7.66325e-07\n",
      "Epoch: 1465700, elapsed: 1.12e+01, train loss: 7.80806e-07, val loss: 1.46962e-06, min loss: 7.66325e-07\n",
      "Epoch: 1465800, elapsed: 1.11e+01, train loss: 1.01262e-06, val loss: 1.59294e-06, min loss: 7.66325e-07\n",
      "Epoch: 1465900, elapsed: 1.09e+01, train loss: 9.08602e-07, val loss: 1.60677e-06, min loss: 7.66325e-07\n",
      "Epoch: 1466000, elapsed: 1.09e+01, train loss: 7.65013e-07, val loss: 1.45199e-06, min loss: 7.65013e-07\n",
      "Epoch: 1466100, elapsed: 1.10e+01, train loss: 7.84203e-07, val loss: 1.45268e-06, min loss: 7.65013e-07\n",
      "Epoch: 1466200, elapsed: 1.09e+01, train loss: 5.38204e-06, val loss: 6.83811e-06, min loss: 7.65013e-07\n",
      "Epoch: 1466300, elapsed: 1.11e+01, train loss: 7.63909e-07, val loss: 1.44235e-06, min loss: 7.63909e-07\n",
      "Epoch: 1466400, elapsed: 1.07e+01, train loss: 1.11058e-06, val loss: 1.94771e-06, min loss: 7.63909e-07\n",
      "Epoch: 1466500, elapsed: 1.07e+01, train loss: 1.34724e-06, val loss: 2.06403e-06, min loss: 7.63909e-07\n",
      "Epoch: 1466600, elapsed: 1.09e+01, train loss: 1.33306e-06, val loss: 2.02493e-06, min loss: 7.63909e-07\n",
      "Epoch: 1466700, elapsed: 1.11e+01, train loss: 8.87577e-07, val loss: 1.55145e-06, min loss: 7.63909e-07\n",
      "Epoch: 1466800, elapsed: 1.10e+01, train loss: 9.67731e-07, val loss: 1.83366e-06, min loss: 7.63909e-07\n",
      "Epoch: 1466900, elapsed: 1.09e+01, train loss: 1.44670e-06, val loss: 2.23173e-06, min loss: 7.63909e-07\n",
      "Epoch: 1467000, elapsed: 1.09e+01, train loss: 1.14831e-06, val loss: 1.83157e-06, min loss: 7.63909e-07\n",
      "Epoch: 1467100, elapsed: 1.09e+01, train loss: 7.71553e-07, val loss: 1.43632e-06, min loss: 7.63909e-07\n",
      "Epoch: 1467200, elapsed: 1.07e+01, train loss: 9.31995e-07, val loss: 1.67399e-06, min loss: 7.63909e-07\n",
      "Epoch: 1467300, elapsed: 1.10e+01, train loss: 7.70621e-07, val loss: 1.43271e-06, min loss: 7.63909e-07\n",
      "Epoch: 1467400, elapsed: 1.09e+01, train loss: 1.14692e-06, val loss: 1.65067e-06, min loss: 7.63909e-07\n",
      "Epoch: 1467500, elapsed: 1.09e+01, train loss: 8.16676e-07, val loss: 1.49536e-06, min loss: 7.63909e-07\n",
      "Epoch: 1467600, elapsed: 1.08e+01, train loss: 7.76213e-07, val loss: 1.46756e-06, min loss: 7.63909e-07\n",
      "Epoch: 1467700, elapsed: 1.06e+01, train loss: 7.69112e-07, val loss: 1.47829e-06, min loss: 7.63909e-07\n",
      "Epoch: 1467800, elapsed: 1.08e+01, train loss: 3.26311e-06, val loss: 3.62954e-06, min loss: 7.63909e-07\n",
      "Epoch: 1467900, elapsed: 1.09e+01, train loss: 9.34958e-07, val loss: 1.73248e-06, min loss: 7.63909e-07\n",
      "Epoch: 1468000, elapsed: 1.07e+01, train loss: 1.46174e-06, val loss: 2.23899e-06, min loss: 7.63909e-07\n",
      "Epoch: 1468100, elapsed: 1.08e+01, train loss: 7.77239e-07, val loss: 1.47162e-06, min loss: 7.63909e-07\n",
      "Epoch: 1468200, elapsed: 1.50e+01, train loss: 7.72308e-07, val loss: 1.45693e-06, min loss: 7.63909e-07\n",
      "Epoch: 1468300, elapsed: 1.11e+01, train loss: 2.90527e-06, val loss: 4.20133e-06, min loss: 7.63909e-07\n",
      "Epoch: 1468400, elapsed: 1.10e+01, train loss: 4.98271e-06, val loss: 5.64993e-06, min loss: 7.63909e-07\n",
      "Epoch: 1468500, elapsed: 1.08e+01, train loss: 3.05435e-06, val loss: 3.90350e-06, min loss: 7.63909e-07\n",
      "Epoch: 1468600, elapsed: 1.08e+01, train loss: 2.68362e-06, val loss: 3.48251e-06, min loss: 7.63909e-07\n",
      "Epoch: 1468700, elapsed: 1.10e+01, train loss: 2.17509e-06, val loss: 2.99761e-06, min loss: 7.63909e-07\n",
      "Epoch: 1468800, elapsed: 1.07e+01, train loss: 1.97765e-06, val loss: 2.77306e-06, min loss: 7.63909e-07\n",
      "Epoch: 1468900, elapsed: 1.10e+01, train loss: 1.84138e-06, val loss: 2.60385e-06, min loss: 7.63909e-07\n",
      "Epoch: 1469000, elapsed: 1.10e+01, train loss: 1.73390e-06, val loss: 2.47525e-06, min loss: 7.63909e-07\n",
      "Epoch: 1469100, elapsed: 1.09e+01, train loss: 2.57555e-06, val loss: 5.26750e-06, min loss: 7.63909e-07\n",
      "Epoch: 1469200, elapsed: 1.08e+01, train loss: 1.58516e-06, val loss: 2.27929e-06, min loss: 7.63909e-07\n",
      "Epoch: 1469300, elapsed: 1.08e+01, train loss: 1.52917e-06, val loss: 2.20370e-06, min loss: 7.63909e-07\n",
      "Epoch: 1469400, elapsed: 1.09e+01, train loss: 1.50072e-06, val loss: 2.13869e-06, min loss: 7.63909e-07\n",
      "Epoch: 1469500, elapsed: 1.08e+01, train loss: 1.43982e-06, val loss: 2.08249e-06, min loss: 7.63909e-07\n",
      "Epoch: 1469600, elapsed: 1.08e+01, train loss: 1.69313e-06, val loss: 2.17833e-06, min loss: 7.63909e-07\n",
      "Epoch: 1469700, elapsed: 1.08e+01, train loss: 1.37031e-06, val loss: 1.98886e-06, min loss: 7.63909e-07\n",
      "Epoch: 1469800, elapsed: 1.08e+01, train loss: 1.34027e-06, val loss: 1.95452e-06, min loss: 7.63909e-07\n",
      "Epoch: 1469900, elapsed: 1.11e+01, train loss: 1.46339e-06, val loss: 2.52832e-06, min loss: 7.63909e-07\n",
      "Epoch: 1470000, elapsed: 1.06e+01, train loss: 1.29041e-06, val loss: 1.89801e-06, min loss: 7.63909e-07\n",
      "Epoch: 1470100, elapsed: 1.27e+01, train loss: 1.26814e-06, val loss: 1.86961e-06, min loss: 7.63909e-07\n",
      "Epoch: 1470200, elapsed: 1.08e+01, train loss: 1.24446e-06, val loss: 1.84791e-06, min loss: 7.63909e-07\n",
      "Epoch: 1470300, elapsed: 1.07e+01, train loss: 1.33086e-06, val loss: 1.81527e-06, min loss: 7.63909e-07\n",
      "Epoch: 1470400, elapsed: 1.08e+01, train loss: 1.20714e-06, val loss: 1.81334e-06, min loss: 7.63909e-07\n",
      "Epoch: 1470500, elapsed: 1.07e+01, train loss: 2.37055e-06, val loss: 3.73981e-06, min loss: 7.63909e-07\n",
      "Epoch: 1470600, elapsed: 1.08e+01, train loss: 1.17415e-06, val loss: 1.77430e-06, min loss: 7.63909e-07\n",
      "Epoch: 1470700, elapsed: 1.08e+01, train loss: 2.08064e-06, val loss: 2.84876e-06, min loss: 7.63909e-07\n",
      "Epoch: 1470800, elapsed: 1.51e+01, train loss: 1.14533e-06, val loss: 1.74842e-06, min loss: 7.63909e-07\n",
      "Epoch: 1470900, elapsed: 1.10e+01, train loss: 1.17958e-06, val loss: 1.77903e-06, min loss: 7.63909e-07\n",
      "Epoch: 1471000, elapsed: 1.10e+01, train loss: 1.85650e-06, val loss: 2.55085e-06, min loss: 7.63909e-07\n",
      "Epoch: 1471100, elapsed: 1.09e+01, train loss: 1.10921e-06, val loss: 1.71638e-06, min loss: 7.63909e-07\n",
      "Epoch: 1471200, elapsed: 1.09e+01, train loss: 1.16081e-06, val loss: 1.72012e-06, min loss: 7.63909e-07\n",
      "Epoch: 1471300, elapsed: 1.08e+01, train loss: 1.65493e-06, val loss: 2.52876e-06, min loss: 7.63909e-07\n",
      "Epoch: 1471400, elapsed: 1.11e+01, train loss: 1.07936e-06, val loss: 1.68855e-06, min loss: 7.63909e-07\n",
      "Epoch: 1471500, elapsed: 1.11e+01, train loss: 1.09188e-06, val loss: 1.71132e-06, min loss: 7.63909e-07\n",
      "Epoch: 1471600, elapsed: 1.11e+01, train loss: 1.12685e-06, val loss: 1.79796e-06, min loss: 7.63909e-07\n",
      "Epoch: 1471700, elapsed: 1.09e+01, train loss: 1.05756e-06, val loss: 1.66838e-06, min loss: 7.63909e-07\n",
      "Epoch: 1471800, elapsed: 1.10e+01, train loss: 1.04730e-06, val loss: 1.65723e-06, min loss: 7.63909e-07\n",
      "Epoch: 1471900, elapsed: 1.08e+01, train loss: 1.07490e-06, val loss: 1.71092e-06, min loss: 7.63909e-07\n",
      "Epoch: 1472000, elapsed: 1.09e+01, train loss: 1.05471e-06, val loss: 1.67710e-06, min loss: 7.63909e-07\n",
      "Epoch: 1472100, elapsed: 1.09e+01, train loss: 1.02911e-06, val loss: 1.64317e-06, min loss: 7.63909e-07\n",
      "Epoch: 1472200, elapsed: 1.09e+01, train loss: 1.02068e-06, val loss: 1.64204e-06, min loss: 7.63909e-07\n",
      "Epoch: 1472300, elapsed: 1.08e+01, train loss: 1.03469e-06, val loss: 1.62989e-06, min loss: 7.63909e-07\n",
      "Epoch: 1472400, elapsed: 1.06e+01, train loss: 1.01167e-06, val loss: 1.62220e-06, min loss: 7.63909e-07\n",
      "Epoch: 1472500, elapsed: 1.07e+01, train loss: 1.01678e-06, val loss: 1.64159e-06, min loss: 7.63909e-07\n",
      "Epoch: 1472600, elapsed: 1.09e+01, train loss: 1.00555e-06, val loss: 1.62874e-06, min loss: 7.63909e-07\n",
      "Epoch: 1472700, elapsed: 1.08e+01, train loss: 9.94766e-07, val loss: 1.60363e-06, min loss: 7.63909e-07\n",
      "Epoch: 1472800, elapsed: 1.07e+01, train loss: 9.97493e-07, val loss: 1.62026e-06, min loss: 7.63909e-07\n",
      "Epoch: 1472900, elapsed: 1.07e+01, train loss: 9.85486e-07, val loss: 1.60451e-06, min loss: 7.63909e-07\n",
      "Epoch: 1473000, elapsed: 1.08e+01, train loss: 9.83061e-07, val loss: 1.58362e-06, min loss: 7.63909e-07\n",
      "Epoch: 1473100, elapsed: 1.06e+01, train loss: 1.03910e-06, val loss: 1.63092e-06, min loss: 7.63909e-07\n",
      "Epoch: 1473200, elapsed: 1.08e+01, train loss: 1.17348e-06, val loss: 1.84202e-06, min loss: 7.63909e-07\n",
      "Epoch: 1473300, elapsed: 1.09e+01, train loss: 9.69697e-07, val loss: 1.57380e-06, min loss: 7.63909e-07\n",
      "Epoch: 1473400, elapsed: 1.09e+01, train loss: 1.19480e-06, val loss: 1.90570e-06, min loss: 7.63909e-07\n",
      "Epoch: 1473500, elapsed: 1.51e+01, train loss: 2.81924e-06, val loss: 3.71575e-06, min loss: 7.63909e-07\n",
      "Epoch: 1473600, elapsed: 1.12e+01, train loss: 1.76491e-06, val loss: 2.50056e-06, min loss: 7.63909e-07\n",
      "Epoch: 1473700, elapsed: 1.09e+01, train loss: 1.48366e-06, val loss: 2.26228e-06, min loss: 7.63909e-07\n",
      "Epoch: 1473800, elapsed: 1.10e+01, train loss: 9.52078e-07, val loss: 1.55476e-06, min loss: 7.63909e-07\n",
      "Epoch: 1473900, elapsed: 1.09e+01, train loss: 9.47579e-07, val loss: 1.55042e-06, min loss: 7.63909e-07\n",
      "Epoch: 1474000, elapsed: 1.08e+01, train loss: 1.19526e-06, val loss: 1.69329e-06, min loss: 7.63909e-07\n",
      "Epoch: 1474100, elapsed: 1.11e+01, train loss: 1.02898e-06, val loss: 1.73485e-06, min loss: 7.63909e-07\n",
      "Epoch: 1474200, elapsed: 1.10e+01, train loss: 9.37846e-07, val loss: 1.54719e-06, min loss: 7.63909e-07\n",
      "Epoch: 1474300, elapsed: 1.09e+01, train loss: 9.57593e-07, val loss: 1.55054e-06, min loss: 7.63909e-07\n",
      "Epoch: 1474400, elapsed: 1.08e+01, train loss: 1.32586e-06, val loss: 1.72336e-06, min loss: 7.63909e-07\n",
      "Epoch: 1474500, elapsed: 1.09e+01, train loss: 9.48451e-07, val loss: 1.53990e-06, min loss: 7.63909e-07\n",
      "Epoch: 1474600, elapsed: 1.09e+01, train loss: 9.47053e-07, val loss: 1.53619e-06, min loss: 7.63909e-07\n",
      "Epoch: 1474700, elapsed: 1.09e+01, train loss: 9.32137e-07, val loss: 1.53334e-06, min loss: 7.63909e-07\n",
      "Epoch: 1474800, elapsed: 1.09e+01, train loss: 9.24692e-07, val loss: 1.53910e-06, min loss: 7.63909e-07\n",
      "Epoch: 1474900, elapsed: 1.08e+01, train loss: 9.65266e-07, val loss: 1.58687e-06, min loss: 7.63909e-07\n",
      "Epoch: 1475000, elapsed: 1.08e+01, train loss: 1.18564e-06, val loss: 1.78294e-06, min loss: 7.63909e-07\n",
      "Epoch: 1475100, elapsed: 1.28e+01, train loss: 1.20357e-06, val loss: 1.78371e-06, min loss: 7.63909e-07\n",
      "Epoch: 1475200, elapsed: 1.07e+01, train loss: 1.80209e-06, val loss: 2.52703e-06, min loss: 7.63909e-07\n",
      "Epoch: 1475300, elapsed: 1.08e+01, train loss: 9.13230e-07, val loss: 1.51764e-06, min loss: 7.63909e-07\n",
      "Epoch: 1475400, elapsed: 1.11e+01, train loss: 9.12499e-07, val loss: 1.51276e-06, min loss: 7.63909e-07\n",
      "Epoch: 1475500, elapsed: 1.07e+01, train loss: 9.10240e-07, val loss: 1.50869e-06, min loss: 7.63909e-07\n",
      "Epoch: 1475600, elapsed: 1.07e+01, train loss: 2.02626e-06, val loss: 1.80550e-06, min loss: 7.63909e-07\n",
      "Epoch: 1475700, elapsed: 1.07e+01, train loss: 9.05229e-07, val loss: 1.51042e-06, min loss: 7.63909e-07\n",
      "Epoch: 1475800, elapsed: 1.07e+01, train loss: 9.29512e-07, val loss: 1.53930e-06, min loss: 7.63909e-07\n",
      "Epoch: 1475900, elapsed: 1.10e+01, train loss: 9.01898e-07, val loss: 1.50680e-06, min loss: 7.63909e-07\n",
      "Epoch: 1476000, elapsed: 1.08e+01, train loss: 9.08963e-07, val loss: 1.49932e-06, min loss: 7.63909e-07\n",
      "Epoch: 1476100, elapsed: 1.08e+01, train loss: 9.26461e-07, val loss: 1.52443e-06, min loss: 7.63909e-07\n",
      "Epoch: 1476200, elapsed: 1.50e+01, train loss: 9.01005e-07, val loss: 1.50791e-06, min loss: 7.63909e-07\n",
      "Epoch: 1476300, elapsed: 1.09e+01, train loss: 2.82324e-06, val loss: 3.58457e-06, min loss: 7.63909e-07\n",
      "Epoch: 1476400, elapsed: 1.08e+01, train loss: 9.02640e-07, val loss: 1.53229e-06, min loss: 7.63909e-07\n",
      "Epoch: 1476500, elapsed: 1.08e+01, train loss: 8.92615e-07, val loss: 1.49994e-06, min loss: 7.63909e-07\n",
      "Epoch: 1476600, elapsed: 1.09e+01, train loss: 8.90900e-07, val loss: 1.49989e-06, min loss: 7.63909e-07\n",
      "Epoch: 1476700, elapsed: 1.08e+01, train loss: 9.52615e-07, val loss: 1.55213e-06, min loss: 7.63909e-07\n",
      "Epoch: 1476800, elapsed: 1.07e+01, train loss: 9.01742e-07, val loss: 1.51708e-06, min loss: 7.63909e-07\n",
      "Epoch: 1476900, elapsed: 1.07e+01, train loss: 1.03627e-06, val loss: 1.59178e-06, min loss: 7.63909e-07\n",
      "Epoch: 1477000, elapsed: 1.09e+01, train loss: 1.15125e-06, val loss: 1.77851e-06, min loss: 7.63909e-07\n",
      "Epoch: 1477100, elapsed: 1.09e+01, train loss: 8.83601e-07, val loss: 1.49048e-06, min loss: 7.63909e-07\n",
      "Epoch: 1477200, elapsed: 1.07e+01, train loss: 8.87273e-07, val loss: 1.49199e-06, min loss: 7.63909e-07\n",
      "Epoch: 1477300, elapsed: 1.08e+01, train loss: 8.99382e-07, val loss: 1.50501e-06, min loss: 7.63909e-07\n",
      "Epoch: 1477400, elapsed: 1.08e+01, train loss: 8.80829e-07, val loss: 1.48356e-06, min loss: 7.63909e-07\n",
      "Epoch: 1477500, elapsed: 1.07e+01, train loss: 9.47546e-07, val loss: 1.59882e-06, min loss: 7.63909e-07\n",
      "Epoch: 1477600, elapsed: 1.09e+01, train loss: 8.97756e-07, val loss: 1.51165e-06, min loss: 7.63909e-07\n",
      "Epoch: 1477700, elapsed: 1.07e+01, train loss: 9.01849e-07, val loss: 1.49207e-06, min loss: 7.63909e-07\n",
      "Epoch: 1477800, elapsed: 1.07e+01, train loss: 1.69175e-06, val loss: 1.58613e-06, min loss: 7.63909e-07\n",
      "Epoch: 1477900, elapsed: 1.09e+01, train loss: 8.82324e-07, val loss: 1.48347e-06, min loss: 7.63909e-07\n",
      "Epoch: 1478000, elapsed: 1.08e+01, train loss: 9.10093e-07, val loss: 1.49360e-06, min loss: 7.63909e-07\n",
      "Epoch: 1478100, elapsed: 1.07e+01, train loss: 9.84042e-07, val loss: 1.57282e-06, min loss: 7.63909e-07\n",
      "Epoch: 1478200, elapsed: 1.08e+01, train loss: 8.84819e-07, val loss: 1.50555e-06, min loss: 7.63909e-07\n",
      "Epoch: 1478300, elapsed: 1.08e+01, train loss: 8.77371e-07, val loss: 1.46603e-06, min loss: 7.63909e-07\n",
      "Epoch: 1478400, elapsed: 1.07e+01, train loss: 8.68013e-07, val loss: 1.47113e-06, min loss: 7.63909e-07\n",
      "Epoch: 1478500, elapsed: 1.08e+01, train loss: 8.72022e-07, val loss: 1.47534e-06, min loss: 7.63909e-07\n",
      "Epoch: 1478600, elapsed: 1.09e+01, train loss: 8.75228e-07, val loss: 1.47084e-06, min loss: 7.63909e-07\n",
      "Epoch: 1478700, elapsed: 1.07e+01, train loss: 1.83065e-06, val loss: 2.06501e-06, min loss: 7.63909e-07\n",
      "Epoch: 1478800, elapsed: 1.49e+01, train loss: 1.73247e-06, val loss: 2.30865e-06, min loss: 7.63909e-07\n",
      "Epoch: 1478900, elapsed: 1.10e+01, train loss: 9.54159e-07, val loss: 1.61681e-06, min loss: 7.63909e-07\n",
      "Epoch: 1479000, elapsed: 1.10e+01, train loss: 8.84759e-07, val loss: 1.46440e-06, min loss: 7.63909e-07\n",
      "Epoch: 1479100, elapsed: 1.08e+01, train loss: 8.60963e-07, val loss: 1.45582e-06, min loss: 7.63909e-07\n",
      "Epoch: 1479200, elapsed: 1.10e+01, train loss: 8.64358e-07, val loss: 1.45663e-06, min loss: 7.63909e-07\n",
      "Epoch: 1479300, elapsed: 1.10e+01, train loss: 1.74423e-06, val loss: 2.32218e-06, min loss: 7.63909e-07\n",
      "Epoch: 1479400, elapsed: 1.11e+01, train loss: 8.62501e-07, val loss: 1.45325e-06, min loss: 7.63909e-07\n",
      "Epoch: 1479500, elapsed: 1.10e+01, train loss: 1.07720e-06, val loss: 1.57020e-06, min loss: 7.63909e-07\n",
      "Epoch: 1479600, elapsed: 1.11e+01, train loss: 9.38238e-07, val loss: 1.57392e-06, min loss: 7.63909e-07\n",
      "Epoch: 1479700, elapsed: 1.09e+01, train loss: 2.09913e-06, val loss: 2.43760e-06, min loss: 7.63909e-07\n",
      "Epoch: 1479800, elapsed: 1.08e+01, train loss: 8.98513e-07, val loss: 1.49827e-06, min loss: 7.63909e-07\n",
      "Epoch: 1479900, elapsed: 1.08e+01, train loss: 9.77987e-07, val loss: 1.70635e-06, min loss: 7.63909e-07\n",
      "Epoch: 1480000, elapsed: 1.09e+01, train loss: 1.91682e-06, val loss: 1.89192e-06, min loss: 7.63909e-07\n",
      "Epoch: 1480100, elapsed: 1.27e+01, train loss: 2.11116e-06, val loss: 2.33455e-06, min loss: 7.63909e-07\n",
      "Epoch: 1480200, elapsed: 1.09e+01, train loss: 1.54054e-06, val loss: 1.98838e-06, min loss: 7.63909e-07\n",
      "Epoch: 1480300, elapsed: 1.09e+01, train loss: 1.19459e-06, val loss: 1.92801e-06, min loss: 7.63909e-07\n",
      "Epoch: 1480400, elapsed: 1.10e+01, train loss: 8.82364e-07, val loss: 1.48623e-06, min loss: 7.63909e-07\n",
      "Epoch: 1480500, elapsed: 1.06e+01, train loss: 9.96429e-07, val loss: 1.64527e-06, min loss: 7.63909e-07\n",
      "Epoch: 1480600, elapsed: 1.08e+01, train loss: 8.51112e-07, val loss: 1.44364e-06, min loss: 7.63909e-07\n",
      "Epoch: 1480700, elapsed: 1.09e+01, train loss: 8.47725e-07, val loss: 1.44104e-06, min loss: 7.63909e-07\n",
      "Epoch: 1480800, elapsed: 1.10e+01, train loss: 1.20074e-06, val loss: 1.93179e-06, min loss: 7.63909e-07\n",
      "Epoch: 1480900, elapsed: 1.07e+01, train loss: 3.17653e-06, val loss: 4.20379e-06, min loss: 7.63909e-07\n",
      "Epoch: 1481000, elapsed: 1.07e+01, train loss: 9.11509e-07, val loss: 1.58641e-06, min loss: 7.63909e-07\n",
      "Epoch: 1481100, elapsed: 1.09e+01, train loss: 8.47730e-07, val loss: 1.45284e-06, min loss: 7.63909e-07\n",
      "Epoch: 1481200, elapsed: 1.10e+01, train loss: 8.46229e-07, val loss: 1.44180e-06, min loss: 7.63909e-07\n",
      "Epoch: 1481300, elapsed: 1.09e+01, train loss: 8.44334e-07, val loss: 1.44758e-06, min loss: 7.63909e-07\n",
      "Epoch: 1481400, elapsed: 1.08e+01, train loss: 9.76306e-07, val loss: 1.53725e-06, min loss: 7.63909e-07\n",
      "Epoch: 1481500, elapsed: 1.52e+01, train loss: 8.41680e-07, val loss: 1.44391e-06, min loss: 7.63909e-07\n",
      "Epoch: 1481600, elapsed: 1.09e+01, train loss: 8.43258e-07, val loss: 1.44096e-06, min loss: 7.63909e-07\n",
      "Epoch: 1481700, elapsed: 1.09e+01, train loss: 1.71587e-06, val loss: 2.29999e-06, min loss: 7.63909e-07\n",
      "Epoch: 1481800, elapsed: 1.10e+01, train loss: 8.86983e-07, val loss: 1.47892e-06, min loss: 7.63909e-07\n",
      "Epoch: 1481900, elapsed: 1.09e+01, train loss: 2.49380e-06, val loss: 3.29158e-06, min loss: 7.63909e-07\n",
      "Epoch: 1482000, elapsed: 1.09e+01, train loss: 1.30979e-06, val loss: 2.17050e-06, min loss: 7.63909e-07\n",
      "Epoch: 1482100, elapsed: 1.09e+01, train loss: 2.45803e-06, val loss: 3.24289e-06, min loss: 7.63909e-07\n",
      "Epoch: 1482200, elapsed: 1.09e+01, train loss: 9.65080e-07, val loss: 1.53769e-06, min loss: 7.63909e-07\n",
      "Epoch: 1482300, elapsed: 1.08e+01, train loss: 1.24381e-06, val loss: 2.07904e-06, min loss: 7.63909e-07\n",
      "Epoch: 1482400, elapsed: 1.08e+01, train loss: 9.32005e-07, val loss: 1.58705e-06, min loss: 7.63909e-07\n",
      "Epoch: 1482500, elapsed: 1.09e+01, train loss: 1.10595e-06, val loss: 1.76706e-06, min loss: 7.63909e-07\n",
      "Epoch: 1482600, elapsed: 1.10e+01, train loss: 3.65419e-06, val loss: 4.42940e-06, min loss: 7.63909e-07\n",
      "Epoch: 1482700, elapsed: 1.09e+01, train loss: 1.56689e-06, val loss: 2.36430e-06, min loss: 7.63909e-07\n",
      "Epoch: 1482800, elapsed: 1.09e+01, train loss: 8.41690e-07, val loss: 1.45915e-06, min loss: 7.63909e-07\n",
      "Epoch: 1482900, elapsed: 1.07e+01, train loss: 8.33702e-07, val loss: 1.43170e-06, min loss: 7.63909e-07\n",
      "Epoch: 1483000, elapsed: 1.09e+01, train loss: 1.21816e-06, val loss: 1.68724e-06, min loss: 7.63909e-07\n",
      "Epoch: 1483100, elapsed: 1.07e+01, train loss: 8.31652e-07, val loss: 1.42630e-06, min loss: 7.63909e-07\n",
      "Epoch: 1483200, elapsed: 1.07e+01, train loss: 8.32651e-07, val loss: 1.43072e-06, min loss: 7.63909e-07\n",
      "Epoch: 1483300, elapsed: 1.09e+01, train loss: 1.23790e-06, val loss: 2.01917e-06, min loss: 7.63909e-07\n",
      "Epoch: 1483400, elapsed: 1.08e+01, train loss: 8.50901e-07, val loss: 1.44459e-06, min loss: 7.63909e-07\n",
      "Epoch: 1483500, elapsed: 1.07e+01, train loss: 1.03483e-06, val loss: 1.59745e-06, min loss: 7.63909e-07\n",
      "Epoch: 1483600, elapsed: 1.07e+01, train loss: 1.14830e-06, val loss: 1.78020e-06, min loss: 7.63909e-07\n",
      "Epoch: 1483700, elapsed: 1.07e+01, train loss: 9.35258e-07, val loss: 1.46303e-06, min loss: 7.63909e-07\n",
      "Epoch: 1483800, elapsed: 1.08e+01, train loss: 1.53087e-06, val loss: 2.07941e-06, min loss: 7.63909e-07\n",
      "Epoch: 1483900, elapsed: 1.08e+01, train loss: 1.65072e-06, val loss: 1.59776e-06, min loss: 7.63909e-07\n",
      "Epoch: 1484000, elapsed: 1.09e+01, train loss: 9.14961e-07, val loss: 1.49944e-06, min loss: 7.63909e-07\n",
      "Epoch: 1484100, elapsed: 1.09e+01, train loss: 8.28276e-07, val loss: 1.42336e-06, min loss: 7.63909e-07\n",
      "Epoch: 1484200, elapsed: 1.51e+01, train loss: 1.03050e-06, val loss: 1.71678e-06, min loss: 7.63909e-07\n",
      "Epoch: 1484300, elapsed: 1.09e+01, train loss: 8.25335e-07, val loss: 1.42167e-06, min loss: 7.63909e-07\n",
      "Epoch: 1484400, elapsed: 1.09e+01, train loss: 8.44158e-07, val loss: 1.41823e-06, min loss: 7.63909e-07\n",
      "Epoch: 1484500, elapsed: 1.09e+01, train loss: 8.24599e-07, val loss: 1.42516e-06, min loss: 7.63909e-07\n",
      "Epoch: 1484600, elapsed: 1.11e+01, train loss: 8.26111e-07, val loss: 1.42490e-06, min loss: 7.63909e-07\n",
      "Epoch: 1484700, elapsed: 1.10e+01, train loss: 1.14835e-06, val loss: 2.01379e-06, min loss: 7.63909e-07\n",
      "Epoch: 1484800, elapsed: 1.11e+01, train loss: 8.23078e-07, val loss: 1.41849e-06, min loss: 7.63909e-07\n",
      "Epoch: 1484900, elapsed: 1.09e+01, train loss: 9.68135e-07, val loss: 1.62141e-06, min loss: 7.63909e-07\n",
      "Epoch: 1485000, elapsed: 1.10e+01, train loss: 1.11023e-06, val loss: 1.47256e-06, min loss: 7.63909e-07\n",
      "Epoch: 1485100, elapsed: 1.27e+01, train loss: 1.32907e-06, val loss: 1.95993e-06, min loss: 7.63909e-07\n",
      "Epoch: 1485200, elapsed: 1.09e+01, train loss: 3.11745e-06, val loss: 2.59909e-06, min loss: 7.63909e-07\n",
      "Epoch: 1485300, elapsed: 1.08e+01, train loss: 8.71057e-07, val loss: 1.47438e-06, min loss: 7.63909e-07\n",
      "Epoch: 1485400, elapsed: 1.10e+01, train loss: 9.15143e-07, val loss: 1.46150e-06, min loss: 7.63909e-07\n",
      "Epoch: 1485500, elapsed: 1.09e+01, train loss: 8.20156e-07, val loss: 1.41413e-06, min loss: 7.63909e-07\n",
      "Epoch: 1485600, elapsed: 1.10e+01, train loss: 8.23798e-07, val loss: 1.42825e-06, min loss: 7.63909e-07\n",
      "Epoch: 1485700, elapsed: 1.08e+01, train loss: 1.19650e-06, val loss: 1.55063e-06, min loss: 7.63909e-07\n",
      "Epoch: 1485800, elapsed: 1.08e+01, train loss: 1.16310e-06, val loss: 1.77033e-06, min loss: 7.63909e-07\n",
      "Epoch: 1485900, elapsed: 1.10e+01, train loss: 8.24029e-07, val loss: 1.42589e-06, min loss: 7.63909e-07\n",
      "Epoch: 1486000, elapsed: 1.08e+01, train loss: 8.22732e-07, val loss: 1.42242e-06, min loss: 7.63909e-07\n",
      "Epoch: 1486100, elapsed: 1.09e+01, train loss: 8.29179e-07, val loss: 1.40074e-06, min loss: 7.63909e-07\n",
      "Epoch: 1486200, elapsed: 1.08e+01, train loss: 2.14992e-06, val loss: 2.82985e-06, min loss: 7.63909e-07\n",
      "Epoch: 1486300, elapsed: 1.08e+01, train loss: 1.10363e-06, val loss: 1.89250e-06, min loss: 7.63909e-07\n",
      "Epoch: 1486400, elapsed: 1.09e+01, train loss: 8.18306e-07, val loss: 1.41731e-06, min loss: 7.63909e-07\n",
      "Epoch: 1486500, elapsed: 1.08e+01, train loss: 8.15799e-07, val loss: 1.41558e-06, min loss: 7.63909e-07\n",
      "Epoch: 1486600, elapsed: 1.07e+01, train loss: 8.16766e-07, val loss: 1.40784e-06, min loss: 7.63909e-07\n",
      "Epoch: 1486700, elapsed: 1.09e+01, train loss: 8.59882e-07, val loss: 1.43396e-06, min loss: 7.63909e-07\n",
      "Epoch: 1486800, elapsed: 1.07e+01, train loss: 1.75436e-06, val loss: 2.52897e-06, min loss: 7.63909e-07\n",
      "Epoch: 1486900, elapsed: 1.52e+01, train loss: 8.14003e-07, val loss: 1.41114e-06, min loss: 7.63909e-07\n",
      "Epoch: 1487000, elapsed: 1.10e+01, train loss: 8.14120e-07, val loss: 1.40755e-06, min loss: 7.63909e-07\n",
      "Epoch: 1487100, elapsed: 1.09e+01, train loss: 1.00312e-06, val loss: 1.95345e-06, min loss: 7.63909e-07\n",
      "Epoch: 1487200, elapsed: 1.08e+01, train loss: 8.12686e-07, val loss: 1.40728e-06, min loss: 7.63909e-07\n",
      "Epoch: 1487300, elapsed: 1.09e+01, train loss: 2.90355e-06, val loss: 2.45346e-06, min loss: 7.63909e-07\n",
      "Epoch: 1487400, elapsed: 1.09e+01, train loss: 8.12908e-07, val loss: 1.40597e-06, min loss: 7.63909e-07\n",
      "Epoch: 1487500, elapsed: 1.08e+01, train loss: 1.13416e-06, val loss: 1.85705e-06, min loss: 7.63909e-07\n",
      "Epoch: 1487600, elapsed: 1.07e+01, train loss: 8.11010e-07, val loss: 1.40647e-06, min loss: 7.63909e-07\n",
      "Epoch: 1487700, elapsed: 1.10e+01, train loss: 8.17392e-07, val loss: 1.40407e-06, min loss: 7.63909e-07\n",
      "Epoch: 1487800, elapsed: 1.08e+01, train loss: 8.64566e-07, val loss: 1.44993e-06, min loss: 7.63909e-07\n",
      "Epoch: 1487900, elapsed: 1.08e+01, train loss: 8.10470e-07, val loss: 1.41153e-06, min loss: 7.63909e-07\n",
      "Epoch: 1488000, elapsed: 1.09e+01, train loss: 8.11619e-07, val loss: 1.40426e-06, min loss: 7.63909e-07\n",
      "Epoch: 1488100, elapsed: 1.08e+01, train loss: 8.20346e-07, val loss: 1.38910e-06, min loss: 7.63909e-07\n",
      "Epoch: 1488200, elapsed: 1.07e+01, train loss: 8.20060e-07, val loss: 1.39783e-06, min loss: 7.63909e-07\n",
      "Epoch: 1488300, elapsed: 1.08e+01, train loss: 1.51940e-06, val loss: 2.25338e-06, min loss: 7.63909e-07\n",
      "Epoch: 1488400, elapsed: 1.10e+01, train loss: 8.08369e-07, val loss: 1.40243e-06, min loss: 7.63909e-07\n",
      "Epoch: 1488500, elapsed: 1.07e+01, train loss: 8.12058e-07, val loss: 1.40492e-06, min loss: 7.63909e-07\n",
      "Epoch: 1488600, elapsed: 1.08e+01, train loss: 8.31702e-07, val loss: 1.45948e-06, min loss: 7.63909e-07\n",
      "Epoch: 1488700, elapsed: 1.07e+01, train loss: 1.60928e-06, val loss: 1.91773e-06, min loss: 7.63909e-07\n",
      "Epoch: 1488800, elapsed: 1.06e+01, train loss: 8.23422e-07, val loss: 1.40428e-06, min loss: 7.63909e-07\n",
      "Epoch: 1488900, elapsed: 1.08e+01, train loss: 8.06339e-07, val loss: 1.40037e-06, min loss: 7.63909e-07\n",
      "Epoch: 1489000, elapsed: 1.08e+01, train loss: 8.17772e-07, val loss: 1.40656e-06, min loss: 7.63909e-07\n",
      "Epoch: 1489100, elapsed: 1.08e+01, train loss: 8.76315e-07, val loss: 1.49583e-06, min loss: 7.63909e-07\n",
      "Epoch: 1489200, elapsed: 1.09e+01, train loss: 2.30989e-06, val loss: 2.46750e-06, min loss: 7.63909e-07\n",
      "Epoch: 1489300, elapsed: 1.09e+01, train loss: 1.54721e-06, val loss: 1.99945e-06, min loss: 7.63909e-07\n",
      "Epoch: 1489400, elapsed: 1.09e+01, train loss: 1.38275e-06, val loss: 2.10962e-06, min loss: 7.63909e-07\n",
      "Epoch: 1489500, elapsed: 1.07e+01, train loss: 8.04630e-07, val loss: 1.39757e-06, min loss: 7.63909e-07\n",
      "Epoch: 1489600, elapsed: 1.51e+01, train loss: 8.20886e-07, val loss: 1.42324e-06, min loss: 7.63909e-07\n",
      "Epoch: 1489700, elapsed: 1.10e+01, train loss: 1.54518e-06, val loss: 2.92864e-06, min loss: 7.63909e-07\n",
      "Epoch: 1489800, elapsed: 1.11e+01, train loss: 8.03340e-07, val loss: 1.39511e-06, min loss: 7.63909e-07\n",
      "Epoch: 1489900, elapsed: 1.10e+01, train loss: 8.10160e-07, val loss: 1.39719e-06, min loss: 7.63909e-07\n",
      "Epoch: 1490000, elapsed: 1.11e+01, train loss: 9.57088e-07, val loss: 1.61491e-06, min loss: 7.63909e-07\n",
      "Epoch: 1490100, elapsed: 1.29e+01, train loss: 1.05032e-06, val loss: 1.54005e-06, min loss: 7.63909e-07\n",
      "Epoch: 1490200, elapsed: 1.10e+01, train loss: 8.03825e-07, val loss: 1.40416e-06, min loss: 7.63909e-07\n",
      "Epoch: 1490300, elapsed: 1.09e+01, train loss: 8.23230e-07, val loss: 1.42055e-06, min loss: 7.63909e-07\n",
      "Epoch: 1490400, elapsed: 1.09e+01, train loss: 1.10058e-06, val loss: 1.68060e-06, min loss: 7.63909e-07\n",
      "Epoch: 1490500, elapsed: 1.09e+01, train loss: 8.01401e-07, val loss: 1.39599e-06, min loss: 7.63909e-07\n",
      "Epoch: 1490600, elapsed: 1.10e+01, train loss: 8.89091e-07, val loss: 1.46305e-06, min loss: 7.63909e-07\n",
      "Epoch: 1490700, elapsed: 1.11e+01, train loss: 2.01442e-06, val loss: 3.18325e-06, min loss: 7.63909e-07\n",
      "Epoch: 1490800, elapsed: 1.09e+01, train loss: 8.00252e-07, val loss: 1.39306e-06, min loss: 7.63909e-07\n",
      "Epoch: 1490900, elapsed: 1.09e+01, train loss: 8.47566e-07, val loss: 1.45272e-06, min loss: 7.63909e-07\n",
      "Epoch: 1491000, elapsed: 1.10e+01, train loss: 8.49543e-07, val loss: 1.50034e-06, min loss: 7.63909e-07\n",
      "Epoch: 1491100, elapsed: 1.06e+01, train loss: 7.99616e-07, val loss: 1.39176e-06, min loss: 7.63909e-07\n",
      "Epoch: 1491200, elapsed: 1.09e+01, train loss: 9.07033e-07, val loss: 1.45117e-06, min loss: 7.63909e-07\n",
      "Epoch: 1491300, elapsed: 1.09e+01, train loss: 1.38311e-06, val loss: 1.99725e-06, min loss: 7.63909e-07\n",
      "Epoch: 1491400, elapsed: 1.07e+01, train loss: 8.15795e-07, val loss: 1.43719e-06, min loss: 7.63909e-07\n",
      "Epoch: 1491500, elapsed: 1.08e+01, train loss: 1.35529e-06, val loss: 1.73715e-06, min loss: 7.63909e-07\n",
      "Epoch: 1491600, elapsed: 1.07e+01, train loss: 2.39778e-06, val loss: 2.74305e-06, min loss: 7.63909e-07\n",
      "Epoch: 1491700, elapsed: 1.08e+01, train loss: 8.52301e-07, val loss: 1.44661e-06, min loss: 7.63909e-07\n",
      "Epoch: 1491800, elapsed: 1.10e+01, train loss: 8.08864e-07, val loss: 1.40146e-06, min loss: 7.63909e-07\n",
      "Epoch: 1491900, elapsed: 1.08e+01, train loss: 2.20754e-06, val loss: 3.12678e-06, min loss: 7.63909e-07\n",
      "Epoch: 1492000, elapsed: 1.09e+01, train loss: 1.87518e-06, val loss: 2.84173e-06, min loss: 7.63909e-07\n",
      "Epoch: 1492100, elapsed: 1.09e+01, train loss: 7.96176e-07, val loss: 1.39067e-06, min loss: 7.63909e-07\n",
      "Epoch: 1492200, elapsed: 1.08e+01, train loss: 8.56677e-07, val loss: 1.49944e-06, min loss: 7.63909e-07\n",
      "Epoch: 1492300, elapsed: 1.52e+01, train loss: 8.69968e-07, val loss: 1.53629e-06, min loss: 7.63909e-07\n",
      "Epoch: 1492400, elapsed: 1.11e+01, train loss: 1.32811e-06, val loss: 1.94919e-06, min loss: 7.63909e-07\n",
      "Epoch: 1492500, elapsed: 1.10e+01, train loss: 1.04270e-06, val loss: 1.80741e-06, min loss: 7.63909e-07\n",
      "Epoch: 1492600, elapsed: 1.12e+01, train loss: 8.90001e-07, val loss: 1.49105e-06, min loss: 7.63909e-07\n",
      "Epoch: 1492700, elapsed: 1.10e+01, train loss: 8.32092e-07, val loss: 1.37965e-06, min loss: 7.63909e-07\n",
      "Epoch: 1492800, elapsed: 1.10e+01, train loss: 7.97309e-07, val loss: 1.39811e-06, min loss: 7.63909e-07\n",
      "Epoch: 1492900, elapsed: 1.10e+01, train loss: 7.95629e-07, val loss: 1.38380e-06, min loss: 7.63909e-07\n",
      "Epoch: 1493000, elapsed: 1.08e+01, train loss: 1.25287e-06, val loss: 1.71138e-06, min loss: 7.63909e-07\n",
      "Epoch: 1493100, elapsed: 1.08e+01, train loss: 8.51419e-07, val loss: 1.41789e-06, min loss: 7.63909e-07\n",
      "Epoch: 1493200, elapsed: 1.10e+01, train loss: 7.99949e-07, val loss: 1.40606e-06, min loss: 7.63909e-07\n",
      "Epoch: 1493300, elapsed: 1.08e+01, train loss: 7.96568e-07, val loss: 1.40339e-06, min loss: 7.63909e-07\n",
      "Epoch: 1493400, elapsed: 1.09e+01, train loss: 8.01870e-07, val loss: 1.39011e-06, min loss: 7.63909e-07\n",
      "Epoch: 1493500, elapsed: 1.08e+01, train loss: 1.09988e-06, val loss: 1.59016e-06, min loss: 7.63909e-07\n",
      "Epoch: 1493600, elapsed: 1.09e+01, train loss: 7.92036e-07, val loss: 1.38440e-06, min loss: 7.63909e-07\n",
      "Epoch: 1493700, elapsed: 1.08e+01, train loss: 7.95721e-07, val loss: 1.39036e-06, min loss: 7.63909e-07\n",
      "Epoch: 1493800, elapsed: 1.08e+01, train loss: 8.07910e-07, val loss: 1.38891e-06, min loss: 7.63909e-07\n",
      "Epoch: 1493900, elapsed: 1.07e+01, train loss: 7.91807e-07, val loss: 1.38536e-06, min loss: 7.63909e-07\n",
      "Epoch: 1494000, elapsed: 1.08e+01, train loss: 9.05532e-07, val loss: 1.45538e-06, min loss: 7.63909e-07\n",
      "Epoch: 1494100, elapsed: 1.08e+01, train loss: 9.77152e-07, val loss: 1.59794e-06, min loss: 7.63909e-07\n",
      "Epoch: 1494200, elapsed: 1.09e+01, train loss: 2.80963e-06, val loss: 2.87791e-06, min loss: 7.63909e-07\n",
      "Epoch: 1494300, elapsed: 1.08e+01, train loss: 3.48714e-06, val loss: 4.01755e-06, min loss: 7.63909e-07\n",
      "Epoch: 1494400, elapsed: 1.06e+01, train loss: 1.08158e-06, val loss: 1.78115e-06, min loss: 7.63909e-07\n",
      "Epoch: 1494500, elapsed: 1.08e+01, train loss: 7.90441e-07, val loss: 1.37728e-06, min loss: 7.63909e-07\n",
      "Epoch: 1494600, elapsed: 1.07e+01, train loss: 7.90802e-07, val loss: 1.38297e-06, min loss: 7.63909e-07\n",
      "Epoch: 1494700, elapsed: 1.08e+01, train loss: 1.06738e-06, val loss: 1.50575e-06, min loss: 7.63909e-07\n",
      "Epoch: 1494800, elapsed: 1.09e+01, train loss: 7.90676e-07, val loss: 1.39722e-06, min loss: 7.63909e-07\n",
      "Epoch: 1494900, elapsed: 1.09e+01, train loss: 8.02091e-07, val loss: 1.37848e-06, min loss: 7.63909e-07\n",
      "Epoch: 1495000, elapsed: 1.52e+01, train loss: 7.93035e-07, val loss: 1.37507e-06, min loss: 7.63909e-07\n",
      "Epoch: 1495100, elapsed: 1.30e+01, train loss: 8.00643e-07, val loss: 1.41755e-06, min loss: 7.63909e-07\n",
      "Epoch: 1495200, elapsed: 1.09e+01, train loss: 8.28948e-07, val loss: 1.37857e-06, min loss: 7.63909e-07\n",
      "Epoch: 1495300, elapsed: 1.09e+01, train loss: 7.89660e-07, val loss: 1.38223e-06, min loss: 7.63909e-07\n",
      "Epoch: 1495400, elapsed: 1.10e+01, train loss: 8.20798e-07, val loss: 1.39934e-06, min loss: 7.63909e-07\n",
      "Epoch: 1495500, elapsed: 1.09e+01, train loss: 8.62086e-07, val loss: 1.41665e-06, min loss: 7.63909e-07\n",
      "Epoch: 1495600, elapsed: 1.10e+01, train loss: 8.58441e-07, val loss: 1.49054e-06, min loss: 7.63909e-07\n",
      "Epoch: 1495700, elapsed: 1.10e+01, train loss: 9.50233e-07, val loss: 1.45046e-06, min loss: 7.63909e-07\n",
      "Epoch: 1495800, elapsed: 1.08e+01, train loss: 9.61395e-07, val loss: 1.60222e-06, min loss: 7.63909e-07\n",
      "Epoch: 1495900, elapsed: 1.10e+01, train loss: 8.02684e-07, val loss: 1.43769e-06, min loss: 7.63909e-07\n",
      "Epoch: 1496000, elapsed: 1.11e+01, train loss: 7.87563e-07, val loss: 1.38361e-06, min loss: 7.63909e-07\n",
      "Epoch: 1496100, elapsed: 1.10e+01, train loss: 8.75850e-07, val loss: 1.38576e-06, min loss: 7.63909e-07\n",
      "Epoch: 1496200, elapsed: 1.08e+01, train loss: 9.48540e-07, val loss: 1.53557e-06, min loss: 7.63909e-07\n",
      "Epoch: 1496300, elapsed: 1.09e+01, train loss: 7.86085e-07, val loss: 1.38188e-06, min loss: 7.63909e-07\n",
      "Epoch: 1496400, elapsed: 1.08e+01, train loss: 2.13301e-06, val loss: 2.77224e-06, min loss: 7.63909e-07\n",
      "Epoch: 1496500, elapsed: 1.10e+01, train loss: 8.24384e-07, val loss: 1.39852e-06, min loss: 7.63909e-07\n",
      "Epoch: 1496600, elapsed: 1.10e+01, train loss: 7.87946e-07, val loss: 1.38799e-06, min loss: 7.63909e-07\n",
      "Epoch: 1496700, elapsed: 1.07e+01, train loss: 7.85133e-07, val loss: 1.37970e-06, min loss: 7.63909e-07\n",
      "Epoch: 1496800, elapsed: 1.09e+01, train loss: 7.92670e-07, val loss: 1.39064e-06, min loss: 7.63909e-07\n",
      "Epoch: 1496900, elapsed: 1.10e+01, train loss: 8.13667e-07, val loss: 1.49059e-06, min loss: 7.63909e-07\n",
      "Epoch: 1497000, elapsed: 1.08e+01, train loss: 3.33589e-06, val loss: 3.46546e-06, min loss: 7.63909e-07\n",
      "Epoch: 1497100, elapsed: 1.07e+01, train loss: 9.91261e-07, val loss: 1.49206e-06, min loss: 7.63909e-07\n",
      "Epoch: 1497200, elapsed: 1.08e+01, train loss: 8.76666e-07, val loss: 1.57656e-06, min loss: 7.63909e-07\n",
      "Epoch: 1497300, elapsed: 1.10e+01, train loss: 2.09599e-06, val loss: 2.88713e-06, min loss: 7.63909e-07\n",
      "Epoch: 1497400, elapsed: 1.08e+01, train loss: 7.85715e-07, val loss: 1.38153e-06, min loss: 7.63909e-07\n",
      "Epoch: 1497500, elapsed: 1.08e+01, train loss: 7.83943e-07, val loss: 1.37291e-06, min loss: 7.63909e-07\n",
      "Epoch: 1497600, elapsed: 1.08e+01, train loss: 7.83053e-07, val loss: 1.37251e-06, min loss: 7.63909e-07\n",
      "Epoch: 1497700, elapsed: 1.54e+01, train loss: 9.97693e-07, val loss: 2.25075e-06, min loss: 7.63909e-07\n",
      "Epoch: 1497800, elapsed: 1.10e+01, train loss: 7.82066e-07, val loss: 1.37390e-06, min loss: 7.63909e-07\n",
      "Epoch: 1497900, elapsed: 1.10e+01, train loss: 7.99489e-07, val loss: 1.37236e-06, min loss: 7.63909e-07\n",
      "Epoch: 1498000, elapsed: 1.11e+01, train loss: 7.87247e-07, val loss: 1.37934e-06, min loss: 7.63909e-07\n",
      "Epoch: 1498100, elapsed: 1.08e+01, train loss: 7.85100e-07, val loss: 1.37420e-06, min loss: 7.63909e-07\n",
      "Epoch: 1498200, elapsed: 1.10e+01, train loss: 1.24219e-06, val loss: 2.09484e-06, min loss: 7.63909e-07\n",
      "Epoch: 1498300, elapsed: 1.09e+01, train loss: 1.11132e-06, val loss: 1.60168e-06, min loss: 7.63909e-07\n",
      "Epoch: 1498400, elapsed: 1.09e+01, train loss: 7.84249e-07, val loss: 1.36437e-06, min loss: 7.63909e-07\n",
      "Epoch: 1498500, elapsed: 1.09e+01, train loss: 7.88488e-07, val loss: 1.39230e-06, min loss: 7.63909e-07\n",
      "Epoch: 1498600, elapsed: 1.10e+01, train loss: 7.99773e-07, val loss: 1.39873e-06, min loss: 7.63909e-07\n",
      "Epoch: 1498700, elapsed: 1.09e+01, train loss: 9.44843e-07, val loss: 1.43599e-06, min loss: 7.63909e-07\n",
      "Epoch: 1498800, elapsed: 1.08e+01, train loss: 3.06572e-06, val loss: 4.20130e-06, min loss: 7.63909e-07\n",
      "Epoch: 1498900, elapsed: 1.08e+01, train loss: 7.85928e-07, val loss: 1.37031e-06, min loss: 7.63909e-07\n",
      "Epoch: 1499000, elapsed: 1.09e+01, train loss: 1.71674e-06, val loss: 2.62317e-06, min loss: 7.63909e-07\n",
      "Epoch: 1499100, elapsed: 1.07e+01, train loss: 8.57777e-07, val loss: 1.47514e-06, min loss: 7.63909e-07\n",
      "Epoch: 1499200, elapsed: 1.10e+01, train loss: 7.85861e-07, val loss: 1.36622e-06, min loss: 7.63909e-07\n",
      "Epoch: 1499300, elapsed: 1.10e+01, train loss: 7.86417e-07, val loss: 1.38035e-06, min loss: 7.63909e-07\n",
      "Epoch: 1499400, elapsed: 1.09e+01, train loss: 7.83152e-07, val loss: 1.36312e-06, min loss: 7.63909e-07\n",
      "Epoch: 1499500, elapsed: 1.08e+01, train loss: 9.20992e-07, val loss: 1.46183e-06, min loss: 7.63909e-07\n",
      "Epoch: 1499600, elapsed: 1.10e+01, train loss: 8.18193e-07, val loss: 1.42916e-06, min loss: 7.63909e-07\n",
      "Epoch: 1499700, elapsed: 1.07e+01, train loss: 2.11858e-06, val loss: 2.74835e-06, min loss: 7.63909e-07\n",
      "Epoch: 1499800, elapsed: 1.09e+01, train loss: 1.18848e-06, val loss: 1.80001e-06, min loss: 7.63909e-07\n",
      "Epoch: 1499900, elapsed: 1.09e+01, train loss: 8.64233e-07, val loss: 1.59740e-06, min loss: 7.63909e-07\n",
      "Epoch: 1500000, elapsed: 1.10e+01, train loss: 8.56188e-07, val loss: 1.49006e-06, min loss: 7.63909e-07\n",
      "Epoch: 1500100, elapsed: 1.27e+01, train loss: 7.84797e-07, val loss: 1.35697e-06, min loss: 7.63909e-07\n",
      "Epoch: 1500200, elapsed: 1.08e+01, train loss: 7.93218e-07, val loss: 1.38480e-06, min loss: 7.63909e-07\n",
      "Epoch: 1500300, elapsed: 1.08e+01, train loss: 3.77703e-06, val loss: 3.59857e-06, min loss: 7.63909e-07\n",
      "Epoch: 1500400, elapsed: 1.52e+01, train loss: 7.98438e-07, val loss: 1.43097e-06, min loss: 7.63909e-07\n",
      "Epoch: 1500500, elapsed: 1.10e+01, train loss: 7.83451e-07, val loss: 1.36135e-06, min loss: 7.63909e-07\n",
      "Epoch: 1500600, elapsed: 1.11e+01, train loss: 7.80205e-07, val loss: 1.36623e-06, min loss: 7.63909e-07\n",
      "Epoch: 1500700, elapsed: 1.11e+01, train loss: 7.80809e-07, val loss: 1.37465e-06, min loss: 7.63909e-07\n",
      "Epoch: 1500800, elapsed: 1.08e+01, train loss: 8.06665e-07, val loss: 1.44209e-06, min loss: 7.63909e-07\n",
      "Epoch: 1500900, elapsed: 1.10e+01, train loss: 1.13380e-06, val loss: 1.70314e-06, min loss: 7.63909e-07\n",
      "Epoch: 1501000, elapsed: 1.10e+01, train loss: 7.76080e-07, val loss: 1.36987e-06, min loss: 7.63909e-07\n",
      "Epoch: 1501100, elapsed: 1.09e+01, train loss: 8.13618e-07, val loss: 1.38636e-06, min loss: 7.63909e-07\n",
      "Epoch: 1501200, elapsed: 1.08e+01, train loss: 8.31948e-07, val loss: 1.48305e-06, min loss: 7.63909e-07\n",
      "Epoch: 1501300, elapsed: 1.11e+01, train loss: 7.80679e-07, val loss: 1.39746e-06, min loss: 7.63909e-07\n",
      "Epoch: 1501400, elapsed: 1.10e+01, train loss: 7.75033e-07, val loss: 1.37486e-06, min loss: 7.63909e-07\n",
      "Epoch: 1501500, elapsed: 1.08e+01, train loss: 8.32343e-07, val loss: 1.48361e-06, min loss: 7.63909e-07\n",
      "Epoch: 1501600, elapsed: 1.08e+01, train loss: 7.77204e-07, val loss: 1.35693e-06, min loss: 7.63909e-07\n",
      "Epoch: 1501700, elapsed: 1.09e+01, train loss: 7.80165e-07, val loss: 1.38031e-06, min loss: 7.63909e-07\n",
      "Epoch: 1501800, elapsed: 1.09e+01, train loss: 7.76047e-07, val loss: 1.37505e-06, min loss: 7.63909e-07\n",
      "Epoch: 1501900, elapsed: 1.09e+01, train loss: 7.93888e-07, val loss: 1.39875e-06, min loss: 7.63909e-07\n",
      "Epoch: 1502000, elapsed: 1.09e+01, train loss: 7.76188e-07, val loss: 1.36439e-06, min loss: 7.63909e-07\n",
      "Epoch: 1502100, elapsed: 1.10e+01, train loss: 7.85112e-07, val loss: 1.37059e-06, min loss: 7.63909e-07\n",
      "Epoch: 1502200, elapsed: 1.06e+01, train loss: 1.16618e-06, val loss: 1.94018e-06, min loss: 7.63909e-07\n",
      "Epoch: 1502300, elapsed: 1.08e+01, train loss: 7.78437e-07, val loss: 1.39190e-06, min loss: 7.63909e-07\n",
      "Epoch: 1502400, elapsed: 1.09e+01, train loss: 7.74876e-07, val loss: 1.36954e-06, min loss: 7.63909e-07\n",
      "Epoch: 1502500, elapsed: 1.08e+01, train loss: 7.74290e-07, val loss: 1.36155e-06, min loss: 7.63909e-07\n",
      "Epoch: 1502600, elapsed: 1.09e+01, train loss: 1.80264e-06, val loss: 2.15314e-06, min loss: 7.63909e-07\n",
      "Epoch: 1502700, elapsed: 1.07e+01, train loss: 1.09034e-06, val loss: 1.92539e-06, min loss: 7.63909e-07\n",
      "Epoch: 1502800, elapsed: 1.09e+01, train loss: 7.74156e-07, val loss: 1.35603e-06, min loss: 7.63909e-07\n",
      "Epoch: 1502900, elapsed: 1.09e+01, train loss: 8.42738e-07, val loss: 1.37941e-06, min loss: 7.63909e-07\n",
      "Epoch: 1503000, elapsed: 1.08e+01, train loss: 1.27559e-06, val loss: 1.86848e-06, min loss: 7.63909e-07\n",
      "Epoch: 1503100, elapsed: 1.52e+01, train loss: 8.09241e-07, val loss: 1.40271e-06, min loss: 7.63909e-07\n",
      "Epoch: 1503200, elapsed: 1.09e+01, train loss: 1.07587e-06, val loss: 1.64619e-06, min loss: 7.63909e-07\n",
      "Epoch: 1503300, elapsed: 1.09e+01, train loss: 8.17369e-07, val loss: 1.39817e-06, min loss: 7.63909e-07\n",
      "Epoch: 1503400, elapsed: 1.09e+01, train loss: 7.82079e-07, val loss: 1.38939e-06, min loss: 7.63909e-07\n",
      "Epoch: 1503500, elapsed: 1.09e+01, train loss: 7.78836e-07, val loss: 1.38960e-06, min loss: 7.63909e-07\n",
      "Epoch: 1503600, elapsed: 1.10e+01, train loss: 7.71134e-07, val loss: 1.36116e-06, min loss: 7.63909e-07\n",
      "Epoch: 1503700, elapsed: 1.11e+01, train loss: 9.18049e-07, val loss: 1.48446e-06, min loss: 7.63909e-07\n",
      "Epoch: 1503800, elapsed: 1.10e+01, train loss: 9.58256e-07, val loss: 1.45734e-06, min loss: 7.63909e-07\n",
      "Epoch: 1503900, elapsed: 1.10e+01, train loss: 8.53944e-07, val loss: 1.46242e-06, min loss: 7.63909e-07\n",
      "Epoch: 1504000, elapsed: 1.09e+01, train loss: 7.82479e-07, val loss: 1.35836e-06, min loss: 7.63909e-07\n",
      "Epoch: 1504100, elapsed: 1.11e+01, train loss: 7.71208e-07, val loss: 1.35391e-06, min loss: 7.63909e-07\n",
      "Epoch: 1504200, elapsed: 1.09e+01, train loss: 7.94256e-07, val loss: 1.39440e-06, min loss: 7.63909e-07\n",
      "Epoch: 1504300, elapsed: 1.10e+01, train loss: 9.21233e-07, val loss: 1.50566e-06, min loss: 7.63909e-07\n",
      "Epoch: 1504400, elapsed: 1.10e+01, train loss: 1.00817e-06, val loss: 1.66491e-06, min loss: 7.63909e-07\n",
      "Epoch: 1504500, elapsed: 1.08e+01, train loss: 7.69197e-07, val loss: 1.35819e-06, min loss: 7.63909e-07\n",
      "Epoch: 1504600, elapsed: 1.08e+01, train loss: 7.74071e-07, val loss: 1.39446e-06, min loss: 7.63909e-07\n",
      "Epoch: 1504700, elapsed: 1.10e+01, train loss: 7.80196e-07, val loss: 1.35957e-06, min loss: 7.63909e-07\n",
      "Epoch: 1504800, elapsed: 1.08e+01, train loss: 8.96068e-07, val loss: 1.56341e-06, min loss: 7.63909e-07\n",
      "Epoch: 1504900, elapsed: 1.08e+01, train loss: 1.10494e-06, val loss: 1.69270e-06, min loss: 7.63909e-07\n",
      "Epoch: 1505000, elapsed: 1.09e+01, train loss: 9.37206e-07, val loss: 1.60053e-06, min loss: 7.63909e-07\n",
      "Epoch: 1505100, elapsed: 1.28e+01, train loss: 7.90040e-07, val loss: 1.42969e-06, min loss: 7.63909e-07\n",
      "Epoch: 1505200, elapsed: 1.07e+01, train loss: 7.91675e-07, val loss: 1.36253e-06, min loss: 7.63909e-07\n",
      "Epoch: 1505300, elapsed: 1.09e+01, train loss: 7.75773e-07, val loss: 1.37408e-06, min loss: 7.63909e-07\n",
      "Epoch: 1505400, elapsed: 1.10e+01, train loss: 7.68639e-07, val loss: 1.35801e-06, min loss: 7.63909e-07\n",
      "Epoch: 1505500, elapsed: 1.09e+01, train loss: 8.55836e-07, val loss: 1.39193e-06, min loss: 7.63909e-07\n",
      "Epoch: 1505600, elapsed: 1.10e+01, train loss: 1.29173e-06, val loss: 1.89702e-06, min loss: 7.63909e-07\n",
      "Epoch: 1505700, elapsed: 1.09e+01, train loss: 8.27854e-07, val loss: 1.36991e-06, min loss: 7.63909e-07\n",
      "Epoch: 1505800, elapsed: 1.52e+01, train loss: 1.01703e-06, val loss: 1.69970e-06, min loss: 7.63909e-07\n",
      "Epoch: 1505900, elapsed: 1.12e+01, train loss: 7.75911e-07, val loss: 1.37123e-06, min loss: 7.63909e-07\n",
      "Epoch: 1506000, elapsed: 1.12e+01, train loss: 1.20563e-06, val loss: 1.61503e-06, min loss: 7.63909e-07\n",
      "Epoch: 1506100, elapsed: 1.09e+01, train loss: 9.80698e-07, val loss: 1.46342e-06, min loss: 7.63909e-07\n",
      "Epoch: 1506200, elapsed: 1.09e+01, train loss: 9.52184e-07, val loss: 1.45039e-06, min loss: 7.63909e-07\n",
      "Epoch: 1506300, elapsed: 1.09e+01, train loss: 1.79983e-06, val loss: 2.02594e-06, min loss: 7.63909e-07\n",
      "Epoch: 1506400, elapsed: 1.10e+01, train loss: 8.80748e-07, val loss: 1.47994e-06, min loss: 7.63909e-07\n",
      "Epoch: 1506500, elapsed: 1.08e+01, train loss: 7.68941e-07, val loss: 1.37607e-06, min loss: 7.63909e-07\n",
      "Epoch: 1506600, elapsed: 1.08e+01, train loss: 1.20066e-06, val loss: 1.69973e-06, min loss: 7.63909e-07\n",
      "Epoch: 1506700, elapsed: 1.11e+01, train loss: 8.22394e-07, val loss: 1.45362e-06, min loss: 7.63909e-07\n",
      "Epoch: 1506800, elapsed: 1.09e+01, train loss: 1.83022e-06, val loss: 2.08911e-06, min loss: 7.63909e-07\n",
      "Epoch: 1506900, elapsed: 1.09e+01, train loss: 7.84432e-07, val loss: 1.40164e-06, min loss: 7.63909e-07\n",
      "Epoch: 1507000, elapsed: 1.09e+01, train loss: 1.17697e-06, val loss: 1.71986e-06, min loss: 7.63909e-07\n",
      "Epoch: 1507100, elapsed: 1.10e+01, train loss: 1.10362e-06, val loss: 1.67519e-06, min loss: 7.63909e-07\n",
      "Epoch: 1507200, elapsed: 1.09e+01, train loss: 1.00443e-06, val loss: 1.68831e-06, min loss: 7.63909e-07\n",
      "Epoch: 1507300, elapsed: 1.08e+01, train loss: 4.53178e-06, val loss: 5.12090e-06, min loss: 7.63909e-07\n",
      "Epoch: 1507400, elapsed: 1.07e+01, train loss: 7.64511e-07, val loss: 1.35598e-06, min loss: 7.63909e-07\n",
      "Epoch: 1507500, elapsed: 1.09e+01, train loss: 7.67608e-07, val loss: 1.36004e-06, min loss: 7.63909e-07\n",
      "Epoch: 1507600, elapsed: 1.08e+01, train loss: 1.10083e-06, val loss: 1.71547e-06, min loss: 7.63909e-07\n",
      "Epoch: 1507700, elapsed: 1.08e+01, train loss: 1.41061e-06, val loss: 1.99222e-06, min loss: 7.63909e-07\n",
      "Epoch: 1507800, elapsed: 1.08e+01, train loss: 1.35911e-06, val loss: 2.04288e-06, min loss: 7.63909e-07\n",
      "Epoch: 1507900, elapsed: 1.09e+01, train loss: 8.68107e-07, val loss: 1.47228e-06, min loss: 7.63909e-07\n",
      "Epoch: 1508000, elapsed: 1.08e+01, train loss: 1.16652e-06, val loss: 2.01609e-06, min loss: 7.63909e-07\n",
      "Epoch: 1508100, elapsed: 1.09e+01, train loss: 1.47461e-06, val loss: 2.04428e-06, min loss: 7.63909e-07\n",
      "Epoch: 1508200, elapsed: 1.08e+01, train loss: 7.82958e-07, val loss: 1.36140e-06, min loss: 7.63909e-07\n",
      "Epoch: 1508300, elapsed: 1.10e+01, train loss: 7.77098e-07, val loss: 1.38016e-06, min loss: 7.63909e-07\n",
      "Epoch: 1508400, elapsed: 1.08e+01, train loss: 8.31987e-07, val loss: 1.45625e-06, min loss: 7.63909e-07\n",
      "Epoch: 1508500, elapsed: 1.52e+01, train loss: 7.70762e-07, val loss: 1.34556e-06, min loss: 7.63909e-07\n",
      "Epoch: 1508600, elapsed: 1.12e+01, train loss: 9.43837e-07, val loss: 1.69316e-06, min loss: 7.63909e-07\n",
      "Epoch: 1508700, elapsed: 1.10e+01, train loss: 7.99385e-07, val loss: 1.36360e-06, min loss: 7.63909e-07\n",
      "Epoch: 1508800, elapsed: 1.11e+01, train loss: 7.72917e-07, val loss: 1.34085e-06, min loss: 7.63909e-07\n",
      "Epoch: 1508900, elapsed: 1.08e+01, train loss: 3.31530e-06, val loss: 4.39273e-06, min loss: 7.63909e-07\n",
      "Epoch: 1509000, elapsed: 1.10e+01, train loss: 7.63639e-07, val loss: 1.35850e-06, min loss: 7.63639e-07\n",
      "Epoch: 1509100, elapsed: 1.11e+01, train loss: 8.51283e-07, val loss: 1.45161e-06, min loss: 7.63639e-07\n",
      "Epoch: 1509200, elapsed: 1.09e+01, train loss: 1.44494e-06, val loss: 2.13777e-06, min loss: 7.63639e-07\n",
      "Epoch: 1509300, elapsed: 1.12e+01, train loss: 7.61337e-07, val loss: 1.34956e-06, min loss: 7.61337e-07\n",
      "Epoch: 1509400, elapsed: 1.09e+01, train loss: 7.63135e-07, val loss: 1.33981e-06, min loss: 7.61337e-07\n",
      "Epoch: 1509500, elapsed: 1.10e+01, train loss: 2.00493e-06, val loss: 2.66866e-06, min loss: 7.61337e-07\n",
      "Epoch: 1509600, elapsed: 1.09e+01, train loss: 8.07947e-07, val loss: 1.43784e-06, min loss: 7.61337e-07\n",
      "Epoch: 1509700, elapsed: 1.10e+01, train loss: 7.71709e-07, val loss: 1.37192e-06, min loss: 7.61337e-07\n",
      "Epoch: 1509800, elapsed: 1.08e+01, train loss: 8.64506e-07, val loss: 1.46457e-06, min loss: 7.61337e-07\n",
      "Epoch: 1509900, elapsed: 1.08e+01, train loss: 1.26661e-06, val loss: 1.60098e-06, min loss: 7.61337e-07\n",
      "Epoch: 1510000, elapsed: 1.10e+01, train loss: 1.81485e-06, val loss: 2.19203e-06, min loss: 7.61337e-07\n",
      "Epoch: 1510100, elapsed: 1.27e+01, train loss: 8.65265e-07, val loss: 1.45902e-06, min loss: 7.61337e-07\n",
      "Epoch: 1510200, elapsed: 1.09e+01, train loss: 8.54890e-07, val loss: 1.40227e-06, min loss: 7.61337e-07\n",
      "Epoch: 1510300, elapsed: 1.07e+01, train loss: 8.00254e-07, val loss: 1.37683e-06, min loss: 7.61337e-07\n",
      "Epoch: 1510400, elapsed: 1.07e+01, train loss: 7.61223e-07, val loss: 1.34247e-06, min loss: 7.61223e-07\n",
      "Epoch: 1510500, elapsed: 1.08e+01, train loss: 7.86541e-07, val loss: 1.35303e-06, min loss: 7.61223e-07\n",
      "Epoch: 1510600, elapsed: 1.10e+01, train loss: 3.42272e-06, val loss: 3.07700e-06, min loss: 7.61223e-07\n",
      "Epoch: 1510700, elapsed: 1.08e+01, train loss: 7.59292e-07, val loss: 1.34676e-06, min loss: 7.59292e-07\n",
      "Epoch: 1510800, elapsed: 1.08e+01, train loss: 7.80494e-07, val loss: 1.35090e-06, min loss: 7.59292e-07\n",
      "Epoch: 1510900, elapsed: 1.06e+01, train loss: 9.50866e-07, val loss: 1.50174e-06, min loss: 7.59292e-07\n",
      "Epoch: 1511000, elapsed: 1.09e+01, train loss: 7.66900e-07, val loss: 1.36411e-06, min loss: 7.59292e-07\n",
      "Epoch: 1511100, elapsed: 1.08e+01, train loss: 7.67396e-07, val loss: 1.35576e-06, min loss: 7.59292e-07\n",
      "Epoch: 1511200, elapsed: 1.52e+01, train loss: 8.32761e-07, val loss: 1.53449e-06, min loss: 7.59292e-07\n",
      "Epoch: 1511300, elapsed: 1.10e+01, train loss: 1.48860e-06, val loss: 2.00138e-06, min loss: 7.59292e-07\n",
      "Epoch: 1511400, elapsed: 1.09e+01, train loss: 1.27573e-06, val loss: 2.20450e-06, min loss: 7.59292e-07\n",
      "Epoch: 1511500, elapsed: 1.10e+01, train loss: 7.94848e-07, val loss: 1.36721e-06, min loss: 7.59292e-07\n",
      "Epoch: 1511600, elapsed: 1.09e+01, train loss: 8.24365e-07, val loss: 1.60512e-06, min loss: 7.59292e-07\n",
      "Epoch: 1511700, elapsed: 1.11e+01, train loss: 8.66961e-07, val loss: 1.52448e-06, min loss: 7.59292e-07\n",
      "Epoch: 1511800, elapsed: 1.09e+01, train loss: 7.58335e-07, val loss: 1.34759e-06, min loss: 7.58335e-07\n",
      "Epoch: 1511900, elapsed: 1.10e+01, train loss: 8.25720e-07, val loss: 1.39190e-06, min loss: 7.58335e-07\n",
      "Epoch: 1512000, elapsed: 1.10e+01, train loss: 7.57768e-07, val loss: 1.34297e-06, min loss: 7.57768e-07\n",
      "Epoch: 1512100, elapsed: 1.08e+01, train loss: 7.57762e-07, val loss: 1.34901e-06, min loss: 7.57762e-07\n",
      "Epoch: 1512200, elapsed: 1.08e+01, train loss: 7.69587e-07, val loss: 1.35351e-06, min loss: 7.57762e-07\n",
      "Epoch: 1512300, elapsed: 1.08e+01, train loss: 8.28325e-07, val loss: 1.36475e-06, min loss: 7.57762e-07\n",
      "Epoch: 1512400, elapsed: 1.08e+01, train loss: 2.72925e-06, val loss: 3.88958e-06, min loss: 7.57762e-07\n",
      "Epoch: 1512500, elapsed: 1.11e+01, train loss: 7.56803e-07, val loss: 1.34414e-06, min loss: 7.56803e-07\n",
      "Epoch: 1512600, elapsed: 1.09e+01, train loss: 7.78474e-07, val loss: 1.40737e-06, min loss: 7.56803e-07\n",
      "Epoch: 1512700, elapsed: 1.07e+01, train loss: 8.93092e-07, val loss: 1.40782e-06, min loss: 7.56803e-07\n",
      "Epoch: 1512800, elapsed: 1.05e+01, train loss: 1.17934e-06, val loss: 2.17073e-06, min loss: 7.56803e-07\n",
      "Epoch: 1512900, elapsed: 1.08e+01, train loss: 7.56482e-07, val loss: 1.34491e-06, min loss: 7.56482e-07\n",
      "Epoch: 1513000, elapsed: 1.06e+01, train loss: 8.06306e-07, val loss: 1.45289e-06, min loss: 7.56482e-07\n",
      "Epoch: 1513100, elapsed: 1.07e+01, train loss: 7.56207e-07, val loss: 1.34542e-06, min loss: 7.56207e-07\n",
      "Epoch: 1513200, elapsed: 1.08e+01, train loss: 8.49629e-07, val loss: 1.69054e-06, min loss: 7.56207e-07\n",
      "Epoch: 1513300, elapsed: 1.08e+01, train loss: 7.70229e-07, val loss: 1.37919e-06, min loss: 7.56207e-07\n",
      "Epoch: 1513400, elapsed: 1.06e+01, train loss: 9.89539e-07, val loss: 1.57087e-06, min loss: 7.56207e-07\n",
      "Epoch: 1513500, elapsed: 1.07e+01, train loss: 1.91361e-06, val loss: 2.52714e-06, min loss: 7.56207e-07\n",
      "Epoch: 1513600, elapsed: 1.07e+01, train loss: 8.90397e-07, val loss: 1.43377e-06, min loss: 7.56207e-07\n",
      "Epoch: 1513700, elapsed: 1.08e+01, train loss: 7.77497e-07, val loss: 1.33804e-06, min loss: 7.56207e-07\n",
      "Epoch: 1513800, elapsed: 1.07e+01, train loss: 1.01450e-06, val loss: 1.49967e-06, min loss: 7.56207e-07\n",
      "Epoch: 1513900, elapsed: 1.08e+01, train loss: 1.91636e-06, val loss: 2.71573e-06, min loss: 7.56207e-07\n",
      "Epoch: 1514000, elapsed: 1.51e+01, train loss: 8.16459e-07, val loss: 1.38104e-06, min loss: 7.56207e-07\n",
      "Epoch: 1514100, elapsed: 1.12e+01, train loss: 7.60467e-07, val loss: 1.33680e-06, min loss: 7.56207e-07\n",
      "Epoch: 1514200, elapsed: 1.10e+01, train loss: 2.11570e-06, val loss: 2.53493e-06, min loss: 7.56207e-07\n",
      "Epoch: 1514300, elapsed: 1.11e+01, train loss: 1.00134e-06, val loss: 1.56428e-06, min loss: 7.56207e-07\n",
      "Epoch: 1514400, elapsed: 1.07e+01, train loss: 7.56113e-07, val loss: 1.34752e-06, min loss: 7.56113e-07\n",
      "Epoch: 1514500, elapsed: 1.11e+01, train loss: 7.54774e-07, val loss: 1.34567e-06, min loss: 7.54774e-07\n",
      "Epoch: 1514600, elapsed: 1.08e+01, train loss: 9.96234e-07, val loss: 1.85780e-06, min loss: 7.54774e-07\n",
      "Epoch: 1514700, elapsed: 1.09e+01, train loss: 7.57283e-07, val loss: 1.36109e-06, min loss: 7.54774e-07\n",
      "Epoch: 1514800, elapsed: 1.10e+01, train loss: 8.34476e-07, val loss: 1.36920e-06, min loss: 7.54774e-07\n",
      "Epoch: 1514900, elapsed: 1.09e+01, train loss: 8.73187e-07, val loss: 1.50791e-06, min loss: 7.54774e-07\n",
      "Epoch: 1515000, elapsed: 1.08e+01, train loss: 1.93392e-06, val loss: 2.65888e-06, min loss: 7.54774e-07\n",
      "Epoch: 1515100, elapsed: 1.29e+01, train loss: 7.66788e-07, val loss: 1.37770e-06, min loss: 7.54774e-07\n",
      "Epoch: 1515200, elapsed: 1.10e+01, train loss: 1.64102e-06, val loss: 2.02485e-06, min loss: 7.54774e-07\n",
      "Epoch: 1515300, elapsed: 1.10e+01, train loss: 1.30133e-06, val loss: 2.27935e-06, min loss: 7.54774e-07\n",
      "Epoch: 1515400, elapsed: 1.10e+01, train loss: 7.57038e-07, val loss: 1.33631e-06, min loss: 7.54774e-07\n",
      "Epoch: 1515500, elapsed: 1.11e+01, train loss: 7.68800e-07, val loss: 1.35137e-06, min loss: 7.54774e-07\n",
      "Epoch: 1515600, elapsed: 1.08e+01, train loss: 8.08008e-07, val loss: 1.32731e-06, min loss: 7.54774e-07\n",
      "Epoch: 1515700, elapsed: 1.09e+01, train loss: 8.48238e-07, val loss: 1.40489e-06, min loss: 7.54774e-07\n",
      "Epoch: 1515800, elapsed: 1.09e+01, train loss: 9.18335e-07, val loss: 1.44205e-06, min loss: 7.54774e-07\n",
      "Epoch: 1515900, elapsed: 1.08e+01, train loss: 8.82738e-07, val loss: 1.49287e-06, min loss: 7.54774e-07\n",
      "Epoch: 1516000, elapsed: 1.08e+01, train loss: 7.85886e-07, val loss: 1.40644e-06, min loss: 7.54774e-07\n",
      "Epoch: 1516100, elapsed: 1.08e+01, train loss: 8.68258e-07, val loss: 1.54024e-06, min loss: 7.54774e-07\n",
      "Epoch: 1516200, elapsed: 1.07e+01, train loss: 7.76211e-07, val loss: 1.35886e-06, min loss: 7.54774e-07\n",
      "Epoch: 1516300, elapsed: 1.08e+01, train loss: 9.59186e-07, val loss: 1.42190e-06, min loss: 7.54774e-07\n",
      "Epoch: 1516400, elapsed: 1.11e+01, train loss: 1.18365e-06, val loss: 1.96697e-06, min loss: 7.54774e-07\n",
      "Epoch: 1516500, elapsed: 1.08e+01, train loss: 1.04519e-06, val loss: 1.87417e-06, min loss: 7.54774e-07\n",
      "Epoch: 1516600, elapsed: 1.10e+01, train loss: 2.44892e-06, val loss: 2.80828e-06, min loss: 7.54774e-07\n",
      "Epoch: 1516700, elapsed: 1.51e+01, train loss: 3.34479e-06, val loss: 4.22731e-06, min loss: 7.54774e-07\n",
      "Epoch: 1516800, elapsed: 1.09e+01, train loss: 7.50970e-07, val loss: 1.34192e-06, min loss: 7.50970e-07\n",
      "Epoch: 1516900, elapsed: 1.12e+01, train loss: 7.51699e-07, val loss: 1.33028e-06, min loss: 7.50970e-07\n",
      "Epoch: 1517000, elapsed: 1.09e+01, train loss: 7.73332e-07, val loss: 1.38871e-06, min loss: 7.50970e-07\n",
      "Epoch: 1517100, elapsed: 1.10e+01, train loss: 2.74777e-06, val loss: 3.83795e-06, min loss: 7.50970e-07\n",
      "Epoch: 1517200, elapsed: 1.11e+01, train loss: 7.50406e-07, val loss: 1.33851e-06, min loss: 7.50406e-07\n",
      "Epoch: 1517300, elapsed: 1.10e+01, train loss: 9.96009e-07, val loss: 1.53037e-06, min loss: 7.50406e-07\n",
      "Epoch: 1517400, elapsed: 1.09e+01, train loss: 7.53265e-07, val loss: 1.36417e-06, min loss: 7.50406e-07\n",
      "Epoch: 1517500, elapsed: 1.09e+01, train loss: 7.50318e-07, val loss: 1.34015e-06, min loss: 7.50318e-07\n",
      "Epoch: 1517600, elapsed: 1.08e+01, train loss: 8.45697e-07, val loss: 1.40029e-06, min loss: 7.50318e-07\n",
      "Epoch: 1517700, elapsed: 1.06e+01, train loss: 7.49770e-07, val loss: 1.33587e-06, min loss: 7.49770e-07\n",
      "Epoch: 1517800, elapsed: 1.10e+01, train loss: 7.50196e-07, val loss: 1.34114e-06, min loss: 7.49770e-07\n",
      "Epoch: 1517900, elapsed: 1.09e+01, train loss: 8.18633e-07, val loss: 1.45851e-06, min loss: 7.49770e-07\n",
      "Epoch: 1518000, elapsed: 1.09e+01, train loss: 7.50704e-07, val loss: 1.33830e-06, min loss: 7.49770e-07\n",
      "Epoch: 1518100, elapsed: 1.11e+01, train loss: 7.50058e-07, val loss: 1.33503e-06, min loss: 7.49770e-07\n",
      "Epoch: 1518200, elapsed: 1.08e+01, train loss: 1.14014e-06, val loss: 1.45964e-06, min loss: 7.49770e-07\n",
      "Epoch: 1518300, elapsed: 1.09e+01, train loss: 7.48933e-07, val loss: 1.33753e-06, min loss: 7.48933e-07\n",
      "Epoch: 1518400, elapsed: 1.10e+01, train loss: 1.22949e-06, val loss: 1.44159e-06, min loss: 7.48933e-07\n",
      "Epoch: 1518500, elapsed: 1.07e+01, train loss: 7.48825e-07, val loss: 1.33618e-06, min loss: 7.48825e-07\n",
      "Epoch: 1518600, elapsed: 1.08e+01, train loss: 7.85511e-07, val loss: 1.45907e-06, min loss: 7.48825e-07\n",
      "Epoch: 1518700, elapsed: 1.09e+01, train loss: 9.14287e-07, val loss: 1.49761e-06, min loss: 7.48825e-07\n",
      "Epoch: 1518800, elapsed: 1.10e+01, train loss: 8.25624e-07, val loss: 1.42905e-06, min loss: 7.48825e-07\n",
      "Epoch: 1518900, elapsed: 1.08e+01, train loss: 9.59450e-07, val loss: 1.50280e-06, min loss: 7.48825e-07\n",
      "Epoch: 1519000, elapsed: 1.08e+01, train loss: 8.58239e-07, val loss: 1.47060e-06, min loss: 7.48825e-07\n",
      "Epoch: 1519100, elapsed: 1.06e+01, train loss: 9.55509e-07, val loss: 1.51858e-06, min loss: 7.48825e-07\n",
      "Epoch: 1519200, elapsed: 1.08e+01, train loss: 8.34601e-07, val loss: 1.42111e-06, min loss: 7.48825e-07\n",
      "Epoch: 1519300, elapsed: 1.08e+01, train loss: 7.65066e-07, val loss: 1.31938e-06, min loss: 7.48825e-07\n",
      "Epoch: 1519400, elapsed: 1.08e+01, train loss: 7.94223e-07, val loss: 1.34789e-06, min loss: 7.48825e-07\n",
      "Epoch: 1519500, elapsed: 1.49e+01, train loss: 9.13340e-07, val loss: 1.51381e-06, min loss: 7.48825e-07\n",
      "Epoch: 1519600, elapsed: 1.09e+01, train loss: 1.15950e-06, val loss: 1.89664e-06, min loss: 7.48825e-07\n",
      "Epoch: 1519700, elapsed: 1.09e+01, train loss: 1.22853e-06, val loss: 1.64591e-06, min loss: 7.48825e-07\n",
      "Epoch: 1519800, elapsed: 1.09e+01, train loss: 1.09318e-06, val loss: 1.61445e-06, min loss: 7.48825e-07\n",
      "Epoch: 1519900, elapsed: 1.10e+01, train loss: 1.03438e-06, val loss: 1.51945e-06, min loss: 7.48825e-07\n",
      "Epoch: 1520000, elapsed: 1.10e+01, train loss: 7.47813e-07, val loss: 1.33387e-06, min loss: 7.47813e-07\n",
      "Epoch: 1520100, elapsed: 1.29e+01, train loss: 7.59437e-07, val loss: 1.33954e-06, min loss: 7.47813e-07\n",
      "Epoch: 1520200, elapsed: 1.08e+01, train loss: 8.22558e-07, val loss: 1.35592e-06, min loss: 7.47813e-07\n",
      "Epoch: 1520300, elapsed: 1.09e+01, train loss: 8.47164e-07, val loss: 1.44555e-06, min loss: 7.47813e-07\n",
      "Epoch: 1520400, elapsed: 1.08e+01, train loss: 9.67887e-07, val loss: 1.64468e-06, min loss: 7.47813e-07\n",
      "Epoch: 1520500, elapsed: 1.08e+01, train loss: 7.52987e-07, val loss: 1.31533e-06, min loss: 7.47813e-07\n",
      "Epoch: 1520600, elapsed: 1.07e+01, train loss: 7.46718e-07, val loss: 1.33182e-06, min loss: 7.46718e-07\n",
      "Epoch: 1520700, elapsed: 1.07e+01, train loss: 8.36799e-07, val loss: 1.37539e-06, min loss: 7.46718e-07\n",
      "Epoch: 1520800, elapsed: 1.09e+01, train loss: 8.25676e-07, val loss: 1.47212e-06, min loss: 7.46718e-07\n",
      "Epoch: 1520900, elapsed: 1.10e+01, train loss: 7.47668e-07, val loss: 1.32807e-06, min loss: 7.46718e-07\n",
      "Epoch: 1521000, elapsed: 1.07e+01, train loss: 7.45913e-07, val loss: 1.33612e-06, min loss: 7.45913e-07\n",
      "Epoch: 1521100, elapsed: 1.09e+01, train loss: 7.82627e-07, val loss: 1.37830e-06, min loss: 7.45913e-07\n",
      "Epoch: 1521200, elapsed: 1.08e+01, train loss: 1.15402e-06, val loss: 2.26078e-06, min loss: 7.45913e-07\n",
      "Epoch: 1521300, elapsed: 1.08e+01, train loss: 7.45152e-07, val loss: 1.33347e-06, min loss: 7.45152e-07\n",
      "Epoch: 1521400, elapsed: 1.08e+01, train loss: 7.54443e-07, val loss: 1.34828e-06, min loss: 7.45152e-07\n",
      "Epoch: 1521500, elapsed: 1.08e+01, train loss: 5.73370e-06, val loss: 6.16211e-06, min loss: 7.45152e-07\n",
      "Epoch: 1521600, elapsed: 1.08e+01, train loss: 7.44930e-07, val loss: 1.33401e-06, min loss: 7.44930e-07\n",
      "Epoch: 1521700, elapsed: 1.07e+01, train loss: 7.67347e-07, val loss: 1.33488e-06, min loss: 7.44930e-07\n",
      "Epoch: 1521800, elapsed: 1.07e+01, train loss: 7.54053e-07, val loss: 1.35785e-06, min loss: 7.44930e-07\n",
      "Epoch: 1521900, elapsed: 1.09e+01, train loss: 7.45044e-07, val loss: 1.33454e-06, min loss: 7.44930e-07\n",
      "Epoch: 1522000, elapsed: 1.08e+01, train loss: 1.06312e-06, val loss: 1.36901e-06, min loss: 7.44930e-07\n",
      "Epoch: 1522100, elapsed: 1.08e+01, train loss: 7.44257e-07, val loss: 1.33492e-06, min loss: 7.44257e-07\n",
      "Epoch: 1522200, elapsed: 1.50e+01, train loss: 7.47101e-07, val loss: 1.34094e-06, min loss: 7.44257e-07\n",
      "Epoch: 1522300, elapsed: 1.09e+01, train loss: 2.37225e-06, val loss: 2.50390e-06, min loss: 7.44257e-07\n",
      "Epoch: 1522400, elapsed: 1.09e+01, train loss: 2.38891e-06, val loss: 2.13349e-06, min loss: 7.44257e-07\n",
      "Epoch: 1522500, elapsed: 1.09e+01, train loss: 7.62049e-07, val loss: 1.38459e-06, min loss: 7.44257e-07\n",
      "Epoch: 1522600, elapsed: 1.09e+01, train loss: 7.45606e-07, val loss: 1.33085e-06, min loss: 7.44257e-07\n",
      "Epoch: 1522700, elapsed: 1.10e+01, train loss: 7.46083e-07, val loss: 1.33935e-06, min loss: 7.44257e-07\n",
      "Epoch: 1522800, elapsed: 1.09e+01, train loss: 7.53735e-07, val loss: 1.37701e-06, min loss: 7.44257e-07\n",
      "Epoch: 1522900, elapsed: 1.08e+01, train loss: 7.99906e-07, val loss: 1.34295e-06, min loss: 7.44257e-07\n",
      "Epoch: 1523000, elapsed: 1.08e+01, train loss: 1.20687e-06, val loss: 1.67774e-06, min loss: 7.44257e-07\n",
      "Epoch: 1523100, elapsed: 1.09e+01, train loss: 7.46811e-07, val loss: 1.32522e-06, min loss: 7.44257e-07\n",
      "Epoch: 1523200, elapsed: 1.09e+01, train loss: 7.44833e-07, val loss: 1.33042e-06, min loss: 7.44257e-07\n",
      "Epoch: 1523300, elapsed: 1.09e+01, train loss: 8.13795e-07, val loss: 1.33246e-06, min loss: 7.44257e-07\n",
      "Epoch: 1523400, elapsed: 1.08e+01, train loss: 7.56982e-07, val loss: 1.34711e-06, min loss: 7.44257e-07\n",
      "Epoch: 1523500, elapsed: 1.07e+01, train loss: 7.49488e-07, val loss: 1.32947e-06, min loss: 7.44257e-07\n",
      "Epoch: 1523600, elapsed: 1.07e+01, train loss: 7.55247e-07, val loss: 1.33718e-06, min loss: 7.44257e-07\n",
      "Epoch: 1523700, elapsed: 1.10e+01, train loss: 7.57691e-07, val loss: 1.31667e-06, min loss: 7.44257e-07\n",
      "Epoch: 1523800, elapsed: 1.09e+01, train loss: 7.70896e-07, val loss: 1.38276e-06, min loss: 7.44257e-07\n",
      "Epoch: 1523900, elapsed: 1.07e+01, train loss: 8.23876e-07, val loss: 1.44451e-06, min loss: 7.44257e-07\n",
      "Epoch: 1524000, elapsed: 1.09e+01, train loss: 3.16635e-06, val loss: 2.49027e-06, min loss: 7.44257e-07\n",
      "Epoch: 1524100, elapsed: 1.07e+01, train loss: 7.41818e-07, val loss: 1.32981e-06, min loss: 7.41818e-07\n",
      "Epoch: 1524200, elapsed: 1.08e+01, train loss: 8.19731e-07, val loss: 1.32178e-06, min loss: 7.41818e-07\n",
      "Epoch: 1524300, elapsed: 1.10e+01, train loss: 7.41535e-07, val loss: 1.32974e-06, min loss: 7.41535e-07\n",
      "Epoch: 1524400, elapsed: 1.08e+01, train loss: 1.13006e-06, val loss: 1.77756e-06, min loss: 7.41535e-07\n",
      "Epoch: 1524500, elapsed: 1.09e+01, train loss: 7.41436e-07, val loss: 1.32976e-06, min loss: 7.41436e-07\n",
      "Epoch: 1524600, elapsed: 1.09e+01, train loss: 9.20903e-07, val loss: 1.95866e-06, min loss: 7.41436e-07\n",
      "Epoch: 1524700, elapsed: 1.09e+01, train loss: 7.41204e-07, val loss: 1.32902e-06, min loss: 7.41204e-07\n",
      "Epoch: 1524800, elapsed: 1.09e+01, train loss: 8.82969e-07, val loss: 1.59253e-06, min loss: 7.41204e-07\n",
      "Epoch: 1524900, elapsed: 1.08e+01, train loss: 7.41891e-07, val loss: 1.32401e-06, min loss: 7.41204e-07\n",
      "Epoch: 1525000, elapsed: 1.53e+01, train loss: 7.43244e-07, val loss: 1.32777e-06, min loss: 7.41204e-07\n",
      "Epoch: 1525100, elapsed: 1.29e+01, train loss: 7.72255e-07, val loss: 1.36305e-06, min loss: 7.41204e-07\n",
      "Epoch: 1525200, elapsed: 1.07e+01, train loss: 2.34154e-06, val loss: 3.43705e-06, min loss: 7.41204e-07\n",
      "Epoch: 1525300, elapsed: 1.10e+01, train loss: 7.54680e-07, val loss: 1.38024e-06, min loss: 7.41204e-07\n",
      "Epoch: 1525400, elapsed: 1.08e+01, train loss: 7.42094e-07, val loss: 1.32733e-06, min loss: 7.41204e-07\n",
      "Epoch: 1525500, elapsed: 1.09e+01, train loss: 7.54221e-07, val loss: 1.31150e-06, min loss: 7.41204e-07\n",
      "Epoch: 1525600, elapsed: 1.08e+01, train loss: 1.02738e-06, val loss: 1.44763e-06, min loss: 7.41204e-07\n",
      "Epoch: 1525700, elapsed: 1.08e+01, train loss: 7.41448e-07, val loss: 1.32362e-06, min loss: 7.41204e-07\n",
      "Epoch: 1525800, elapsed: 1.06e+01, train loss: 7.39902e-07, val loss: 1.32658e-06, min loss: 7.39902e-07\n",
      "Epoch: 1525900, elapsed: 1.10e+01, train loss: 7.45876e-07, val loss: 1.35742e-06, min loss: 7.39902e-07\n",
      "Epoch: 1526000, elapsed: 1.10e+01, train loss: 7.40995e-07, val loss: 1.33039e-06, min loss: 7.39902e-07\n",
      "Epoch: 1526100, elapsed: 1.08e+01, train loss: 7.80963e-07, val loss: 1.33284e-06, min loss: 7.39902e-07\n",
      "Epoch: 1526200, elapsed: 1.11e+01, train loss: 7.40264e-07, val loss: 1.33589e-06, min loss: 7.39902e-07\n",
      "Epoch: 1526300, elapsed: 1.09e+01, train loss: 7.40545e-07, val loss: 1.33263e-06, min loss: 7.39902e-07\n",
      "Epoch: 1526400, elapsed: 1.08e+01, train loss: 7.39233e-07, val loss: 1.32977e-06, min loss: 7.39233e-07\n",
      "Epoch: 1526500, elapsed: 1.08e+01, train loss: 7.40908e-07, val loss: 1.33150e-06, min loss: 7.39233e-07\n",
      "Epoch: 1526600, elapsed: 1.07e+01, train loss: 7.89548e-07, val loss: 1.42722e-06, min loss: 7.39233e-07\n",
      "Epoch: 1526700, elapsed: 1.09e+01, train loss: 2.14067e-06, val loss: 1.83147e-06, min loss: 7.39233e-07\n",
      "Epoch: 1526800, elapsed: 1.08e+01, train loss: 7.42863e-07, val loss: 1.34532e-06, min loss: 7.39233e-07\n",
      "Epoch: 1526900, elapsed: 1.09e+01, train loss: 7.39196e-07, val loss: 1.33325e-06, min loss: 7.39196e-07\n",
      "Epoch: 1527000, elapsed: 1.10e+01, train loss: 8.70332e-07, val loss: 1.33688e-06, min loss: 7.39196e-07\n",
      "Epoch: 1527100, elapsed: 1.09e+01, train loss: 7.38204e-07, val loss: 1.32689e-06, min loss: 7.38204e-07\n",
      "Epoch: 1527200, elapsed: 1.08e+01, train loss: 7.59039e-07, val loss: 1.35307e-06, min loss: 7.38204e-07\n",
      "Epoch: 1527300, elapsed: 1.08e+01, train loss: 1.19286e-06, val loss: 2.16669e-06, min loss: 7.38204e-07\n",
      "Epoch: 1527400, elapsed: 1.09e+01, train loss: 8.60544e-07, val loss: 1.58149e-06, min loss: 7.38204e-07\n",
      "Epoch: 1527500, elapsed: 1.10e+01, train loss: 1.00230e-06, val loss: 1.43803e-06, min loss: 7.38204e-07\n",
      "Epoch: 1527600, elapsed: 1.09e+01, train loss: 8.47891e-07, val loss: 1.43082e-06, min loss: 7.38204e-07\n",
      "Epoch: 1527700, elapsed: 1.52e+01, train loss: 7.38533e-07, val loss: 1.33045e-06, min loss: 7.38204e-07\n",
      "Epoch: 1527800, elapsed: 1.10e+01, train loss: 7.41535e-07, val loss: 1.32307e-06, min loss: 7.38204e-07\n",
      "Epoch: 1527900, elapsed: 1.11e+01, train loss: 7.52149e-07, val loss: 1.37447e-06, min loss: 7.38204e-07\n",
      "Epoch: 1528000, elapsed: 1.09e+01, train loss: 7.56441e-07, val loss: 1.34319e-06, min loss: 7.38204e-07\n",
      "Epoch: 1528100, elapsed: 1.09e+01, train loss: 1.03524e-06, val loss: 1.59391e-06, min loss: 7.38204e-07\n",
      "Epoch: 1528200, elapsed: 1.09e+01, train loss: 1.11451e-06, val loss: 1.82336e-06, min loss: 7.38204e-07\n",
      "Epoch: 1528300, elapsed: 1.10e+01, train loss: 8.28279e-07, val loss: 1.38709e-06, min loss: 7.38204e-07\n",
      "Epoch: 1528400, elapsed: 1.09e+01, train loss: 8.04306e-07, val loss: 1.39548e-06, min loss: 7.38204e-07\n",
      "Epoch: 1528500, elapsed: 1.08e+01, train loss: 7.90010e-07, val loss: 1.32823e-06, min loss: 7.38204e-07\n",
      "Epoch: 1528600, elapsed: 1.07e+01, train loss: 7.46082e-07, val loss: 1.37874e-06, min loss: 7.38204e-07\n",
      "Epoch: 1528700, elapsed: 1.10e+01, train loss: 8.26529e-07, val loss: 1.41293e-06, min loss: 7.38204e-07\n",
      "Epoch: 1528800, elapsed: 1.08e+01, train loss: 1.04532e-06, val loss: 1.80205e-06, min loss: 7.38204e-07\n",
      "Epoch: 1528900, elapsed: 1.10e+01, train loss: 7.54473e-07, val loss: 1.39046e-06, min loss: 7.38204e-07\n",
      "Epoch: 1529000, elapsed: 1.10e+01, train loss: 1.02697e-06, val loss: 1.43167e-06, min loss: 7.38204e-07\n",
      "Epoch: 1529100, elapsed: 1.09e+01, train loss: 1.23062e-06, val loss: 1.87163e-06, min loss: 7.38204e-07\n",
      "Epoch: 1529200, elapsed: 1.08e+01, train loss: 8.18702e-07, val loss: 1.32804e-06, min loss: 7.38204e-07\n",
      "Epoch: 1529300, elapsed: 1.10e+01, train loss: 8.58352e-07, val loss: 1.39422e-06, min loss: 7.38204e-07\n",
      "Epoch: 1529400, elapsed: 1.09e+01, train loss: 7.79875e-07, val loss: 1.33221e-06, min loss: 7.38204e-07\n",
      "Epoch: 1529500, elapsed: 1.09e+01, train loss: 7.53601e-07, val loss: 1.38853e-06, min loss: 7.38204e-07\n",
      "Epoch: 1529600, elapsed: 1.09e+01, train loss: 7.57634e-07, val loss: 1.35812e-06, min loss: 7.38204e-07\n",
      "Epoch: 1529700, elapsed: 1.08e+01, train loss: 8.21361e-07, val loss: 1.34142e-06, min loss: 7.38204e-07\n",
      "Epoch: 1529800, elapsed: 1.07e+01, train loss: 1.03113e-06, val loss: 1.68292e-06, min loss: 7.38204e-07\n",
      "Epoch: 1529900, elapsed: 1.09e+01, train loss: 1.71816e-06, val loss: 1.91448e-06, min loss: 7.38204e-07\n",
      "Epoch: 1530000, elapsed: 1.10e+01, train loss: 8.25402e-07, val loss: 1.34366e-06, min loss: 7.38204e-07\n",
      "Epoch: 1530100, elapsed: 1.28e+01, train loss: 8.05110e-07, val loss: 1.42524e-06, min loss: 7.38204e-07\n",
      "Epoch: 1530200, elapsed: 1.11e+01, train loss: 8.39665e-07, val loss: 1.39260e-06, min loss: 7.38204e-07\n",
      "Epoch: 1530300, elapsed: 1.06e+01, train loss: 7.51877e-07, val loss: 1.32042e-06, min loss: 7.38204e-07\n",
      "Epoch: 1530400, elapsed: 1.07e+01, train loss: 7.34994e-07, val loss: 1.31854e-06, min loss: 7.34994e-07\n",
      "Epoch: 1530500, elapsed: 1.50e+01, train loss: 7.38264e-07, val loss: 1.32598e-06, min loss: 7.34994e-07\n",
      "Epoch: 1530600, elapsed: 1.10e+01, train loss: 8.41569e-07, val loss: 1.47471e-06, min loss: 7.34994e-07\n",
      "Epoch: 1530700, elapsed: 1.10e+01, train loss: 1.19785e-06, val loss: 1.86828e-06, min loss: 7.34994e-07\n",
      "Epoch: 1530800, elapsed: 1.09e+01, train loss: 7.47741e-07, val loss: 1.37047e-06, min loss: 7.34994e-07\n",
      "Epoch: 1530900, elapsed: 1.11e+01, train loss: 7.57742e-07, val loss: 1.31232e-06, min loss: 7.34994e-07\n",
      "Epoch: 1531000, elapsed: 1.11e+01, train loss: 7.36594e-07, val loss: 1.31442e-06, min loss: 7.34994e-07\n",
      "Epoch: 1531100, elapsed: 1.10e+01, train loss: 7.70440e-07, val loss: 1.31819e-06, min loss: 7.34994e-07\n",
      "Epoch: 1531200, elapsed: 1.09e+01, train loss: 1.08719e-06, val loss: 1.64877e-06, min loss: 7.34994e-07\n",
      "Epoch: 1531300, elapsed: 1.08e+01, train loss: 8.01487e-07, val loss: 1.34819e-06, min loss: 7.34994e-07\n",
      "Epoch: 1531400, elapsed: 1.09e+01, train loss: 8.36041e-07, val loss: 1.40290e-06, min loss: 7.34994e-07\n",
      "Epoch: 1531500, elapsed: 1.09e+01, train loss: 7.45157e-07, val loss: 1.35398e-06, min loss: 7.34994e-07\n",
      "Epoch: 1531600, elapsed: 1.10e+01, train loss: 7.66830e-07, val loss: 1.36988e-06, min loss: 7.34994e-07\n",
      "Epoch: 1531700, elapsed: 1.09e+01, train loss: 7.35401e-07, val loss: 1.31569e-06, min loss: 7.34994e-07\n",
      "Epoch: 1531800, elapsed: 1.09e+01, train loss: 7.75217e-07, val loss: 1.37753e-06, min loss: 7.34994e-07\n",
      "Epoch: 1531900, elapsed: 1.07e+01, train loss: 9.73015e-07, val loss: 1.76076e-06, min loss: 7.34994e-07\n",
      "Epoch: 1532000, elapsed: 1.08e+01, train loss: 9.07380e-07, val loss: 1.47037e-06, min loss: 7.34994e-07\n",
      "Epoch: 1532100, elapsed: 1.09e+01, train loss: 7.40848e-07, val loss: 1.34941e-06, min loss: 7.34994e-07\n",
      "Epoch: 1532200, elapsed: 1.10e+01, train loss: 7.33418e-07, val loss: 1.31845e-06, min loss: 7.33418e-07\n",
      "Epoch: 1532300, elapsed: 1.09e+01, train loss: 8.26130e-07, val loss: 1.49349e-06, min loss: 7.33418e-07\n",
      "Epoch: 1532400, elapsed: 1.10e+01, train loss: 1.98083e-06, val loss: 3.07563e-06, min loss: 7.33418e-07\n",
      "Epoch: 1532500, elapsed: 1.09e+01, train loss: 1.05164e-06, val loss: 1.57962e-06, min loss: 7.33418e-07\n",
      "Epoch: 1532600, elapsed: 1.09e+01, train loss: 8.92284e-07, val loss: 1.57751e-06, min loss: 7.33418e-07\n",
      "Epoch: 1532700, elapsed: 1.09e+01, train loss: 9.54494e-07, val loss: 1.46655e-06, min loss: 7.33418e-07\n",
      "Epoch: 1532800, elapsed: 1.08e+01, train loss: 7.32388e-07, val loss: 1.32088e-06, min loss: 7.32388e-07\n",
      "Epoch: 1532900, elapsed: 1.09e+01, train loss: 7.34499e-07, val loss: 1.33171e-06, min loss: 7.32388e-07\n",
      "Epoch: 1533000, elapsed: 1.09e+01, train loss: 7.86173e-07, val loss: 1.39245e-06, min loss: 7.32388e-07\n",
      "Epoch: 1533100, elapsed: 1.08e+01, train loss: 2.67043e-06, val loss: 3.86339e-06, min loss: 7.32388e-07\n",
      "Epoch: 1533200, elapsed: 1.48e+01, train loss: 7.31869e-07, val loss: 1.32656e-06, min loss: 7.31869e-07\n",
      "Epoch: 1533300, elapsed: 1.10e+01, train loss: 1.30763e-06, val loss: 1.88267e-06, min loss: 7.31869e-07\n",
      "Epoch: 1533400, elapsed: 1.10e+01, train loss: 2.03803e-06, val loss: 1.95714e-06, min loss: 7.31869e-07\n",
      "Epoch: 1533500, elapsed: 1.08e+01, train loss: 1.19206e-06, val loss: 1.83049e-06, min loss: 7.31869e-07\n",
      "Epoch: 1533600, elapsed: 1.11e+01, train loss: 7.79625e-07, val loss: 1.41673e-06, min loss: 7.31869e-07\n",
      "Epoch: 1533700, elapsed: 1.11e+01, train loss: 1.20362e-06, val loss: 1.76920e-06, min loss: 7.31869e-07\n",
      "Epoch: 1533800, elapsed: 1.08e+01, train loss: 6.16290e-06, val loss: 5.28110e-06, min loss: 7.31869e-07\n",
      "Epoch: 1533900, elapsed: 1.10e+01, train loss: 7.30612e-07, val loss: 1.31983e-06, min loss: 7.30612e-07\n",
      "Epoch: 1534000, elapsed: 1.08e+01, train loss: 1.58526e-06, val loss: 1.77323e-06, min loss: 7.30612e-07\n",
      "Epoch: 1534100, elapsed: 1.09e+01, train loss: 8.83832e-07, val loss: 1.46707e-06, min loss: 7.30612e-07\n",
      "Epoch: 1534200, elapsed: 1.10e+01, train loss: 8.03458e-07, val loss: 1.46667e-06, min loss: 7.30612e-07\n",
      "Epoch: 1534300, elapsed: 1.07e+01, train loss: 7.51429e-07, val loss: 1.33014e-06, min loss: 7.30612e-07\n",
      "Epoch: 1534400, elapsed: 1.10e+01, train loss: 1.61985e-06, val loss: 2.01034e-06, min loss: 7.30612e-07\n",
      "Epoch: 1534500, elapsed: 1.09e+01, train loss: 7.92110e-07, val loss: 1.42325e-06, min loss: 7.30612e-07\n",
      "Epoch: 1534600, elapsed: 1.08e+01, train loss: 7.84558e-07, val loss: 1.41582e-06, min loss: 7.30612e-07\n",
      "Epoch: 1534700, elapsed: 1.08e+01, train loss: 7.68362e-07, val loss: 1.33044e-06, min loss: 7.30612e-07\n",
      "Epoch: 1534800, elapsed: 1.11e+01, train loss: 7.60660e-07, val loss: 1.38044e-06, min loss: 7.30612e-07\n",
      "Epoch: 1534900, elapsed: 1.10e+01, train loss: 7.72195e-07, val loss: 1.32206e-06, min loss: 7.30612e-07\n",
      "Epoch: 1535000, elapsed: 1.08e+01, train loss: 9.99489e-07, val loss: 1.82171e-06, min loss: 7.30612e-07\n",
      "Epoch: 1535100, elapsed: 1.28e+01, train loss: 7.46598e-07, val loss: 1.29721e-06, min loss: 7.30612e-07\n",
      "Epoch: 1535200, elapsed: 1.07e+01, train loss: 7.30481e-07, val loss: 1.33015e-06, min loss: 7.30481e-07\n",
      "Epoch: 1535300, elapsed: 1.09e+01, train loss: 8.09675e-07, val loss: 1.42140e-06, min loss: 7.30481e-07\n",
      "Epoch: 1535400, elapsed: 1.09e+01, train loss: 7.29450e-07, val loss: 1.32154e-06, min loss: 7.29450e-07\n",
      "Epoch: 1535500, elapsed: 1.06e+01, train loss: 8.09649e-07, val loss: 1.47121e-06, min loss: 7.29450e-07\n",
      "Epoch: 1535600, elapsed: 1.10e+01, train loss: 1.22885e-06, val loss: 1.73683e-06, min loss: 7.29450e-07\n",
      "Epoch: 1535700, elapsed: 1.10e+01, train loss: 7.28960e-07, val loss: 1.32006e-06, min loss: 7.28960e-07\n",
      "Epoch: 1535800, elapsed: 1.07e+01, train loss: 7.28750e-07, val loss: 1.32170e-06, min loss: 7.28750e-07\n",
      "Epoch: 1535900, elapsed: 1.08e+01, train loss: 8.68805e-07, val loss: 1.49541e-06, min loss: 7.28750e-07\n",
      "Epoch: 1536000, elapsed: 1.52e+01, train loss: 8.05185e-07, val loss: 1.35056e-06, min loss: 7.28750e-07\n",
      "Epoch: 1536100, elapsed: 1.11e+01, train loss: 7.37703e-07, val loss: 1.39054e-06, min loss: 7.28750e-07\n",
      "Epoch: 1536200, elapsed: 1.09e+01, train loss: 8.72702e-07, val loss: 1.44494e-06, min loss: 7.28750e-07\n",
      "Epoch: 1536300, elapsed: 1.09e+01, train loss: 7.64400e-07, val loss: 1.31399e-06, min loss: 7.28750e-07\n",
      "Epoch: 1536400, elapsed: 1.07e+01, train loss: 7.35833e-07, val loss: 1.33928e-06, min loss: 7.28750e-07\n",
      "Epoch: 1536500, elapsed: 1.11e+01, train loss: 7.33934e-07, val loss: 1.33882e-06, min loss: 7.28750e-07\n",
      "Epoch: 1536600, elapsed: 1.07e+01, train loss: 9.33883e-07, val loss: 1.66803e-06, min loss: 7.28750e-07\n",
      "Epoch: 1536700, elapsed: 1.09e+01, train loss: 1.08400e-06, val loss: 1.73316e-06, min loss: 7.28750e-07\n",
      "Epoch: 1536800, elapsed: 1.09e+01, train loss: 3.07421e-06, val loss: 3.76676e-06, min loss: 7.28750e-07\n",
      "Epoch: 1536900, elapsed: 1.09e+01, train loss: 7.51258e-07, val loss: 1.33495e-06, min loss: 7.28750e-07\n",
      "Epoch: 1537000, elapsed: 1.08e+01, train loss: 7.28330e-07, val loss: 1.31491e-06, min loss: 7.28330e-07\n",
      "Epoch: 1537100, elapsed: 1.09e+01, train loss: 7.31464e-07, val loss: 1.32866e-06, min loss: 7.28330e-07\n",
      "Epoch: 1537200, elapsed: 1.07e+01, train loss: 7.43783e-07, val loss: 1.34540e-06, min loss: 7.28330e-07\n",
      "Epoch: 1537300, elapsed: 1.10e+01, train loss: 9.33825e-07, val loss: 1.42902e-06, min loss: 7.28330e-07\n",
      "Epoch: 1537400, elapsed: 1.09e+01, train loss: 1.16562e-06, val loss: 1.87092e-06, min loss: 7.28330e-07\n",
      "Epoch: 1537500, elapsed: 1.08e+01, train loss: 1.73036e-06, val loss: 2.68798e-06, min loss: 7.28330e-07\n",
      "Epoch: 1537600, elapsed: 1.09e+01, train loss: 8.39790e-07, val loss: 1.64081e-06, min loss: 7.28330e-07\n",
      "Epoch: 1537700, elapsed: 1.10e+01, train loss: 4.37405e-06, val loss: 4.96792e-06, min loss: 7.28330e-07\n",
      "Epoch: 1537800, elapsed: 1.08e+01, train loss: 7.36768e-07, val loss: 1.33638e-06, min loss: 7.28330e-07\n",
      "Epoch: 1537900, elapsed: 1.09e+01, train loss: 7.41352e-07, val loss: 1.34754e-06, min loss: 7.28330e-07\n",
      "Epoch: 1538000, elapsed: 1.05e+01, train loss: 7.81612e-07, val loss: 1.31335e-06, min loss: 7.28330e-07\n",
      "Epoch: 1538100, elapsed: 1.08e+01, train loss: 9.96152e-07, val loss: 1.47014e-06, min loss: 7.28330e-07\n",
      "Epoch: 1538200, elapsed: 1.08e+01, train loss: 1.27836e-06, val loss: 2.03152e-06, min loss: 7.28330e-07\n",
      "Epoch: 1538300, elapsed: 1.07e+01, train loss: 7.25902e-07, val loss: 1.31289e-06, min loss: 7.25902e-07\n",
      "Epoch: 1538400, elapsed: 1.08e+01, train loss: 7.30452e-07, val loss: 1.31095e-06, min loss: 7.25902e-07\n",
      "Epoch: 1538500, elapsed: 1.11e+01, train loss: 7.34219e-07, val loss: 1.31937e-06, min loss: 7.25902e-07\n",
      "Epoch: 1538600, elapsed: 1.09e+01, train loss: 7.29122e-07, val loss: 1.31651e-06, min loss: 7.25902e-07\n",
      "Epoch: 1538700, elapsed: 1.07e+01, train loss: 7.35353e-07, val loss: 1.34360e-06, min loss: 7.25902e-07\n",
      "Epoch: 1538800, elapsed: 1.51e+01, train loss: 7.28471e-07, val loss: 1.30405e-06, min loss: 7.25902e-07\n",
      "Epoch: 1538900, elapsed: 1.10e+01, train loss: 7.26390e-07, val loss: 1.31113e-06, min loss: 7.25902e-07\n",
      "Epoch: 1539000, elapsed: 1.08e+01, train loss: 1.04905e-06, val loss: 1.63055e-06, min loss: 7.25902e-07\n",
      "Epoch: 1539100, elapsed: 1.08e+01, train loss: 7.24816e-07, val loss: 1.31651e-06, min loss: 7.24816e-07\n",
      "Epoch: 1539200, elapsed: 1.11e+01, train loss: 7.25822e-07, val loss: 1.31458e-06, min loss: 7.24816e-07\n",
      "Epoch: 1539300, elapsed: 1.11e+01, train loss: 1.08484e-06, val loss: 1.81235e-06, min loss: 7.24816e-07\n",
      "Epoch: 1539400, elapsed: 1.07e+01, train loss: 7.24436e-07, val loss: 1.31624e-06, min loss: 7.24436e-07\n",
      "Epoch: 1539500, elapsed: 1.11e+01, train loss: 7.25363e-07, val loss: 1.32329e-06, min loss: 7.24436e-07\n",
      "Epoch: 1539600, elapsed: 1.08e+01, train loss: 7.35065e-07, val loss: 1.30662e-06, min loss: 7.24436e-07\n",
      "Epoch: 1539700, elapsed: 1.10e+01, train loss: 8.10441e-07, val loss: 1.62820e-06, min loss: 7.24436e-07\n",
      "Epoch: 1539800, elapsed: 1.10e+01, train loss: 8.68776e-07, val loss: 1.57512e-06, min loss: 7.24436e-07\n",
      "Epoch: 1539900, elapsed: 1.09e+01, train loss: 9.47021e-07, val loss: 1.46057e-06, min loss: 7.24436e-07\n",
      "Epoch: 1540000, elapsed: 1.09e+01, train loss: 7.78578e-07, val loss: 1.36835e-06, min loss: 7.24436e-07\n",
      "Epoch: 1540100, elapsed: 1.28e+01, train loss: 2.79908e-06, val loss: 3.74581e-06, min loss: 7.24436e-07\n",
      "Epoch: 1540200, elapsed: 1.08e+01, train loss: 1.12943e-06, val loss: 1.77029e-06, min loss: 7.24436e-07\n",
      "Epoch: 1540300, elapsed: 1.09e+01, train loss: 7.94466e-07, val loss: 1.30553e-06, min loss: 7.24436e-07\n",
      "Epoch: 1540400, elapsed: 1.09e+01, train loss: 7.88450e-07, val loss: 1.38503e-06, min loss: 7.24436e-07\n",
      "Epoch: 1540500, elapsed: 1.10e+01, train loss: 1.33503e-06, val loss: 1.83578e-06, min loss: 7.24436e-07\n",
      "Epoch: 1540600, elapsed: 1.07e+01, train loss: 7.45271e-07, val loss: 1.30305e-06, min loss: 7.24436e-07\n",
      "Epoch: 1540700, elapsed: 1.09e+01, train loss: 7.33665e-07, val loss: 1.31351e-06, min loss: 7.24436e-07\n",
      "Epoch: 1540800, elapsed: 1.08e+01, train loss: 7.61075e-07, val loss: 1.33375e-06, min loss: 7.24436e-07\n",
      "Epoch: 1540900, elapsed: 1.08e+01, train loss: 8.37939e-07, val loss: 1.47834e-06, min loss: 7.24436e-07\n",
      "Epoch: 1541000, elapsed: 1.07e+01, train loss: 7.68737e-07, val loss: 1.36497e-06, min loss: 7.24436e-07\n",
      "Epoch: 1541100, elapsed: 1.08e+01, train loss: 7.67700e-07, val loss: 1.33523e-06, min loss: 7.24436e-07\n",
      "Epoch: 1541200, elapsed: 1.08e+01, train loss: 8.92711e-07, val loss: 1.41408e-06, min loss: 7.24436e-07\n",
      "Epoch: 1541300, elapsed: 1.08e+01, train loss: 3.47292e-06, val loss: 2.92506e-06, min loss: 7.24436e-07\n",
      "Epoch: 1541400, elapsed: 1.09e+01, train loss: 9.08379e-07, val loss: 1.54093e-06, min loss: 7.24436e-07\n",
      "Epoch: 1541500, elapsed: 1.49e+01, train loss: 1.11225e-06, val loss: 1.94317e-06, min loss: 7.24436e-07\n",
      "Epoch: 1541600, elapsed: 1.12e+01, train loss: 8.29316e-07, val loss: 1.41399e-06, min loss: 7.24436e-07\n",
      "Epoch: 1541700, elapsed: 1.11e+01, train loss: 7.36901e-07, val loss: 1.35692e-06, min loss: 7.24436e-07\n",
      "Epoch: 1541800, elapsed: 1.07e+01, train loss: 7.27844e-07, val loss: 1.29652e-06, min loss: 7.24436e-07\n",
      "Epoch: 1541900, elapsed: 1.08e+01, train loss: 7.22151e-07, val loss: 1.31405e-06, min loss: 7.22151e-07\n",
      "Epoch: 1542000, elapsed: 1.10e+01, train loss: 1.04812e-06, val loss: 1.51363e-06, min loss: 7.22151e-07\n",
      "Epoch: 1542100, elapsed: 1.08e+01, train loss: 8.04752e-07, val loss: 1.41921e-06, min loss: 7.22151e-07\n",
      "Epoch: 1542200, elapsed: 1.08e+01, train loss: 2.00143e-06, val loss: 1.39358e-06, min loss: 7.22151e-07\n",
      "Epoch: 1542300, elapsed: 1.06e+01, train loss: 7.21488e-07, val loss: 1.31184e-06, min loss: 7.21488e-07\n",
      "Epoch: 1542400, elapsed: 1.08e+01, train loss: 7.27514e-07, val loss: 1.31481e-06, min loss: 7.21488e-07\n",
      "Epoch: 1542500, elapsed: 1.10e+01, train loss: 7.50937e-07, val loss: 1.38956e-06, min loss: 7.21488e-07\n",
      "Epoch: 1542600, elapsed: 1.08e+01, train loss: 8.16762e-07, val loss: 1.44765e-06, min loss: 7.21488e-07\n",
      "Epoch: 1542700, elapsed: 1.08e+01, train loss: 2.21743e-06, val loss: 2.54723e-06, min loss: 7.21488e-07\n",
      "Epoch: 1542800, elapsed: 1.07e+01, train loss: 7.99132e-07, val loss: 1.33314e-06, min loss: 7.21488e-07\n",
      "Epoch: 1542900, elapsed: 1.06e+01, train loss: 8.29934e-07, val loss: 1.43718e-06, min loss: 7.21488e-07\n",
      "Epoch: 1543000, elapsed: 1.09e+01, train loss: 7.27338e-07, val loss: 1.30060e-06, min loss: 7.21488e-07\n",
      "Epoch: 1543100, elapsed: 1.08e+01, train loss: 7.21721e-07, val loss: 1.30569e-06, min loss: 7.21488e-07\n",
      "Epoch: 1543200, elapsed: 1.07e+01, train loss: 7.25005e-07, val loss: 1.30361e-06, min loss: 7.21488e-07\n",
      "Epoch: 1543300, elapsed: 1.09e+01, train loss: 7.29668e-07, val loss: 1.29125e-06, min loss: 7.21488e-07\n",
      "Epoch: 1543400, elapsed: 1.07e+01, train loss: 7.30557e-07, val loss: 1.30570e-06, min loss: 7.21488e-07\n",
      "Epoch: 1543500, elapsed: 1.09e+01, train loss: 1.02435e-06, val loss: 1.67682e-06, min loss: 7.21488e-07\n",
      "Epoch: 1543600, elapsed: 1.09e+01, train loss: 8.63108e-07, val loss: 1.38840e-06, min loss: 7.21488e-07\n",
      "Epoch: 1543700, elapsed: 1.06e+01, train loss: 1.00270e-06, val loss: 1.48890e-06, min loss: 7.21488e-07\n",
      "Epoch: 1543800, elapsed: 1.08e+01, train loss: 7.33551e-07, val loss: 1.31451e-06, min loss: 7.21488e-07\n",
      "Epoch: 1543900, elapsed: 1.06e+01, train loss: 1.06905e-06, val loss: 1.80429e-06, min loss: 7.21488e-07\n",
      "Epoch: 1544000, elapsed: 1.09e+01, train loss: 9.71812e-07, val loss: 1.40744e-06, min loss: 7.21488e-07\n",
      "Epoch: 1544100, elapsed: 1.07e+01, train loss: 8.30716e-07, val loss: 1.53776e-06, min loss: 7.21488e-07\n",
      "Epoch: 1544200, elapsed: 1.08e+01, train loss: 7.82375e-07, val loss: 1.36801e-06, min loss: 7.21488e-07\n",
      "Epoch: 1544300, elapsed: 1.08e+01, train loss: 1.17114e-06, val loss: 1.55078e-06, min loss: 7.21488e-07\n",
      "Epoch: 1544400, elapsed: 1.54e+01, train loss: 7.21712e-07, val loss: 1.31398e-06, min loss: 7.21488e-07\n",
      "Epoch: 1544500, elapsed: 1.10e+01, train loss: 7.21880e-07, val loss: 1.31537e-06, min loss: 7.21488e-07\n",
      "Epoch: 1544600, elapsed: 1.11e+01, train loss: 7.22783e-07, val loss: 1.30277e-06, min loss: 7.21488e-07\n",
      "Epoch: 1544700, elapsed: 1.10e+01, train loss: 7.32741e-07, val loss: 1.33606e-06, min loss: 7.21488e-07\n",
      "Epoch: 1544800, elapsed: 1.09e+01, train loss: 8.29689e-07, val loss: 1.48481e-06, min loss: 7.21488e-07\n",
      "Epoch: 1544900, elapsed: 1.10e+01, train loss: 1.13175e-06, val loss: 1.71390e-06, min loss: 7.21488e-07\n",
      "Epoch: 1545000, elapsed: 1.10e+01, train loss: 4.70537e-06, val loss: 3.57950e-06, min loss: 7.21488e-07\n",
      "Epoch: 1545100, elapsed: 1.29e+01, train loss: 7.18450e-07, val loss: 1.30955e-06, min loss: 7.18450e-07\n",
      "Epoch: 1545200, elapsed: 1.08e+01, train loss: 9.00172e-07, val loss: 1.42692e-06, min loss: 7.18450e-07\n",
      "Epoch: 1545300, elapsed: 1.06e+01, train loss: 7.20077e-07, val loss: 1.30240e-06, min loss: 7.18450e-07\n",
      "Epoch: 1545400, elapsed: 1.09e+01, train loss: 7.19886e-07, val loss: 1.31858e-06, min loss: 7.18450e-07\n",
      "Epoch: 1545500, elapsed: 1.10e+01, train loss: 7.43879e-07, val loss: 1.34290e-06, min loss: 7.18450e-07\n",
      "Epoch: 1545600, elapsed: 1.07e+01, train loss: 8.93188e-07, val loss: 1.48664e-06, min loss: 7.18450e-07\n",
      "Epoch: 1545700, elapsed: 1.09e+01, train loss: 8.47387e-07, val loss: 1.46652e-06, min loss: 7.18450e-07\n",
      "Epoch: 1545800, elapsed: 1.10e+01, train loss: 7.18015e-07, val loss: 1.30715e-06, min loss: 7.18015e-07\n",
      "Epoch: 1545900, elapsed: 1.07e+01, train loss: 7.32248e-07, val loss: 1.33404e-06, min loss: 7.18015e-07\n",
      "Epoch: 1546000, elapsed: 1.09e+01, train loss: 7.46050e-07, val loss: 1.29643e-06, min loss: 7.18015e-07\n",
      "Epoch: 1546100, elapsed: 1.09e+01, train loss: 8.61704e-07, val loss: 1.44305e-06, min loss: 7.18015e-07\n",
      "Epoch: 1546200, elapsed: 1.06e+01, train loss: 1.35147e-06, val loss: 1.80031e-06, min loss: 7.18015e-07\n",
      "Epoch: 1546300, elapsed: 1.08e+01, train loss: 1.59619e-06, val loss: 2.21315e-06, min loss: 7.18015e-07\n",
      "Epoch: 1546400, elapsed: 1.10e+01, train loss: 8.08461e-07, val loss: 1.46358e-06, min loss: 7.18015e-07\n",
      "Epoch: 1546500, elapsed: 1.08e+01, train loss: 7.20912e-07, val loss: 1.32920e-06, min loss: 7.18015e-07\n",
      "Epoch: 1546600, elapsed: 1.08e+01, train loss: 8.12866e-07, val loss: 1.47404e-06, min loss: 7.18015e-07\n",
      "Epoch: 1546700, elapsed: 1.08e+01, train loss: 7.64601e-07, val loss: 1.30457e-06, min loss: 7.18015e-07\n",
      "Epoch: 1546800, elapsed: 1.10e+01, train loss: 7.17976e-07, val loss: 1.31395e-06, min loss: 7.17976e-07\n",
      "Epoch: 1546900, elapsed: 1.08e+01, train loss: 1.81012e-06, val loss: 2.05811e-06, min loss: 7.17976e-07\n",
      "Epoch: 1547000, elapsed: 1.07e+01, train loss: 7.16661e-07, val loss: 1.31122e-06, min loss: 7.16661e-07\n",
      "Epoch: 1547100, elapsed: 1.51e+01, train loss: 7.19974e-07, val loss: 1.31646e-06, min loss: 7.16661e-07\n",
      "Epoch: 1547200, elapsed: 1.13e+01, train loss: 1.51075e-06, val loss: 1.46146e-06, min loss: 7.16661e-07\n",
      "Epoch: 1547300, elapsed: 1.10e+01, train loss: 1.27552e-06, val loss: 1.73511e-06, min loss: 7.16661e-07\n",
      "Epoch: 1547400, elapsed: 1.08e+01, train loss: 7.70739e-07, val loss: 1.40567e-06, min loss: 7.16661e-07\n",
      "Epoch: 1547500, elapsed: 1.08e+01, train loss: 7.19685e-07, val loss: 1.30037e-06, min loss: 7.16661e-07\n",
      "Epoch: 1547600, elapsed: 1.09e+01, train loss: 7.16298e-07, val loss: 1.31045e-06, min loss: 7.16298e-07\n",
      "Epoch: 1547700, elapsed: 1.09e+01, train loss: 7.18251e-07, val loss: 1.32336e-06, min loss: 7.16298e-07\n",
      "Epoch: 1547800, elapsed: 1.10e+01, train loss: 9.65448e-07, val loss: 1.56925e-06, min loss: 7.16298e-07\n",
      "Epoch: 1547900, elapsed: 1.10e+01, train loss: 9.31907e-07, val loss: 1.51371e-06, min loss: 7.16298e-07\n",
      "Epoch: 1548000, elapsed: 1.08e+01, train loss: 8.38169e-07, val loss: 1.64174e-06, min loss: 7.16298e-07\n",
      "Epoch: 1548100, elapsed: 1.09e+01, train loss: 1.26316e-06, val loss: 1.79669e-06, min loss: 7.16298e-07\n",
      "Epoch: 1548200, elapsed: 1.08e+01, train loss: 4.03383e-06, val loss: 3.80413e-06, min loss: 7.16298e-07\n",
      "Epoch: 1548300, elapsed: 1.07e+01, train loss: 7.47559e-07, val loss: 1.33268e-06, min loss: 7.16298e-07\n",
      "Epoch: 1548400, elapsed: 1.07e+01, train loss: 7.15241e-07, val loss: 1.30619e-06, min loss: 7.15241e-07\n",
      "Epoch: 1548500, elapsed: 1.09e+01, train loss: 7.42095e-07, val loss: 1.30855e-06, min loss: 7.15241e-07\n",
      "Epoch: 1548600, elapsed: 1.09e+01, train loss: 9.98564e-07, val loss: 1.64030e-06, min loss: 7.15241e-07\n",
      "Epoch: 1548700, elapsed: 1.09e+01, train loss: 7.49524e-07, val loss: 1.35078e-06, min loss: 7.15241e-07\n",
      "Epoch: 1548800, elapsed: 1.10e+01, train loss: 7.16205e-07, val loss: 1.31678e-06, min loss: 7.15241e-07\n",
      "Epoch: 1548900, elapsed: 1.08e+01, train loss: 9.10894e-07, val loss: 1.44128e-06, min loss: 7.15241e-07\n",
      "Epoch: 1549000, elapsed: 1.09e+01, train loss: 7.95269e-07, val loss: 1.44562e-06, min loss: 7.15241e-07\n",
      "Epoch: 1549100, elapsed: 1.08e+01, train loss: 7.14729e-07, val loss: 1.30294e-06, min loss: 7.14729e-07\n",
      "Epoch: 1549200, elapsed: 1.08e+01, train loss: 7.80946e-07, val loss: 1.38859e-06, min loss: 7.14729e-07\n",
      "Epoch: 1549300, elapsed: 1.09e+01, train loss: 7.78532e-07, val loss: 1.35885e-06, min loss: 7.14729e-07\n",
      "Epoch: 1549400, elapsed: 1.07e+01, train loss: 7.94241e-07, val loss: 1.44368e-06, min loss: 7.14729e-07\n",
      "Epoch: 1549500, elapsed: 1.09e+01, train loss: 7.14625e-07, val loss: 1.30930e-06, min loss: 7.14625e-07\n",
      "Epoch: 1549600, elapsed: 1.10e+01, train loss: 7.67464e-07, val loss: 1.36680e-06, min loss: 7.14625e-07\n",
      "Epoch: 1549700, elapsed: 1.08e+01, train loss: 7.20755e-07, val loss: 1.29353e-06, min loss: 7.14625e-07\n",
      "Epoch: 1549800, elapsed: 1.08e+01, train loss: 7.14703e-07, val loss: 1.31996e-06, min loss: 7.14625e-07\n",
      "Epoch: 1549900, elapsed: 1.52e+01, train loss: 1.06929e-06, val loss: 1.69227e-06, min loss: 7.14625e-07\n",
      "Epoch: 1550000, elapsed: 1.10e+01, train loss: 7.13689e-07, val loss: 1.30725e-06, min loss: 7.13689e-07\n",
      "Epoch: 1550100, elapsed: 1.28e+01, train loss: 7.49880e-07, val loss: 1.30003e-06, min loss: 7.13689e-07\n",
      "Epoch: 1550200, elapsed: 1.10e+01, train loss: 9.02001e-07, val loss: 1.63080e-06, min loss: 7.13689e-07\n",
      "Epoch: 1550300, elapsed: 1.10e+01, train loss: 1.13988e-06, val loss: 1.68003e-06, min loss: 7.13689e-07\n",
      "Epoch: 1550400, elapsed: 1.09e+01, train loss: 7.21950e-07, val loss: 1.28808e-06, min loss: 7.13689e-07\n",
      "Epoch: 1550500, elapsed: 1.10e+01, train loss: 7.19848e-07, val loss: 1.32476e-06, min loss: 7.13689e-07\n",
      "Epoch: 1550600, elapsed: 1.08e+01, train loss: 7.33695e-07, val loss: 1.30383e-06, min loss: 7.13689e-07\n",
      "Epoch: 1550700, elapsed: 1.09e+01, train loss: 7.40254e-07, val loss: 1.32849e-06, min loss: 7.13689e-07\n",
      "Epoch: 1550800, elapsed: 1.09e+01, train loss: 8.00188e-07, val loss: 1.38153e-06, min loss: 7.13689e-07\n",
      "Epoch: 1550900, elapsed: 1.10e+01, train loss: 7.29709e-07, val loss: 1.35485e-06, min loss: 7.13689e-07\n",
      "Epoch: 1551000, elapsed: 1.10e+01, train loss: 7.13129e-07, val loss: 1.30841e-06, min loss: 7.13129e-07\n",
      "Epoch: 1551100, elapsed: 1.09e+01, train loss: 8.52868e-07, val loss: 1.51379e-06, min loss: 7.13129e-07\n",
      "Epoch: 1551200, elapsed: 1.09e+01, train loss: 7.12343e-07, val loss: 1.30815e-06, min loss: 7.12343e-07\n",
      "Epoch: 1551300, elapsed: 1.07e+01, train loss: 7.18534e-07, val loss: 1.29743e-06, min loss: 7.12343e-07\n",
      "Epoch: 1551400, elapsed: 1.09e+01, train loss: 7.71246e-07, val loss: 1.40315e-06, min loss: 7.12343e-07\n",
      "Epoch: 1551500, elapsed: 1.08e+01, train loss: 8.02710e-07, val loss: 1.34651e-06, min loss: 7.12343e-07\n",
      "Epoch: 1551600, elapsed: 1.09e+01, train loss: 1.22753e-06, val loss: 1.62877e-06, min loss: 7.12343e-07\n",
      "Epoch: 1551700, elapsed: 1.07e+01, train loss: 7.24684e-07, val loss: 1.30915e-06, min loss: 7.12343e-07\n",
      "Epoch: 1551800, elapsed: 1.08e+01, train loss: 7.13385e-07, val loss: 1.30938e-06, min loss: 7.12343e-07\n",
      "Epoch: 1551900, elapsed: 1.08e+01, train loss: 7.34921e-07, val loss: 1.33349e-06, min loss: 7.12343e-07\n",
      "Epoch: 1552000, elapsed: 1.08e+01, train loss: 7.40652e-07, val loss: 1.30287e-06, min loss: 7.12343e-07\n",
      "Epoch: 1552100, elapsed: 1.08e+01, train loss: 1.09866e-06, val loss: 1.82345e-06, min loss: 7.12343e-07\n",
      "Epoch: 1552200, elapsed: 1.08e+01, train loss: 7.24433e-07, val loss: 1.31870e-06, min loss: 7.12343e-07\n",
      "Epoch: 1552300, elapsed: 1.06e+01, train loss: 7.37472e-07, val loss: 1.33882e-06, min loss: 7.12343e-07\n",
      "Epoch: 1552400, elapsed: 1.10e+01, train loss: 7.18212e-07, val loss: 1.32315e-06, min loss: 7.12343e-07\n",
      "Epoch: 1552500, elapsed: 1.08e+01, train loss: 7.11785e-07, val loss: 1.30243e-06, min loss: 7.11785e-07\n",
      "Epoch: 1552600, elapsed: 1.09e+01, train loss: 7.56886e-07, val loss: 1.37155e-06, min loss: 7.11785e-07\n",
      "Epoch: 1552700, elapsed: 1.51e+01, train loss: 7.79206e-07, val loss: 1.33435e-06, min loss: 7.11785e-07\n",
      "Epoch: 1552800, elapsed: 1.12e+01, train loss: 7.13716e-07, val loss: 1.29277e-06, min loss: 7.11785e-07\n",
      "Epoch: 1552900, elapsed: 1.10e+01, train loss: 7.42935e-07, val loss: 1.36931e-06, min loss: 7.11785e-07\n",
      "Epoch: 1553000, elapsed: 1.09e+01, train loss: 7.10284e-07, val loss: 1.30493e-06, min loss: 7.10284e-07\n",
      "Epoch: 1553100, elapsed: 1.09e+01, train loss: 7.17568e-07, val loss: 1.31556e-06, min loss: 7.10284e-07\n",
      "Epoch: 1553200, elapsed: 1.11e+01, train loss: 7.32868e-07, val loss: 1.28603e-06, min loss: 7.10284e-07\n",
      "Epoch: 1553300, elapsed: 1.09e+01, train loss: 7.09952e-07, val loss: 1.30459e-06, min loss: 7.09952e-07\n",
      "Epoch: 1553400, elapsed: 1.09e+01, train loss: 7.23921e-07, val loss: 1.31530e-06, min loss: 7.09952e-07\n",
      "Epoch: 1553500, elapsed: 1.10e+01, train loss: 2.17746e-06, val loss: 2.68207e-06, min loss: 7.09952e-07\n",
      "Epoch: 1553600, elapsed: 1.08e+01, train loss: 8.66961e-07, val loss: 1.28056e-06, min loss: 7.09952e-07\n",
      "Epoch: 1553700, elapsed: 1.10e+01, train loss: 2.44447e-06, val loss: 2.47071e-06, min loss: 7.09952e-07\n",
      "Epoch: 1553800, elapsed: 1.09e+01, train loss: 3.15042e-06, val loss: 3.61300e-06, min loss: 7.09952e-07\n",
      "Epoch: 1553900, elapsed: 1.08e+01, train loss: 1.13136e-06, val loss: 1.53540e-06, min loss: 7.09952e-07\n",
      "Epoch: 1554000, elapsed: 1.08e+01, train loss: 1.74969e-06, val loss: 2.34202e-06, min loss: 7.09952e-07\n",
      "Epoch: 1554100, elapsed: 1.09e+01, train loss: 1.57799e-06, val loss: 2.02842e-06, min loss: 7.09952e-07\n",
      "Epoch: 1554200, elapsed: 1.08e+01, train loss: 1.13168e-06, val loss: 1.47611e-06, min loss: 7.09952e-07\n",
      "Epoch: 1554300, elapsed: 1.10e+01, train loss: 7.50966e-07, val loss: 1.41053e-06, min loss: 7.09952e-07\n",
      "Epoch: 1554400, elapsed: 1.11e+01, train loss: 8.47710e-07, val loss: 1.49015e-06, min loss: 7.09952e-07\n",
      "Epoch: 1554500, elapsed: 1.08e+01, train loss: 1.29115e-06, val loss: 1.55307e-06, min loss: 7.09952e-07\n",
      "Epoch: 1554600, elapsed: 1.10e+01, train loss: 7.95761e-07, val loss: 1.38689e-06, min loss: 7.09952e-07\n",
      "Epoch: 1554700, elapsed: 1.09e+01, train loss: 7.74929e-07, val loss: 1.47811e-06, min loss: 7.09952e-07\n",
      "Epoch: 1554800, elapsed: 1.08e+01, train loss: 9.37013e-07, val loss: 1.49659e-06, min loss: 7.09952e-07\n",
      "Epoch: 1554900, elapsed: 1.10e+01, train loss: 1.54304e-06, val loss: 2.11687e-06, min loss: 7.09952e-07\n",
      "Epoch: 1555000, elapsed: 1.10e+01, train loss: 1.38382e-06, val loss: 2.22065e-06, min loss: 7.09952e-07\n",
      "Epoch: 1555100, elapsed: 1.30e+01, train loss: 2.44350e-06, val loss: 2.64393e-06, min loss: 7.09952e-07\n",
      "Epoch: 1555200, elapsed: 1.08e+01, train loss: 7.34278e-07, val loss: 1.30660e-06, min loss: 7.09952e-07\n",
      "Epoch: 1555300, elapsed: 1.09e+01, train loss: 7.29930e-07, val loss: 1.28662e-06, min loss: 7.09952e-07\n",
      "Epoch: 1555400, elapsed: 1.08e+01, train loss: 7.19318e-07, val loss: 1.33014e-06, min loss: 7.09952e-07\n",
      "Epoch: 1555500, elapsed: 1.51e+01, train loss: 7.52229e-07, val loss: 1.35975e-06, min loss: 7.09952e-07\n",
      "Epoch: 1555600, elapsed: 1.09e+01, train loss: 8.08907e-07, val loss: 1.44029e-06, min loss: 7.09952e-07\n",
      "Epoch: 1555700, elapsed: 1.09e+01, train loss: 7.14153e-07, val loss: 1.29590e-06, min loss: 7.09952e-07\n",
      "Epoch: 1555800, elapsed: 1.11e+01, train loss: 7.14661e-07, val loss: 1.31675e-06, min loss: 7.09952e-07\n",
      "Epoch: 1555900, elapsed: 1.10e+01, train loss: 7.38975e-07, val loss: 1.31347e-06, min loss: 7.09952e-07\n",
      "Epoch: 1556000, elapsed: 1.09e+01, train loss: 1.54478e-06, val loss: 2.13021e-06, min loss: 7.09952e-07\n",
      "Epoch: 1556100, elapsed: 1.10e+01, train loss: 9.63087e-07, val loss: 1.60359e-06, min loss: 7.09952e-07\n",
      "Epoch: 1556200, elapsed: 1.09e+01, train loss: 7.22272e-07, val loss: 1.30605e-06, min loss: 7.09952e-07\n",
      "Epoch: 1556300, elapsed: 1.10e+01, train loss: 7.11197e-07, val loss: 1.29036e-06, min loss: 7.09952e-07\n",
      "Epoch: 1556400, elapsed: 1.09e+01, train loss: 7.19111e-07, val loss: 1.30229e-06, min loss: 7.09952e-07\n",
      "Epoch: 1556500, elapsed: 1.08e+01, train loss: 7.73881e-07, val loss: 1.34080e-06, min loss: 7.09952e-07\n",
      "Epoch: 1556600, elapsed: 1.10e+01, train loss: 7.71847e-07, val loss: 1.31173e-06, min loss: 7.09952e-07\n",
      "Epoch: 1556700, elapsed: 1.08e+01, train loss: 7.44896e-07, val loss: 1.30723e-06, min loss: 7.09952e-07\n",
      "Epoch: 1556800, elapsed: 1.08e+01, train loss: 8.60652e-07, val loss: 1.42159e-06, min loss: 7.09952e-07\n",
      "Epoch: 1556900, elapsed: 1.08e+01, train loss: 8.19313e-07, val loss: 1.30255e-06, min loss: 7.09952e-07\n",
      "Epoch: 1557000, elapsed: 1.08e+01, train loss: 1.29841e-06, val loss: 1.76539e-06, min loss: 7.09952e-07\n",
      "Epoch: 1557100, elapsed: 1.09e+01, train loss: 1.03982e-06, val loss: 1.79289e-06, min loss: 7.09952e-07\n",
      "Epoch: 1557200, elapsed: 1.08e+01, train loss: 1.19672e-06, val loss: 2.06393e-06, min loss: 7.09952e-07\n",
      "Epoch: 1557300, elapsed: 1.09e+01, train loss: 7.06891e-07, val loss: 1.30375e-06, min loss: 7.06891e-07\n",
      "Epoch: 1557400, elapsed: 1.07e+01, train loss: 7.25115e-07, val loss: 1.35363e-06, min loss: 7.06891e-07\n",
      "Epoch: 1557500, elapsed: 1.06e+01, train loss: 2.64300e-06, val loss: 2.30587e-06, min loss: 7.06891e-07\n",
      "Epoch: 1557600, elapsed: 1.07e+01, train loss: 7.05845e-07, val loss: 1.30275e-06, min loss: 7.05845e-07\n",
      "Epoch: 1557700, elapsed: 1.08e+01, train loss: 7.08463e-07, val loss: 1.30093e-06, min loss: 7.05845e-07\n",
      "Epoch: 1557800, elapsed: 1.09e+01, train loss: 1.29738e-06, val loss: 2.46278e-06, min loss: 7.05845e-07\n",
      "Epoch: 1557900, elapsed: 1.08e+01, train loss: 7.05497e-07, val loss: 1.29966e-06, min loss: 7.05497e-07\n",
      "Epoch: 1558000, elapsed: 1.09e+01, train loss: 7.06354e-07, val loss: 1.31228e-06, min loss: 7.05497e-07\n",
      "Epoch: 1558100, elapsed: 1.09e+01, train loss: 1.06054e-06, val loss: 1.66680e-06, min loss: 7.05497e-07\n",
      "Epoch: 1558200, elapsed: 1.08e+01, train loss: 7.65601e-07, val loss: 1.31429e-06, min loss: 7.05497e-07\n",
      "Epoch: 1558300, elapsed: 1.50e+01, train loss: 8.42764e-07, val loss: 1.47903e-06, min loss: 7.05497e-07\n",
      "Epoch: 1558400, elapsed: 1.11e+01, train loss: 1.40223e-06, val loss: 2.32111e-06, min loss: 7.05497e-07\n",
      "Epoch: 1558500, elapsed: 1.09e+01, train loss: 1.10688e-06, val loss: 1.79180e-06, min loss: 7.05497e-07\n",
      "Epoch: 1558600, elapsed: 1.10e+01, train loss: 7.84147e-07, val loss: 1.40991e-06, min loss: 7.05497e-07\n",
      "Epoch: 1558700, elapsed: 1.10e+01, train loss: 7.47036e-07, val loss: 1.32482e-06, min loss: 7.05497e-07\n",
      "Epoch: 1558800, elapsed: 1.08e+01, train loss: 7.57829e-07, val loss: 1.39489e-06, min loss: 7.05497e-07\n",
      "Epoch: 1558900, elapsed: 1.09e+01, train loss: 7.06666e-07, val loss: 1.29609e-06, min loss: 7.05497e-07\n",
      "Epoch: 1559000, elapsed: 1.10e+01, train loss: 4.05653e-06, val loss: 3.78399e-06, min loss: 7.05497e-07\n",
      "Epoch: 1559100, elapsed: 1.08e+01, train loss: 7.04395e-07, val loss: 1.30110e-06, min loss: 7.04395e-07\n",
      "Epoch: 1559200, elapsed: 1.10e+01, train loss: 1.66590e-06, val loss: 2.66486e-06, min loss: 7.04395e-07\n",
      "Epoch: 1559300, elapsed: 1.09e+01, train loss: 8.94779e-07, val loss: 1.51691e-06, min loss: 7.04395e-07\n",
      "Epoch: 1559400, elapsed: 1.10e+01, train loss: 1.09515e-06, val loss: 1.57420e-06, min loss: 7.04395e-07\n",
      "Epoch: 1559500, elapsed: 1.07e+01, train loss: 1.75987e-06, val loss: 1.85422e-06, min loss: 7.04395e-07\n",
      "Epoch: 1559600, elapsed: 1.08e+01, train loss: 7.29766e-07, val loss: 1.34550e-06, min loss: 7.04395e-07\n",
      "Epoch: 1559700, elapsed: 1.09e+01, train loss: 7.03939e-07, val loss: 1.30160e-06, min loss: 7.03939e-07\n",
      "Epoch: 1559800, elapsed: 1.08e+01, train loss: 7.86285e-07, val loss: 1.46575e-06, min loss: 7.03939e-07\n",
      "Epoch: 1559900, elapsed: 1.10e+01, train loss: 8.73449e-07, val loss: 1.57088e-06, min loss: 7.03939e-07\n",
      "Epoch: 1560000, elapsed: 1.09e+01, train loss: 2.63181e-06, val loss: 3.13097e-06, min loss: 7.03939e-07\n",
      "Epoch: 1560100, elapsed: 1.27e+01, train loss: 1.02422e-06, val loss: 1.68111e-06, min loss: 7.03939e-07\n",
      "Epoch: 1560200, elapsed: 1.08e+01, train loss: 3.46184e-06, val loss: 4.13794e-06, min loss: 7.03939e-07\n",
      "Epoch: 1560300, elapsed: 1.10e+01, train loss: 7.93246e-07, val loss: 1.37393e-06, min loss: 7.03939e-07\n",
      "Epoch: 1560400, elapsed: 1.09e+01, train loss: 8.47609e-07, val loss: 1.42269e-06, min loss: 7.03939e-07\n",
      "Epoch: 1560500, elapsed: 1.05e+01, train loss: 7.08216e-07, val loss: 1.33024e-06, min loss: 7.03939e-07\n",
      "Epoch: 1560600, elapsed: 1.04e+01, train loss: 7.34036e-07, val loss: 1.36510e-06, min loss: 7.03939e-07\n",
      "Epoch: 1560700, elapsed: 1.03e+01, train loss: 8.36688e-07, val loss: 1.40859e-06, min loss: 7.03939e-07\n",
      "Epoch: 1560800, elapsed: 1.05e+01, train loss: 1.32800e-06, val loss: 2.22466e-06, min loss: 7.03939e-07\n",
      "Epoch: 1560900, elapsed: 1.05e+01, train loss: 1.17961e-06, val loss: 2.14005e-06, min loss: 7.03939e-07\n",
      "Epoch: 1561000, elapsed: 1.04e+01, train loss: 7.02825e-07, val loss: 1.29757e-06, min loss: 7.02825e-07\n",
      "Epoch: 1561100, elapsed: 1.45e+01, train loss: 7.17216e-07, val loss: 1.29198e-06, min loss: 7.02825e-07\n",
      "Epoch: 1561200, elapsed: 1.06e+01, train loss: 7.02116e-07, val loss: 1.29897e-06, min loss: 7.02116e-07\n",
      "Epoch: 1561300, elapsed: 1.05e+01, train loss: 7.21428e-07, val loss: 1.32869e-06, min loss: 7.02116e-07\n",
      "Epoch: 1561400, elapsed: 1.05e+01, train loss: 7.04790e-07, val loss: 1.31174e-06, min loss: 7.02116e-07\n",
      "Epoch: 1561500, elapsed: 1.05e+01, train loss: 7.03730e-07, val loss: 1.29615e-06, min loss: 7.02116e-07\n",
      "Epoch: 1561600, elapsed: 1.05e+01, train loss: 7.20038e-07, val loss: 1.27827e-06, min loss: 7.02116e-07\n",
      "Epoch: 1561700, elapsed: 1.03e+01, train loss: 7.42612e-07, val loss: 1.35222e-06, min loss: 7.02116e-07\n",
      "Epoch: 1561800, elapsed: 1.06e+01, train loss: 7.74059e-07, val loss: 1.35698e-06, min loss: 7.02116e-07\n",
      "Epoch: 1561900, elapsed: 1.09e+01, train loss: 9.87874e-07, val loss: 1.49340e-06, min loss: 7.02116e-07\n",
      "Epoch: 1562000, elapsed: 1.07e+01, train loss: 1.00683e-06, val loss: 1.45596e-06, min loss: 7.02116e-07\n",
      "Epoch: 1562100, elapsed: 1.09e+01, train loss: 8.68633e-07, val loss: 1.30395e-06, min loss: 7.02116e-07\n",
      "Epoch: 1562200, elapsed: 1.07e+01, train loss: 1.12538e-06, val loss: 1.64793e-06, min loss: 7.02116e-07\n",
      "Epoch: 1562300, elapsed: 1.07e+01, train loss: 1.23159e-06, val loss: 1.73313e-06, min loss: 7.02116e-07\n",
      "Epoch: 1562400, elapsed: 1.07e+01, train loss: 7.16365e-07, val loss: 1.28191e-06, min loss: 7.02116e-07\n",
      "Epoch: 1562500, elapsed: 1.06e+01, train loss: 9.16880e-07, val loss: 1.59194e-06, min loss: 7.02116e-07\n",
      "Epoch: 1562600, elapsed: 1.06e+01, train loss: 1.10971e-06, val loss: 1.91422e-06, min loss: 7.02116e-07\n",
      "Epoch: 1562700, elapsed: 1.06e+01, train loss: 7.03691e-07, val loss: 1.29258e-06, min loss: 7.02116e-07\n",
      "Epoch: 1562800, elapsed: 1.06e+01, train loss: 9.00868e-07, val loss: 1.47668e-06, min loss: 7.02116e-07\n",
      "Epoch: 1562900, elapsed: 1.06e+01, train loss: 2.42254e-06, val loss: 2.80098e-06, min loss: 7.02116e-07\n",
      "Epoch: 1563000, elapsed: 1.04e+01, train loss: 1.68360e-06, val loss: 2.14191e-06, min loss: 7.02116e-07\n",
      "Epoch: 1563100, elapsed: 1.05e+01, train loss: 8.41423e-07, val loss: 1.51268e-06, min loss: 7.02116e-07\n",
      "Epoch: 1563200, elapsed: 1.07e+01, train loss: 7.95243e-07, val loss: 1.42064e-06, min loss: 7.02116e-07\n",
      "Epoch: 1563300, elapsed: 1.05e+01, train loss: 7.21442e-07, val loss: 1.27454e-06, min loss: 7.02116e-07\n",
      "Epoch: 1563400, elapsed: 1.05e+01, train loss: 7.00096e-07, val loss: 1.29704e-06, min loss: 7.00096e-07\n",
      "Epoch: 1563500, elapsed: 1.06e+01, train loss: 7.04301e-07, val loss: 1.30369e-06, min loss: 7.00096e-07\n",
      "Epoch: 1563600, elapsed: 1.04e+01, train loss: 7.38923e-07, val loss: 1.36402e-06, min loss: 7.00096e-07\n",
      "Epoch: 1563700, elapsed: 1.04e+01, train loss: 1.34985e-06, val loss: 1.88640e-06, min loss: 7.00096e-07\n",
      "Epoch: 1563800, elapsed: 1.05e+01, train loss: 6.99790e-07, val loss: 1.29560e-06, min loss: 6.99790e-07\n",
      "Epoch: 1563900, elapsed: 1.03e+01, train loss: 6.99971e-07, val loss: 1.30108e-06, min loss: 6.99790e-07\n",
      "Epoch: 1564000, elapsed: 1.49e+01, train loss: 7.13960e-07, val loss: 1.30785e-06, min loss: 6.99790e-07\n",
      "Epoch: 1564100, elapsed: 1.09e+01, train loss: 7.04246e-07, val loss: 1.28386e-06, min loss: 6.99790e-07\n",
      "Epoch: 1564200, elapsed: 1.06e+01, train loss: 7.01182e-07, val loss: 1.30468e-06, min loss: 6.99790e-07\n",
      "Epoch: 1564300, elapsed: 1.07e+01, train loss: 7.03149e-07, val loss: 1.29081e-06, min loss: 6.99790e-07\n",
      "Epoch: 1564400, elapsed: 1.08e+01, train loss: 7.85718e-07, val loss: 1.36909e-06, min loss: 6.99790e-07\n",
      "Epoch: 1564500, elapsed: 1.08e+01, train loss: 8.15017e-07, val loss: 1.46963e-06, min loss: 6.99790e-07\n",
      "Epoch: 1564600, elapsed: 1.07e+01, train loss: 1.08913e-06, val loss: 1.74744e-06, min loss: 6.99790e-07\n",
      "Epoch: 1564700, elapsed: 1.05e+01, train loss: 7.04668e-07, val loss: 1.31665e-06, min loss: 6.99790e-07\n",
      "Epoch: 1564800, elapsed: 1.05e+01, train loss: 5.78857e-06, val loss: 5.90605e-06, min loss: 6.99790e-07\n",
      "Epoch: 1564900, elapsed: 1.05e+01, train loss: 6.98749e-07, val loss: 1.29741e-06, min loss: 6.98749e-07\n",
      "Epoch: 1565000, elapsed: 1.09e+01, train loss: 7.32686e-07, val loss: 1.37771e-06, min loss: 6.98749e-07\n",
      "Epoch: 1565100, elapsed: 1.24e+01, train loss: 6.17212e-06, val loss: 6.49652e-06, min loss: 6.98749e-07\n",
      "Epoch: 1565200, elapsed: 1.06e+01, train loss: 6.98320e-07, val loss: 1.29530e-06, min loss: 6.98320e-07\n",
      "Epoch: 1565300, elapsed: 1.05e+01, train loss: 9.07236e-07, val loss: 1.45755e-06, min loss: 6.98320e-07\n",
      "Epoch: 1565400, elapsed: 1.07e+01, train loss: 7.75197e-07, val loss: 1.36572e-06, min loss: 6.98320e-07\n",
      "Epoch: 1565500, elapsed: 1.06e+01, train loss: 6.98213e-07, val loss: 1.29612e-06, min loss: 6.98213e-07\n",
      "Epoch: 1565600, elapsed: 1.06e+01, train loss: 7.95475e-07, val loss: 1.35153e-06, min loss: 6.98213e-07\n",
      "Epoch: 1565700, elapsed: 1.07e+01, train loss: 7.47857e-07, val loss: 1.40733e-06, min loss: 6.98213e-07\n",
      "Epoch: 1565800, elapsed: 1.07e+01, train loss: 9.83724e-07, val loss: 1.86929e-06, min loss: 6.98213e-07\n",
      "Epoch: 1565900, elapsed: 1.06e+01, train loss: 7.21832e-07, val loss: 1.32093e-06, min loss: 6.98213e-07\n",
      "Epoch: 1566000, elapsed: 1.05e+01, train loss: 6.98376e-07, val loss: 1.30565e-06, min loss: 6.98213e-07\n",
      "Epoch: 1566100, elapsed: 1.05e+01, train loss: 9.34197e-07, val loss: 1.47329e-06, min loss: 6.98213e-07\n",
      "Epoch: 1566200, elapsed: 1.06e+01, train loss: 7.75938e-07, val loss: 1.33862e-06, min loss: 6.98213e-07\n",
      "Epoch: 1566300, elapsed: 1.06e+01, train loss: 1.02335e-06, val loss: 1.67358e-06, min loss: 6.98213e-07\n",
      "Epoch: 1566400, elapsed: 1.05e+01, train loss: 6.98417e-07, val loss: 1.29619e-06, min loss: 6.98213e-07\n",
      "Epoch: 1566500, elapsed: 1.06e+01, train loss: 7.99146e-07, val loss: 1.39607e-06, min loss: 6.98213e-07\n",
      "Epoch: 1566600, elapsed: 1.05e+01, train loss: 7.45886e-07, val loss: 1.40865e-06, min loss: 6.98213e-07\n",
      "Epoch: 1566700, elapsed: 1.06e+01, train loss: 1.31011e-06, val loss: 2.22020e-06, min loss: 6.98213e-07\n",
      "Epoch: 1566800, elapsed: 1.51e+01, train loss: 6.99822e-07, val loss: 1.29718e-06, min loss: 6.98213e-07\n",
      "Epoch: 1566900, elapsed: 1.05e+01, train loss: 6.98954e-07, val loss: 1.30046e-06, min loss: 6.98213e-07\n",
      "Epoch: 1567000, elapsed: 1.07e+01, train loss: 6.97273e-07, val loss: 1.29629e-06, min loss: 6.97273e-07\n",
      "Epoch: 1567100, elapsed: 1.07e+01, train loss: 8.55706e-07, val loss: 1.44540e-06, min loss: 6.97273e-07\n",
      "Epoch: 1567200, elapsed: 1.06e+01, train loss: 8.60226e-07, val loss: 1.87293e-06, min loss: 6.97273e-07\n",
      "Epoch: 1567300, elapsed: 1.05e+01, train loss: 8.52424e-07, val loss: 1.63839e-06, min loss: 6.97273e-07\n",
      "Epoch: 1567400, elapsed: 1.06e+01, train loss: 7.05051e-07, val loss: 1.28750e-06, min loss: 6.97273e-07\n",
      "Epoch: 1567500, elapsed: 1.07e+01, train loss: 9.30481e-07, val loss: 1.72135e-06, min loss: 6.97273e-07\n",
      "Epoch: 1567600, elapsed: 1.06e+01, train loss: 6.96649e-07, val loss: 1.28976e-06, min loss: 6.96649e-07\n",
      "Epoch: 1567700, elapsed: 1.05e+01, train loss: 8.01335e-07, val loss: 1.44700e-06, min loss: 6.96649e-07\n",
      "Epoch: 1567800, elapsed: 1.07e+01, train loss: 6.95862e-07, val loss: 1.29677e-06, min loss: 6.95862e-07\n",
      "Epoch: 1567900, elapsed: 1.07e+01, train loss: 7.05320e-07, val loss: 1.32156e-06, min loss: 6.95862e-07\n",
      "Epoch: 1568000, elapsed: 1.04e+01, train loss: 8.30861e-07, val loss: 1.45796e-06, min loss: 6.95862e-07\n",
      "Epoch: 1568100, elapsed: 1.06e+01, train loss: 6.95884e-07, val loss: 1.29917e-06, min loss: 6.95862e-07\n",
      "Epoch: 1568200, elapsed: 1.05e+01, train loss: 6.99895e-07, val loss: 1.30052e-06, min loss: 6.95862e-07\n",
      "Epoch: 1568300, elapsed: 1.08e+01, train loss: 6.95407e-07, val loss: 1.29201e-06, min loss: 6.95407e-07\n",
      "Epoch: 1568400, elapsed: 1.06e+01, train loss: 7.11193e-07, val loss: 1.31907e-06, min loss: 6.95407e-07\n",
      "Epoch: 1568500, elapsed: 1.04e+01, train loss: 8.04458e-07, val loss: 1.32717e-06, min loss: 6.95407e-07\n",
      "Epoch: 1568600, elapsed: 1.06e+01, train loss: 8.96038e-07, val loss: 1.69020e-06, min loss: 6.95407e-07\n",
      "Epoch: 1568700, elapsed: 1.05e+01, train loss: 3.77168e-06, val loss: 3.79099e-06, min loss: 6.95407e-07\n",
      "Epoch: 1568800, elapsed: 1.04e+01, train loss: 7.65349e-07, val loss: 1.43738e-06, min loss: 6.95407e-07\n",
      "Epoch: 1568900, elapsed: 1.04e+01, train loss: 6.95962e-07, val loss: 1.29316e-06, min loss: 6.95407e-07\n",
      "Epoch: 1569000, elapsed: 1.06e+01, train loss: 6.96209e-07, val loss: 1.29873e-06, min loss: 6.95407e-07\n",
      "Epoch: 1569100, elapsed: 1.05e+01, train loss: 7.05024e-07, val loss: 1.30508e-06, min loss: 6.95407e-07\n",
      "Epoch: 1569200, elapsed: 1.06e+01, train loss: 8.36923e-07, val loss: 1.38052e-06, min loss: 6.95407e-07\n",
      "Epoch: 1569300, elapsed: 1.05e+01, train loss: 8.10523e-07, val loss: 1.36542e-06, min loss: 6.95407e-07\n",
      "Epoch: 1569400, elapsed: 1.05e+01, train loss: 1.41148e-06, val loss: 1.79818e-06, min loss: 6.95407e-07\n",
      "Epoch: 1569500, elapsed: 1.07e+01, train loss: 1.72758e-06, val loss: 1.65571e-06, min loss: 6.95407e-07\n",
      "Epoch: 1569600, elapsed: 1.47e+01, train loss: 1.45596e-06, val loss: 2.05253e-06, min loss: 6.95407e-07\n",
      "Epoch: 1569700, elapsed: 1.09e+01, train loss: 7.26434e-07, val loss: 1.27949e-06, min loss: 6.95407e-07\n",
      "Epoch: 1569800, elapsed: 1.06e+01, train loss: 6.98355e-07, val loss: 1.31246e-06, min loss: 6.95407e-07\n",
      "Epoch: 1569900, elapsed: 1.09e+01, train loss: 1.53629e-06, val loss: 2.46822e-06, min loss: 6.95407e-07\n",
      "Epoch: 1570000, elapsed: 1.06e+01, train loss: 6.94189e-07, val loss: 1.29869e-06, min loss: 6.94189e-07\n",
      "Epoch: 1570100, elapsed: 1.26e+01, train loss: 6.95250e-07, val loss: 1.30153e-06, min loss: 6.94189e-07\n",
      "Epoch: 1570200, elapsed: 1.06e+01, train loss: 8.32805e-07, val loss: 1.39773e-06, min loss: 6.94189e-07\n",
      "Epoch: 1570300, elapsed: 1.05e+01, train loss: 3.27498e-06, val loss: 3.06189e-06, min loss: 6.94189e-07\n",
      "Epoch: 1570400, elapsed: 1.06e+01, train loss: 6.93811e-07, val loss: 1.29075e-06, min loss: 6.93811e-07\n",
      "Epoch: 1570500, elapsed: 1.05e+01, train loss: 6.95484e-07, val loss: 1.28978e-06, min loss: 6.93811e-07\n",
      "Epoch: 1570600, elapsed: 1.06e+01, train loss: 1.48745e-06, val loss: 1.66920e-06, min loss: 6.93811e-07\n",
      "Epoch: 1570700, elapsed: 1.05e+01, train loss: 1.34451e-06, val loss: 1.97524e-06, min loss: 6.93811e-07\n",
      "Epoch: 1570800, elapsed: 1.05e+01, train loss: 1.07226e-06, val loss: 2.24295e-06, min loss: 6.93811e-07\n",
      "Epoch: 1570900, elapsed: 1.05e+01, train loss: 8.09803e-07, val loss: 1.40023e-06, min loss: 6.93811e-07\n",
      "Epoch: 1571000, elapsed: 1.07e+01, train loss: 9.44550e-07, val loss: 1.29483e-06, min loss: 6.93811e-07\n",
      "Epoch: 1571100, elapsed: 1.06e+01, train loss: 8.61792e-07, val loss: 1.71971e-06, min loss: 6.93811e-07\n",
      "Epoch: 1571200, elapsed: 1.05e+01, train loss: 2.37457e-06, val loss: 3.31123e-06, min loss: 6.93811e-07\n",
      "Epoch: 1571300, elapsed: 1.07e+01, train loss: 1.14750e-06, val loss: 1.54212e-06, min loss: 6.93811e-07\n",
      "Epoch: 1571400, elapsed: 1.03e+01, train loss: 9.53672e-07, val loss: 1.53249e-06, min loss: 6.93811e-07\n",
      "Epoch: 1571500, elapsed: 1.05e+01, train loss: 7.51847e-07, val loss: 1.33705e-06, min loss: 6.93811e-07\n",
      "Epoch: 1571600, elapsed: 1.04e+01, train loss: 1.04699e-06, val loss: 1.74633e-06, min loss: 6.93811e-07\n",
      "Epoch: 1571700, elapsed: 1.05e+01, train loss: 7.01955e-07, val loss: 1.31853e-06, min loss: 6.93811e-07\n",
      "Epoch: 1571800, elapsed: 1.04e+01, train loss: 6.93846e-07, val loss: 1.28763e-06, min loss: 6.93811e-07\n",
      "Epoch: 1571900, elapsed: 1.05e+01, train loss: 7.09682e-07, val loss: 1.29096e-06, min loss: 6.93811e-07\n",
      "Epoch: 1572000, elapsed: 1.04e+01, train loss: 7.10338e-07, val loss: 1.29154e-06, min loss: 6.93811e-07\n",
      "Epoch: 1572100, elapsed: 1.06e+01, train loss: 8.87730e-07, val loss: 1.46259e-06, min loss: 6.93811e-07\n",
      "Epoch: 1572200, elapsed: 1.05e+01, train loss: 1.69476e-06, val loss: 1.97767e-06, min loss: 6.93811e-07\n",
      "Epoch: 1572300, elapsed: 1.05e+01, train loss: 1.79429e-06, val loss: 2.24192e-06, min loss: 6.93811e-07\n",
      "Epoch: 1572400, elapsed: 1.46e+01, train loss: 1.08173e-06, val loss: 1.87350e-06, min loss: 6.93811e-07\n",
      "Epoch: 1572500, elapsed: 1.06e+01, train loss: 7.03514e-07, val loss: 1.29251e-06, min loss: 6.93811e-07\n",
      "Epoch: 1572600, elapsed: 1.05e+01, train loss: 6.92892e-07, val loss: 1.29095e-06, min loss: 6.92892e-07\n",
      "Epoch: 1572700, elapsed: 1.06e+01, train loss: 7.09179e-07, val loss: 1.28411e-06, min loss: 6.92892e-07\n",
      "Epoch: 1572800, elapsed: 1.07e+01, train loss: 7.88802e-07, val loss: 1.64200e-06, min loss: 6.92892e-07\n",
      "Epoch: 1572900, elapsed: 1.07e+01, train loss: 6.91117e-07, val loss: 1.29139e-06, min loss: 6.91117e-07\n",
      "Epoch: 1573000, elapsed: 1.06e+01, train loss: 7.00627e-07, val loss: 1.29043e-06, min loss: 6.91117e-07\n",
      "Epoch: 1573100, elapsed: 1.06e+01, train loss: 7.36880e-07, val loss: 1.37127e-06, min loss: 6.91117e-07\n",
      "Epoch: 1573200, elapsed: 1.06e+01, train loss: 7.22355e-07, val loss: 1.29926e-06, min loss: 6.91117e-07\n",
      "Epoch: 1573300, elapsed: 1.05e+01, train loss: 6.91040e-07, val loss: 1.29732e-06, min loss: 6.91040e-07\n",
      "Epoch: 1573400, elapsed: 1.05e+01, train loss: 7.17630e-07, val loss: 1.34385e-06, min loss: 6.91040e-07\n",
      "Epoch: 1573500, elapsed: 1.04e+01, train loss: 7.30937e-07, val loss: 1.40537e-06, min loss: 6.91040e-07\n",
      "Epoch: 1573600, elapsed: 1.06e+01, train loss: 6.90658e-07, val loss: 1.28662e-06, min loss: 6.90658e-07\n",
      "Epoch: 1573700, elapsed: 1.04e+01, train loss: 6.96184e-07, val loss: 1.31432e-06, min loss: 6.90658e-07\n",
      "Epoch: 1573800, elapsed: 1.04e+01, train loss: 6.98139e-07, val loss: 1.32018e-06, min loss: 6.90658e-07\n",
      "Epoch: 1573900, elapsed: 1.05e+01, train loss: 6.91523e-07, val loss: 1.29748e-06, min loss: 6.90658e-07\n",
      "Epoch: 1574000, elapsed: 1.04e+01, train loss: 7.01060e-07, val loss: 1.28926e-06, min loss: 6.90658e-07\n",
      "Epoch: 1574100, elapsed: 1.06e+01, train loss: 7.37241e-07, val loss: 1.37851e-06, min loss: 6.90658e-07\n",
      "Epoch: 1574200, elapsed: 1.08e+01, train loss: 8.34827e-07, val loss: 1.43647e-06, min loss: 6.90658e-07\n",
      "Epoch: 1574300, elapsed: 1.04e+01, train loss: 1.11180e-06, val loss: 1.66898e-06, min loss: 6.90658e-07\n",
      "Epoch: 1574400, elapsed: 1.05e+01, train loss: 7.17929e-07, val loss: 1.32802e-06, min loss: 6.90658e-07\n",
      "Epoch: 1574500, elapsed: 1.04e+01, train loss: 2.61827e-06, val loss: 1.89261e-06, min loss: 6.90658e-07\n",
      "Epoch: 1574600, elapsed: 1.05e+01, train loss: 6.89521e-07, val loss: 1.29199e-06, min loss: 6.89521e-07\n",
      "Epoch: 1574700, elapsed: 1.05e+01, train loss: 6.98511e-07, val loss: 1.31250e-06, min loss: 6.89521e-07\n",
      "Epoch: 1574800, elapsed: 1.06e+01, train loss: 7.52234e-07, val loss: 1.37945e-06, min loss: 6.89521e-07\n",
      "Epoch: 1574900, elapsed: 1.06e+01, train loss: 9.14958e-07, val loss: 1.65728e-06, min loss: 6.89521e-07\n",
      "Epoch: 1575000, elapsed: 1.06e+01, train loss: 8.77522e-07, val loss: 1.51903e-06, min loss: 6.89521e-07\n",
      "Epoch: 1575100, elapsed: 1.23e+01, train loss: 8.70851e-07, val loss: 1.43319e-06, min loss: 6.89521e-07\n",
      "Epoch: 1575200, elapsed: 1.46e+01, train loss: 8.73337e-07, val loss: 1.52789e-06, min loss: 6.89521e-07\n",
      "Epoch: 1575300, elapsed: 1.06e+01, train loss: 7.18689e-07, val loss: 1.31220e-06, min loss: 6.89521e-07\n",
      "Epoch: 1575400, elapsed: 1.07e+01, train loss: 6.89017e-07, val loss: 1.29363e-06, min loss: 6.89017e-07\n",
      "Epoch: 1575500, elapsed: 1.06e+01, train loss: 6.94035e-07, val loss: 1.29825e-06, min loss: 6.89017e-07\n",
      "Epoch: 1575600, elapsed: 1.06e+01, train loss: 2.41838e-06, val loss: 2.85758e-06, min loss: 6.89017e-07\n",
      "Epoch: 1575700, elapsed: 1.06e+01, train loss: 1.23907e-06, val loss: 2.00879e-06, min loss: 6.89017e-07\n",
      "Epoch: 1575800, elapsed: 1.07e+01, train loss: 7.13276e-07, val loss: 1.28905e-06, min loss: 6.89017e-07\n",
      "Epoch: 1575900, elapsed: 1.06e+01, train loss: 7.63152e-07, val loss: 1.39045e-06, min loss: 6.89017e-07\n",
      "Epoch: 1576000, elapsed: 1.07e+01, train loss: 7.00777e-07, val loss: 1.28402e-06, min loss: 6.89017e-07\n",
      "Epoch: 1576100, elapsed: 1.04e+01, train loss: 6.93592e-07, val loss: 1.31909e-06, min loss: 6.89017e-07\n",
      "Epoch: 1576200, elapsed: 1.08e+01, train loss: 1.18830e-06, val loss: 1.42363e-06, min loss: 6.89017e-07\n",
      "Epoch: 1576300, elapsed: 1.09e+01, train loss: 7.24295e-07, val loss: 1.27970e-06, min loss: 6.89017e-07\n",
      "Epoch: 1576400, elapsed: 1.05e+01, train loss: 6.88393e-07, val loss: 1.28824e-06, min loss: 6.88393e-07\n",
      "Epoch: 1576500, elapsed: 1.06e+01, train loss: 7.21803e-07, val loss: 1.31172e-06, min loss: 6.88393e-07\n",
      "Epoch: 1576600, elapsed: 1.06e+01, train loss: 1.21990e-06, val loss: 2.05649e-06, min loss: 6.88393e-07\n",
      "Epoch: 1576700, elapsed: 1.06e+01, train loss: 6.87455e-07, val loss: 1.28857e-06, min loss: 6.87455e-07\n",
      "Epoch: 1576800, elapsed: 1.04e+01, train loss: 6.99199e-07, val loss: 1.30342e-06, min loss: 6.87455e-07\n",
      "Epoch: 1576900, elapsed: 1.05e+01, train loss: 1.61661e-06, val loss: 2.35854e-06, min loss: 6.87455e-07\n",
      "Epoch: 1577000, elapsed: 1.04e+01, train loss: 1.29642e-06, val loss: 2.18583e-06, min loss: 6.87455e-07\n",
      "Epoch: 1577100, elapsed: 1.04e+01, train loss: 7.17707e-07, val loss: 1.30100e-06, min loss: 6.87455e-07\n",
      "Epoch: 1577200, elapsed: 1.05e+01, train loss: 1.70706e-06, val loss: 2.33407e-06, min loss: 6.87455e-07\n",
      "Epoch: 1577300, elapsed: 1.09e+01, train loss: 2.38302e-06, val loss: 3.38219e-06, min loss: 6.87455e-07\n",
      "Epoch: 1577400, elapsed: 1.06e+01, train loss: 7.86368e-07, val loss: 1.35681e-06, min loss: 6.87455e-07\n",
      "Epoch: 1577500, elapsed: 1.05e+01, train loss: 6.98225e-07, val loss: 1.27792e-06, min loss: 6.87455e-07\n",
      "Epoch: 1577600, elapsed: 1.06e+01, train loss: 1.61349e-06, val loss: 2.38626e-06, min loss: 6.87455e-07\n",
      "Epoch: 1577700, elapsed: 1.04e+01, train loss: 6.86599e-07, val loss: 1.28821e-06, min loss: 6.86599e-07\n",
      "Epoch: 1577800, elapsed: 1.06e+01, train loss: 6.95012e-07, val loss: 1.29044e-06, min loss: 6.86599e-07\n",
      "Epoch: 1577900, elapsed: 1.05e+01, train loss: 1.60408e-06, val loss: 2.65738e-06, min loss: 6.86599e-07\n",
      "Epoch: 1578000, elapsed: 1.06e+01, train loss: 6.87009e-07, val loss: 1.28745e-06, min loss: 6.86599e-07\n",
      "Epoch: 1578100, elapsed: 1.48e+01, train loss: 6.93452e-07, val loss: 1.27187e-06, min loss: 6.86599e-07\n",
      "Epoch: 1578200, elapsed: 1.08e+01, train loss: 1.28073e-06, val loss: 1.70480e-06, min loss: 6.86599e-07\n",
      "Epoch: 1578300, elapsed: 1.06e+01, train loss: 6.86804e-07, val loss: 1.29298e-06, min loss: 6.86599e-07\n",
      "Epoch: 1578400, elapsed: 1.06e+01, train loss: 6.87342e-07, val loss: 1.29604e-06, min loss: 6.86599e-07\n",
      "Epoch: 1578500, elapsed: 1.07e+01, train loss: 7.11837e-07, val loss: 1.34825e-06, min loss: 6.86599e-07\n",
      "Epoch: 1578600, elapsed: 1.06e+01, train loss: 7.07878e-07, val loss: 1.31441e-06, min loss: 6.86599e-07\n",
      "Epoch: 1578700, elapsed: 1.07e+01, train loss: 6.96215e-07, val loss: 1.30033e-06, min loss: 6.86599e-07\n",
      "Epoch: 1578800, elapsed: 1.08e+01, train loss: 6.98076e-07, val loss: 1.32626e-06, min loss: 6.86599e-07\n",
      "Epoch: 1578900, elapsed: 1.07e+01, train loss: 8.96189e-07, val loss: 1.63671e-06, min loss: 6.86599e-07\n",
      "Epoch: 1579000, elapsed: 1.04e+01, train loss: 1.84105e-06, val loss: 2.61458e-06, min loss: 6.86599e-07\n",
      "Epoch: 1579100, elapsed: 1.06e+01, train loss: 6.86814e-07, val loss: 1.29085e-06, min loss: 6.86599e-07\n",
      "Epoch: 1579200, elapsed: 1.05e+01, train loss: 6.91918e-07, val loss: 1.29718e-06, min loss: 6.86599e-07\n",
      "Epoch: 1579300, elapsed: 1.06e+01, train loss: 2.93615e-06, val loss: 3.45924e-06, min loss: 6.86599e-07\n",
      "Epoch: 1579400, elapsed: 1.05e+01, train loss: 8.86610e-07, val loss: 1.40280e-06, min loss: 6.86599e-07\n",
      "Epoch: 1579500, elapsed: 1.04e+01, train loss: 6.85733e-07, val loss: 1.28426e-06, min loss: 6.85733e-07\n",
      "Epoch: 1579600, elapsed: 1.05e+01, train loss: 9.50143e-07, val loss: 1.58639e-06, min loss: 6.85733e-07\n",
      "Epoch: 1579700, elapsed: 1.04e+01, train loss: 1.16872e-06, val loss: 1.52473e-06, min loss: 6.85733e-07\n",
      "Epoch: 1579800, elapsed: 1.05e+01, train loss: 7.06237e-07, val loss: 1.31888e-06, min loss: 6.85733e-07\n",
      "Epoch: 1579900, elapsed: 1.05e+01, train loss: 7.28904e-07, val loss: 1.31270e-06, min loss: 6.85733e-07\n",
      "Epoch: 1580000, elapsed: 1.06e+01, train loss: 6.84608e-07, val loss: 1.28415e-06, min loss: 6.84608e-07\n",
      "Epoch: 1580100, elapsed: 1.24e+01, train loss: 7.00349e-07, val loss: 1.29064e-06, min loss: 6.84608e-07\n",
      "Epoch: 1580200, elapsed: 1.05e+01, train loss: 6.97642e-07, val loss: 1.26809e-06, min loss: 6.84608e-07\n",
      "Epoch: 1580300, elapsed: 1.06e+01, train loss: 8.47325e-07, val loss: 1.53922e-06, min loss: 6.84608e-07\n",
      "Epoch: 1580400, elapsed: 1.05e+01, train loss: 7.84117e-07, val loss: 1.29328e-06, min loss: 6.84608e-07\n",
      "Epoch: 1580500, elapsed: 1.04e+01, train loss: 6.86123e-07, val loss: 1.29681e-06, min loss: 6.84608e-07\n",
      "Epoch: 1580600, elapsed: 1.05e+01, train loss: 7.40777e-07, val loss: 1.42695e-06, min loss: 6.84608e-07\n",
      "Epoch: 1580700, elapsed: 1.04e+01, train loss: 7.90627e-07, val loss: 1.38848e-06, min loss: 6.84608e-07\n",
      "Epoch: 1580800, elapsed: 1.03e+01, train loss: 1.57602e-06, val loss: 1.93876e-06, min loss: 6.84608e-07\n",
      "Epoch: 1580900, elapsed: 1.46e+01, train loss: 7.82075e-07, val loss: 1.34797e-06, min loss: 6.84608e-07\n",
      "Epoch: 1581000, elapsed: 1.07e+01, train loss: 6.84103e-07, val loss: 1.28954e-06, min loss: 6.84103e-07\n",
      "Epoch: 1581100, elapsed: 1.06e+01, train loss: 6.88973e-07, val loss: 1.30528e-06, min loss: 6.84103e-07\n",
      "Epoch: 1581200, elapsed: 1.08e+01, train loss: 8.10538e-07, val loss: 1.56271e-06, min loss: 6.84103e-07\n",
      "Epoch: 1581300, elapsed: 1.05e+01, train loss: 6.96792e-07, val loss: 1.27713e-06, min loss: 6.84103e-07\n",
      "Epoch: 1581400, elapsed: 1.06e+01, train loss: 6.97075e-07, val loss: 1.32281e-06, min loss: 6.84103e-07\n",
      "Epoch: 1581500, elapsed: 1.06e+01, train loss: 6.83481e-07, val loss: 1.28626e-06, min loss: 6.83481e-07\n",
      "Epoch: 1581600, elapsed: 1.08e+01, train loss: 7.08074e-07, val loss: 1.27590e-06, min loss: 6.83481e-07\n",
      "Epoch: 1581700, elapsed: 1.06e+01, train loss: 7.21869e-07, val loss: 1.27434e-06, min loss: 6.83481e-07\n",
      "Epoch: 1581800, elapsed: 1.08e+01, train loss: 1.20218e-06, val loss: 2.09137e-06, min loss: 6.83481e-07\n",
      "Epoch: 1581900, elapsed: 1.05e+01, train loss: 2.42739e-06, val loss: 3.35603e-06, min loss: 6.83481e-07\n",
      "Epoch: 1582000, elapsed: 1.08e+01, train loss: 1.24761e-06, val loss: 1.76749e-06, min loss: 6.83481e-07\n",
      "Epoch: 1582100, elapsed: 1.04e+01, train loss: 7.31060e-07, val loss: 1.34963e-06, min loss: 6.83481e-07\n",
      "Epoch: 1582200, elapsed: 1.04e+01, train loss: 7.10803e-07, val loss: 1.32768e-06, min loss: 6.83481e-07\n",
      "Epoch: 1582300, elapsed: 1.06e+01, train loss: 6.83151e-07, val loss: 1.29059e-06, min loss: 6.83151e-07\n",
      "Epoch: 1582400, elapsed: 1.05e+01, train loss: 7.13541e-07, val loss: 1.32901e-06, min loss: 6.83151e-07\n",
      "Epoch: 1582500, elapsed: 1.06e+01, train loss: 7.22912e-07, val loss: 1.34278e-06, min loss: 6.83151e-07\n",
      "Epoch: 1582600, elapsed: 1.07e+01, train loss: 8.90090e-07, val loss: 1.62095e-06, min loss: 6.83151e-07\n",
      "Epoch: 1582700, elapsed: 1.05e+01, train loss: 3.31090e-06, val loss: 4.71213e-06, min loss: 6.83151e-07\n",
      "Epoch: 1582800, elapsed: 1.05e+01, train loss: 6.81975e-07, val loss: 1.28515e-06, min loss: 6.81975e-07\n",
      "Epoch: 1582900, elapsed: 1.04e+01, train loss: 6.87578e-07, val loss: 1.28994e-06, min loss: 6.81975e-07\n",
      "Epoch: 1583000, elapsed: 1.02e+01, train loss: 6.81745e-07, val loss: 1.28851e-06, min loss: 6.81745e-07\n",
      "Epoch: 1583100, elapsed: 1.05e+01, train loss: 6.86869e-07, val loss: 1.29830e-06, min loss: 6.81745e-07\n",
      "Epoch: 1583200, elapsed: 1.06e+01, train loss: 8.62796e-07, val loss: 1.39517e-06, min loss: 6.81745e-07\n",
      "Epoch: 1583300, elapsed: 1.07e+01, train loss: 6.85260e-07, val loss: 1.27093e-06, min loss: 6.81745e-07\n",
      "Epoch: 1583400, elapsed: 1.07e+01, train loss: 6.83975e-07, val loss: 1.28614e-06, min loss: 6.81745e-07\n",
      "Epoch: 1583500, elapsed: 1.05e+01, train loss: 7.33765e-07, val loss: 1.40479e-06, min loss: 6.81745e-07\n",
      "Epoch: 1583600, elapsed: 1.05e+01, train loss: 6.94304e-07, val loss: 1.32375e-06, min loss: 6.81745e-07\n",
      "Epoch: 1583700, elapsed: 1.07e+01, train loss: 6.87114e-07, val loss: 1.30007e-06, min loss: 6.81745e-07\n",
      "Epoch: 1583800, elapsed: 1.47e+01, train loss: 6.87837e-07, val loss: 1.27706e-06, min loss: 6.81745e-07\n",
      "Epoch: 1583900, elapsed: 1.10e+01, train loss: 2.65092e-06, val loss: 3.57991e-06, min loss: 6.81745e-07\n",
      "Epoch: 1584000, elapsed: 1.06e+01, train loss: 6.81183e-07, val loss: 1.28199e-06, min loss: 6.81183e-07\n",
      "Epoch: 1584100, elapsed: 1.08e+01, train loss: 7.30898e-07, val loss: 1.36181e-06, min loss: 6.81183e-07\n",
      "Epoch: 1584200, elapsed: 1.06e+01, train loss: 1.36656e-06, val loss: 2.27430e-06, min loss: 6.81183e-07\n",
      "Epoch: 1584300, elapsed: 1.06e+01, train loss: 6.87707e-07, val loss: 1.28226e-06, min loss: 6.81183e-07\n",
      "Epoch: 1584400, elapsed: 1.07e+01, train loss: 6.82676e-07, val loss: 1.29241e-06, min loss: 6.81183e-07\n",
      "Epoch: 1584500, elapsed: 1.05e+01, train loss: 6.91773e-07, val loss: 1.30369e-06, min loss: 6.81183e-07\n",
      "Epoch: 1584600, elapsed: 1.07e+01, train loss: 6.86740e-07, val loss: 1.27549e-06, min loss: 6.81183e-07\n",
      "Epoch: 1584700, elapsed: 1.07e+01, train loss: 1.28563e-06, val loss: 1.62788e-06, min loss: 6.81183e-07\n",
      "Epoch: 1584800, elapsed: 1.05e+01, train loss: 1.35448e-06, val loss: 1.81051e-06, min loss: 6.81183e-07\n",
      "Epoch: 1584900, elapsed: 1.06e+01, train loss: 9.15298e-07, val loss: 1.57957e-06, min loss: 6.81183e-07\n",
      "Epoch: 1585000, elapsed: 1.05e+01, train loss: 6.87401e-07, val loss: 1.27654e-06, min loss: 6.81183e-07\n",
      "Epoch: 1585100, elapsed: 1.24e+01, train loss: 6.86671e-07, val loss: 1.29989e-06, min loss: 6.81183e-07\n",
      "Epoch: 1585200, elapsed: 1.02e+01, train loss: 6.82346e-07, val loss: 1.27826e-06, min loss: 6.81183e-07\n",
      "Epoch: 1585300, elapsed: 1.03e+01, train loss: 6.94959e-07, val loss: 1.28373e-06, min loss: 6.81183e-07\n",
      "Epoch: 1585400, elapsed: 1.04e+01, train loss: 8.63466e-07, val loss: 1.50111e-06, min loss: 6.81183e-07\n",
      "Epoch: 1585500, elapsed: 1.06e+01, train loss: 7.72771e-07, val loss: 1.35227e-06, min loss: 6.81183e-07\n",
      "Epoch: 1585600, elapsed: 1.04e+01, train loss: 6.80460e-07, val loss: 1.28120e-06, min loss: 6.80460e-07\n",
      "Epoch: 1585700, elapsed: 1.06e+01, train loss: 8.58859e-07, val loss: 1.42392e-06, min loss: 6.80460e-07\n",
      "Epoch: 1585800, elapsed: 1.03e+01, train loss: 1.01984e-06, val loss: 1.64842e-06, min loss: 6.80460e-07\n",
      "Epoch: 1585900, elapsed: 1.04e+01, train loss: 1.55373e-06, val loss: 1.67404e-06, min loss: 6.80460e-07\n",
      "Epoch: 1586000, elapsed: 1.03e+01, train loss: 1.05737e-06, val loss: 2.34622e-06, min loss: 6.80460e-07\n",
      "Epoch: 1586100, elapsed: 1.06e+01, train loss: 6.78982e-07, val loss: 1.28384e-06, min loss: 6.78982e-07\n",
      "Epoch: 1586200, elapsed: 1.06e+01, train loss: 6.93968e-07, val loss: 1.26705e-06, min loss: 6.78982e-07\n",
      "Epoch: 1586300, elapsed: 1.03e+01, train loss: 8.49777e-07, val loss: 1.40290e-06, min loss: 6.78982e-07\n",
      "Epoch: 1586400, elapsed: 1.02e+01, train loss: 1.01198e-06, val loss: 1.67714e-06, min loss: 6.78982e-07\n",
      "Epoch: 1586500, elapsed: 1.03e+01, train loss: 6.79470e-07, val loss: 1.28288e-06, min loss: 6.78982e-07\n",
      "Epoch: 1586600, elapsed: 1.48e+01, train loss: 7.86015e-07, val loss: 1.40414e-06, min loss: 6.78982e-07\n",
      "Epoch: 1586700, elapsed: 1.08e+01, train loss: 1.08154e-06, val loss: 1.88234e-06, min loss: 6.78982e-07\n",
      "Epoch: 1586800, elapsed: 1.08e+01, train loss: 6.79272e-07, val loss: 1.28628e-06, min loss: 6.78982e-07\n",
      "Epoch: 1586900, elapsed: 1.08e+01, train loss: 6.78876e-07, val loss: 1.28706e-06, min loss: 6.78876e-07\n",
      "Epoch: 1587000, elapsed: 1.07e+01, train loss: 6.84411e-07, val loss: 1.27822e-06, min loss: 6.78876e-07\n",
      "Epoch: 1587100, elapsed: 1.08e+01, train loss: 7.07302e-07, val loss: 1.33519e-06, min loss: 6.78876e-07\n",
      "Epoch: 1587200, elapsed: 1.06e+01, train loss: 6.97495e-07, val loss: 1.31811e-06, min loss: 6.78876e-07\n",
      "Epoch: 1587300, elapsed: 1.06e+01, train loss: 1.43749e-06, val loss: 2.31842e-06, min loss: 6.78876e-07\n",
      "Epoch: 1587400, elapsed: 1.06e+01, train loss: 7.42730e-07, val loss: 1.32341e-06, min loss: 6.78876e-07\n",
      "Epoch: 1587500, elapsed: 1.05e+01, train loss: 6.81637e-07, val loss: 1.30621e-06, min loss: 6.78876e-07\n",
      "Epoch: 1587600, elapsed: 1.06e+01, train loss: 7.79132e-07, val loss: 1.36380e-06, min loss: 6.78876e-07\n",
      "Epoch: 1587700, elapsed: 1.04e+01, train loss: 7.43669e-07, val loss: 1.29766e-06, min loss: 6.78876e-07\n",
      "Epoch: 1587800, elapsed: 1.03e+01, train loss: 7.44953e-07, val loss: 1.31504e-06, min loss: 6.78876e-07\n",
      "Epoch: 1587900, elapsed: 1.05e+01, train loss: 8.11028e-07, val loss: 1.41146e-06, min loss: 6.78876e-07\n",
      "Epoch: 1588000, elapsed: 1.07e+01, train loss: 1.28649e-06, val loss: 1.85335e-06, min loss: 6.78876e-07\n",
      "Epoch: 1588100, elapsed: 1.05e+01, train loss: 1.17853e-06, val loss: 1.54567e-06, min loss: 6.78876e-07\n",
      "Epoch: 1588200, elapsed: 1.05e+01, train loss: 1.23602e-06, val loss: 1.67954e-06, min loss: 6.78876e-07\n",
      "Epoch: 1588300, elapsed: 1.03e+01, train loss: 6.81614e-07, val loss: 1.28171e-06, min loss: 6.78876e-07\n",
      "Epoch: 1588400, elapsed: 1.04e+01, train loss: 6.78323e-07, val loss: 1.28549e-06, min loss: 6.78323e-07\n",
      "Epoch: 1588500, elapsed: 1.06e+01, train loss: 6.84243e-07, val loss: 1.27958e-06, min loss: 6.78323e-07\n",
      "Epoch: 1588600, elapsed: 1.05e+01, train loss: 7.06698e-07, val loss: 1.32768e-06, min loss: 6.78323e-07\n",
      "Epoch: 1588700, elapsed: 1.03e+01, train loss: 1.85073e-06, val loss: 2.80006e-06, min loss: 6.78323e-07\n",
      "Epoch: 1588800, elapsed: 1.05e+01, train loss: 1.31795e-06, val loss: 1.63338e-06, min loss: 6.78323e-07\n",
      "Epoch: 1588900, elapsed: 1.04e+01, train loss: 7.32687e-07, val loss: 1.28225e-06, min loss: 6.78323e-07\n",
      "Epoch: 1589000, elapsed: 1.06e+01, train loss: 6.82929e-07, val loss: 1.28203e-06, min loss: 6.78323e-07\n",
      "Epoch: 1589100, elapsed: 1.03e+01, train loss: 8.66992e-07, val loss: 1.48837e-06, min loss: 6.78323e-07\n",
      "Epoch: 1589200, elapsed: 1.05e+01, train loss: 7.23197e-07, val loss: 1.30007e-06, min loss: 6.78323e-07\n",
      "Epoch: 1589300, elapsed: 1.07e+01, train loss: 7.21156e-07, val loss: 1.28009e-06, min loss: 6.78323e-07\n",
      "Epoch: 1589400, elapsed: 1.04e+01, train loss: 9.96124e-07, val loss: 1.39052e-06, min loss: 6.78323e-07\n",
      "Epoch: 1589500, elapsed: 1.47e+01, train loss: 6.76375e-07, val loss: 1.28629e-06, min loss: 6.76375e-07\n",
      "Epoch: 1589600, elapsed: 1.05e+01, train loss: 6.76204e-07, val loss: 1.28223e-06, min loss: 6.76204e-07\n",
      "Epoch: 1589700, elapsed: 1.06e+01, train loss: 1.27765e-06, val loss: 2.13523e-06, min loss: 6.76204e-07\n",
      "Epoch: 1589800, elapsed: 1.04e+01, train loss: 6.77258e-07, val loss: 1.27616e-06, min loss: 6.76204e-07\n",
      "Epoch: 1589900, elapsed: 1.06e+01, train loss: 6.77869e-07, val loss: 1.28705e-06, min loss: 6.76204e-07\n",
      "Epoch: 1590000, elapsed: 1.06e+01, train loss: 6.88951e-07, val loss: 1.29544e-06, min loss: 6.76204e-07\n",
      "Epoch: 1590100, elapsed: 1.24e+01, train loss: 7.54051e-07, val loss: 1.33269e-06, min loss: 6.76204e-07\n",
      "Epoch: 1590200, elapsed: 1.04e+01, train loss: 6.79250e-07, val loss: 1.27868e-06, min loss: 6.76204e-07\n",
      "Epoch: 1590300, elapsed: 1.04e+01, train loss: 9.61360e-07, val loss: 1.60488e-06, min loss: 6.76204e-07\n",
      "Epoch: 1590400, elapsed: 1.04e+01, train loss: 7.98851e-07, val loss: 1.31434e-06, min loss: 6.76204e-07\n",
      "Epoch: 1590500, elapsed: 1.06e+01, train loss: 9.36659e-07, val loss: 1.73101e-06, min loss: 6.76204e-07\n",
      "Epoch: 1590600, elapsed: 1.05e+01, train loss: 6.76567e-07, val loss: 1.27888e-06, min loss: 6.76204e-07\n",
      "Epoch: 1590700, elapsed: 1.05e+01, train loss: 2.19269e-06, val loss: 2.56607e-06, min loss: 6.76204e-07\n",
      "Epoch: 1590800, elapsed: 1.06e+01, train loss: 6.75167e-07, val loss: 1.28166e-06, min loss: 6.75167e-07\n",
      "Epoch: 1590900, elapsed: 1.05e+01, train loss: 6.75582e-07, val loss: 1.28477e-06, min loss: 6.75167e-07\n",
      "Epoch: 1591000, elapsed: 1.06e+01, train loss: 6.76923e-07, val loss: 1.28391e-06, min loss: 6.75167e-07\n",
      "Epoch: 1591100, elapsed: 1.06e+01, train loss: 8.21226e-07, val loss: 1.34521e-06, min loss: 6.75167e-07\n",
      "Epoch: 1591200, elapsed: 1.04e+01, train loss: 9.79858e-07, val loss: 1.60195e-06, min loss: 6.75167e-07\n",
      "Epoch: 1591300, elapsed: 1.06e+01, train loss: 1.15166e-06, val loss: 1.76231e-06, min loss: 6.75167e-07\n",
      "Epoch: 1591400, elapsed: 1.05e+01, train loss: 3.32557e-06, val loss: 2.56315e-06, min loss: 6.75167e-07\n",
      "Epoch: 1591500, elapsed: 1.06e+01, train loss: 9.23823e-07, val loss: 1.39429e-06, min loss: 6.75167e-07\n",
      "Epoch: 1591600, elapsed: 1.05e+01, train loss: 6.92653e-07, val loss: 1.31509e-06, min loss: 6.75167e-07\n",
      "Epoch: 1591700, elapsed: 1.05e+01, train loss: 1.18079e-06, val loss: 1.85026e-06, min loss: 6.75167e-07\n",
      "Epoch: 1591800, elapsed: 1.04e+01, train loss: 2.53087e-06, val loss: 2.18122e-06, min loss: 6.75167e-07\n",
      "Epoch: 1591900, elapsed: 1.06e+01, train loss: 7.02972e-07, val loss: 1.35266e-06, min loss: 6.75167e-07\n",
      "Epoch: 1592000, elapsed: 1.07e+01, train loss: 6.99365e-07, val loss: 1.28027e-06, min loss: 6.75167e-07\n",
      "Epoch: 1592100, elapsed: 1.04e+01, train loss: 6.78337e-07, val loss: 1.29492e-06, min loss: 6.75167e-07\n",
      "Epoch: 1592200, elapsed: 1.05e+01, train loss: 6.94835e-07, val loss: 1.28684e-06, min loss: 6.75167e-07\n",
      "Epoch: 1592300, elapsed: 1.06e+01, train loss: 7.90365e-07, val loss: 1.33270e-06, min loss: 6.75167e-07\n",
      "Epoch: 1592400, elapsed: 1.47e+01, train loss: 6.78516e-07, val loss: 1.29042e-06, min loss: 6.75167e-07\n",
      "Epoch: 1592500, elapsed: 1.07e+01, train loss: 7.44607e-07, val loss: 1.43833e-06, min loss: 6.75167e-07\n",
      "Epoch: 1592600, elapsed: 1.06e+01, train loss: 1.68943e-06, val loss: 2.52163e-06, min loss: 6.75167e-07\n",
      "Epoch: 1592700, elapsed: 1.06e+01, train loss: 6.73486e-07, val loss: 1.28163e-06, min loss: 6.73486e-07\n",
      "Epoch: 1592800, elapsed: 1.07e+01, train loss: 6.73780e-07, val loss: 1.28067e-06, min loss: 6.73486e-07\n",
      "Epoch: 1592900, elapsed: 1.05e+01, train loss: 7.43110e-07, val loss: 1.37752e-06, min loss: 6.73486e-07\n",
      "Epoch: 1593000, elapsed: 1.06e+01, train loss: 8.37137e-07, val loss: 1.40870e-06, min loss: 6.73486e-07\n",
      "Epoch: 1593100, elapsed: 1.04e+01, train loss: 6.98511e-07, val loss: 1.29769e-06, min loss: 6.73486e-07\n",
      "Epoch: 1593200, elapsed: 1.04e+01, train loss: 7.30119e-07, val loss: 1.40975e-06, min loss: 6.73486e-07\n",
      "Epoch: 1593300, elapsed: 1.07e+01, train loss: 6.75105e-07, val loss: 1.29012e-06, min loss: 6.73486e-07\n",
      "Epoch: 1593400, elapsed: 1.06e+01, train loss: 6.82620e-07, val loss: 1.29100e-06, min loss: 6.73486e-07\n",
      "Epoch: 1593500, elapsed: 1.05e+01, train loss: 8.94424e-07, val loss: 1.43155e-06, min loss: 6.73486e-07\n",
      "Epoch: 1593600, elapsed: 1.06e+01, train loss: 7.34385e-07, val loss: 1.34526e-06, min loss: 6.73486e-07\n",
      "Epoch: 1593700, elapsed: 1.04e+01, train loss: 1.97315e-06, val loss: 2.80533e-06, min loss: 6.73486e-07\n",
      "Epoch: 1593800, elapsed: 1.05e+01, train loss: 7.68713e-07, val loss: 1.40286e-06, min loss: 6.73486e-07\n",
      "Epoch: 1593900, elapsed: 1.04e+01, train loss: 6.82836e-07, val loss: 1.28081e-06, min loss: 6.73486e-07\n",
      "Epoch: 1594000, elapsed: 1.05e+01, train loss: 6.84013e-07, val loss: 1.26872e-06, min loss: 6.73486e-07\n",
      "Epoch: 1594100, elapsed: 1.07e+01, train loss: 9.43895e-07, val loss: 1.58780e-06, min loss: 6.73486e-07\n",
      "Epoch: 1594200, elapsed: 1.04e+01, train loss: 6.47797e-06, val loss: 5.91076e-06, min loss: 6.73486e-07\n",
      "Epoch: 1594300, elapsed: 1.06e+01, train loss: 8.37339e-07, val loss: 1.36986e-06, min loss: 6.73486e-07\n",
      "Epoch: 1594400, elapsed: 1.03e+01, train loss: 1.55911e-06, val loss: 2.43804e-06, min loss: 6.73486e-07\n",
      "Epoch: 1594500, elapsed: 1.02e+01, train loss: 8.41920e-07, val loss: 1.47155e-06, min loss: 6.73486e-07\n",
      "Epoch: 1594600, elapsed: 1.07e+01, train loss: 6.84037e-07, val loss: 1.28518e-06, min loss: 6.73486e-07\n",
      "Epoch: 1594700, elapsed: 1.05e+01, train loss: 8.09458e-07, val loss: 1.31653e-06, min loss: 6.73486e-07\n",
      "Epoch: 1594800, elapsed: 1.04e+01, train loss: 8.58832e-07, val loss: 1.34504e-06, min loss: 6.73486e-07\n",
      "Epoch: 1594900, elapsed: 1.05e+01, train loss: 7.28069e-07, val loss: 1.39727e-06, min loss: 6.73486e-07\n",
      "Epoch: 1595000, elapsed: 1.06e+01, train loss: 8.03918e-07, val loss: 1.41524e-06, min loss: 6.73486e-07\n",
      "Epoch: 1595100, elapsed: 1.21e+01, train loss: 6.87436e-07, val loss: 1.32269e-06, min loss: 6.73486e-07\n",
      "Epoch: 1595200, elapsed: 1.47e+01, train loss: 1.09197e-06, val loss: 1.70862e-06, min loss: 6.73486e-07\n",
      "Epoch: 1595300, elapsed: 1.10e+01, train loss: 9.14503e-07, val loss: 1.54331e-06, min loss: 6.73486e-07\n",
      "Epoch: 1595400, elapsed: 1.07e+01, train loss: 6.72855e-07, val loss: 1.27565e-06, min loss: 6.72855e-07\n",
      "Epoch: 1595500, elapsed: 1.07e+01, train loss: 7.46196e-07, val loss: 1.40453e-06, min loss: 6.72855e-07\n",
      "Epoch: 1595600, elapsed: 1.04e+01, train loss: 6.81117e-07, val loss: 1.26999e-06, min loss: 6.72855e-07\n",
      "Epoch: 1595700, elapsed: 1.07e+01, train loss: 9.18998e-07, val loss: 1.46681e-06, min loss: 6.72855e-07\n",
      "Epoch: 1595800, elapsed: 1.07e+01, train loss: 6.79262e-07, val loss: 1.29893e-06, min loss: 6.72855e-07\n",
      "Epoch: 1595900, elapsed: 1.05e+01, train loss: 8.36599e-07, val loss: 1.46829e-06, min loss: 6.72855e-07\n",
      "Epoch: 1596000, elapsed: 1.06e+01, train loss: 6.71133e-07, val loss: 1.28354e-06, min loss: 6.71133e-07\n",
      "Epoch: 1596100, elapsed: 1.04e+01, train loss: 6.75008e-07, val loss: 1.30044e-06, min loss: 6.71133e-07\n",
      "Epoch: 1596200, elapsed: 1.06e+01, train loss: 6.77950e-07, val loss: 1.30874e-06, min loss: 6.71133e-07\n",
      "Epoch: 1596300, elapsed: 1.05e+01, train loss: 2.18478e-06, val loss: 1.99353e-06, min loss: 6.71133e-07\n",
      "Epoch: 1596400, elapsed: 1.09e+01, train loss: 6.74138e-07, val loss: 1.26296e-06, min loss: 6.71133e-07\n",
      "Epoch: 1596500, elapsed: 1.07e+01, train loss: 8.66154e-07, val loss: 1.28556e-06, min loss: 6.71133e-07\n",
      "Epoch: 1596600, elapsed: 1.06e+01, train loss: 2.68007e-06, val loss: 2.80703e-06, min loss: 6.71133e-07\n",
      "Epoch: 1596700, elapsed: 1.04e+01, train loss: 1.24650e-06, val loss: 1.98239e-06, min loss: 6.71133e-07\n",
      "Epoch: 1596800, elapsed: 1.05e+01, train loss: 6.87640e-07, val loss: 1.34095e-06, min loss: 6.71133e-07\n",
      "Epoch: 1596900, elapsed: 1.07e+01, train loss: 9.11505e-07, val loss: 1.49591e-06, min loss: 6.71133e-07\n",
      "Epoch: 1597000, elapsed: 1.04e+01, train loss: 2.14511e-06, val loss: 2.83759e-06, min loss: 6.71133e-07\n",
      "Epoch: 1597100, elapsed: 1.03e+01, train loss: 7.23258e-07, val loss: 1.32959e-06, min loss: 6.71133e-07\n",
      "Epoch: 1597200, elapsed: 1.02e+01, train loss: 7.16809e-07, val loss: 1.35596e-06, min loss: 6.71133e-07\n",
      "Epoch: 1597300, elapsed: 1.05e+01, train loss: 7.44667e-07, val loss: 1.29652e-06, min loss: 6.71133e-07\n",
      "Epoch: 1597400, elapsed: 1.04e+01, train loss: 2.14021e-06, val loss: 3.05078e-06, min loss: 6.71133e-07\n",
      "Epoch: 1597500, elapsed: 1.05e+01, train loss: 1.19726e-06, val loss: 1.76553e-06, min loss: 6.71133e-07\n",
      "Epoch: 1597600, elapsed: 1.04e+01, train loss: 1.91055e-06, val loss: 2.47788e-06, min loss: 6.71133e-07\n",
      "Epoch: 1597700, elapsed: 1.03e+01, train loss: 8.93590e-07, val loss: 1.66969e-06, min loss: 6.71133e-07\n",
      "Epoch: 1597800, elapsed: 1.05e+01, train loss: 8.45167e-07, val loss: 1.43353e-06, min loss: 6.71133e-07\n",
      "Epoch: 1597900, elapsed: 1.05e+01, train loss: 2.25441e-06, val loss: 2.56213e-06, min loss: 6.71133e-07\n",
      "Epoch: 1598000, elapsed: 1.04e+01, train loss: 4.49251e-06, val loss: 4.96546e-06, min loss: 6.71133e-07\n",
      "Epoch: 1598100, elapsed: 1.45e+01, train loss: 6.74323e-07, val loss: 1.29232e-06, min loss: 6.71133e-07\n",
      "Epoch: 1598200, elapsed: 1.07e+01, train loss: 6.75859e-07, val loss: 1.27263e-06, min loss: 6.71133e-07\n",
      "Epoch: 1598300, elapsed: 1.06e+01, train loss: 7.89437e-07, val loss: 1.37425e-06, min loss: 6.71133e-07\n",
      "Epoch: 1598400, elapsed: 1.06e+01, train loss: 7.20209e-07, val loss: 1.29812e-06, min loss: 6.71133e-07\n",
      "Epoch: 1598500, elapsed: 1.06e+01, train loss: 1.96881e-06, val loss: 2.71423e-06, min loss: 6.71133e-07\n",
      "Epoch: 1598600, elapsed: 1.05e+01, train loss: 6.76455e-07, val loss: 1.29742e-06, min loss: 6.71133e-07\n",
      "Epoch: 1598700, elapsed: 1.04e+01, train loss: 6.69711e-07, val loss: 1.27315e-06, min loss: 6.69711e-07\n",
      "Epoch: 1598800, elapsed: 1.06e+01, train loss: 6.71662e-07, val loss: 1.27351e-06, min loss: 6.69711e-07\n",
      "Epoch: 1598900, elapsed: 1.04e+01, train loss: 9.77076e-07, val loss: 1.41141e-06, min loss: 6.69711e-07\n",
      "Epoch: 1599000, elapsed: 1.04e+01, train loss: 1.17900e-06, val loss: 1.93848e-06, min loss: 6.69711e-07\n",
      "Epoch: 1599100, elapsed: 1.06e+01, train loss: 7.39529e-07, val loss: 1.31153e-06, min loss: 6.69711e-07\n",
      "Epoch: 1599200, elapsed: 1.06e+01, train loss: 1.66944e-06, val loss: 2.32797e-06, min loss: 6.69711e-07\n",
      "Epoch: 1599300, elapsed: 1.05e+01, train loss: 7.57188e-07, val loss: 1.44925e-06, min loss: 6.69711e-07\n",
      "Epoch: 1599400, elapsed: 1.06e+01, train loss: 1.74238e-06, val loss: 2.41978e-06, min loss: 6.69711e-07\n",
      "Epoch: 1599500, elapsed: 1.05e+01, train loss: 8.97474e-07, val loss: 1.53494e-06, min loss: 6.69711e-07\n",
      "Epoch: 1599600, elapsed: 1.06e+01, train loss: 2.39674e-06, val loss: 2.53540e-06, min loss: 6.69711e-07\n",
      "Epoch: 1599700, elapsed: 1.05e+01, train loss: 1.01903e-06, val loss: 1.78648e-06, min loss: 6.69711e-07\n",
      "Epoch: 1599800, elapsed: 1.03e+01, train loss: 7.88004e-07, val loss: 1.43949e-06, min loss: 6.69711e-07\n",
      "Epoch: 1599900, elapsed: 1.04e+01, train loss: 7.92285e-07, val loss: 1.42353e-06, min loss: 6.69711e-07\n",
      "Epoch: 1600000, elapsed: 1.05e+01, train loss: 8.47182e-07, val loss: 1.43344e-06, min loss: 6.69711e-07\n",
      "Epoch: 1600100, elapsed: 1.21e+01, train loss: 6.85657e-07, val loss: 1.32119e-06, min loss: 6.69711e-07\n",
      "Epoch: 1600200, elapsed: 1.03e+01, train loss: 6.68067e-07, val loss: 1.27289e-06, min loss: 6.68067e-07\n",
      "Epoch: 1600300, elapsed: 1.05e+01, train loss: 6.81161e-07, val loss: 1.28382e-06, min loss: 6.68067e-07\n",
      "Epoch: 1600400, elapsed: 1.03e+01, train loss: 6.75862e-07, val loss: 1.30013e-06, min loss: 6.68067e-07\n",
      "Epoch: 1600500, elapsed: 1.03e+01, train loss: 8.35928e-07, val loss: 1.37991e-06, min loss: 6.68067e-07\n",
      "Epoch: 1600600, elapsed: 1.06e+01, train loss: 1.19781e-06, val loss: 2.20425e-06, min loss: 6.68067e-07\n",
      "Epoch: 1600700, elapsed: 1.05e+01, train loss: 6.66556e-07, val loss: 1.27982e-06, min loss: 6.66556e-07\n",
      "Epoch: 1600800, elapsed: 1.06e+01, train loss: 6.83989e-07, val loss: 1.28587e-06, min loss: 6.66556e-07\n",
      "Epoch: 1600900, elapsed: 1.05e+01, train loss: 2.10368e-06, val loss: 3.17269e-06, min loss: 6.66556e-07\n",
      "Epoch: 1601000, elapsed: 1.45e+01, train loss: 6.66435e-07, val loss: 1.27738e-06, min loss: 6.66435e-07\n",
      "Epoch: 1601100, elapsed: 1.08e+01, train loss: 6.86378e-07, val loss: 1.27698e-06, min loss: 6.66435e-07\n",
      "Epoch: 1601200, elapsed: 1.05e+01, train loss: 6.70524e-07, val loss: 1.28119e-06, min loss: 6.66435e-07\n",
      "Epoch: 1601300, elapsed: 1.06e+01, train loss: 9.23322e-07, val loss: 1.46438e-06, min loss: 6.66435e-07\n",
      "Epoch: 1601400, elapsed: 1.06e+01, train loss: 6.66532e-07, val loss: 1.28333e-06, min loss: 6.66435e-07\n",
      "Epoch: 1601500, elapsed: 1.05e+01, train loss: 6.71775e-07, val loss: 1.26986e-06, min loss: 6.66435e-07\n",
      "Epoch: 1601600, elapsed: 1.05e+01, train loss: 6.89773e-07, val loss: 1.27164e-06, min loss: 6.66435e-07\n",
      "Epoch: 1601700, elapsed: 1.04e+01, train loss: 7.79024e-07, val loss: 1.46860e-06, min loss: 6.66435e-07\n",
      "Epoch: 1601800, elapsed: 1.03e+01, train loss: 1.01980e-06, val loss: 1.53949e-06, min loss: 6.66435e-07\n",
      "Epoch: 1601900, elapsed: 1.05e+01, train loss: 7.04003e-07, val loss: 1.28360e-06, min loss: 6.66435e-07\n",
      "Epoch: 1602000, elapsed: 1.05e+01, train loss: 6.80379e-07, val loss: 1.32134e-06, min loss: 6.66435e-07\n",
      "Epoch: 1602100, elapsed: 1.05e+01, train loss: 1.93163e-06, val loss: 2.52498e-06, min loss: 6.66435e-07\n",
      "Epoch: 1602200, elapsed: 1.02e+01, train loss: 3.39518e-06, val loss: 3.41881e-06, min loss: 6.66435e-07\n",
      "Epoch: 1602300, elapsed: 1.06e+01, train loss: 6.72398e-07, val loss: 1.28216e-06, min loss: 6.66435e-07\n",
      "Epoch: 1602400, elapsed: 1.04e+01, train loss: 6.65427e-07, val loss: 1.28099e-06, min loss: 6.65427e-07\n",
      "Epoch: 1602500, elapsed: 1.06e+01, train loss: 6.68278e-07, val loss: 1.27211e-06, min loss: 6.65427e-07\n",
      "Epoch: 1602600, elapsed: 1.04e+01, train loss: 1.27136e-06, val loss: 2.13447e-06, min loss: 6.65427e-07\n",
      "Epoch: 1602700, elapsed: 1.05e+01, train loss: 6.65223e-07, val loss: 1.27546e-06, min loss: 6.65223e-07\n",
      "Epoch: 1602800, elapsed: 1.04e+01, train loss: 6.76537e-07, val loss: 1.29579e-06, min loss: 6.65223e-07\n",
      "Epoch: 1602900, elapsed: 1.04e+01, train loss: 6.64704e-07, val loss: 1.27438e-06, min loss: 6.64704e-07\n",
      "Epoch: 1603000, elapsed: 1.04e+01, train loss: 6.81807e-07, val loss: 1.26713e-06, min loss: 6.64704e-07\n",
      "Epoch: 1603100, elapsed: 1.03e+01, train loss: 6.86378e-07, val loss: 1.28385e-06, min loss: 6.64704e-07\n",
      "Epoch: 1603200, elapsed: 1.04e+01, train loss: 4.35117e-06, val loss: 3.93804e-06, min loss: 6.64704e-07\n",
      "Epoch: 1603300, elapsed: 1.04e+01, train loss: 6.64370e-07, val loss: 1.27659e-06, min loss: 6.64370e-07\n",
      "Epoch: 1603400, elapsed: 1.04e+01, train loss: 6.68689e-07, val loss: 1.27667e-06, min loss: 6.64370e-07\n",
      "Epoch: 1603500, elapsed: 1.04e+01, train loss: 6.69085e-07, val loss: 1.26034e-06, min loss: 6.64370e-07\n",
      "Epoch: 1603600, elapsed: 1.04e+01, train loss: 7.28568e-07, val loss: 1.33370e-06, min loss: 6.64370e-07\n",
      "Epoch: 1603700, elapsed: 1.04e+01, train loss: 1.19407e-06, val loss: 1.79875e-06, min loss: 6.64370e-07\n",
      "Epoch: 1603800, elapsed: 1.05e+01, train loss: 6.81800e-07, val loss: 1.30541e-06, min loss: 6.64370e-07\n",
      "Epoch: 1603900, elapsed: 1.47e+01, train loss: 6.80348e-07, val loss: 1.27629e-06, min loss: 6.64370e-07\n",
      "Epoch: 1604000, elapsed: 1.06e+01, train loss: 3.39113e-06, val loss: 3.86726e-06, min loss: 6.64370e-07\n",
      "Epoch: 1604100, elapsed: 1.05e+01, train loss: 1.15159e-06, val loss: 2.06135e-06, min loss: 6.64370e-07\n",
      "Epoch: 1604200, elapsed: 1.06e+01, train loss: 1.37676e-06, val loss: 2.24158e-06, min loss: 6.64370e-07\n",
      "Epoch: 1604300, elapsed: 1.05e+01, train loss: 8.00872e-07, val loss: 1.38870e-06, min loss: 6.64370e-07\n",
      "Epoch: 1604400, elapsed: 1.05e+01, train loss: 6.66970e-07, val loss: 1.26374e-06, min loss: 6.64370e-07\n",
      "Epoch: 1604500, elapsed: 1.06e+01, train loss: 1.09288e-06, val loss: 1.43473e-06, min loss: 6.64370e-07\n",
      "Epoch: 1604600, elapsed: 1.06e+01, train loss: 6.74816e-07, val loss: 1.34177e-06, min loss: 6.64370e-07\n",
      "Epoch: 1604700, elapsed: 1.06e+01, train loss: 3.18008e-06, val loss: 3.20416e-06, min loss: 6.64370e-07\n",
      "Epoch: 1604800, elapsed: 1.06e+01, train loss: 6.67230e-07, val loss: 1.27513e-06, min loss: 6.64370e-07\n",
      "Epoch: 1604900, elapsed: 1.06e+01, train loss: 6.64431e-07, val loss: 1.26869e-06, min loss: 6.64370e-07\n",
      "Epoch: 1605000, elapsed: 1.05e+01, train loss: 8.25073e-07, val loss: 1.58014e-06, min loss: 6.64370e-07\n",
      "Epoch: 1605100, elapsed: 1.23e+01, train loss: 1.92693e-06, val loss: 2.67833e-06, min loss: 6.64370e-07\n",
      "Epoch: 1605200, elapsed: 1.04e+01, train loss: 6.63320e-07, val loss: 1.27217e-06, min loss: 6.63320e-07\n",
      "Epoch: 1605300, elapsed: 1.06e+01, train loss: 6.63970e-07, val loss: 1.27340e-06, min loss: 6.63320e-07\n",
      "Epoch: 1605400, elapsed: 1.05e+01, train loss: 7.13815e-07, val loss: 1.30292e-06, min loss: 6.63320e-07\n",
      "Epoch: 1605500, elapsed: 1.04e+01, train loss: 8.02323e-07, val loss: 1.39590e-06, min loss: 6.63320e-07\n",
      "Epoch: 1605600, elapsed: 1.07e+01, train loss: 7.00874e-07, val loss: 1.27835e-06, min loss: 6.63320e-07\n",
      "Epoch: 1605700, elapsed: 1.04e+01, train loss: 6.70871e-07, val loss: 1.28953e-06, min loss: 6.63320e-07\n",
      "Epoch: 1605800, elapsed: 1.04e+01, train loss: 7.69580e-07, val loss: 1.38284e-06, min loss: 6.63320e-07\n",
      "Epoch: 1605900, elapsed: 1.05e+01, train loss: 6.74562e-07, val loss: 1.28992e-06, min loss: 6.63320e-07\n",
      "Epoch: 1606000, elapsed: 1.03e+01, train loss: 7.73026e-07, val loss: 1.46079e-06, min loss: 6.63320e-07\n",
      "Epoch: 1606100, elapsed: 1.03e+01, train loss: 8.93969e-07, val loss: 1.68150e-06, min loss: 6.63320e-07\n",
      "Epoch: 1606200, elapsed: 1.04e+01, train loss: 9.88097e-07, val loss: 1.82805e-06, min loss: 6.63320e-07\n",
      "Epoch: 1606300, elapsed: 1.03e+01, train loss: 3.05958e-06, val loss: 3.39541e-06, min loss: 6.63320e-07\n",
      "Epoch: 1606400, elapsed: 1.04e+01, train loss: 1.62217e-06, val loss: 2.42379e-06, min loss: 6.63320e-07\n",
      "Epoch: 1606500, elapsed: 1.05e+01, train loss: 1.31551e-06, val loss: 1.61326e-06, min loss: 6.63320e-07\n",
      "Epoch: 1606600, elapsed: 1.04e+01, train loss: 2.57045e-06, val loss: 2.62681e-06, min loss: 6.63320e-07\n",
      "Epoch: 1606700, elapsed: 1.05e+01, train loss: 7.97691e-07, val loss: 1.38613e-06, min loss: 6.63320e-07\n",
      "Epoch: 1606800, elapsed: 1.49e+01, train loss: 6.61776e-07, val loss: 1.27476e-06, min loss: 6.61776e-07\n",
      "Epoch: 1606900, elapsed: 1.05e+01, train loss: 8.04154e-07, val loss: 1.32178e-06, min loss: 6.61776e-07\n",
      "Epoch: 1607000, elapsed: 1.06e+01, train loss: 8.47437e-07, val loss: 1.39862e-06, min loss: 6.61776e-07\n",
      "Epoch: 1607100, elapsed: 1.04e+01, train loss: 9.91735e-07, val loss: 1.40941e-06, min loss: 6.61776e-07\n",
      "Epoch: 1607200, elapsed: 1.06e+01, train loss: 6.65651e-07, val loss: 1.27848e-06, min loss: 6.61776e-07\n",
      "Epoch: 1607300, elapsed: 1.05e+01, train loss: 6.62487e-07, val loss: 1.27036e-06, min loss: 6.61776e-07\n",
      "Epoch: 1607400, elapsed: 1.05e+01, train loss: 9.49198e-07, val loss: 1.45201e-06, min loss: 6.61776e-07\n",
      "Epoch: 1607500, elapsed: 1.05e+01, train loss: 6.60831e-07, val loss: 1.27347e-06, min loss: 6.60831e-07\n",
      "Epoch: 1607600, elapsed: 1.06e+01, train loss: 1.95812e-06, val loss: 3.05199e-06, min loss: 6.60831e-07\n",
      "Epoch: 1607700, elapsed: 1.03e+01, train loss: 6.60671e-07, val loss: 1.26994e-06, min loss: 6.60671e-07\n",
      "Epoch: 1607800, elapsed: 1.07e+01, train loss: 8.60126e-07, val loss: 1.39487e-06, min loss: 6.60671e-07\n",
      "Epoch: 1607900, elapsed: 1.05e+01, train loss: 6.60479e-07, val loss: 1.27323e-06, min loss: 6.60479e-07\n",
      "Epoch: 1608000, elapsed: 1.02e+01, train loss: 6.63691e-07, val loss: 1.27173e-06, min loss: 6.60479e-07\n",
      "Epoch: 1608100, elapsed: 1.05e+01, train loss: 2.79526e-06, val loss: 4.13230e-06, min loss: 6.60479e-07\n",
      "Epoch: 1608200, elapsed: 1.06e+01, train loss: 6.60388e-07, val loss: 1.27104e-06, min loss: 6.60388e-07\n",
      "Epoch: 1608300, elapsed: 1.06e+01, train loss: 6.83178e-07, val loss: 1.28582e-06, min loss: 6.60388e-07\n",
      "Epoch: 1608400, elapsed: 1.05e+01, train loss: 6.79948e-07, val loss: 1.27276e-06, min loss: 6.60388e-07\n",
      "Epoch: 1608500, elapsed: 1.03e+01, train loss: 7.52869e-07, val loss: 1.28267e-06, min loss: 6.60388e-07\n",
      "Epoch: 1608600, elapsed: 1.05e+01, train loss: 6.61659e-07, val loss: 1.27166e-06, min loss: 6.60388e-07\n",
      "Epoch: 1608700, elapsed: 1.04e+01, train loss: 7.01582e-07, val loss: 1.34855e-06, min loss: 6.60388e-07\n",
      "Epoch: 1608800, elapsed: 1.02e+01, train loss: 8.15054e-07, val loss: 1.37090e-06, min loss: 6.60388e-07\n",
      "Epoch: 1608900, elapsed: 1.04e+01, train loss: 2.60577e-06, val loss: 3.55039e-06, min loss: 6.60388e-07\n",
      "Epoch: 1609000, elapsed: 1.04e+01, train loss: 6.99461e-07, val loss: 1.36025e-06, min loss: 6.60388e-07\n",
      "Epoch: 1609100, elapsed: 1.02e+01, train loss: 6.63776e-07, val loss: 1.27440e-06, min loss: 6.60388e-07\n",
      "Epoch: 1609200, elapsed: 1.05e+01, train loss: 6.68185e-07, val loss: 1.28944e-06, min loss: 6.60388e-07\n",
      "Epoch: 1609300, elapsed: 1.05e+01, train loss: 6.64334e-07, val loss: 1.27647e-06, min loss: 6.60388e-07\n",
      "Epoch: 1609400, elapsed: 1.05e+01, train loss: 6.65010e-07, val loss: 1.27668e-06, min loss: 6.60388e-07\n",
      "Epoch: 1609500, elapsed: 1.04e+01, train loss: 8.13379e-07, val loss: 1.30706e-06, min loss: 6.60388e-07\n",
      "Epoch: 1609600, elapsed: 1.03e+01, train loss: 6.76098e-07, val loss: 1.30227e-06, min loss: 6.60388e-07\n",
      "Epoch: 1609700, elapsed: 1.46e+01, train loss: 7.93884e-07, val loss: 1.29363e-06, min loss: 6.60388e-07\n",
      "Epoch: 1609800, elapsed: 1.08e+01, train loss: 1.47720e-06, val loss: 2.33254e-06, min loss: 6.60388e-07\n",
      "Epoch: 1609900, elapsed: 1.05e+01, train loss: 8.04488e-07, val loss: 1.49518e-06, min loss: 6.60388e-07\n",
      "Epoch: 1610000, elapsed: 1.06e+01, train loss: 8.79938e-07, val loss: 1.77368e-06, min loss: 6.60388e-07\n",
      "Epoch: 1610100, elapsed: 1.23e+01, train loss: 8.32581e-07, val loss: 1.48818e-06, min loss: 6.60388e-07\n",
      "Epoch: 1610200, elapsed: 1.04e+01, train loss: 8.29530e-07, val loss: 1.45982e-06, min loss: 6.60388e-07\n",
      "Epoch: 1610300, elapsed: 1.05e+01, train loss: 7.58806e-07, val loss: 1.36441e-06, min loss: 6.60388e-07\n",
      "Epoch: 1610400, elapsed: 1.05e+01, train loss: 3.04657e-06, val loss: 3.53363e-06, min loss: 6.60388e-07\n",
      "Epoch: 1610500, elapsed: 1.04e+01, train loss: 8.27521e-07, val loss: 1.42700e-06, min loss: 6.60388e-07\n",
      "Epoch: 1610600, elapsed: 1.06e+01, train loss: 6.62252e-07, val loss: 1.28985e-06, min loss: 6.60388e-07\n",
      "Epoch: 1610700, elapsed: 1.06e+01, train loss: 6.58734e-07, val loss: 1.26574e-06, min loss: 6.58734e-07\n",
      "Epoch: 1610800, elapsed: 1.05e+01, train loss: 6.65460e-07, val loss: 1.27345e-06, min loss: 6.58734e-07\n",
      "Epoch: 1610900, elapsed: 1.05e+01, train loss: 7.76984e-07, val loss: 1.40604e-06, min loss: 6.58734e-07\n",
      "Epoch: 1611000, elapsed: 1.04e+01, train loss: 1.23777e-06, val loss: 1.48045e-06, min loss: 6.58734e-07\n",
      "Epoch: 1611100, elapsed: 1.05e+01, train loss: 7.07322e-07, val loss: 1.27096e-06, min loss: 6.58734e-07\n",
      "Epoch: 1611200, elapsed: 1.04e+01, train loss: 8.01651e-07, val loss: 1.29848e-06, min loss: 6.58734e-07\n",
      "Epoch: 1611300, elapsed: 1.04e+01, train loss: 6.93018e-07, val loss: 1.30108e-06, min loss: 6.58734e-07\n",
      "Epoch: 1611400, elapsed: 1.04e+01, train loss: 1.10217e-06, val loss: 1.43916e-06, min loss: 6.58734e-07\n",
      "Epoch: 1611500, elapsed: 1.04e+01, train loss: 6.57632e-07, val loss: 1.26765e-06, min loss: 6.57632e-07\n",
      "Epoch: 1611600, elapsed: 1.03e+01, train loss: 6.60933e-07, val loss: 1.27916e-06, min loss: 6.57632e-07\n",
      "Epoch: 1611700, elapsed: 1.01e+01, train loss: 1.54039e-06, val loss: 2.55482e-06, min loss: 6.57632e-07\n",
      "Epoch: 1611800, elapsed: 1.04e+01, train loss: 6.81862e-07, val loss: 1.31675e-06, min loss: 6.57632e-07\n",
      "Epoch: 1611900, elapsed: 1.03e+01, train loss: 6.58968e-07, val loss: 1.26818e-06, min loss: 6.57632e-07\n",
      "Epoch: 1612000, elapsed: 1.04e+01, train loss: 6.60422e-07, val loss: 1.27874e-06, min loss: 6.57632e-07\n",
      "Epoch: 1612100, elapsed: 1.04e+01, train loss: 6.59387e-07, val loss: 1.27292e-06, min loss: 6.57632e-07\n",
      "Epoch: 1612200, elapsed: 1.03e+01, train loss: 6.67860e-07, val loss: 1.29471e-06, min loss: 6.57632e-07\n",
      "Epoch: 1612300, elapsed: 1.03e+01, train loss: 6.64417e-07, val loss: 1.26516e-06, min loss: 6.57632e-07\n",
      "Epoch: 1612400, elapsed: 1.04e+01, train loss: 6.78370e-07, val loss: 1.30445e-06, min loss: 6.57632e-07\n",
      "Epoch: 1612500, elapsed: 1.04e+01, train loss: 7.41408e-07, val loss: 1.41071e-06, min loss: 6.57632e-07\n",
      "Epoch: 1612600, elapsed: 1.49e+01, train loss: 6.81578e-07, val loss: 1.29150e-06, min loss: 6.57632e-07\n",
      "Epoch: 1612700, elapsed: 1.07e+01, train loss: 7.83042e-07, val loss: 1.30421e-06, min loss: 6.57632e-07\n",
      "Epoch: 1612800, elapsed: 1.06e+01, train loss: 7.03823e-07, val loss: 1.31656e-06, min loss: 6.57632e-07\n",
      "Epoch: 1612900, elapsed: 1.05e+01, train loss: 6.70610e-07, val loss: 1.28117e-06, min loss: 6.57632e-07\n",
      "Epoch: 1613000, elapsed: 1.07e+01, train loss: 6.59147e-07, val loss: 1.27671e-06, min loss: 6.57632e-07\n",
      "Epoch: 1613100, elapsed: 1.05e+01, train loss: 6.61003e-07, val loss: 1.28650e-06, min loss: 6.57632e-07\n",
      "Epoch: 1613200, elapsed: 1.05e+01, train loss: 7.08867e-07, val loss: 1.32957e-06, min loss: 6.57632e-07\n",
      "Epoch: 1613300, elapsed: 1.05e+01, train loss: 9.80710e-07, val loss: 1.39583e-06, min loss: 6.57632e-07\n",
      "Epoch: 1613400, elapsed: 1.04e+01, train loss: 6.66957e-07, val loss: 1.30107e-06, min loss: 6.57632e-07\n",
      "Epoch: 1613500, elapsed: 1.07e+01, train loss: 6.56383e-07, val loss: 1.27206e-06, min loss: 6.56383e-07\n",
      "Epoch: 1613600, elapsed: 1.04e+01, train loss: 6.66087e-07, val loss: 1.30246e-06, min loss: 6.56383e-07\n",
      "Epoch: 1613700, elapsed: 1.05e+01, train loss: 2.75736e-06, val loss: 2.18532e-06, min loss: 6.56383e-07\n",
      "Epoch: 1613800, elapsed: 1.04e+01, train loss: 6.56235e-07, val loss: 1.26586e-06, min loss: 6.56235e-07\n",
      "Epoch: 1613900, elapsed: 1.05e+01, train loss: 6.59510e-07, val loss: 1.26423e-06, min loss: 6.56235e-07\n",
      "Epoch: 1614000, elapsed: 1.05e+01, train loss: 8.17776e-07, val loss: 1.44614e-06, min loss: 6.56235e-07\n",
      "Epoch: 1614100, elapsed: 1.04e+01, train loss: 6.63873e-07, val loss: 1.27521e-06, min loss: 6.56235e-07\n",
      "Epoch: 1614200, elapsed: 1.04e+01, train loss: 6.60005e-07, val loss: 1.26182e-06, min loss: 6.56235e-07\n",
      "Epoch: 1614300, elapsed: 1.05e+01, train loss: 8.06698e-07, val loss: 1.46294e-06, min loss: 6.56235e-07\n",
      "Epoch: 1614400, elapsed: 1.04e+01, train loss: 7.21519e-07, val loss: 1.30131e-06, min loss: 6.56235e-07\n",
      "Epoch: 1614500, elapsed: 1.03e+01, train loss: 6.86726e-07, val loss: 1.25530e-06, min loss: 6.56235e-07\n",
      "Epoch: 1614600, elapsed: 1.04e+01, train loss: 9.21841e-07, val loss: 1.57298e-06, min loss: 6.56235e-07\n",
      "Epoch: 1614700, elapsed: 1.05e+01, train loss: 1.13323e-06, val loss: 1.70198e-06, min loss: 6.56235e-07\n",
      "Epoch: 1614800, elapsed: 1.05e+01, train loss: 6.64524e-07, val loss: 1.26516e-06, min loss: 6.56235e-07\n",
      "Epoch: 1614900, elapsed: 1.02e+01, train loss: 6.85124e-07, val loss: 1.33028e-06, min loss: 6.56235e-07\n",
      "Epoch: 1615000, elapsed: 1.05e+01, train loss: 8.22374e-07, val loss: 1.49568e-06, min loss: 6.56235e-07\n",
      "Epoch: 1615100, elapsed: 1.23e+01, train loss: 6.69188e-07, val loss: 1.30768e-06, min loss: 6.56235e-07\n",
      "Epoch: 1615200, elapsed: 1.04e+01, train loss: 8.93277e-07, val loss: 1.28008e-06, min loss: 6.56235e-07\n",
      "Epoch: 1615300, elapsed: 1.05e+01, train loss: 1.16775e-06, val loss: 2.23442e-06, min loss: 6.56235e-07\n",
      "Epoch: 1615400, elapsed: 1.48e+01, train loss: 6.54712e-07, val loss: 1.26640e-06, min loss: 6.54712e-07\n",
      "Epoch: 1615500, elapsed: 1.06e+01, train loss: 6.61322e-07, val loss: 1.25671e-06, min loss: 6.54712e-07\n",
      "Epoch: 1615600, elapsed: 1.05e+01, train loss: 4.89668e-06, val loss: 6.39468e-06, min loss: 6.54712e-07\n",
      "Epoch: 1615700, elapsed: 1.04e+01, train loss: 6.57758e-07, val loss: 1.27912e-06, min loss: 6.54712e-07\n",
      "Epoch: 1615800, elapsed: 1.04e+01, train loss: 8.30368e-07, val loss: 1.30955e-06, min loss: 6.54712e-07\n",
      "Epoch: 1615900, elapsed: 1.04e+01, train loss: 1.74823e-06, val loss: 2.79039e-06, min loss: 6.54712e-07\n",
      "Epoch: 1616000, elapsed: 1.06e+01, train loss: 1.05825e-06, val loss: 1.42418e-06, min loss: 6.54712e-07\n",
      "Epoch: 1616100, elapsed: 1.05e+01, train loss: 6.55878e-07, val loss: 1.26052e-06, min loss: 6.54712e-07\n",
      "Epoch: 1616200, elapsed: 1.05e+01, train loss: 8.14817e-07, val loss: 1.33572e-06, min loss: 6.54712e-07\n",
      "Epoch: 1616300, elapsed: 1.06e+01, train loss: 7.11157e-07, val loss: 1.36295e-06, min loss: 6.54712e-07\n",
      "Epoch: 1616400, elapsed: 1.04e+01, train loss: 6.97153e-07, val loss: 1.35777e-06, min loss: 6.54712e-07\n",
      "Epoch: 1616500, elapsed: 1.04e+01, train loss: 6.71277e-07, val loss: 1.29011e-06, min loss: 6.54712e-07\n",
      "Epoch: 1616600, elapsed: 1.04e+01, train loss: 6.63046e-07, val loss: 1.29614e-06, min loss: 6.54712e-07\n",
      "Epoch: 1616700, elapsed: 1.06e+01, train loss: 6.54606e-07, val loss: 1.26614e-06, min loss: 6.54606e-07\n",
      "Epoch: 1616800, elapsed: 1.05e+01, train loss: 6.54230e-07, val loss: 1.26650e-06, min loss: 6.54230e-07\n",
      "Epoch: 1616900, elapsed: 1.04e+01, train loss: 6.57188e-07, val loss: 1.27427e-06, min loss: 6.54230e-07\n",
      "Epoch: 1617000, elapsed: 1.03e+01, train loss: 7.39398e-07, val loss: 1.33722e-06, min loss: 6.54230e-07\n",
      "Epoch: 1617100, elapsed: 1.06e+01, train loss: 1.02636e-06, val loss: 1.66911e-06, min loss: 6.54230e-07\n",
      "Epoch: 1617200, elapsed: 1.04e+01, train loss: 6.62363e-07, val loss: 1.25843e-06, min loss: 6.54230e-07\n",
      "Epoch: 1617300, elapsed: 1.02e+01, train loss: 6.53432e-07, val loss: 1.26888e-06, min loss: 6.53432e-07\n",
      "Epoch: 1617400, elapsed: 1.06e+01, train loss: 6.69342e-07, val loss: 1.31636e-06, min loss: 6.53432e-07\n",
      "Epoch: 1617500, elapsed: 1.04e+01, train loss: 7.45839e-07, val loss: 1.35106e-06, min loss: 6.53432e-07\n",
      "Epoch: 1617600, elapsed: 1.03e+01, train loss: 1.60990e-06, val loss: 2.35469e-06, min loss: 6.53432e-07\n",
      "Epoch: 1617700, elapsed: 1.03e+01, train loss: 6.55440e-07, val loss: 1.27730e-06, min loss: 6.53432e-07\n",
      "Epoch: 1617800, elapsed: 1.02e+01, train loss: 6.53824e-07, val loss: 1.27665e-06, min loss: 6.53432e-07\n",
      "Epoch: 1617900, elapsed: 1.04e+01, train loss: 6.79991e-07, val loss: 1.30209e-06, min loss: 6.53432e-07\n",
      "Epoch: 1618000, elapsed: 1.01e+01, train loss: 6.89534e-07, val loss: 1.30826e-06, min loss: 6.53432e-07\n",
      "Epoch: 1618100, elapsed: 1.05e+01, train loss: 2.12420e-06, val loss: 2.40734e-06, min loss: 6.53432e-07\n",
      "Epoch: 1618200, elapsed: 1.04e+01, train loss: 6.52280e-07, val loss: 1.26450e-06, min loss: 6.52280e-07\n",
      "Epoch: 1618300, elapsed: 1.04e+01, train loss: 7.78103e-07, val loss: 1.40965e-06, min loss: 6.52280e-07\n",
      "Epoch: 1618400, elapsed: 1.49e+01, train loss: 6.52129e-07, val loss: 1.26466e-06, min loss: 6.52129e-07\n",
      "Epoch: 1618500, elapsed: 1.05e+01, train loss: 7.18073e-07, val loss: 1.31513e-06, min loss: 6.52129e-07\n",
      "Epoch: 1618600, elapsed: 1.06e+01, train loss: 6.51977e-07, val loss: 1.26428e-06, min loss: 6.51977e-07\n",
      "Epoch: 1618700, elapsed: 1.05e+01, train loss: 6.57710e-07, val loss: 1.26432e-06, min loss: 6.51977e-07\n",
      "Epoch: 1618800, elapsed: 1.07e+01, train loss: 1.51716e-06, val loss: 1.79422e-06, min loss: 6.51977e-07\n",
      "Epoch: 1618900, elapsed: 1.06e+01, train loss: 7.97255e-07, val loss: 1.49846e-06, min loss: 6.51977e-07\n",
      "Epoch: 1619000, elapsed: 1.06e+01, train loss: 6.60008e-07, val loss: 1.28736e-06, min loss: 6.51977e-07\n",
      "Epoch: 1619100, elapsed: 1.05e+01, train loss: 6.79636e-07, val loss: 1.29037e-06, min loss: 6.51977e-07\n",
      "Epoch: 1619200, elapsed: 1.06e+01, train loss: 6.59547e-07, val loss: 1.26882e-06, min loss: 6.51977e-07\n",
      "Epoch: 1619300, elapsed: 1.04e+01, train loss: 6.57360e-07, val loss: 1.28498e-06, min loss: 6.51977e-07\n",
      "Epoch: 1619400, elapsed: 1.06e+01, train loss: 1.00439e-06, val loss: 1.73961e-06, min loss: 6.51977e-07\n",
      "Epoch: 1619500, elapsed: 1.04e+01, train loss: 2.49651e-06, val loss: 3.10149e-06, min loss: 6.51977e-07\n",
      "Epoch: 1619600, elapsed: 1.04e+01, train loss: 1.62844e-06, val loss: 2.36037e-06, min loss: 6.51977e-07\n",
      "Epoch: 1619700, elapsed: 1.04e+01, train loss: 2.40431e-06, val loss: 2.37052e-06, min loss: 6.51977e-07\n",
      "Epoch: 1619800, elapsed: 1.04e+01, train loss: 6.74997e-07, val loss: 1.32922e-06, min loss: 6.51977e-07\n",
      "Epoch: 1619900, elapsed: 1.06e+01, train loss: 6.52174e-07, val loss: 1.26276e-06, min loss: 6.51977e-07\n",
      "Epoch: 1620000, elapsed: 1.05e+01, train loss: 6.55294e-07, val loss: 1.25496e-06, min loss: 6.51977e-07\n",
      "Epoch: 1620100, elapsed: 1.21e+01, train loss: 6.61568e-07, val loss: 1.25415e-06, min loss: 6.51977e-07\n",
      "Epoch: 1620200, elapsed: 1.04e+01, train loss: 6.66471e-07, val loss: 1.28983e-06, min loss: 6.51977e-07\n",
      "Epoch: 1620300, elapsed: 1.02e+01, train loss: 6.97022e-07, val loss: 1.38467e-06, min loss: 6.51977e-07\n",
      "Epoch: 1620400, elapsed: 1.04e+01, train loss: 6.71059e-07, val loss: 1.27044e-06, min loss: 6.51977e-07\n",
      "Epoch: 1620500, elapsed: 1.03e+01, train loss: 6.61415e-07, val loss: 1.28034e-06, min loss: 6.51977e-07\n",
      "Epoch: 1620600, elapsed: 1.03e+01, train loss: 9.06807e-07, val loss: 1.57392e-06, min loss: 6.51977e-07\n",
      "Epoch: 1620700, elapsed: 1.02e+01, train loss: 1.74798e-06, val loss: 2.28484e-06, min loss: 6.51977e-07\n",
      "Epoch: 1620800, elapsed: 1.05e+01, train loss: 7.24178e-07, val loss: 1.33729e-06, min loss: 6.51977e-07\n",
      "Epoch: 1620900, elapsed: 1.04e+01, train loss: 6.79389e-07, val loss: 1.26864e-06, min loss: 6.51977e-07\n",
      "Epoch: 1621000, elapsed: 1.03e+01, train loss: 1.21529e-06, val loss: 1.81858e-06, min loss: 6.51977e-07\n",
      "Epoch: 1621100, elapsed: 1.03e+01, train loss: 6.95722e-07, val loss: 1.33812e-06, min loss: 6.51977e-07\n",
      "Epoch: 1621200, elapsed: 1.05e+01, train loss: 8.24731e-07, val loss: 1.48657e-06, min loss: 6.51977e-07\n",
      "Epoch: 1621300, elapsed: 1.50e+01, train loss: 7.13387e-07, val loss: 1.37323e-06, min loss: 6.51977e-07\n",
      "Epoch: 1621400, elapsed: 1.05e+01, train loss: 1.83119e-06, val loss: 2.48210e-06, min loss: 6.51977e-07\n",
      "Epoch: 1621500, elapsed: 1.05e+01, train loss: 1.20320e-06, val loss: 1.74959e-06, min loss: 6.51977e-07\n",
      "Epoch: 1621600, elapsed: 1.05e+01, train loss: 7.59303e-07, val loss: 1.24556e-06, min loss: 6.51977e-07\n",
      "Epoch: 1621700, elapsed: 1.04e+01, train loss: 7.13770e-07, val loss: 1.32264e-06, min loss: 6.51977e-07\n",
      "Epoch: 1621800, elapsed: 1.06e+01, train loss: 1.18965e-06, val loss: 1.67915e-06, min loss: 6.51977e-07\n",
      "Epoch: 1621900, elapsed: 1.05e+01, train loss: 6.61076e-07, val loss: 1.24341e-06, min loss: 6.51977e-07\n",
      "Epoch: 1622000, elapsed: 1.06e+01, train loss: 6.50615e-07, val loss: 1.27634e-06, min loss: 6.50615e-07\n",
      "Epoch: 1622100, elapsed: 1.05e+01, train loss: 6.71008e-07, val loss: 1.28542e-06, min loss: 6.50615e-07\n",
      "Epoch: 1622200, elapsed: 1.04e+01, train loss: 1.68651e-06, val loss: 2.11534e-06, min loss: 6.50615e-07\n",
      "Epoch: 1622300, elapsed: 1.06e+01, train loss: 7.30246e-07, val loss: 1.33731e-06, min loss: 6.50615e-07\n",
      "Epoch: 1622400, elapsed: 1.05e+01, train loss: 6.59236e-07, val loss: 1.27455e-06, min loss: 6.50615e-07\n",
      "Epoch: 1622500, elapsed: 1.05e+01, train loss: 1.85256e-06, val loss: 2.18884e-06, min loss: 6.50615e-07\n",
      "Epoch: 1622600, elapsed: 1.05e+01, train loss: 1.18792e-06, val loss: 1.75139e-06, min loss: 6.50615e-07\n",
      "Epoch: 1622700, elapsed: 1.03e+01, train loss: 7.44302e-07, val loss: 1.27189e-06, min loss: 6.50615e-07\n",
      "Epoch: 1622800, elapsed: 1.04e+01, train loss: 6.94039e-07, val loss: 1.34087e-06, min loss: 6.50615e-07\n",
      "Epoch: 1622900, elapsed: 1.05e+01, train loss: 6.50009e-07, val loss: 1.26955e-06, min loss: 6.50009e-07\n",
      "Epoch: 1623000, elapsed: 1.04e+01, train loss: 6.68155e-07, val loss: 1.30523e-06, min loss: 6.50009e-07\n",
      "Epoch: 1623100, elapsed: 1.04e+01, train loss: 9.38031e-07, val loss: 1.70941e-06, min loss: 6.50009e-07\n",
      "Epoch: 1623200, elapsed: 1.03e+01, train loss: 6.48737e-07, val loss: 1.25971e-06, min loss: 6.48737e-07\n",
      "Epoch: 1623300, elapsed: 1.04e+01, train loss: 6.90103e-07, val loss: 1.25589e-06, min loss: 6.48737e-07\n",
      "Epoch: 1623400, elapsed: 1.04e+01, train loss: 7.02460e-07, val loss: 1.29202e-06, min loss: 6.48737e-07\n",
      "Epoch: 1623500, elapsed: 1.05e+01, train loss: 2.06429e-06, val loss: 2.09953e-06, min loss: 6.48737e-07\n",
      "Epoch: 1623600, elapsed: 1.05e+01, train loss: 1.20193e-06, val loss: 1.83165e-06, min loss: 6.48737e-07\n",
      "Epoch: 1623700, elapsed: 1.03e+01, train loss: 8.62279e-07, val loss: 1.47196e-06, min loss: 6.48737e-07\n",
      "Epoch: 1623800, elapsed: 1.05e+01, train loss: 1.00339e-06, val loss: 1.53670e-06, min loss: 6.48737e-07\n",
      "Epoch: 1623900, elapsed: 1.05e+01, train loss: 6.63598e-07, val loss: 1.26473e-06, min loss: 6.48737e-07\n",
      "Epoch: 1624000, elapsed: 1.03e+01, train loss: 6.55967e-07, val loss: 1.28715e-06, min loss: 6.48737e-07\n",
      "Epoch: 1624100, elapsed: 1.03e+01, train loss: 9.57188e-07, val loss: 1.42540e-06, min loss: 6.48737e-07\n",
      "Epoch: 1624200, elapsed: 1.45e+01, train loss: 7.71164e-07, val loss: 1.27126e-06, min loss: 6.48737e-07\n",
      "Epoch: 1624300, elapsed: 1.05e+01, train loss: 1.62961e-06, val loss: 1.95113e-06, min loss: 6.48737e-07\n",
      "Epoch: 1624400, elapsed: 1.05e+01, train loss: 7.16092e-07, val loss: 1.32816e-06, min loss: 6.48737e-07\n",
      "Epoch: 1624500, elapsed: 1.04e+01, train loss: 7.32313e-07, val loss: 1.27940e-06, min loss: 6.48737e-07\n",
      "Epoch: 1624600, elapsed: 1.06e+01, train loss: 7.05355e-07, val loss: 1.32615e-06, min loss: 6.48737e-07\n",
      "Epoch: 1624700, elapsed: 1.04e+01, train loss: 6.73992e-07, val loss: 1.33947e-06, min loss: 6.48737e-07\n",
      "Epoch: 1624800, elapsed: 1.04e+01, train loss: 2.55988e-06, val loss: 2.47592e-06, min loss: 6.48737e-07\n",
      "Epoch: 1624900, elapsed: 1.05e+01, train loss: 6.63681e-07, val loss: 1.25063e-06, min loss: 6.48737e-07\n",
      "Epoch: 1625000, elapsed: 1.03e+01, train loss: 6.50251e-07, val loss: 1.26901e-06, min loss: 6.48737e-07\n",
      "Epoch: 1625100, elapsed: 1.24e+01, train loss: 8.75300e-07, val loss: 1.60842e-06, min loss: 6.48737e-07\n",
      "Epoch: 1625200, elapsed: 1.05e+01, train loss: 7.23382e-07, val loss: 1.32319e-06, min loss: 6.48737e-07\n",
      "Epoch: 1625300, elapsed: 1.04e+01, train loss: 6.66919e-07, val loss: 1.29234e-06, min loss: 6.48737e-07\n",
      "Epoch: 1625400, elapsed: 1.04e+01, train loss: 8.32625e-07, val loss: 1.46208e-06, min loss: 6.48737e-07\n",
      "Epoch: 1625500, elapsed: 1.05e+01, train loss: 6.65906e-07, val loss: 1.30326e-06, min loss: 6.48737e-07\n",
      "Epoch: 1625600, elapsed: 1.03e+01, train loss: 1.18027e-06, val loss: 1.58084e-06, min loss: 6.48737e-07\n",
      "Epoch: 1625700, elapsed: 1.04e+01, train loss: 6.50874e-07, val loss: 1.28190e-06, min loss: 6.48737e-07\n",
      "Epoch: 1625800, elapsed: 1.02e+01, train loss: 6.49571e-07, val loss: 1.25337e-06, min loss: 6.48737e-07\n",
      "Epoch: 1625900, elapsed: 1.03e+01, train loss: 6.66508e-07, val loss: 1.25144e-06, min loss: 6.48737e-07\n",
      "Epoch: 1626000, elapsed: 1.04e+01, train loss: 8.01977e-07, val loss: 1.37003e-06, min loss: 6.48737e-07\n",
      "Epoch: 1626100, elapsed: 1.04e+01, train loss: 9.02710e-07, val loss: 1.54577e-06, min loss: 6.48737e-07\n",
      "Epoch: 1626200, elapsed: 1.04e+01, train loss: 6.51994e-07, val loss: 1.27939e-06, min loss: 6.48737e-07\n",
      "Epoch: 1626300, elapsed: 1.04e+01, train loss: 6.46339e-07, val loss: 1.25638e-06, min loss: 6.46339e-07\n",
      "Epoch: 1626400, elapsed: 1.03e+01, train loss: 6.59067e-07, val loss: 1.26075e-06, min loss: 6.46339e-07\n",
      "Epoch: 1626500, elapsed: 1.03e+01, train loss: 6.83005e-07, val loss: 1.28556e-06, min loss: 6.46339e-07\n",
      "Epoch: 1626600, elapsed: 1.04e+01, train loss: 6.57571e-07, val loss: 1.30561e-06, min loss: 6.46339e-07\n",
      "Epoch: 1626700, elapsed: 1.03e+01, train loss: 6.49410e-07, val loss: 1.25722e-06, min loss: 6.46339e-07\n",
      "Epoch: 1626800, elapsed: 1.04e+01, train loss: 1.36925e-06, val loss: 1.98771e-06, min loss: 6.46339e-07\n",
      "Epoch: 1626900, elapsed: 1.04e+01, train loss: 6.46511e-07, val loss: 1.26022e-06, min loss: 6.46339e-07\n",
      "Epoch: 1627000, elapsed: 1.04e+01, train loss: 6.64759e-07, val loss: 1.25933e-06, min loss: 6.46339e-07\n",
      "Epoch: 1627100, elapsed: 1.45e+01, train loss: 6.45528e-07, val loss: 1.25906e-06, min loss: 6.45528e-07\n",
      "Epoch: 1627200, elapsed: 1.05e+01, train loss: 6.49807e-07, val loss: 1.25148e-06, min loss: 6.45528e-07\n",
      "Epoch: 1627300, elapsed: 1.05e+01, train loss: 7.23431e-07, val loss: 1.47500e-06, min loss: 6.45528e-07\n",
      "Epoch: 1627400, elapsed: 1.04e+01, train loss: 6.45668e-07, val loss: 1.26351e-06, min loss: 6.45528e-07\n",
      "Epoch: 1627500, elapsed: 1.06e+01, train loss: 1.38019e-06, val loss: 1.48473e-06, min loss: 6.45528e-07\n",
      "Epoch: 1627600, elapsed: 1.06e+01, train loss: 6.45240e-07, val loss: 1.25733e-06, min loss: 6.45240e-07\n",
      "Epoch: 1627700, elapsed: 1.04e+01, train loss: 1.09519e-06, val loss: 1.54192e-06, min loss: 6.45240e-07\n",
      "Epoch: 1627800, elapsed: 1.07e+01, train loss: 6.45015e-07, val loss: 1.26054e-06, min loss: 6.45015e-07\n",
      "Epoch: 1627900, elapsed: 1.05e+01, train loss: 7.75124e-07, val loss: 1.45663e-06, min loss: 6.45015e-07\n",
      "Epoch: 1628000, elapsed: 1.05e+01, train loss: 6.67925e-07, val loss: 1.29591e-06, min loss: 6.45015e-07\n",
      "Epoch: 1628100, elapsed: 1.05e+01, train loss: 6.53669e-07, val loss: 1.27112e-06, min loss: 6.45015e-07\n",
      "Epoch: 1628200, elapsed: 1.05e+01, train loss: 6.46628e-07, val loss: 1.26513e-06, min loss: 6.45015e-07\n",
      "Epoch: 1628300, elapsed: 1.04e+01, train loss: 6.46968e-07, val loss: 1.25702e-06, min loss: 6.45015e-07\n",
      "Epoch: 1628400, elapsed: 1.04e+01, train loss: 6.74121e-07, val loss: 1.25738e-06, min loss: 6.45015e-07\n",
      "Epoch: 1628500, elapsed: 1.04e+01, train loss: 6.59342e-07, val loss: 1.26410e-06, min loss: 6.45015e-07\n",
      "Epoch: 1628600, elapsed: 1.03e+01, train loss: 6.52902e-07, val loss: 1.27997e-06, min loss: 6.45015e-07\n",
      "Epoch: 1628700, elapsed: 1.05e+01, train loss: 6.45621e-07, val loss: 1.25881e-06, min loss: 6.45015e-07\n",
      "Epoch: 1628800, elapsed: 1.04e+01, train loss: 1.12107e-06, val loss: 1.32578e-06, min loss: 6.45015e-07\n",
      "Epoch: 1628900, elapsed: 1.03e+01, train loss: 1.09139e-06, val loss: 1.58775e-06, min loss: 6.45015e-07\n",
      "Epoch: 1629000, elapsed: 1.04e+01, train loss: 1.13754e-06, val loss: 1.44015e-06, min loss: 6.45015e-07\n",
      "Epoch: 1629100, elapsed: 1.03e+01, train loss: 2.49638e-06, val loss: 2.85324e-06, min loss: 6.45015e-07\n",
      "Epoch: 1629200, elapsed: 1.05e+01, train loss: 2.53353e-06, val loss: 2.74769e-06, min loss: 6.45015e-07\n",
      "Epoch: 1629300, elapsed: 1.04e+01, train loss: 1.07763e-06, val loss: 2.17345e-06, min loss: 6.45015e-07\n",
      "Epoch: 1629400, elapsed: 1.04e+01, train loss: 6.82998e-07, val loss: 1.32321e-06, min loss: 6.45015e-07\n",
      "Epoch: 1629500, elapsed: 1.04e+01, train loss: 6.46139e-07, val loss: 1.25381e-06, min loss: 6.45015e-07\n",
      "Epoch: 1629600, elapsed: 1.05e+01, train loss: 6.44997e-07, val loss: 1.26063e-06, min loss: 6.44997e-07\n",
      "Epoch: 1629700, elapsed: 1.04e+01, train loss: 7.59504e-07, val loss: 1.27164e-06, min loss: 6.44997e-07\n",
      "Epoch: 1629800, elapsed: 1.04e+01, train loss: 6.43491e-07, val loss: 1.25698e-06, min loss: 6.43491e-07\n",
      "Epoch: 1629900, elapsed: 1.04e+01, train loss: 6.48559e-07, val loss: 1.26652e-06, min loss: 6.43491e-07\n",
      "Epoch: 1630000, elapsed: 1.05e+01, train loss: 7.69081e-07, val loss: 1.42737e-06, min loss: 6.43491e-07\n",
      "Epoch: 1630100, elapsed: 1.68e+01, train loss: 1.53772e-06, val loss: 2.05760e-06, min loss: 6.43491e-07\n",
      "Epoch: 1630200, elapsed: 1.05e+01, train loss: 6.63060e-07, val loss: 1.29044e-06, min loss: 6.43491e-07\n",
      "Epoch: 1630300, elapsed: 1.07e+01, train loss: 7.14275e-07, val loss: 1.39464e-06, min loss: 6.43491e-07\n",
      "Epoch: 1630400, elapsed: 1.05e+01, train loss: 6.95312e-07, val loss: 1.28471e-06, min loss: 6.43491e-07\n",
      "Epoch: 1630500, elapsed: 1.05e+01, train loss: 7.83467e-07, val loss: 1.35573e-06, min loss: 6.43491e-07\n",
      "Epoch: 1630600, elapsed: 1.03e+01, train loss: 6.43641e-07, val loss: 1.25715e-06, min loss: 6.43491e-07\n",
      "Epoch: 1630700, elapsed: 1.07e+01, train loss: 6.75486e-07, val loss: 1.31494e-06, min loss: 6.43491e-07\n",
      "Epoch: 1630800, elapsed: 1.06e+01, train loss: 7.00853e-07, val loss: 1.25743e-06, min loss: 6.43491e-07\n",
      "Epoch: 1630900, elapsed: 1.03e+01, train loss: 6.45154e-07, val loss: 1.24849e-06, min loss: 6.43491e-07\n",
      "Epoch: 1631000, elapsed: 1.04e+01, train loss: 6.54608e-07, val loss: 1.27261e-06, min loss: 6.43491e-07\n",
      "Epoch: 1631100, elapsed: 1.04e+01, train loss: 6.48314e-07, val loss: 1.26312e-06, min loss: 6.43491e-07\n",
      "Epoch: 1631200, elapsed: 1.04e+01, train loss: 7.31292e-07, val loss: 1.32234e-06, min loss: 6.43491e-07\n",
      "Epoch: 1631300, elapsed: 1.04e+01, train loss: 1.88873e-06, val loss: 2.72053e-06, min loss: 6.43491e-07\n",
      "Epoch: 1631400, elapsed: 1.03e+01, train loss: 1.11802e-06, val loss: 1.44969e-06, min loss: 6.43491e-07\n",
      "Epoch: 1631500, elapsed: 1.04e+01, train loss: 8.90181e-07, val loss: 1.49459e-06, min loss: 6.43491e-07\n",
      "Epoch: 1631600, elapsed: 1.03e+01, train loss: 7.49174e-07, val loss: 1.43219e-06, min loss: 6.43491e-07\n",
      "Epoch: 1631700, elapsed: 1.03e+01, train loss: 1.24975e-06, val loss: 2.05396e-06, min loss: 6.43491e-07\n",
      "Epoch: 1631800, elapsed: 1.03e+01, train loss: 2.63042e-06, val loss: 3.11362e-06, min loss: 6.43491e-07\n",
      "Epoch: 1631900, elapsed: 1.04e+01, train loss: 6.81368e-07, val loss: 1.34578e-06, min loss: 6.43491e-07\n",
      "Epoch: 1632000, elapsed: 1.03e+01, train loss: 6.66923e-07, val loss: 1.31291e-06, min loss: 6.43491e-07\n",
      "Epoch: 1632100, elapsed: 1.03e+01, train loss: 6.42724e-07, val loss: 1.25144e-06, min loss: 6.42724e-07\n",
      "Epoch: 1632200, elapsed: 1.04e+01, train loss: 6.49815e-07, val loss: 1.25359e-06, min loss: 6.42724e-07\n",
      "Epoch: 1632300, elapsed: 1.03e+01, train loss: 2.25150e-06, val loss: 2.91677e-06, min loss: 6.42724e-07\n",
      "Epoch: 1632400, elapsed: 1.03e+01, train loss: 6.42255e-07, val loss: 1.25357e-06, min loss: 6.42255e-07\n",
      "Epoch: 1632500, elapsed: 1.03e+01, train loss: 6.45002e-07, val loss: 1.25750e-06, min loss: 6.42255e-07\n",
      "Epoch: 1632600, elapsed: 1.03e+01, train loss: 7.38338e-07, val loss: 1.33461e-06, min loss: 6.42255e-07\n",
      "Epoch: 1632700, elapsed: 1.03e+01, train loss: 9.74276e-07, val loss: 1.54477e-06, min loss: 6.42255e-07\n",
      "Epoch: 1632800, elapsed: 1.04e+01, train loss: 6.64730e-07, val loss: 1.30578e-06, min loss: 6.42255e-07\n",
      "Epoch: 1632900, elapsed: 1.03e+01, train loss: 7.00391e-07, val loss: 1.27454e-06, min loss: 6.42255e-07\n",
      "Epoch: 1633000, elapsed: 1.47e+01, train loss: 7.05025e-07, val loss: 1.32785e-06, min loss: 6.42255e-07\n",
      "Epoch: 1633100, elapsed: 1.07e+01, train loss: 1.56893e-06, val loss: 1.95959e-06, min loss: 6.42255e-07\n",
      "Epoch: 1633200, elapsed: 1.07e+01, train loss: 8.46601e-07, val loss: 1.68332e-06, min loss: 6.42255e-07\n",
      "Epoch: 1633300, elapsed: 1.06e+01, train loss: 1.60573e-06, val loss: 2.89117e-06, min loss: 6.42255e-07\n",
      "Epoch: 1633400, elapsed: 1.05e+01, train loss: 6.41019e-07, val loss: 1.25526e-06, min loss: 6.41019e-07\n",
      "Epoch: 1633500, elapsed: 1.05e+01, train loss: 7.03501e-07, val loss: 1.25666e-06, min loss: 6.41019e-07\n",
      "Epoch: 1633600, elapsed: 1.05e+01, train loss: 6.70782e-07, val loss: 1.27372e-06, min loss: 6.41019e-07\n",
      "Epoch: 1633700, elapsed: 1.06e+01, train loss: 6.48777e-07, val loss: 1.27449e-06, min loss: 6.41019e-07\n",
      "Epoch: 1633800, elapsed: 1.04e+01, train loss: 6.41844e-07, val loss: 1.26442e-06, min loss: 6.41019e-07\n",
      "Epoch: 1633900, elapsed: 1.04e+01, train loss: 7.44902e-07, val loss: 1.29326e-06, min loss: 6.41019e-07\n",
      "Epoch: 1634000, elapsed: 1.05e+01, train loss: 6.92910e-07, val loss: 1.33464e-06, min loss: 6.41019e-07\n",
      "Epoch: 1634100, elapsed: 1.04e+01, train loss: 6.99740e-07, val loss: 1.35736e-06, min loss: 6.41019e-07\n",
      "Epoch: 1634200, elapsed: 1.06e+01, train loss: 9.53413e-07, val loss: 1.72493e-06, min loss: 6.41019e-07\n",
      "Epoch: 1634300, elapsed: 1.05e+01, train loss: 7.85326e-07, val loss: 1.47997e-06, min loss: 6.41019e-07\n",
      "Epoch: 1634400, elapsed: 1.04e+01, train loss: 6.63241e-07, val loss: 1.26716e-06, min loss: 6.41019e-07\n",
      "Epoch: 1634500, elapsed: 1.05e+01, train loss: 6.47410e-07, val loss: 1.27033e-06, min loss: 6.41019e-07\n",
      "Epoch: 1634600, elapsed: 1.03e+01, train loss: 6.43426e-07, val loss: 1.26617e-06, min loss: 6.41019e-07\n",
      "Epoch: 1634700, elapsed: 1.05e+01, train loss: 6.47775e-07, val loss: 1.24946e-06, min loss: 6.41019e-07\n",
      "Epoch: 1634800, elapsed: 1.05e+01, train loss: 6.84581e-07, val loss: 1.30395e-06, min loss: 6.41019e-07\n",
      "Epoch: 1634900, elapsed: 1.04e+01, train loss: 8.04235e-07, val loss: 1.45728e-06, min loss: 6.41019e-07\n",
      "Epoch: 1635000, elapsed: 1.06e+01, train loss: 9.68429e-07, val loss: 1.78154e-06, min loss: 6.41019e-07\n",
      "Epoch: 1635100, elapsed: 1.23e+01, train loss: 6.45057e-07, val loss: 1.24714e-06, min loss: 6.41019e-07\n",
      "Epoch: 1635200, elapsed: 1.03e+01, train loss: 7.00997e-07, val loss: 1.26246e-06, min loss: 6.41019e-07\n",
      "Epoch: 1635300, elapsed: 1.04e+01, train loss: 3.55944e-06, val loss: 4.34687e-06, min loss: 6.41019e-07\n",
      "Epoch: 1635400, elapsed: 1.04e+01, train loss: 6.39797e-07, val loss: 1.25493e-06, min loss: 6.39797e-07\n",
      "Epoch: 1635500, elapsed: 1.03e+01, train loss: 6.52323e-07, val loss: 1.25834e-06, min loss: 6.39797e-07\n",
      "Epoch: 1635600, elapsed: 1.05e+01, train loss: 2.38899e-06, val loss: 2.87108e-06, min loss: 6.39797e-07\n",
      "Epoch: 1635700, elapsed: 1.04e+01, train loss: 6.39695e-07, val loss: 1.25733e-06, min loss: 6.39695e-07\n",
      "Epoch: 1635800, elapsed: 1.06e+01, train loss: 6.47663e-07, val loss: 1.25719e-06, min loss: 6.39695e-07\n",
      "Epoch: 1635900, elapsed: 1.48e+01, train loss: 6.96929e-07, val loss: 1.29787e-06, min loss: 6.39695e-07\n",
      "Epoch: 1636000, elapsed: 1.08e+01, train loss: 2.00722e-06, val loss: 2.93944e-06, min loss: 6.39695e-07\n",
      "Epoch: 1636100, elapsed: 1.05e+01, train loss: 1.26923e-06, val loss: 1.61932e-06, min loss: 6.39695e-07\n",
      "Epoch: 1636200, elapsed: 1.06e+01, train loss: 3.74297e-06, val loss: 4.27031e-06, min loss: 6.39695e-07\n",
      "Epoch: 1636300, elapsed: 1.06e+01, train loss: 6.40331e-07, val loss: 1.24428e-06, min loss: 6.39695e-07\n",
      "Epoch: 1636400, elapsed: 1.05e+01, train loss: 6.41045e-07, val loss: 1.25900e-06, min loss: 6.39695e-07\n",
      "Epoch: 1636500, elapsed: 1.05e+01, train loss: 1.00239e-06, val loss: 1.82210e-06, min loss: 6.39695e-07\n",
      "Epoch: 1636600, elapsed: 1.05e+01, train loss: 6.38730e-07, val loss: 1.25268e-06, min loss: 6.38730e-07\n",
      "Epoch: 1636700, elapsed: 1.04e+01, train loss: 1.37870e-06, val loss: 1.42570e-06, min loss: 6.38730e-07\n",
      "Epoch: 1636800, elapsed: 1.05e+01, train loss: 6.43179e-07, val loss: 1.25529e-06, min loss: 6.38730e-07\n",
      "Epoch: 1636900, elapsed: 1.04e+01, train loss: 1.13993e-06, val loss: 1.91205e-06, min loss: 6.38730e-07\n",
      "Epoch: 1637000, elapsed: 1.06e+01, train loss: 6.38367e-07, val loss: 1.25344e-06, min loss: 6.38367e-07\n",
      "Epoch: 1637100, elapsed: 1.04e+01, train loss: 7.44401e-07, val loss: 1.30326e-06, min loss: 6.38367e-07\n",
      "Epoch: 1637200, elapsed: 1.04e+01, train loss: 6.43575e-07, val loss: 1.24853e-06, min loss: 6.38367e-07\n",
      "Epoch: 1637300, elapsed: 1.03e+01, train loss: 6.38293e-07, val loss: 1.25107e-06, min loss: 6.38293e-07\n",
      "Epoch: 1637400, elapsed: 1.02e+01, train loss: 7.11522e-07, val loss: 1.26490e-06, min loss: 6.38293e-07\n",
      "Epoch: 1637500, elapsed: 1.04e+01, train loss: 2.51238e-06, val loss: 3.07211e-06, min loss: 6.38293e-07\n",
      "Epoch: 1637600, elapsed: 1.04e+01, train loss: 6.43531e-07, val loss: 1.25072e-06, min loss: 6.38293e-07\n",
      "Epoch: 1637700, elapsed: 1.04e+01, train loss: 6.38457e-07, val loss: 1.25058e-06, min loss: 6.38293e-07\n",
      "Epoch: 1637800, elapsed: 1.03e+01, train loss: 7.13487e-07, val loss: 1.25708e-06, min loss: 6.38293e-07\n",
      "Epoch: 1637900, elapsed: 1.04e+01, train loss: 1.83710e-06, val loss: 2.80777e-06, min loss: 6.38293e-07\n",
      "Epoch: 1638000, elapsed: 1.03e+01, train loss: 6.38364e-07, val loss: 1.25230e-06, min loss: 6.38293e-07\n",
      "Epoch: 1638100, elapsed: 1.04e+01, train loss: 6.38841e-07, val loss: 1.24428e-06, min loss: 6.38293e-07\n",
      "Epoch: 1638200, elapsed: 1.03e+01, train loss: 1.10396e-06, val loss: 1.70268e-06, min loss: 6.38293e-07\n",
      "Epoch: 1638300, elapsed: 1.04e+01, train loss: 6.37879e-07, val loss: 1.24904e-06, min loss: 6.37879e-07\n",
      "Epoch: 1638400, elapsed: 1.02e+01, train loss: 6.38450e-07, val loss: 1.25473e-06, min loss: 6.37879e-07\n",
      "Epoch: 1638500, elapsed: 1.04e+01, train loss: 7.55584e-07, val loss: 1.35769e-06, min loss: 6.37879e-07\n",
      "Epoch: 1638600, elapsed: 1.05e+01, train loss: 6.37490e-07, val loss: 1.25365e-06, min loss: 6.37490e-07\n",
      "Epoch: 1638700, elapsed: 1.04e+01, train loss: 6.40454e-07, val loss: 1.26403e-06, min loss: 6.37490e-07\n",
      "Epoch: 1638800, elapsed: 1.03e+01, train loss: 2.75383e-06, val loss: 3.29394e-06, min loss: 6.37490e-07\n",
      "Epoch: 1638900, elapsed: 1.50e+01, train loss: 2.46938e-06, val loss: 3.43808e-06, min loss: 6.37490e-07\n",
      "Epoch: 1639000, elapsed: 1.06e+01, train loss: 7.51053e-07, val loss: 1.46655e-06, min loss: 6.37490e-07\n",
      "Epoch: 1639100, elapsed: 1.05e+01, train loss: 7.25598e-07, val loss: 1.45595e-06, min loss: 6.37490e-07\n",
      "Epoch: 1639200, elapsed: 1.06e+01, train loss: 6.37052e-07, val loss: 1.25335e-06, min loss: 6.37052e-07\n",
      "Epoch: 1639300, elapsed: 1.04e+01, train loss: 6.43110e-07, val loss: 1.25810e-06, min loss: 6.37052e-07\n",
      "Epoch: 1639400, elapsed: 1.05e+01, train loss: 6.49310e-07, val loss: 1.27667e-06, min loss: 6.37052e-07\n",
      "Epoch: 1639500, elapsed: 1.06e+01, train loss: 6.45940e-07, val loss: 1.26084e-06, min loss: 6.37052e-07\n",
      "Epoch: 1639600, elapsed: 1.05e+01, train loss: 7.38689e-07, val loss: 1.35572e-06, min loss: 6.37052e-07\n",
      "Epoch: 1639700, elapsed: 1.04e+01, train loss: 9.68864e-07, val loss: 1.54375e-06, min loss: 6.37052e-07\n",
      "Epoch: 1639800, elapsed: 1.06e+01, train loss: 7.67809e-07, val loss: 1.31763e-06, min loss: 6.37052e-07\n",
      "Epoch: 1639900, elapsed: 1.05e+01, train loss: 6.50407e-07, val loss: 1.24770e-06, min loss: 6.37052e-07\n",
      "Epoch: 1640000, elapsed: 1.05e+01, train loss: 6.37253e-07, val loss: 1.25555e-06, min loss: 6.37052e-07\n",
      "Epoch: 1640100, elapsed: 1.24e+01, train loss: 6.53692e-07, val loss: 1.26795e-06, min loss: 6.37052e-07\n",
      "Epoch: 1640200, elapsed: 1.04e+01, train loss: 1.18622e-06, val loss: 1.73737e-06, min loss: 6.37052e-07\n",
      "Epoch: 1640300, elapsed: 1.05e+01, train loss: 6.36681e-07, val loss: 1.24781e-06, min loss: 6.36681e-07\n",
      "Epoch: 1640400, elapsed: 1.04e+01, train loss: 9.97990e-07, val loss: 1.33989e-06, min loss: 6.36681e-07\n",
      "Epoch: 1640500, elapsed: 1.03e+01, train loss: 7.11003e-07, val loss: 1.36556e-06, min loss: 6.36681e-07\n",
      "Epoch: 1640600, elapsed: 1.04e+01, train loss: 6.49111e-07, val loss: 1.26280e-06, min loss: 6.36681e-07\n",
      "Epoch: 1640700, elapsed: 1.04e+01, train loss: 2.89659e-06, val loss: 3.58419e-06, min loss: 6.36681e-07\n",
      "Epoch: 1640800, elapsed: 1.04e+01, train loss: 6.35698e-07, val loss: 1.24856e-06, min loss: 6.35698e-07\n",
      "Epoch: 1640900, elapsed: 1.04e+01, train loss: 6.69801e-07, val loss: 1.26256e-06, min loss: 6.35698e-07\n",
      "Epoch: 1641000, elapsed: 1.03e+01, train loss: 6.46653e-07, val loss: 1.24464e-06, min loss: 6.35698e-07\n",
      "Epoch: 1641100, elapsed: 1.04e+01, train loss: 8.03553e-07, val loss: 1.30064e-06, min loss: 6.35698e-07\n",
      "Epoch: 1641200, elapsed: 1.05e+01, train loss: 3.54255e-06, val loss: 4.70143e-06, min loss: 6.35698e-07\n",
      "Epoch: 1641300, elapsed: 1.05e+01, train loss: 1.07403e-06, val loss: 1.93554e-06, min loss: 6.35698e-07\n",
      "Epoch: 1641400, elapsed: 1.05e+01, train loss: 6.73051e-07, val loss: 1.28178e-06, min loss: 6.35698e-07\n",
      "Epoch: 1641500, elapsed: 1.05e+01, train loss: 1.11194e-06, val loss: 1.55041e-06, min loss: 6.35698e-07\n",
      "Epoch: 1641600, elapsed: 1.05e+01, train loss: 1.09528e-06, val loss: 1.62581e-06, min loss: 6.35698e-07\n",
      "Epoch: 1641700, elapsed: 1.06e+01, train loss: 6.58510e-07, val loss: 1.23106e-06, min loss: 6.35698e-07\n",
      "Epoch: 1641800, elapsed: 1.48e+01, train loss: 6.35114e-07, val loss: 1.24737e-06, min loss: 6.35114e-07\n",
      "Epoch: 1641900, elapsed: 1.06e+01, train loss: 6.38961e-07, val loss: 1.23441e-06, min loss: 6.35114e-07\n",
      "Epoch: 1642000, elapsed: 1.05e+01, train loss: 6.36854e-07, val loss: 1.25187e-06, min loss: 6.35114e-07\n",
      "Epoch: 1642100, elapsed: 1.06e+01, train loss: 8.21440e-07, val loss: 1.47962e-06, min loss: 6.35114e-07\n",
      "Epoch: 1642200, elapsed: 1.05e+01, train loss: 2.42814e-06, val loss: 1.96747e-06, min loss: 6.35114e-07\n",
      "Epoch: 1642300, elapsed: 1.04e+01, train loss: 6.34689e-07, val loss: 1.24785e-06, min loss: 6.34689e-07\n",
      "Epoch: 1642400, elapsed: 1.04e+01, train loss: 6.85973e-07, val loss: 1.32363e-06, min loss: 6.34689e-07\n",
      "Epoch: 1642500, elapsed: 1.07e+01, train loss: 6.37424e-07, val loss: 1.26043e-06, min loss: 6.34689e-07\n",
      "Epoch: 1642600, elapsed: 1.05e+01, train loss: 6.35705e-07, val loss: 1.24680e-06, min loss: 6.34689e-07\n",
      "Epoch: 1642700, elapsed: 1.05e+01, train loss: 1.25306e-06, val loss: 1.60789e-06, min loss: 6.34689e-07\n",
      "Epoch: 1642800, elapsed: 1.04e+01, train loss: 6.34639e-07, val loss: 1.27643e-06, min loss: 6.34639e-07\n",
      "Epoch: 1642900, elapsed: 1.04e+01, train loss: 2.22754e-06, val loss: 2.01740e-06, min loss: 6.34639e-07\n",
      "Epoch: 1643000, elapsed: 1.05e+01, train loss: 6.64702e-07, val loss: 1.32607e-06, min loss: 6.34639e-07\n",
      "Epoch: 1643100, elapsed: 1.05e+01, train loss: 7.44089e-07, val loss: 1.32645e-06, min loss: 6.34639e-07\n",
      "Epoch: 1643200, elapsed: 1.05e+01, train loss: 6.70218e-07, val loss: 1.30395e-06, min loss: 6.34639e-07\n",
      "Epoch: 1643300, elapsed: 1.05e+01, train loss: 6.34811e-07, val loss: 1.24681e-06, min loss: 6.34639e-07\n",
      "Epoch: 1643400, elapsed: 1.03e+01, train loss: 6.34282e-07, val loss: 1.25105e-06, min loss: 6.34282e-07\n",
      "Epoch: 1643500, elapsed: 1.05e+01, train loss: 1.05649e-06, val loss: 1.67912e-06, min loss: 6.34282e-07\n",
      "Epoch: 1643600, elapsed: 1.04e+01, train loss: 6.34770e-07, val loss: 1.25803e-06, min loss: 6.34282e-07\n",
      "Epoch: 1643700, elapsed: 1.03e+01, train loss: 6.34464e-07, val loss: 1.24528e-06, min loss: 6.34282e-07\n",
      "Epoch: 1643800, elapsed: 1.04e+01, train loss: 1.84310e-06, val loss: 1.89232e-06, min loss: 6.34282e-07\n",
      "Epoch: 1643900, elapsed: 1.03e+01, train loss: 6.44647e-07, val loss: 1.29095e-06, min loss: 6.34282e-07\n",
      "Epoch: 1644000, elapsed: 1.04e+01, train loss: 7.41900e-07, val loss: 1.41343e-06, min loss: 6.34282e-07\n",
      "Epoch: 1644100, elapsed: 1.03e+01, train loss: 6.45441e-07, val loss: 1.25861e-06, min loss: 6.34282e-07\n",
      "Epoch: 1644200, elapsed: 1.05e+01, train loss: 4.07527e-06, val loss: 3.91749e-06, min loss: 6.34282e-07\n",
      "Epoch: 1644300, elapsed: 1.04e+01, train loss: 6.33535e-07, val loss: 1.24980e-06, min loss: 6.33535e-07\n",
      "Epoch: 1644400, elapsed: 1.04e+01, train loss: 6.36453e-07, val loss: 1.23987e-06, min loss: 6.33535e-07\n",
      "Epoch: 1644500, elapsed: 1.05e+01, train loss: 1.00583e-06, val loss: 1.76094e-06, min loss: 6.33535e-07\n",
      "Epoch: 1644600, elapsed: 1.04e+01, train loss: 6.33032e-07, val loss: 1.24759e-06, min loss: 6.33032e-07\n",
      "Epoch: 1644700, elapsed: 1.03e+01, train loss: 6.97671e-07, val loss: 1.28923e-06, min loss: 6.33032e-07\n",
      "Epoch: 1644800, elapsed: 1.49e+01, train loss: 9.96775e-07, val loss: 1.43950e-06, min loss: 6.33032e-07\n",
      "Epoch: 1644900, elapsed: 1.06e+01, train loss: 2.64311e-06, val loss: 3.81277e-06, min loss: 6.33032e-07\n",
      "Epoch: 1645000, elapsed: 1.06e+01, train loss: 6.73348e-07, val loss: 1.31170e-06, min loss: 6.33032e-07\n",
      "Epoch: 1645100, elapsed: 1.24e+01, train loss: 6.33261e-07, val loss: 1.24319e-06, min loss: 6.33032e-07\n",
      "Epoch: 1645200, elapsed: 1.05e+01, train loss: 6.38813e-07, val loss: 1.26023e-06, min loss: 6.33032e-07\n",
      "Epoch: 1645300, elapsed: 1.04e+01, train loss: 6.40748e-07, val loss: 1.26890e-06, min loss: 6.33032e-07\n",
      "Epoch: 1645400, elapsed: 1.04e+01, train loss: 6.83872e-07, val loss: 1.32015e-06, min loss: 6.33032e-07\n",
      "Epoch: 1645500, elapsed: 1.05e+01, train loss: 6.77229e-07, val loss: 1.33829e-06, min loss: 6.33032e-07\n",
      "Epoch: 1645600, elapsed: 1.05e+01, train loss: 6.33023e-07, val loss: 1.24020e-06, min loss: 6.33023e-07\n",
      "Epoch: 1645700, elapsed: 1.05e+01, train loss: 7.13613e-07, val loss: 1.31823e-06, min loss: 6.33023e-07\n",
      "Epoch: 1645800, elapsed: 1.03e+01, train loss: 8.39202e-07, val loss: 1.37761e-06, min loss: 6.33023e-07\n",
      "Epoch: 1645900, elapsed: 1.05e+01, train loss: 2.23580e-06, val loss: 2.95457e-06, min loss: 6.33023e-07\n",
      "Epoch: 1646000, elapsed: 1.03e+01, train loss: 1.22348e-06, val loss: 1.78435e-06, min loss: 6.33023e-07\n",
      "Epoch: 1646100, elapsed: 1.05e+01, train loss: 8.28854e-07, val loss: 1.45225e-06, min loss: 6.33023e-07\n",
      "Epoch: 1646200, elapsed: 1.04e+01, train loss: 6.38611e-07, val loss: 1.26495e-06, min loss: 6.33023e-07\n",
      "Epoch: 1646300, elapsed: 1.03e+01, train loss: 6.34440e-07, val loss: 1.25092e-06, min loss: 6.33023e-07\n",
      "Epoch: 1646400, elapsed: 1.05e+01, train loss: 6.39373e-07, val loss: 1.25708e-06, min loss: 6.33023e-07\n",
      "Epoch: 1646500, elapsed: 1.04e+01, train loss: 7.05378e-07, val loss: 1.36366e-06, min loss: 6.33023e-07\n",
      "Epoch: 1646600, elapsed: 1.05e+01, train loss: 7.43226e-07, val loss: 1.28489e-06, min loss: 6.33023e-07\n",
      "Epoch: 1646700, elapsed: 1.07e+01, train loss: 6.75587e-07, val loss: 1.27582e-06, min loss: 6.33023e-07\n",
      "Epoch: 1646800, elapsed: 1.05e+01, train loss: 6.51016e-07, val loss: 1.23888e-06, min loss: 6.33023e-07\n",
      "Epoch: 1646900, elapsed: 1.04e+01, train loss: 6.51771e-07, val loss: 1.28632e-06, min loss: 6.33023e-07\n",
      "Epoch: 1647000, elapsed: 1.05e+01, train loss: 6.69834e-07, val loss: 1.27831e-06, min loss: 6.33023e-07\n",
      "Epoch: 1647100, elapsed: 1.04e+01, train loss: 6.39960e-07, val loss: 1.24225e-06, min loss: 6.33023e-07\n",
      "Epoch: 1647200, elapsed: 1.04e+01, train loss: 6.43247e-07, val loss: 1.24936e-06, min loss: 6.33023e-07\n",
      "Epoch: 1647300, elapsed: 1.05e+01, train loss: 6.48179e-07, val loss: 1.28431e-06, min loss: 6.33023e-07\n",
      "Epoch: 1647400, elapsed: 1.04e+01, train loss: 7.91970e-07, val loss: 1.28252e-06, min loss: 6.33023e-07\n",
      "Epoch: 1647500, elapsed: 1.05e+01, train loss: 1.68587e-06, val loss: 2.07235e-06, min loss: 6.33023e-07\n",
      "Epoch: 1647600, elapsed: 1.05e+01, train loss: 7.00819e-07, val loss: 1.29060e-06, min loss: 6.33023e-07\n",
      "Epoch: 1647700, elapsed: 1.47e+01, train loss: 6.46261e-07, val loss: 1.26069e-06, min loss: 6.33023e-07\n",
      "Epoch: 1647800, elapsed: 1.07e+01, train loss: 6.69138e-07, val loss: 1.23479e-06, min loss: 6.33023e-07\n",
      "Epoch: 1647900, elapsed: 1.06e+01, train loss: 6.31244e-07, val loss: 1.24969e-06, min loss: 6.31244e-07\n",
      "Epoch: 1648000, elapsed: 1.06e+01, train loss: 6.36634e-07, val loss: 1.24305e-06, min loss: 6.31244e-07\n",
      "Epoch: 1648100, elapsed: 1.06e+01, train loss: 6.47891e-07, val loss: 1.23875e-06, min loss: 6.31244e-07\n",
      "Epoch: 1648200, elapsed: 1.04e+01, train loss: 8.33916e-07, val loss: 1.67179e-06, min loss: 6.31244e-07\n",
      "Epoch: 1648300, elapsed: 1.06e+01, train loss: 6.31387e-07, val loss: 1.24219e-06, min loss: 6.31244e-07\n",
      "Epoch: 1648400, elapsed: 1.06e+01, train loss: 6.31027e-07, val loss: 1.24802e-06, min loss: 6.31027e-07\n",
      "Epoch: 1648500, elapsed: 1.05e+01, train loss: 6.84170e-07, val loss: 1.30001e-06, min loss: 6.31027e-07\n",
      "Epoch: 1648600, elapsed: 1.03e+01, train loss: 6.31974e-07, val loss: 1.24326e-06, min loss: 6.31027e-07\n",
      "Epoch: 1648700, elapsed: 1.06e+01, train loss: 6.84639e-07, val loss: 1.34812e-06, min loss: 6.31027e-07\n",
      "Epoch: 1648800, elapsed: 1.06e+01, train loss: 6.95516e-07, val loss: 1.36646e-06, min loss: 6.31027e-07\n",
      "Epoch: 1648900, elapsed: 1.04e+01, train loss: 6.97260e-07, val loss: 1.34517e-06, min loss: 6.31027e-07\n",
      "Epoch: 1649000, elapsed: 1.05e+01, train loss: 6.67159e-07, val loss: 1.22810e-06, min loss: 6.31027e-07\n",
      "Epoch: 1649100, elapsed: 1.04e+01, train loss: 6.32753e-07, val loss: 1.25765e-06, min loss: 6.31027e-07\n",
      "Epoch: 1649200, elapsed: 1.04e+01, train loss: 6.41025e-07, val loss: 1.27262e-06, min loss: 6.31027e-07\n",
      "Epoch: 1649300, elapsed: 1.04e+01, train loss: 8.90056e-07, val loss: 1.33299e-06, min loss: 6.31027e-07\n",
      "Epoch: 1649400, elapsed: 1.05e+01, train loss: 6.43506e-07, val loss: 1.28823e-06, min loss: 6.31027e-07\n",
      "Epoch: 1649500, elapsed: 1.04e+01, train loss: 6.33688e-07, val loss: 1.24685e-06, min loss: 6.31027e-07\n",
      "Epoch: 1649600, elapsed: 1.06e+01, train loss: 6.47250e-07, val loss: 1.26879e-06, min loss: 6.31027e-07\n",
      "Epoch: 1649700, elapsed: 1.04e+01, train loss: 7.86041e-07, val loss: 1.38139e-06, min loss: 6.31027e-07\n",
      "Epoch: 1649800, elapsed: 1.04e+01, train loss: 7.05091e-07, val loss: 1.28642e-06, min loss: 6.31027e-07\n",
      "Epoch: 1649900, elapsed: 1.05e+01, train loss: 7.27092e-07, val loss: 1.31575e-06, min loss: 6.31027e-07\n",
      "Epoch: 1650000, elapsed: 1.03e+01, train loss: 1.01261e-06, val loss: 1.66478e-06, min loss: 6.31027e-07\n",
      "Epoch: 1650100, elapsed: 1.23e+01, train loss: 1.15537e-06, val loss: 1.49308e-06, min loss: 6.31027e-07\n",
      "Epoch: 1650200, elapsed: 1.03e+01, train loss: 6.40855e-07, val loss: 1.23667e-06, min loss: 6.31027e-07\n",
      "Epoch: 1650300, elapsed: 1.04e+01, train loss: 6.58634e-07, val loss: 1.30626e-06, min loss: 6.31027e-07\n",
      "Epoch: 1650400, elapsed: 1.05e+01, train loss: 6.36992e-07, val loss: 1.25122e-06, min loss: 6.31027e-07\n",
      "Epoch: 1650500, elapsed: 1.03e+01, train loss: 9.48689e-07, val loss: 1.66974e-06, min loss: 6.31027e-07\n",
      "Epoch: 1650600, elapsed: 1.04e+01, train loss: 1.00466e-06, val loss: 1.72714e-06, min loss: 6.31027e-07\n",
      "Epoch: 1650700, elapsed: 1.49e+01, train loss: 6.63265e-07, val loss: 1.30084e-06, min loss: 6.31027e-07\n",
      "Epoch: 1650800, elapsed: 1.04e+01, train loss: 7.21524e-07, val loss: 1.32082e-06, min loss: 6.31027e-07\n",
      "Epoch: 1650900, elapsed: 1.04e+01, train loss: 1.58124e-06, val loss: 1.47902e-06, min loss: 6.31027e-07\n",
      "Epoch: 1651000, elapsed: 1.05e+01, train loss: 6.29314e-07, val loss: 1.23802e-06, min loss: 6.29314e-07\n",
      "Epoch: 1651100, elapsed: 1.05e+01, train loss: 6.42717e-07, val loss: 1.31345e-06, min loss: 6.29314e-07\n",
      "Epoch: 1651200, elapsed: 1.05e+01, train loss: 6.28496e-07, val loss: 1.24237e-06, min loss: 6.28496e-07\n",
      "Epoch: 1651300, elapsed: 1.05e+01, train loss: 6.30628e-07, val loss: 1.24866e-06, min loss: 6.28496e-07\n",
      "Epoch: 1651400, elapsed: 1.05e+01, train loss: 1.22185e-06, val loss: 1.80869e-06, min loss: 6.28496e-07\n",
      "Epoch: 1651500, elapsed: 1.03e+01, train loss: 6.28405e-07, val loss: 1.24371e-06, min loss: 6.28405e-07\n",
      "Epoch: 1651600, elapsed: 1.04e+01, train loss: 6.30203e-07, val loss: 1.25482e-06, min loss: 6.28405e-07\n",
      "Epoch: 1651700, elapsed: 1.05e+01, train loss: 6.49392e-07, val loss: 1.26432e-06, min loss: 6.28405e-07\n",
      "Epoch: 1651800, elapsed: 1.05e+01, train loss: 9.34573e-07, val loss: 1.74099e-06, min loss: 6.28405e-07\n",
      "Epoch: 1651900, elapsed: 1.03e+01, train loss: 6.28293e-07, val loss: 1.23907e-06, min loss: 6.28293e-07\n",
      "Epoch: 1652000, elapsed: 1.04e+01, train loss: 6.34819e-07, val loss: 1.22327e-06, min loss: 6.28293e-07\n",
      "Epoch: 1652100, elapsed: 1.05e+01, train loss: 1.37128e-06, val loss: 1.38211e-06, min loss: 6.28293e-07\n",
      "Epoch: 1652200, elapsed: 1.05e+01, train loss: 7.71107e-07, val loss: 1.51110e-06, min loss: 6.28293e-07\n",
      "Epoch: 1652300, elapsed: 1.03e+01, train loss: 6.62961e-07, val loss: 1.28506e-06, min loss: 6.28293e-07\n",
      "Epoch: 1652400, elapsed: 1.04e+01, train loss: 6.42324e-07, val loss: 1.28229e-06, min loss: 6.28293e-07\n",
      "Epoch: 1652500, elapsed: 1.03e+01, train loss: 1.63656e-06, val loss: 2.75181e-06, min loss: 6.28293e-07\n",
      "Epoch: 1652600, elapsed: 1.03e+01, train loss: 6.27732e-07, val loss: 1.24262e-06, min loss: 6.27732e-07\n",
      "Epoch: 1652700, elapsed: 1.05e+01, train loss: 1.56611e-06, val loss: 2.55137e-06, min loss: 6.27732e-07\n",
      "Epoch: 1652800, elapsed: 1.03e+01, train loss: 6.29460e-07, val loss: 1.23783e-06, min loss: 6.27732e-07\n",
      "Epoch: 1652900, elapsed: 1.03e+01, train loss: 6.33196e-07, val loss: 1.24839e-06, min loss: 6.27732e-07\n",
      "Epoch: 1653000, elapsed: 1.04e+01, train loss: 6.33998e-07, val loss: 1.25715e-06, min loss: 6.27732e-07\n",
      "Epoch: 1653100, elapsed: 1.03e+01, train loss: 8.99079e-07, val loss: 1.59588e-06, min loss: 6.27732e-07\n",
      "Epoch: 1653200, elapsed: 1.04e+01, train loss: 6.32874e-07, val loss: 1.25605e-06, min loss: 6.27732e-07\n",
      "Epoch: 1653300, elapsed: 1.02e+01, train loss: 6.39527e-07, val loss: 1.23101e-06, min loss: 6.27732e-07\n",
      "Epoch: 1653400, elapsed: 1.03e+01, train loss: 6.93274e-07, val loss: 1.23750e-06, min loss: 6.27732e-07\n",
      "Epoch: 1653500, elapsed: 1.02e+01, train loss: 3.25260e-06, val loss: 3.17697e-06, min loss: 6.27732e-07\n",
      "Epoch: 1653600, elapsed: 1.03e+01, train loss: 1.33702e-06, val loss: 2.43099e-06, min loss: 6.27732e-07\n",
      "Epoch: 1653700, elapsed: 1.48e+01, train loss: 7.48660e-07, val loss: 1.38964e-06, min loss: 6.27732e-07\n",
      "Epoch: 1653800, elapsed: 1.06e+01, train loss: 3.31687e-06, val loss: 4.18296e-06, min loss: 6.27732e-07\n",
      "Epoch: 1653900, elapsed: 1.05e+01, train loss: 6.26780e-07, val loss: 1.24129e-06, min loss: 6.26780e-07\n",
      "Epoch: 1654000, elapsed: 1.03e+01, train loss: 6.31541e-07, val loss: 1.23784e-06, min loss: 6.26780e-07\n",
      "Epoch: 1654100, elapsed: 1.06e+01, train loss: 6.42622e-07, val loss: 1.24784e-06, min loss: 6.26780e-07\n",
      "Epoch: 1654200, elapsed: 1.07e+01, train loss: 6.36410e-07, val loss: 1.22267e-06, min loss: 6.26780e-07\n",
      "Epoch: 1654300, elapsed: 1.05e+01, train loss: 7.28120e-07, val loss: 1.37476e-06, min loss: 6.26780e-07\n",
      "Epoch: 1654400, elapsed: 1.03e+01, train loss: 6.36796e-07, val loss: 1.23551e-06, min loss: 6.26780e-07\n",
      "Epoch: 1654500, elapsed: 1.06e+01, train loss: 6.44119e-07, val loss: 1.27023e-06, min loss: 6.26780e-07\n",
      "Epoch: 1654600, elapsed: 1.05e+01, train loss: 6.56219e-07, val loss: 1.25590e-06, min loss: 6.26780e-07\n",
      "Epoch: 1654700, elapsed: 1.04e+01, train loss: 1.61158e-06, val loss: 2.75487e-06, min loss: 6.26780e-07\n",
      "Epoch: 1654800, elapsed: 1.06e+01, train loss: 6.38976e-07, val loss: 1.25328e-06, min loss: 6.26780e-07\n",
      "Epoch: 1654900, elapsed: 1.04e+01, train loss: 6.98159e-07, val loss: 1.33866e-06, min loss: 6.26780e-07\n",
      "Epoch: 1655000, elapsed: 1.03e+01, train loss: 6.27414e-07, val loss: 1.24439e-06, min loss: 6.26780e-07\n",
      "Epoch: 1655100, elapsed: 1.23e+01, train loss: 1.65643e-06, val loss: 2.27481e-06, min loss: 6.26780e-07\n",
      "Epoch: 1655200, elapsed: 1.04e+01, train loss: 6.38026e-07, val loss: 1.26217e-06, min loss: 6.26780e-07\n",
      "Epoch: 1655300, elapsed: 1.04e+01, train loss: 1.03751e-06, val loss: 1.78569e-06, min loss: 6.26780e-07\n",
      "Epoch: 1655400, elapsed: 1.06e+01, train loss: 9.75764e-07, val loss: 1.70744e-06, min loss: 6.26780e-07\n",
      "Epoch: 1655500, elapsed: 1.04e+01, train loss: 6.46883e-07, val loss: 1.28309e-06, min loss: 6.26780e-07\n",
      "Epoch: 1655600, elapsed: 1.05e+01, train loss: 6.46672e-07, val loss: 1.28401e-06, min loss: 6.26780e-07\n",
      "Epoch: 1655700, elapsed: 1.05e+01, train loss: 6.47403e-07, val loss: 1.26874e-06, min loss: 6.26780e-07\n",
      "Epoch: 1655800, elapsed: 1.04e+01, train loss: 6.65596e-07, val loss: 1.25647e-06, min loss: 6.26780e-07\n",
      "Epoch: 1655900, elapsed: 1.05e+01, train loss: 8.88091e-07, val loss: 1.31578e-06, min loss: 6.26780e-07\n",
      "Epoch: 1656000, elapsed: 1.04e+01, train loss: 6.58994e-07, val loss: 1.23086e-06, min loss: 6.26780e-07\n",
      "Epoch: 1656100, elapsed: 1.05e+01, train loss: 6.25778e-07, val loss: 1.23645e-06, min loss: 6.25778e-07\n",
      "Epoch: 1656200, elapsed: 1.05e+01, train loss: 6.60977e-07, val loss: 1.24851e-06, min loss: 6.25778e-07\n",
      "Epoch: 1656300, elapsed: 1.04e+01, train loss: 9.08168e-07, val loss: 1.39692e-06, min loss: 6.25778e-07\n",
      "Epoch: 1656400, elapsed: 1.04e+01, train loss: 6.86862e-07, val loss: 1.30882e-06, min loss: 6.25778e-07\n",
      "Epoch: 1656500, elapsed: 1.05e+01, train loss: 7.96385e-07, val loss: 1.46379e-06, min loss: 6.25778e-07\n",
      "Epoch: 1656600, elapsed: 1.49e+01, train loss: 1.00990e-06, val loss: 1.70731e-06, min loss: 6.25778e-07\n",
      "Epoch: 1656700, elapsed: 1.06e+01, train loss: 1.57287e-06, val loss: 2.17528e-06, min loss: 6.25778e-07\n",
      "Epoch: 1656800, elapsed: 1.08e+01, train loss: 1.61243e-06, val loss: 1.50249e-06, min loss: 6.25778e-07\n",
      "Epoch: 1656900, elapsed: 1.05e+01, train loss: 6.50412e-07, val loss: 1.26370e-06, min loss: 6.25778e-07\n",
      "Epoch: 1657000, elapsed: 1.04e+01, train loss: 6.25360e-07, val loss: 1.23652e-06, min loss: 6.25360e-07\n",
      "Epoch: 1657100, elapsed: 1.05e+01, train loss: 6.33765e-07, val loss: 1.24451e-06, min loss: 6.25360e-07\n",
      "Epoch: 1657200, elapsed: 1.05e+01, train loss: 3.06290e-06, val loss: 4.35937e-06, min loss: 6.25360e-07\n",
      "Epoch: 1657300, elapsed: 1.04e+01, train loss: 6.28036e-07, val loss: 1.25063e-06, min loss: 6.25360e-07\n",
      "Epoch: 1657400, elapsed: 1.06e+01, train loss: 6.33915e-07, val loss: 1.23411e-06, min loss: 6.25360e-07\n",
      "Epoch: 1657500, elapsed: 1.06e+01, train loss: 9.64788e-07, val loss: 1.46482e-06, min loss: 6.25360e-07\n",
      "Epoch: 1657600, elapsed: 1.06e+01, train loss: 1.35512e-06, val loss: 1.77923e-06, min loss: 6.25360e-07\n",
      "Epoch: 1657700, elapsed: 1.03e+01, train loss: 6.24248e-07, val loss: 1.23875e-06, min loss: 6.24248e-07\n",
      "Epoch: 1657800, elapsed: 1.04e+01, train loss: 6.27831e-07, val loss: 1.23833e-06, min loss: 6.24248e-07\n",
      "Epoch: 1657900, elapsed: 1.05e+01, train loss: 7.08315e-07, val loss: 1.45302e-06, min loss: 6.24248e-07\n",
      "Epoch: 1658000, elapsed: 1.05e+01, train loss: 6.24100e-07, val loss: 1.23762e-06, min loss: 6.24100e-07\n",
      "Epoch: 1658100, elapsed: 1.05e+01, train loss: 1.82522e-06, val loss: 1.93770e-06, min loss: 6.24100e-07\n",
      "Epoch: 1658200, elapsed: 1.05e+01, train loss: 6.24086e-07, val loss: 1.23818e-06, min loss: 6.24086e-07\n",
      "Epoch: 1658300, elapsed: 1.05e+01, train loss: 6.32124e-07, val loss: 1.27726e-06, min loss: 6.24086e-07\n",
      "Epoch: 1658400, elapsed: 1.04e+01, train loss: 6.87813e-07, val loss: 1.33457e-06, min loss: 6.24086e-07\n",
      "Epoch: 1658500, elapsed: 1.04e+01, train loss: 6.36155e-07, val loss: 1.24958e-06, min loss: 6.24086e-07\n",
      "Epoch: 1658600, elapsed: 1.06e+01, train loss: 6.26745e-07, val loss: 1.24657e-06, min loss: 6.24086e-07\n",
      "Epoch: 1658700, elapsed: 1.04e+01, train loss: 6.33920e-07, val loss: 1.22277e-06, min loss: 6.24086e-07\n",
      "Epoch: 1658800, elapsed: 1.04e+01, train loss: 2.99664e-06, val loss: 2.38134e-06, min loss: 6.24086e-07\n",
      "Epoch: 1658900, elapsed: 1.05e+01, train loss: 6.79345e-07, val loss: 1.31381e-06, min loss: 6.24086e-07\n",
      "Epoch: 1659000, elapsed: 1.05e+01, train loss: 6.33746e-07, val loss: 1.25515e-06, min loss: 6.24086e-07\n",
      "Epoch: 1659100, elapsed: 1.03e+01, train loss: 1.53754e-06, val loss: 1.72656e-06, min loss: 6.24086e-07\n",
      "Epoch: 1659200, elapsed: 1.04e+01, train loss: 1.58741e-06, val loss: 2.19876e-06, min loss: 6.24086e-07\n",
      "Epoch: 1659300, elapsed: 1.05e+01, train loss: 6.44524e-07, val loss: 1.30522e-06, min loss: 6.24086e-07\n",
      "Epoch: 1659400, elapsed: 1.04e+01, train loss: 6.26070e-07, val loss: 1.24273e-06, min loss: 6.24086e-07\n",
      "Epoch: 1659500, elapsed: 1.05e+01, train loss: 6.26769e-07, val loss: 1.23465e-06, min loss: 6.24086e-07\n",
      "Epoch: 1659600, elapsed: 1.48e+01, train loss: 7.40514e-07, val loss: 1.37306e-06, min loss: 6.24086e-07\n",
      "Epoch: 1659700, elapsed: 1.06e+01, train loss: 6.52304e-07, val loss: 1.30024e-06, min loss: 6.24086e-07\n",
      "Epoch: 1659800, elapsed: 1.06e+01, train loss: 8.47193e-07, val loss: 1.54531e-06, min loss: 6.24086e-07\n",
      "Epoch: 1659900, elapsed: 1.06e+01, train loss: 6.33814e-07, val loss: 1.25610e-06, min loss: 6.24086e-07\n",
      "Epoch: 1660000, elapsed: 1.05e+01, train loss: 6.25249e-07, val loss: 1.22298e-06, min loss: 6.24086e-07\n",
      "Epoch: 1660100, elapsed: 1.25e+01, train loss: 1.07931e-06, val loss: 1.80869e-06, min loss: 6.24086e-07\n",
      "Epoch: 1660200, elapsed: 1.05e+01, train loss: 6.81185e-07, val loss: 1.22550e-06, min loss: 6.24086e-07\n",
      "Epoch: 1660300, elapsed: 1.06e+01, train loss: 6.60711e-07, val loss: 1.28034e-06, min loss: 6.24086e-07\n",
      "Epoch: 1660400, elapsed: 1.05e+01, train loss: 6.93254e-07, val loss: 1.24411e-06, min loss: 6.24086e-07\n",
      "Epoch: 1660500, elapsed: 1.05e+01, train loss: 9.07069e-07, val loss: 1.92894e-06, min loss: 6.24086e-07\n",
      "Epoch: 1660600, elapsed: 1.04e+01, train loss: 1.00936e-06, val loss: 1.72999e-06, min loss: 6.24086e-07\n",
      "Epoch: 1660700, elapsed: 1.04e+01, train loss: 6.25852e-07, val loss: 1.23762e-06, min loss: 6.24086e-07\n",
      "Epoch: 1660800, elapsed: 1.04e+01, train loss: 6.75744e-07, val loss: 1.28247e-06, min loss: 6.24086e-07\n",
      "Epoch: 1660900, elapsed: 1.04e+01, train loss: 7.72602e-07, val loss: 1.41396e-06, min loss: 6.24086e-07\n",
      "Epoch: 1661000, elapsed: 1.04e+01, train loss: 4.27004e-06, val loss: 4.59563e-06, min loss: 6.24086e-07\n",
      "Epoch: 1661100, elapsed: 1.03e+01, train loss: 7.37732e-07, val loss: 1.35952e-06, min loss: 6.24086e-07\n",
      "Epoch: 1661200, elapsed: 1.05e+01, train loss: 6.26918e-07, val loss: 1.25454e-06, min loss: 6.24086e-07\n",
      "Epoch: 1661300, elapsed: 1.05e+01, train loss: 1.25532e-06, val loss: 1.60102e-06, min loss: 6.24086e-07\n",
      "Epoch: 1661400, elapsed: 1.04e+01, train loss: 6.21849e-07, val loss: 1.23406e-06, min loss: 6.21849e-07\n",
      "Epoch: 1661500, elapsed: 1.03e+01, train loss: 6.37020e-07, val loss: 1.27138e-06, min loss: 6.21849e-07\n",
      "Epoch: 1661600, elapsed: 1.05e+01, train loss: 1.09867e-06, val loss: 1.54712e-06, min loss: 6.21849e-07\n",
      "Epoch: 1661700, elapsed: 1.05e+01, train loss: 6.60844e-07, val loss: 1.29867e-06, min loss: 6.21849e-07\n",
      "Epoch: 1661800, elapsed: 1.05e+01, train loss: 6.21756e-07, val loss: 1.23436e-06, min loss: 6.21756e-07\n",
      "Epoch: 1661900, elapsed: 1.04e+01, train loss: 6.27608e-07, val loss: 1.22477e-06, min loss: 6.21756e-07\n",
      "Epoch: 1662000, elapsed: 1.05e+01, train loss: 6.94936e-07, val loss: 1.22072e-06, min loss: 6.21756e-07\n",
      "Epoch: 1662100, elapsed: 1.05e+01, train loss: 9.74744e-07, val loss: 1.55894e-06, min loss: 6.21756e-07\n",
      "Epoch: 1662200, elapsed: 1.04e+01, train loss: 6.77740e-07, val loss: 1.29097e-06, min loss: 6.21756e-07\n",
      "Epoch: 1662300, elapsed: 1.04e+01, train loss: 6.37004e-07, val loss: 1.27483e-06, min loss: 6.21756e-07\n",
      "Epoch: 1662400, elapsed: 1.05e+01, train loss: 6.22217e-07, val loss: 1.23366e-06, min loss: 6.21756e-07\n",
      "Epoch: 1662500, elapsed: 1.04e+01, train loss: 6.27137e-07, val loss: 1.22197e-06, min loss: 6.21756e-07\n",
      "Epoch: 1662600, elapsed: 1.50e+01, train loss: 6.42405e-07, val loss: 1.27130e-06, min loss: 6.21756e-07\n",
      "Epoch: 1662700, elapsed: 1.06e+01, train loss: 7.52262e-07, val loss: 1.39238e-06, min loss: 6.21756e-07\n",
      "Epoch: 1662800, elapsed: 1.07e+01, train loss: 6.85571e-07, val loss: 1.37516e-06, min loss: 6.21756e-07\n",
      "Epoch: 1662900, elapsed: 1.07e+01, train loss: 6.24310e-07, val loss: 1.24435e-06, min loss: 6.21756e-07\n",
      "Epoch: 1663000, elapsed: 1.05e+01, train loss: 6.55306e-07, val loss: 1.26462e-06, min loss: 6.21756e-07\n",
      "Epoch: 1663100, elapsed: 1.06e+01, train loss: 6.54522e-07, val loss: 1.32170e-06, min loss: 6.21756e-07\n",
      "Epoch: 1663200, elapsed: 1.05e+01, train loss: 6.40571e-07, val loss: 1.25000e-06, min loss: 6.21756e-07\n",
      "Epoch: 1663300, elapsed: 1.06e+01, train loss: 6.56958e-07, val loss: 1.24665e-06, min loss: 6.21756e-07\n",
      "Epoch: 1663400, elapsed: 1.06e+01, train loss: 6.66717e-07, val loss: 1.26316e-06, min loss: 6.21756e-07\n",
      "Epoch: 1663500, elapsed: 1.06e+01, train loss: 2.72337e-06, val loss: 3.88937e-06, min loss: 6.21756e-07\n",
      "Epoch: 1663600, elapsed: 1.04e+01, train loss: 6.20662e-07, val loss: 1.23274e-06, min loss: 6.20662e-07\n",
      "Epoch: 1663700, elapsed: 1.05e+01, train loss: 6.33113e-07, val loss: 1.24165e-06, min loss: 6.20662e-07\n",
      "Epoch: 1663800, elapsed: 1.04e+01, train loss: 1.17005e-06, val loss: 1.56614e-06, min loss: 6.20662e-07\n",
      "Epoch: 1663900, elapsed: 1.03e+01, train loss: 6.22069e-07, val loss: 1.23415e-06, min loss: 6.20662e-07\n",
      "Epoch: 1664000, elapsed: 1.06e+01, train loss: 6.35896e-07, val loss: 1.25678e-06, min loss: 6.20662e-07\n",
      "Epoch: 1664100, elapsed: 1.03e+01, train loss: 7.01086e-07, val loss: 1.26739e-06, min loss: 6.20662e-07\n",
      "Epoch: 1664200, elapsed: 1.03e+01, train loss: 1.57626e-06, val loss: 1.53368e-06, min loss: 6.20662e-07\n",
      "Epoch: 1664300, elapsed: 1.06e+01, train loss: 6.21116e-07, val loss: 1.22864e-06, min loss: 6.20662e-07\n",
      "Epoch: 1664400, elapsed: 1.04e+01, train loss: 6.22736e-07, val loss: 1.23276e-06, min loss: 6.20662e-07\n",
      "Epoch: 1664500, elapsed: 1.04e+01, train loss: 6.65862e-07, val loss: 1.24941e-06, min loss: 6.20662e-07\n",
      "Epoch: 1664600, elapsed: 1.05e+01, train loss: 8.51854e-07, val loss: 1.56950e-06, min loss: 6.20662e-07\n",
      "Epoch: 1664700, elapsed: 1.04e+01, train loss: 1.78923e-06, val loss: 2.62113e-06, min loss: 6.20662e-07\n",
      "Epoch: 1664800, elapsed: 1.04e+01, train loss: 9.81902e-07, val loss: 1.64844e-06, min loss: 6.20662e-07\n",
      "Epoch: 1664900, elapsed: 1.04e+01, train loss: 6.23464e-07, val loss: 1.22259e-06, min loss: 6.20662e-07\n",
      "Epoch: 1665000, elapsed: 1.03e+01, train loss: 6.19743e-07, val loss: 1.23088e-06, min loss: 6.19743e-07\n",
      "Epoch: 1665100, elapsed: 1.23e+01, train loss: 6.37591e-07, val loss: 1.26743e-06, min loss: 6.19743e-07\n",
      "Epoch: 1665200, elapsed: 1.03e+01, train loss: 8.42553e-07, val loss: 1.39657e-06, min loss: 6.19743e-07\n",
      "Epoch: 1665300, elapsed: 1.05e+01, train loss: 8.67422e-07, val loss: 1.47936e-06, min loss: 6.19743e-07\n",
      "Epoch: 1665400, elapsed: 1.05e+01, train loss: 6.20628e-07, val loss: 1.24126e-06, min loss: 6.19743e-07\n",
      "Epoch: 1665500, elapsed: 1.49e+01, train loss: 6.22417e-07, val loss: 1.23518e-06, min loss: 6.19743e-07\n",
      "Epoch: 1665600, elapsed: 1.06e+01, train loss: 6.57787e-07, val loss: 1.37753e-06, min loss: 6.19743e-07\n",
      "Epoch: 1665700, elapsed: 1.07e+01, train loss: 8.10891e-07, val loss: 1.44228e-06, min loss: 6.19743e-07\n",
      "Epoch: 1665800, elapsed: 1.05e+01, train loss: 6.19834e-07, val loss: 1.22879e-06, min loss: 6.19743e-07\n",
      "Epoch: 1665900, elapsed: 1.07e+01, train loss: 1.66840e-06, val loss: 2.00729e-06, min loss: 6.19743e-07\n",
      "Epoch: 1666000, elapsed: 1.03e+01, train loss: 7.21507e-07, val loss: 1.27545e-06, min loss: 6.19743e-07\n",
      "Epoch: 1666100, elapsed: 1.06e+01, train loss: 6.25923e-07, val loss: 1.24925e-06, min loss: 6.19743e-07\n",
      "Epoch: 1666200, elapsed: 1.07e+01, train loss: 6.22965e-07, val loss: 1.21992e-06, min loss: 6.19743e-07\n",
      "Epoch: 1666300, elapsed: 1.04e+01, train loss: 6.21323e-07, val loss: 1.23795e-06, min loss: 6.19743e-07\n",
      "Epoch: 1666400, elapsed: 1.07e+01, train loss: 6.31662e-07, val loss: 1.26121e-06, min loss: 6.19743e-07\n",
      "Epoch: 1666500, elapsed: 1.06e+01, train loss: 6.43307e-07, val loss: 1.29119e-06, min loss: 6.19743e-07\n",
      "Epoch: 1666600, elapsed: 1.05e+01, train loss: 6.19335e-07, val loss: 1.23090e-06, min loss: 6.19335e-07\n",
      "Epoch: 1666700, elapsed: 1.06e+01, train loss: 6.19419e-07, val loss: 1.24004e-06, min loss: 6.19335e-07\n",
      "Epoch: 1666800, elapsed: 1.05e+01, train loss: 9.20833e-07, val loss: 1.49280e-06, min loss: 6.19335e-07\n",
      "Epoch: 1666900, elapsed: 1.06e+01, train loss: 9.39741e-07, val loss: 1.55903e-06, min loss: 6.19335e-07\n",
      "Epoch: 1667000, elapsed: 1.05e+01, train loss: 6.63581e-07, val loss: 1.28595e-06, min loss: 6.19335e-07\n",
      "Epoch: 1667100, elapsed: 1.05e+01, train loss: 6.29064e-07, val loss: 1.24723e-06, min loss: 6.19335e-07\n",
      "Epoch: 1667200, elapsed: 1.04e+01, train loss: 6.56002e-07, val loss: 1.25502e-06, min loss: 6.19335e-07\n",
      "Epoch: 1667300, elapsed: 1.06e+01, train loss: 6.65309e-07, val loss: 1.27042e-06, min loss: 6.19335e-07\n",
      "Epoch: 1667400, elapsed: 1.06e+01, train loss: 6.22195e-07, val loss: 1.24487e-06, min loss: 6.19335e-07\n",
      "Epoch: 1667500, elapsed: 1.04e+01, train loss: 6.20358e-07, val loss: 1.23361e-06, min loss: 6.19335e-07\n",
      "Epoch: 1667600, elapsed: 1.06e+01, train loss: 8.95252e-07, val loss: 1.37816e-06, min loss: 6.19335e-07\n",
      "Epoch: 1667700, elapsed: 1.05e+01, train loss: 7.66880e-07, val loss: 1.47044e-06, min loss: 6.19335e-07\n",
      "Epoch: 1667800, elapsed: 1.05e+01, train loss: 7.60908e-07, val loss: 1.40080e-06, min loss: 6.19335e-07\n",
      "Epoch: 1667900, elapsed: 1.04e+01, train loss: 8.06458e-07, val loss: 1.44017e-06, min loss: 6.19335e-07\n",
      "Epoch: 1668000, elapsed: 1.05e+01, train loss: 6.22387e-07, val loss: 1.24683e-06, min loss: 6.19335e-07\n",
      "Epoch: 1668100, elapsed: 1.05e+01, train loss: 6.29491e-07, val loss: 1.25764e-06, min loss: 6.19335e-07\n",
      "Epoch: 1668200, elapsed: 1.06e+01, train loss: 7.65472e-07, val loss: 1.41048e-06, min loss: 6.19335e-07\n",
      "Epoch: 1668300, elapsed: 1.04e+01, train loss: 8.52318e-07, val loss: 1.56283e-06, min loss: 6.19335e-07\n",
      "Epoch: 1668400, elapsed: 1.06e+01, train loss: 6.17908e-07, val loss: 1.22910e-06, min loss: 6.17908e-07\n",
      "Epoch: 1668500, elapsed: 1.03e+01, train loss: 6.26941e-07, val loss: 1.24427e-06, min loss: 6.17908e-07\n",
      "Epoch: 1668600, elapsed: 1.49e+01, train loss: 1.72996e-06, val loss: 2.48904e-06, min loss: 6.17908e-07\n",
      "Epoch: 1668700, elapsed: 1.08e+01, train loss: 7.53294e-07, val loss: 1.27691e-06, min loss: 6.17908e-07\n",
      "Epoch: 1668800, elapsed: 1.07e+01, train loss: 1.99779e-06, val loss: 1.89293e-06, min loss: 6.17908e-07\n",
      "Epoch: 1668900, elapsed: 1.05e+01, train loss: 8.48429e-07, val loss: 1.41707e-06, min loss: 6.17908e-07\n",
      "Epoch: 1669000, elapsed: 1.06e+01, train loss: 1.36034e-06, val loss: 2.08427e-06, min loss: 6.17908e-07\n",
      "Epoch: 1669100, elapsed: 1.06e+01, train loss: 6.33059e-07, val loss: 1.23540e-06, min loss: 6.17908e-07\n",
      "Epoch: 1669200, elapsed: 1.06e+01, train loss: 6.17389e-07, val loss: 1.23434e-06, min loss: 6.17389e-07\n",
      "Epoch: 1669300, elapsed: 1.05e+01, train loss: 6.31101e-07, val loss: 1.22774e-06, min loss: 6.17389e-07\n",
      "Epoch: 1669400, elapsed: 1.05e+01, train loss: 6.44000e-07, val loss: 1.26160e-06, min loss: 6.17389e-07\n",
      "Epoch: 1669500, elapsed: 1.06e+01, train loss: 1.01022e-06, val loss: 1.54270e-06, min loss: 6.17389e-07\n",
      "Epoch: 1669600, elapsed: 1.06e+01, train loss: 7.25825e-07, val loss: 1.38139e-06, min loss: 6.17389e-07\n",
      "Epoch: 1669700, elapsed: 1.05e+01, train loss: 6.56655e-07, val loss: 1.32468e-06, min loss: 6.17389e-07\n",
      "Epoch: 1669800, elapsed: 1.05e+01, train loss: 6.39752e-07, val loss: 1.25940e-06, min loss: 6.17389e-07\n",
      "Epoch: 1669900, elapsed: 1.04e+01, train loss: 6.24079e-07, val loss: 1.24823e-06, min loss: 6.17389e-07\n",
      "Epoch: 1670000, elapsed: 1.05e+01, train loss: 6.32063e-07, val loss: 1.24673e-06, min loss: 6.17389e-07\n",
      "Epoch: 1670100, elapsed: 1.22e+01, train loss: 1.29214e-06, val loss: 1.73243e-06, min loss: 6.17389e-07\n",
      "Epoch: 1670200, elapsed: 1.04e+01, train loss: 7.33972e-07, val loss: 1.37930e-06, min loss: 6.17389e-07\n",
      "Epoch: 1670300, elapsed: 1.03e+01, train loss: 6.34100e-07, val loss: 1.24866e-06, min loss: 6.17389e-07\n",
      "Epoch: 1670400, elapsed: 1.05e+01, train loss: 6.85408e-07, val loss: 1.28272e-06, min loss: 6.17389e-07\n",
      "Epoch: 1670500, elapsed: 1.05e+01, train loss: 6.26910e-07, val loss: 1.26155e-06, min loss: 6.17389e-07\n",
      "Epoch: 1670600, elapsed: 1.04e+01, train loss: 7.48895e-07, val loss: 1.29637e-06, min loss: 6.17389e-07\n",
      "Epoch: 1670700, elapsed: 1.04e+01, train loss: 9.22647e-07, val loss: 1.36683e-06, min loss: 6.17389e-07\n",
      "Epoch: 1670800, elapsed: 1.04e+01, train loss: 8.61132e-07, val loss: 1.85883e-06, min loss: 6.17389e-07\n",
      "Epoch: 1670900, elapsed: 1.04e+01, train loss: 6.20475e-07, val loss: 1.23033e-06, min loss: 6.17389e-07\n",
      "Epoch: 1671000, elapsed: 1.05e+01, train loss: 6.18220e-07, val loss: 1.22112e-06, min loss: 6.17389e-07\n",
      "Epoch: 1671100, elapsed: 1.04e+01, train loss: 9.53670e-07, val loss: 1.31345e-06, min loss: 6.17389e-07\n",
      "Epoch: 1671200, elapsed: 1.05e+01, train loss: 6.41949e-07, val loss: 1.21157e-06, min loss: 6.17389e-07\n",
      "Epoch: 1671300, elapsed: 1.05e+01, train loss: 6.17055e-07, val loss: 1.22761e-06, min loss: 6.17055e-07\n",
      "Epoch: 1671400, elapsed: 1.05e+01, train loss: 6.38638e-07, val loss: 1.24893e-06, min loss: 6.17055e-07\n",
      "Epoch: 1671500, elapsed: 1.49e+01, train loss: 7.10795e-07, val loss: 1.51179e-06, min loss: 6.17055e-07\n",
      "Epoch: 1671600, elapsed: 1.05e+01, train loss: 6.42449e-07, val loss: 1.29392e-06, min loss: 6.17055e-07\n",
      "Epoch: 1671700, elapsed: 1.05e+01, train loss: 6.40786e-07, val loss: 1.27989e-06, min loss: 6.17055e-07\n",
      "Epoch: 1671800, elapsed: 1.06e+01, train loss: 7.00461e-07, val loss: 1.29578e-06, min loss: 6.17055e-07\n",
      "Epoch: 1671900, elapsed: 1.04e+01, train loss: 6.16357e-07, val loss: 1.22904e-06, min loss: 6.16357e-07\n",
      "Epoch: 1672000, elapsed: 1.05e+01, train loss: 6.59931e-07, val loss: 1.27259e-06, min loss: 6.16357e-07\n",
      "Epoch: 1672100, elapsed: 1.05e+01, train loss: 1.27046e-06, val loss: 2.05249e-06, min loss: 6.16357e-07\n",
      "Epoch: 1672200, elapsed: 1.04e+01, train loss: 6.19939e-07, val loss: 1.22979e-06, min loss: 6.16357e-07\n",
      "Epoch: 1672300, elapsed: 1.03e+01, train loss: 7.21014e-07, val loss: 1.28021e-06, min loss: 6.16357e-07\n",
      "Epoch: 1672400, elapsed: 1.05e+01, train loss: 8.80514e-07, val loss: 1.55333e-06, min loss: 6.16357e-07\n",
      "Epoch: 1672500, elapsed: 1.04e+01, train loss: 8.08434e-07, val loss: 1.46309e-06, min loss: 6.16357e-07\n",
      "Epoch: 1672600, elapsed: 1.04e+01, train loss: 6.19637e-07, val loss: 1.21347e-06, min loss: 6.16357e-07\n",
      "Epoch: 1672700, elapsed: 1.05e+01, train loss: 6.81104e-07, val loss: 1.25341e-06, min loss: 6.16357e-07\n",
      "Epoch: 1672800, elapsed: 1.04e+01, train loss: 6.42493e-07, val loss: 1.27389e-06, min loss: 6.16357e-07\n",
      "Epoch: 1672900, elapsed: 1.04e+01, train loss: 8.76121e-07, val loss: 1.34801e-06, min loss: 6.16357e-07\n",
      "Epoch: 1673000, elapsed: 1.04e+01, train loss: 7.94530e-07, val loss: 1.36108e-06, min loss: 6.16357e-07\n",
      "Epoch: 1673100, elapsed: 1.03e+01, train loss: 6.85276e-07, val loss: 1.30501e-06, min loss: 6.16357e-07\n",
      "Epoch: 1673200, elapsed: 1.04e+01, train loss: 1.40299e-06, val loss: 1.63434e-06, min loss: 6.16357e-07\n",
      "Epoch: 1673300, elapsed: 1.04e+01, train loss: 1.56187e-06, val loss: 1.71397e-06, min loss: 6.16357e-07\n",
      "Epoch: 1673400, elapsed: 1.03e+01, train loss: 6.59610e-07, val loss: 1.25124e-06, min loss: 6.16357e-07\n",
      "Epoch: 1673500, elapsed: 1.05e+01, train loss: 7.40294e-07, val loss: 1.40485e-06, min loss: 6.16357e-07\n",
      "Epoch: 1673600, elapsed: 1.04e+01, train loss: 7.59673e-07, val loss: 1.45038e-06, min loss: 6.16357e-07\n",
      "Epoch: 1673700, elapsed: 1.03e+01, train loss: 1.00798e-06, val loss: 1.61401e-06, min loss: 6.16357e-07\n",
      "Epoch: 1673800, elapsed: 1.04e+01, train loss: 7.79823e-07, val loss: 1.42654e-06, min loss: 6.16357e-07\n",
      "Epoch: 1673900, elapsed: 1.03e+01, train loss: 8.25762e-07, val loss: 1.54168e-06, min loss: 6.16357e-07\n",
      "Epoch: 1674000, elapsed: 1.03e+01, train loss: 6.14411e-07, val loss: 1.22247e-06, min loss: 6.14411e-07\n",
      "Epoch: 1674100, elapsed: 1.04e+01, train loss: 2.22510e-06, val loss: 2.88247e-06, min loss: 6.14411e-07\n",
      "Epoch: 1674200, elapsed: 1.03e+01, train loss: 1.09100e-06, val loss: 1.74851e-06, min loss: 6.14411e-07\n",
      "Epoch: 1674300, elapsed: 1.04e+01, train loss: 2.56769e-06, val loss: 3.53397e-06, min loss: 6.14411e-07\n",
      "Epoch: 1674400, elapsed: 1.02e+01, train loss: 6.38066e-07, val loss: 1.28307e-06, min loss: 6.14411e-07\n",
      "Epoch: 1674500, elapsed: 1.01e+01, train loss: 6.14604e-07, val loss: 1.23122e-06, min loss: 6.14411e-07\n",
      "Epoch: 1674600, elapsed: 1.51e+01, train loss: 6.16936e-07, val loss: 1.21528e-06, min loss: 6.14411e-07\n",
      "Epoch: 1674700, elapsed: 1.05e+01, train loss: 6.43350e-07, val loss: 1.21016e-06, min loss: 6.14411e-07\n",
      "Epoch: 1674800, elapsed: 1.05e+01, train loss: 6.36509e-07, val loss: 1.27407e-06, min loss: 6.14411e-07\n",
      "Epoch: 1674900, elapsed: 1.06e+01, train loss: 7.21715e-07, val loss: 1.40131e-06, min loss: 6.14411e-07\n",
      "Epoch: 1675000, elapsed: 1.06e+01, train loss: 3.82836e-06, val loss: 3.37855e-06, min loss: 6.14411e-07\n",
      "Epoch: 1675100, elapsed: 1.23e+01, train loss: 6.13621e-07, val loss: 1.22208e-06, min loss: 6.13621e-07\n",
      "Epoch: 1675200, elapsed: 1.04e+01, train loss: 6.19996e-07, val loss: 1.21798e-06, min loss: 6.13621e-07\n",
      "Epoch: 1675300, elapsed: 1.05e+01, train loss: 6.52640e-07, val loss: 1.36781e-06, min loss: 6.13621e-07\n",
      "Epoch: 1675400, elapsed: 1.04e+01, train loss: 6.13183e-07, val loss: 1.22627e-06, min loss: 6.13183e-07\n",
      "Epoch: 1675500, elapsed: 1.05e+01, train loss: 6.37583e-07, val loss: 1.25329e-06, min loss: 6.13183e-07\n",
      "Epoch: 1675600, elapsed: 1.05e+01, train loss: 3.52969e-06, val loss: 4.31811e-06, min loss: 6.13183e-07\n",
      "Epoch: 1675700, elapsed: 1.06e+01, train loss: 9.39757e-07, val loss: 1.67090e-06, min loss: 6.13183e-07\n",
      "Epoch: 1675800, elapsed: 1.05e+01, train loss: 6.16835e-07, val loss: 1.23764e-06, min loss: 6.13183e-07\n",
      "Epoch: 1675900, elapsed: 1.05e+01, train loss: 6.16582e-07, val loss: 1.21693e-06, min loss: 6.13183e-07\n",
      "Epoch: 1676000, elapsed: 1.05e+01, train loss: 1.02468e-06, val loss: 1.36231e-06, min loss: 6.13183e-07\n",
      "Epoch: 1676100, elapsed: 1.04e+01, train loss: 6.15602e-07, val loss: 1.23936e-06, min loss: 6.13183e-07\n",
      "Epoch: 1676200, elapsed: 1.05e+01, train loss: 6.14335e-07, val loss: 1.22340e-06, min loss: 6.13183e-07\n",
      "Epoch: 1676300, elapsed: 1.05e+01, train loss: 1.16312e-06, val loss: 1.40093e-06, min loss: 6.13183e-07\n",
      "Epoch: 1676400, elapsed: 1.04e+01, train loss: 7.46321e-07, val loss: 1.29596e-06, min loss: 6.13183e-07\n",
      "Epoch: 1676500, elapsed: 1.05e+01, train loss: 1.61536e-06, val loss: 2.13308e-06, min loss: 6.13183e-07\n",
      "Epoch: 1676600, elapsed: 1.04e+01, train loss: 1.89946e-06, val loss: 2.90089e-06, min loss: 6.13183e-07\n",
      "Epoch: 1676700, elapsed: 1.05e+01, train loss: 6.12353e-07, val loss: 1.22301e-06, min loss: 6.12353e-07\n",
      "Epoch: 1676800, elapsed: 1.04e+01, train loss: 6.22900e-07, val loss: 1.22559e-06, min loss: 6.12353e-07\n",
      "Epoch: 1676900, elapsed: 1.03e+01, train loss: 6.14070e-07, val loss: 1.22181e-06, min loss: 6.12353e-07\n",
      "Epoch: 1677000, elapsed: 1.04e+01, train loss: 6.21791e-07, val loss: 1.24078e-06, min loss: 6.12353e-07\n",
      "Epoch: 1677100, elapsed: 1.06e+01, train loss: 6.31837e-07, val loss: 1.23510e-06, min loss: 6.12353e-07\n",
      "Epoch: 1677200, elapsed: 1.04e+01, train loss: 6.90815e-07, val loss: 1.27852e-06, min loss: 6.12353e-07\n",
      "Epoch: 1677300, elapsed: 1.03e+01, train loss: 6.81042e-07, val loss: 1.25067e-06, min loss: 6.12353e-07\n",
      "Epoch: 1677400, elapsed: 1.02e+01, train loss: 6.25762e-07, val loss: 1.22881e-06, min loss: 6.12353e-07\n",
      "Epoch: 1677500, elapsed: 1.02e+01, train loss: 6.17746e-07, val loss: 1.23858e-06, min loss: 6.12353e-07\n",
      "Epoch: 1677600, elapsed: 1.50e+01, train loss: 6.14612e-07, val loss: 1.21505e-06, min loss: 6.12353e-07\n",
      "Epoch: 1677700, elapsed: 1.04e+01, train loss: 6.82031e-07, val loss: 1.34664e-06, min loss: 6.12353e-07\n",
      "Epoch: 1677800, elapsed: 1.06e+01, train loss: 1.12997e-06, val loss: 1.76587e-06, min loss: 6.12353e-07\n",
      "Epoch: 1677900, elapsed: 1.06e+01, train loss: 6.11718e-07, val loss: 1.22393e-06, min loss: 6.11718e-07\n",
      "Epoch: 1678000, elapsed: 1.05e+01, train loss: 6.64353e-07, val loss: 1.26726e-06, min loss: 6.11718e-07\n",
      "Epoch: 1678100, elapsed: 1.06e+01, train loss: 6.40420e-07, val loss: 1.24363e-06, min loss: 6.11718e-07\n",
      "Epoch: 1678200, elapsed: 1.05e+01, train loss: 6.34265e-07, val loss: 1.22286e-06, min loss: 6.11718e-07\n",
      "Epoch: 1678300, elapsed: 1.05e+01, train loss: 6.12248e-07, val loss: 1.23027e-06, min loss: 6.11718e-07\n",
      "Epoch: 1678400, elapsed: 1.06e+01, train loss: 6.17629e-07, val loss: 1.23252e-06, min loss: 6.11718e-07\n",
      "Epoch: 1678500, elapsed: 1.04e+01, train loss: 6.56684e-07, val loss: 1.29993e-06, min loss: 6.11718e-07\n",
      "Epoch: 1678600, elapsed: 1.04e+01, train loss: 6.57864e-07, val loss: 1.21762e-06, min loss: 6.11718e-07\n",
      "Epoch: 1678700, elapsed: 1.06e+01, train loss: 7.78915e-07, val loss: 1.30887e-06, min loss: 6.11718e-07\n",
      "Epoch: 1678800, elapsed: 1.05e+01, train loss: 7.09618e-07, val loss: 1.34791e-06, min loss: 6.11718e-07\n",
      "Epoch: 1678900, elapsed: 1.05e+01, train loss: 6.22672e-07, val loss: 1.21580e-06, min loss: 6.11718e-07\n",
      "Epoch: 1679000, elapsed: 1.06e+01, train loss: 8.05743e-07, val loss: 1.41219e-06, min loss: 6.11718e-07\n",
      "Epoch: 1679100, elapsed: 1.05e+01, train loss: 1.96243e-06, val loss: 2.42827e-06, min loss: 6.11718e-07\n",
      "Epoch: 1679200, elapsed: 1.06e+01, train loss: 6.25420e-07, val loss: 1.26064e-06, min loss: 6.11718e-07\n",
      "Epoch: 1679300, elapsed: 1.04e+01, train loss: 7.67628e-07, val loss: 1.36329e-06, min loss: 6.11718e-07\n",
      "Epoch: 1679400, elapsed: 1.06e+01, train loss: 7.55290e-07, val loss: 1.29683e-06, min loss: 6.11718e-07\n",
      "Epoch: 1679500, elapsed: 1.05e+01, train loss: 6.61034e-07, val loss: 1.34270e-06, min loss: 6.11718e-07\n",
      "Epoch: 1679600, elapsed: 1.05e+01, train loss: 3.40011e-06, val loss: 3.30561e-06, min loss: 6.11718e-07\n",
      "Epoch: 1679700, elapsed: 1.03e+01, train loss: 7.90225e-07, val loss: 1.51763e-06, min loss: 6.11718e-07\n",
      "Epoch: 1679800, elapsed: 1.04e+01, train loss: 6.39837e-07, val loss: 1.21986e-06, min loss: 6.11718e-07\n",
      "Epoch: 1679900, elapsed: 1.04e+01, train loss: 6.45559e-07, val loss: 1.26589e-06, min loss: 6.11718e-07\n",
      "Epoch: 1680000, elapsed: 1.04e+01, train loss: 9.45046e-07, val loss: 1.38296e-06, min loss: 6.11718e-07\n",
      "Epoch: 1680100, elapsed: 1.23e+01, train loss: 1.15781e-06, val loss: 1.48223e-06, min loss: 6.11718e-07\n",
      "Epoch: 1680200, elapsed: 1.05e+01, train loss: 1.29156e-06, val loss: 2.21459e-06, min loss: 6.11718e-07\n",
      "Epoch: 1680300, elapsed: 1.04e+01, train loss: 6.82781e-07, val loss: 1.33198e-06, min loss: 6.11718e-07\n",
      "Epoch: 1680400, elapsed: 1.06e+01, train loss: 6.15017e-07, val loss: 1.23188e-06, min loss: 6.11718e-07\n",
      "Epoch: 1680500, elapsed: 1.03e+01, train loss: 6.32363e-07, val loss: 1.25771e-06, min loss: 6.11718e-07\n",
      "Epoch: 1680600, elapsed: 1.47e+01, train loss: 6.52158e-07, val loss: 1.35379e-06, min loss: 6.11718e-07\n",
      "Epoch: 1680700, elapsed: 1.06e+01, train loss: 6.22813e-07, val loss: 1.22292e-06, min loss: 6.11718e-07\n",
      "Epoch: 1680800, elapsed: 1.06e+01, train loss: 6.13413e-07, val loss: 1.22642e-06, min loss: 6.11718e-07\n",
      "Epoch: 1680900, elapsed: 1.06e+01, train loss: 6.16315e-07, val loss: 1.24919e-06, min loss: 6.11718e-07\n",
      "Epoch: 1681000, elapsed: 1.06e+01, train loss: 7.15523e-07, val loss: 1.40435e-06, min loss: 6.11718e-07\n",
      "Epoch: 1681100, elapsed: 1.05e+01, train loss: 6.10394e-07, val loss: 1.22010e-06, min loss: 6.10394e-07\n",
      "Epoch: 1681200, elapsed: 1.05e+01, train loss: 6.11741e-07, val loss: 1.22373e-06, min loss: 6.10394e-07\n",
      "Epoch: 1681300, elapsed: 1.06e+01, train loss: 6.33260e-07, val loss: 1.22891e-06, min loss: 6.10394e-07\n",
      "Epoch: 1681400, elapsed: 1.06e+01, train loss: 6.23332e-07, val loss: 1.21833e-06, min loss: 6.10394e-07\n",
      "Epoch: 1681500, elapsed: 1.05e+01, train loss: 6.17411e-07, val loss: 1.23382e-06, min loss: 6.10394e-07\n",
      "Epoch: 1681600, elapsed: 1.06e+01, train loss: 6.13450e-07, val loss: 1.21248e-06, min loss: 6.10394e-07\n",
      "Epoch: 1681700, elapsed: 1.05e+01, train loss: 6.09849e-07, val loss: 1.22036e-06, min loss: 6.09849e-07\n",
      "Epoch: 1681800, elapsed: 1.07e+01, train loss: 6.71351e-07, val loss: 1.24147e-06, min loss: 6.09849e-07\n",
      "Epoch: 1681900, elapsed: 1.04e+01, train loss: 6.15531e-07, val loss: 1.24258e-06, min loss: 6.09849e-07\n",
      "Epoch: 1682000, elapsed: 1.05e+01, train loss: 6.13164e-07, val loss: 1.23240e-06, min loss: 6.09849e-07\n",
      "Epoch: 1682100, elapsed: 1.05e+01, train loss: 6.49095e-07, val loss: 1.29202e-06, min loss: 6.09849e-07\n",
      "Epoch: 1682200, elapsed: 1.05e+01, train loss: 6.63885e-07, val loss: 1.25205e-06, min loss: 6.09849e-07\n",
      "Epoch: 1682300, elapsed: 1.05e+01, train loss: 8.15892e-07, val loss: 1.27882e-06, min loss: 6.09849e-07\n",
      "Epoch: 1682400, elapsed: 1.05e+01, train loss: 6.09395e-07, val loss: 1.22159e-06, min loss: 6.09395e-07\n",
      "Epoch: 1682500, elapsed: 1.05e+01, train loss: 6.46082e-07, val loss: 1.31838e-06, min loss: 6.09395e-07\n",
      "Epoch: 1682600, elapsed: 1.06e+01, train loss: 6.08801e-07, val loss: 1.22005e-06, min loss: 6.08801e-07\n",
      "Epoch: 1682700, elapsed: 1.05e+01, train loss: 7.76955e-07, val loss: 1.36942e-06, min loss: 6.08801e-07\n",
      "Epoch: 1682800, elapsed: 1.04e+01, train loss: 9.56379e-07, val loss: 1.64800e-06, min loss: 6.08801e-07\n",
      "Epoch: 1682900, elapsed: 1.03e+01, train loss: 6.19131e-07, val loss: 1.24570e-06, min loss: 6.08801e-07\n",
      "Epoch: 1683000, elapsed: 1.05e+01, train loss: 6.67578e-07, val loss: 1.31007e-06, min loss: 6.08801e-07\n",
      "Epoch: 1683100, elapsed: 1.06e+01, train loss: 1.99972e-06, val loss: 2.32208e-06, min loss: 6.08801e-07\n",
      "Epoch: 1683200, elapsed: 1.06e+01, train loss: 9.05650e-07, val loss: 1.55566e-06, min loss: 6.08801e-07\n",
      "Epoch: 1683300, elapsed: 1.06e+01, train loss: 4.19341e-06, val loss: 4.19116e-06, min loss: 6.08801e-07\n",
      "Epoch: 1683400, elapsed: 1.06e+01, train loss: 6.34218e-07, val loss: 1.24145e-06, min loss: 6.08801e-07\n",
      "Epoch: 1683500, elapsed: 1.06e+01, train loss: 6.18800e-07, val loss: 1.23612e-06, min loss: 6.08801e-07\n",
      "Epoch: 1683600, elapsed: 1.46e+01, train loss: 6.95299e-07, val loss: 1.34787e-06, min loss: 6.08801e-07\n",
      "Epoch: 1683700, elapsed: 1.06e+01, train loss: 8.35890e-07, val loss: 1.52165e-06, min loss: 6.08801e-07\n",
      "Epoch: 1683800, elapsed: 1.06e+01, train loss: 8.49615e-07, val loss: 1.53080e-06, min loss: 6.08801e-07\n",
      "Epoch: 1683900, elapsed: 1.07e+01, train loss: 7.37326e-07, val loss: 1.48008e-06, min loss: 6.08801e-07\n",
      "Epoch: 1684000, elapsed: 1.06e+01, train loss: 2.61494e-06, val loss: 3.54624e-06, min loss: 6.08801e-07\n",
      "Epoch: 1684100, elapsed: 1.07e+01, train loss: 6.41352e-07, val loss: 1.23679e-06, min loss: 6.08801e-07\n",
      "Epoch: 1684200, elapsed: 1.06e+01, train loss: 6.91570e-07, val loss: 1.23626e-06, min loss: 6.08801e-07\n",
      "Epoch: 1684300, elapsed: 1.06e+01, train loss: 6.39579e-07, val loss: 1.29448e-06, min loss: 6.08801e-07\n",
      "Epoch: 1684400, elapsed: 1.06e+01, train loss: 6.97463e-07, val loss: 1.25488e-06, min loss: 6.08801e-07\n",
      "Epoch: 1684500, elapsed: 1.05e+01, train loss: 1.48331e-06, val loss: 2.17875e-06, min loss: 6.08801e-07\n",
      "Epoch: 1684600, elapsed: 1.06e+01, train loss: 6.09148e-07, val loss: 1.21853e-06, min loss: 6.08801e-07\n",
      "Epoch: 1684700, elapsed: 1.07e+01, train loss: 6.22329e-07, val loss: 1.23947e-06, min loss: 6.08801e-07\n",
      "Epoch: 1684800, elapsed: 1.04e+01, train loss: 8.74950e-07, val loss: 1.42035e-06, min loss: 6.08801e-07\n",
      "Epoch: 1684900, elapsed: 1.05e+01, train loss: 6.44652e-07, val loss: 1.32726e-06, min loss: 6.08801e-07\n",
      "Epoch: 1685000, elapsed: 1.05e+01, train loss: 6.20916e-07, val loss: 1.21005e-06, min loss: 6.08801e-07\n",
      "Epoch: 1685100, elapsed: 1.26e+01, train loss: 6.76375e-07, val loss: 1.27069e-06, min loss: 6.08801e-07\n",
      "Epoch: 1685200, elapsed: 1.04e+01, train loss: 9.71736e-07, val loss: 1.60110e-06, min loss: 6.08801e-07\n",
      "Epoch: 1685300, elapsed: 1.05e+01, train loss: 7.97315e-07, val loss: 1.50561e-06, min loss: 6.08801e-07\n",
      "Epoch: 1685400, elapsed: 1.04e+01, train loss: 6.50014e-07, val loss: 1.29050e-06, min loss: 6.08801e-07\n",
      "Epoch: 1685500, elapsed: 1.05e+01, train loss: 6.19300e-07, val loss: 1.22087e-06, min loss: 6.08801e-07\n",
      "Epoch: 1685600, elapsed: 1.05e+01, train loss: 6.10600e-07, val loss: 1.23200e-06, min loss: 6.08801e-07\n",
      "Epoch: 1685700, elapsed: 1.04e+01, train loss: 6.08957e-07, val loss: 1.21602e-06, min loss: 6.08801e-07\n",
      "Epoch: 1685800, elapsed: 1.03e+01, train loss: 6.07093e-07, val loss: 1.22128e-06, min loss: 6.07093e-07\n",
      "Epoch: 1685900, elapsed: 1.06e+01, train loss: 6.30025e-07, val loss: 1.27477e-06, min loss: 6.07093e-07\n",
      "Epoch: 1686000, elapsed: 1.03e+01, train loss: 7.77792e-07, val loss: 1.31590e-06, min loss: 6.07093e-07\n",
      "Epoch: 1686100, elapsed: 1.05e+01, train loss: 9.49297e-07, val loss: 1.58209e-06, min loss: 6.07093e-07\n",
      "Epoch: 1686200, elapsed: 1.04e+01, train loss: 6.07352e-07, val loss: 1.21624e-06, min loss: 6.07093e-07\n",
      "Epoch: 1686300, elapsed: 1.04e+01, train loss: 6.12729e-07, val loss: 1.21956e-06, min loss: 6.07093e-07\n",
      "Epoch: 1686400, elapsed: 1.06e+01, train loss: 9.42746e-07, val loss: 1.25750e-06, min loss: 6.07093e-07\n",
      "Epoch: 1686500, elapsed: 1.05e+01, train loss: 6.08160e-07, val loss: 1.21983e-06, min loss: 6.07093e-07\n",
      "Epoch: 1686600, elapsed: 1.47e+01, train loss: 6.83683e-07, val loss: 1.22075e-06, min loss: 6.07093e-07\n",
      "Epoch: 1686700, elapsed: 1.07e+01, train loss: 1.29197e-06, val loss: 2.19007e-06, min loss: 6.07093e-07\n",
      "Epoch: 1686800, elapsed: 1.05e+01, train loss: 6.06868e-07, val loss: 1.21698e-06, min loss: 6.06868e-07\n",
      "Epoch: 1686900, elapsed: 1.05e+01, train loss: 6.66166e-07, val loss: 1.23449e-06, min loss: 6.06868e-07\n",
      "Epoch: 1687000, elapsed: 1.04e+01, train loss: 6.09253e-07, val loss: 1.21914e-06, min loss: 6.06868e-07\n",
      "Epoch: 1687100, elapsed: 1.06e+01, train loss: 6.21459e-07, val loss: 1.24938e-06, min loss: 6.06868e-07\n",
      "Epoch: 1687200, elapsed: 1.05e+01, train loss: 9.24089e-07, val loss: 1.23971e-06, min loss: 6.06868e-07\n",
      "Epoch: 1687300, elapsed: 1.05e+01, train loss: 6.66395e-07, val loss: 1.27168e-06, min loss: 6.06868e-07\n",
      "Epoch: 1687400, elapsed: 1.05e+01, train loss: 6.11241e-07, val loss: 1.23071e-06, min loss: 6.06868e-07\n",
      "Epoch: 1687500, elapsed: 1.05e+01, train loss: 2.03356e-06, val loss: 2.20816e-06, min loss: 6.06868e-07\n",
      "Epoch: 1687600, elapsed: 1.05e+01, train loss: 6.11545e-07, val loss: 1.20233e-06, min loss: 6.06868e-07\n",
      "Epoch: 1687700, elapsed: 1.08e+01, train loss: 6.35893e-07, val loss: 1.26908e-06, min loss: 6.06868e-07\n",
      "Epoch: 1687800, elapsed: 1.07e+01, train loss: 6.28683e-07, val loss: 1.21153e-06, min loss: 6.06868e-07\n",
      "Epoch: 1687900, elapsed: 1.05e+01, train loss: 6.35679e-07, val loss: 1.21769e-06, min loss: 6.06868e-07\n",
      "Epoch: 1688000, elapsed: 1.06e+01, train loss: 6.07215e-07, val loss: 1.22826e-06, min loss: 6.06868e-07\n",
      "Epoch: 1688100, elapsed: 1.07e+01, train loss: 6.39150e-07, val loss: 1.26558e-06, min loss: 6.06868e-07\n",
      "Epoch: 1688200, elapsed: 1.05e+01, train loss: 6.07377e-07, val loss: 1.21099e-06, min loss: 6.06868e-07\n",
      "Epoch: 1688300, elapsed: 1.06e+01, train loss: 6.14770e-07, val loss: 1.23423e-06, min loss: 6.06868e-07\n",
      "Epoch: 1688400, elapsed: 1.06e+01, train loss: 6.55931e-07, val loss: 1.26993e-06, min loss: 6.06868e-07\n",
      "Epoch: 1688500, elapsed: 1.07e+01, train loss: 6.25672e-07, val loss: 1.21779e-06, min loss: 6.06868e-07\n",
      "Epoch: 1688600, elapsed: 1.05e+01, train loss: 8.43909e-07, val loss: 1.31285e-06, min loss: 6.06868e-07\n",
      "Epoch: 1688700, elapsed: 1.05e+01, train loss: 6.83843e-07, val loss: 1.28641e-06, min loss: 6.06868e-07\n",
      "Epoch: 1688800, elapsed: 1.06e+01, train loss: 6.06536e-07, val loss: 1.21486e-06, min loss: 6.06536e-07\n",
      "Epoch: 1688900, elapsed: 1.05e+01, train loss: 6.09071e-07, val loss: 1.22549e-06, min loss: 6.06536e-07\n",
      "Epoch: 1689000, elapsed: 1.06e+01, train loss: 8.07062e-07, val loss: 1.45707e-06, min loss: 6.06536e-07\n",
      "Epoch: 1689100, elapsed: 1.05e+01, train loss: 6.05071e-07, val loss: 1.21526e-06, min loss: 6.05071e-07\n",
      "Epoch: 1689200, elapsed: 1.04e+01, train loss: 6.06265e-07, val loss: 1.20833e-06, min loss: 6.05071e-07\n",
      "Epoch: 1689300, elapsed: 1.06e+01, train loss: 6.05201e-07, val loss: 1.21810e-06, min loss: 6.05071e-07\n",
      "Epoch: 1689400, elapsed: 1.06e+01, train loss: 6.09789e-07, val loss: 1.20215e-06, min loss: 6.05071e-07\n",
      "Epoch: 1689500, elapsed: 1.05e+01, train loss: 6.36717e-07, val loss: 1.23263e-06, min loss: 6.05071e-07\n",
      "Epoch: 1689600, elapsed: 1.06e+01, train loss: 6.97865e-07, val loss: 1.38993e-06, min loss: 6.05071e-07\n",
      "Epoch: 1689700, elapsed: 1.50e+01, train loss: 6.45983e-07, val loss: 1.28985e-06, min loss: 6.05071e-07\n",
      "Epoch: 1689800, elapsed: 1.05e+01, train loss: 6.32502e-07, val loss: 1.33626e-06, min loss: 6.05071e-07\n",
      "Epoch: 1689900, elapsed: 1.08e+01, train loss: 1.01065e-06, val loss: 1.49025e-06, min loss: 6.05071e-07\n",
      "Epoch: 1690000, elapsed: 1.08e+01, train loss: 6.05119e-07, val loss: 1.21084e-06, min loss: 6.05071e-07\n",
      "Epoch: 1690100, elapsed: 1.29e+01, train loss: 6.38542e-07, val loss: 1.24601e-06, min loss: 6.05071e-07\n",
      "Epoch: 1690200, elapsed: 1.06e+01, train loss: 6.16936e-07, val loss: 1.22847e-06, min loss: 6.05071e-07\n",
      "Epoch: 1690300, elapsed: 1.07e+01, train loss: 6.05664e-07, val loss: 1.21563e-06, min loss: 6.05071e-07\n",
      "Epoch: 1690400, elapsed: 1.11e+01, train loss: 6.41131e-07, val loss: 1.22172e-06, min loss: 6.05071e-07\n",
      "Epoch: 1690500, elapsed: 1.09e+01, train loss: 1.55298e-06, val loss: 2.26672e-06, min loss: 6.05071e-07\n",
      "Epoch: 1690600, elapsed: 1.08e+01, train loss: 6.52951e-07, val loss: 1.27218e-06, min loss: 6.05071e-07\n",
      "Epoch: 1690700, elapsed: 1.08e+01, train loss: 6.06009e-07, val loss: 1.22037e-06, min loss: 6.05071e-07\n",
      "Epoch: 1690800, elapsed: 1.08e+01, train loss: 6.06076e-07, val loss: 1.21545e-06, min loss: 6.05071e-07\n",
      "Epoch: 1690900, elapsed: 1.09e+01, train loss: 7.33550e-07, val loss: 1.36731e-06, min loss: 6.05071e-07\n",
      "Epoch: 1691000, elapsed: 1.08e+01, train loss: 6.04017e-07, val loss: 1.21482e-06, min loss: 6.04017e-07\n",
      "Epoch: 1691100, elapsed: 1.08e+01, train loss: 7.83143e-07, val loss: 1.26565e-06, min loss: 6.04017e-07\n",
      "Epoch: 1691200, elapsed: 1.08e+01, train loss: 6.03902e-07, val loss: 1.21509e-06, min loss: 6.03902e-07\n",
      "Epoch: 1691300, elapsed: 1.06e+01, train loss: 6.08665e-07, val loss: 1.23575e-06, min loss: 6.03902e-07\n",
      "Epoch: 1691400, elapsed: 1.08e+01, train loss: 6.37272e-07, val loss: 1.21377e-06, min loss: 6.03902e-07\n",
      "Epoch: 1691500, elapsed: 1.09e+01, train loss: 6.23306e-07, val loss: 1.28937e-06, min loss: 6.03902e-07\n",
      "Epoch: 1691600, elapsed: 1.09e+01, train loss: 6.11435e-07, val loss: 1.23463e-06, min loss: 6.03902e-07\n",
      "Epoch: 1691700, elapsed: 1.08e+01, train loss: 6.05381e-07, val loss: 1.20708e-06, min loss: 6.03902e-07\n",
      "Epoch: 1691800, elapsed: 1.09e+01, train loss: 6.42160e-07, val loss: 1.24656e-06, min loss: 6.03902e-07\n",
      "Epoch: 1691900, elapsed: 1.11e+01, train loss: 2.79878e-06, val loss: 2.66935e-06, min loss: 6.03902e-07\n",
      "Epoch: 1692000, elapsed: 1.11e+01, train loss: 6.11976e-07, val loss: 1.23524e-06, min loss: 6.03902e-07\n",
      "Epoch: 1692100, elapsed: 1.07e+01, train loss: 6.03965e-07, val loss: 1.21842e-06, min loss: 6.03902e-07\n",
      "Epoch: 1692200, elapsed: 1.10e+01, train loss: 6.40351e-07, val loss: 1.28740e-06, min loss: 6.03902e-07\n",
      "Epoch: 1692300, elapsed: 1.08e+01, train loss: 6.99849e-07, val loss: 1.42084e-06, min loss: 6.03902e-07\n",
      "Epoch: 1692400, elapsed: 1.09e+01, train loss: 6.52742e-07, val loss: 1.30941e-06, min loss: 6.03902e-07\n",
      "Epoch: 1692500, elapsed: 1.09e+01, train loss: 6.65813e-07, val loss: 1.23316e-06, min loss: 6.03902e-07\n",
      "Epoch: 1692600, elapsed: 1.10e+01, train loss: 6.04726e-07, val loss: 1.21147e-06, min loss: 6.03902e-07\n",
      "Epoch: 1692700, elapsed: 1.59e+01, train loss: 6.03285e-07, val loss: 1.21042e-06, min loss: 6.03285e-07\n",
      "Epoch: 1692800, elapsed: 1.10e+01, train loss: 6.16745e-07, val loss: 1.19397e-06, min loss: 6.03285e-07\n",
      "Epoch: 1692900, elapsed: 1.09e+01, train loss: 6.10741e-07, val loss: 1.23196e-06, min loss: 6.03285e-07\n",
      "Epoch: 1693000, elapsed: 1.11e+01, train loss: 6.19731e-07, val loss: 1.25158e-06, min loss: 6.03285e-07\n",
      "Epoch: 1693100, elapsed: 1.11e+01, train loss: 6.66227e-07, val loss: 1.44902e-06, min loss: 6.03285e-07\n",
      "Epoch: 1693200, elapsed: 1.10e+01, train loss: 6.95408e-07, val loss: 1.30883e-06, min loss: 6.03285e-07\n",
      "Epoch: 1693300, elapsed: 1.10e+01, train loss: 6.40015e-07, val loss: 1.22888e-06, min loss: 6.03285e-07\n",
      "Epoch: 1693400, elapsed: 1.10e+01, train loss: 7.13118e-07, val loss: 1.43083e-06, min loss: 6.03285e-07\n",
      "Epoch: 1693500, elapsed: 1.09e+01, train loss: 6.02688e-07, val loss: 1.21490e-06, min loss: 6.02688e-07\n",
      "Epoch: 1693600, elapsed: 1.10e+01, train loss: 6.04362e-07, val loss: 1.20886e-06, min loss: 6.02688e-07\n",
      "Epoch: 1693700, elapsed: 1.12e+01, train loss: 6.96279e-07, val loss: 1.39183e-06, min loss: 6.02688e-07\n",
      "Epoch: 1693800, elapsed: 1.09e+01, train loss: 6.02431e-07, val loss: 1.21258e-06, min loss: 6.02431e-07\n",
      "Epoch: 1693900, elapsed: 1.09e+01, train loss: 6.27059e-07, val loss: 1.27436e-06, min loss: 6.02431e-07\n",
      "Epoch: 1694000, elapsed: 1.08e+01, train loss: 6.02285e-07, val loss: 1.21224e-06, min loss: 6.02285e-07\n",
      "Epoch: 1694100, elapsed: 1.09e+01, train loss: 6.10294e-07, val loss: 1.22204e-06, min loss: 6.02285e-07\n",
      "Epoch: 1694200, elapsed: 1.09e+01, train loss: 9.35022e-07, val loss: 1.71333e-06, min loss: 6.02285e-07\n",
      "Epoch: 1694300, elapsed: 1.08e+01, train loss: 7.02560e-07, val loss: 1.24352e-06, min loss: 6.02285e-07\n",
      "Epoch: 1694400, elapsed: 1.08e+01, train loss: 6.23462e-07, val loss: 1.23008e-06, min loss: 6.02285e-07\n",
      "Epoch: 1694500, elapsed: 1.09e+01, train loss: 6.10911e-07, val loss: 1.23994e-06, min loss: 6.02285e-07\n",
      "Epoch: 1694600, elapsed: 1.08e+01, train loss: 6.24626e-07, val loss: 1.21556e-06, min loss: 6.02285e-07\n",
      "Epoch: 1694700, elapsed: 1.09e+01, train loss: 6.40413e-07, val loss: 1.26136e-06, min loss: 6.02285e-07\n",
      "Epoch: 1694800, elapsed: 1.09e+01, train loss: 6.08817e-07, val loss: 1.22464e-06, min loss: 6.02285e-07\n",
      "Epoch: 1694900, elapsed: 1.09e+01, train loss: 6.11112e-07, val loss: 1.23381e-06, min loss: 6.02285e-07\n",
      "Epoch: 1695000, elapsed: 1.08e+01, train loss: 8.12641e-07, val loss: 1.47261e-06, min loss: 6.02285e-07\n",
      "Epoch: 1695100, elapsed: 1.29e+01, train loss: 6.40346e-07, val loss: 1.30144e-06, min loss: 6.02285e-07\n",
      "Epoch: 1695200, elapsed: 1.08e+01, train loss: 6.03020e-07, val loss: 1.21684e-06, min loss: 6.02285e-07\n",
      "Epoch: 1695300, elapsed: 1.08e+01, train loss: 7.43294e-07, val loss: 1.22825e-06, min loss: 6.02285e-07\n",
      "Epoch: 1695400, elapsed: 1.08e+01, train loss: 8.95239e-07, val loss: 1.40345e-06, min loss: 6.02285e-07\n",
      "Epoch: 1695500, elapsed: 1.08e+01, train loss: 6.02440e-07, val loss: 1.20916e-06, min loss: 6.02285e-07\n",
      "Epoch: 1695600, elapsed: 1.08e+01, train loss: 6.26748e-07, val loss: 1.20508e-06, min loss: 6.02285e-07\n",
      "Epoch: 1695700, elapsed: 1.56e+01, train loss: 6.05056e-07, val loss: 1.22312e-06, min loss: 6.02285e-07\n",
      "Epoch: 1695800, elapsed: 1.11e+01, train loss: 6.43779e-07, val loss: 1.24667e-06, min loss: 6.02285e-07\n",
      "Epoch: 1695900, elapsed: 1.10e+01, train loss: 7.25074e-07, val loss: 1.25972e-06, min loss: 6.02285e-07\n",
      "Epoch: 1696000, elapsed: 1.11e+01, train loss: 7.36092e-07, val loss: 1.32967e-06, min loss: 6.02285e-07\n",
      "Epoch: 1696100, elapsed: 1.09e+01, train loss: 8.18581e-07, val loss: 1.51155e-06, min loss: 6.02285e-07\n",
      "Epoch: 1696200, elapsed: 1.11e+01, train loss: 6.88064e-07, val loss: 1.39879e-06, min loss: 6.02285e-07\n",
      "Epoch: 1696300, elapsed: 1.10e+01, train loss: 6.02734e-07, val loss: 1.20788e-06, min loss: 6.02285e-07\n",
      "Epoch: 1696400, elapsed: 1.11e+01, train loss: 6.01952e-07, val loss: 1.22005e-06, min loss: 6.01952e-07\n",
      "Epoch: 1696500, elapsed: 1.09e+01, train loss: 6.14459e-07, val loss: 1.23775e-06, min loss: 6.01952e-07\n",
      "Epoch: 1696600, elapsed: 1.08e+01, train loss: 6.95536e-07, val loss: 1.32015e-06, min loss: 6.01952e-07\n",
      "Epoch: 1696700, elapsed: 1.09e+01, train loss: 6.29739e-07, val loss: 1.26209e-06, min loss: 6.01952e-07\n",
      "Epoch: 1696800, elapsed: 1.10e+01, train loss: 6.07433e-07, val loss: 1.23060e-06, min loss: 6.01952e-07\n",
      "Epoch: 1696900, elapsed: 1.09e+01, train loss: 8.00750e-07, val loss: 1.37212e-06, min loss: 6.01952e-07\n",
      "Epoch: 1697000, elapsed: 1.08e+01, train loss: 8.84317e-07, val loss: 1.57262e-06, min loss: 6.01952e-07\n",
      "Epoch: 1697100, elapsed: 1.09e+01, train loss: 7.08772e-07, val loss: 1.30508e-06, min loss: 6.01952e-07\n",
      "Epoch: 1697200, elapsed: 1.08e+01, train loss: 1.11608e-06, val loss: 1.97449e-06, min loss: 6.01952e-07\n",
      "Epoch: 1697300, elapsed: 1.10e+01, train loss: 6.46938e-07, val loss: 1.28740e-06, min loss: 6.01952e-07\n",
      "Epoch: 1697400, elapsed: 1.08e+01, train loss: 6.12179e-07, val loss: 1.22352e-06, min loss: 6.01952e-07\n",
      "Epoch: 1697500, elapsed: 1.09e+01, train loss: 1.00777e-06, val loss: 1.69188e-06, min loss: 6.01952e-07\n",
      "Epoch: 1697600, elapsed: 1.08e+01, train loss: 6.70978e-07, val loss: 1.34825e-06, min loss: 6.01952e-07\n",
      "Epoch: 1697700, elapsed: 1.11e+01, train loss: 6.85048e-07, val loss: 1.33712e-06, min loss: 6.01952e-07\n",
      "Epoch: 1697800, elapsed: 1.08e+01, train loss: 6.23407e-07, val loss: 1.25643e-06, min loss: 6.01952e-07\n",
      "Epoch: 1697900, elapsed: 1.07e+01, train loss: 7.76916e-07, val loss: 1.32342e-06, min loss: 6.01952e-07\n",
      "Epoch: 1698000, elapsed: 1.07e+01, train loss: 7.17931e-07, val loss: 1.30905e-06, min loss: 6.01952e-07\n",
      "Epoch: 1698100, elapsed: 1.09e+01, train loss: 6.56418e-07, val loss: 1.28871e-06, min loss: 6.01952e-07\n",
      "Epoch: 1698200, elapsed: 1.08e+01, train loss: 6.01037e-07, val loss: 1.20694e-06, min loss: 6.01037e-07\n",
      "Epoch: 1698300, elapsed: 1.10e+01, train loss: 6.00252e-07, val loss: 1.21620e-06, min loss: 6.00252e-07\n",
      "Epoch: 1698400, elapsed: 1.07e+01, train loss: 6.76583e-07, val loss: 1.36463e-06, min loss: 6.00252e-07\n",
      "Epoch: 1698500, elapsed: 1.08e+01, train loss: 7.07093e-07, val loss: 1.35474e-06, min loss: 6.00252e-07\n",
      "Epoch: 1698600, elapsed: 1.10e+01, train loss: 5.01571e-06, val loss: 4.38524e-06, min loss: 6.00252e-07\n",
      "Epoch: 1698700, elapsed: 1.09e+01, train loss: 5.99803e-07, val loss: 1.20954e-06, min loss: 5.99803e-07\n",
      "Epoch: 1698800, elapsed: 1.57e+01, train loss: 6.71934e-07, val loss: 1.24020e-06, min loss: 5.99803e-07\n",
      "Epoch: 1698900, elapsed: 1.10e+01, train loss: 6.90129e-07, val loss: 1.34507e-06, min loss: 5.99803e-07\n",
      "Epoch: 1699000, elapsed: 1.10e+01, train loss: 9.35759e-07, val loss: 1.41465e-06, min loss: 5.99803e-07\n",
      "Epoch: 1699100, elapsed: 1.09e+01, train loss: 1.32478e-06, val loss: 1.84200e-06, min loss: 5.99803e-07\n",
      "Epoch: 1699200, elapsed: 1.11e+01, train loss: 7.10725e-07, val loss: 1.55633e-06, min loss: 5.99803e-07\n",
      "Epoch: 1699300, elapsed: 1.09e+01, train loss: 6.86544e-07, val loss: 1.32098e-06, min loss: 5.99803e-07\n",
      "Epoch: 1699400, elapsed: 1.08e+01, train loss: 6.45272e-07, val loss: 1.22664e-06, min loss: 5.99803e-07\n",
      "Epoch: 1699500, elapsed: 1.11e+01, train loss: 6.89955e-07, val loss: 1.40810e-06, min loss: 5.99803e-07\n",
      "Epoch: 1699600, elapsed: 1.09e+01, train loss: 6.54237e-07, val loss: 1.24470e-06, min loss: 5.99803e-07\n",
      "Epoch: 1699700, elapsed: 1.08e+01, train loss: 6.26096e-07, val loss: 1.25319e-06, min loss: 5.99803e-07\n",
      "Epoch: 1699800, elapsed: 1.10e+01, train loss: 6.13793e-07, val loss: 1.23034e-06, min loss: 5.99803e-07\n",
      "Epoch: 1699900, elapsed: 1.09e+01, train loss: 6.00678e-07, val loss: 1.20634e-06, min loss: 5.99803e-07\n",
      "Epoch: 1700000, elapsed: 1.12e+01, train loss: 6.01822e-07, val loss: 1.20975e-06, min loss: 5.99803e-07\n",
      "Epoch: 1700100, elapsed: 1.67e+01, train loss: 7.22004e-07, val loss: 1.28582e-06, min loss: 5.99803e-07\n",
      "Epoch: 1700200, elapsed: 1.11e+01, train loss: 7.74790e-07, val loss: 1.36267e-06, min loss: 5.99803e-07\n",
      "Epoch: 1700300, elapsed: 1.09e+01, train loss: 8.63080e-07, val loss: 1.53270e-06, min loss: 5.99803e-07\n",
      "Epoch: 1700400, elapsed: 1.08e+01, train loss: 6.06183e-07, val loss: 1.22914e-06, min loss: 5.99803e-07\n",
      "Epoch: 1700500, elapsed: 1.10e+01, train loss: 5.98907e-07, val loss: 1.20964e-06, min loss: 5.98907e-07\n",
      "Epoch: 1700600, elapsed: 1.10e+01, train loss: 6.26908e-07, val loss: 1.24966e-06, min loss: 5.98907e-07\n",
      "Epoch: 1700700, elapsed: 1.11e+01, train loss: 6.13457e-07, val loss: 1.20987e-06, min loss: 5.98907e-07\n",
      "Epoch: 1700800, elapsed: 1.08e+01, train loss: 6.24158e-07, val loss: 1.26483e-06, min loss: 5.98907e-07\n",
      "Epoch: 1700900, elapsed: 1.08e+01, train loss: 7.20934e-07, val loss: 1.47159e-06, min loss: 5.98907e-07\n",
      "Epoch: 1701000, elapsed: 1.09e+01, train loss: 6.01293e-07, val loss: 1.20576e-06, min loss: 5.98907e-07\n",
      "Epoch: 1701100, elapsed: 1.08e+01, train loss: 6.04848e-07, val loss: 1.20955e-06, min loss: 5.98907e-07\n",
      "Epoch: 1701200, elapsed: 1.09e+01, train loss: 6.47397e-07, val loss: 1.25756e-06, min loss: 5.98907e-07\n",
      "Epoch: 1701300, elapsed: 1.10e+01, train loss: 6.03220e-07, val loss: 1.21594e-06, min loss: 5.98907e-07\n",
      "Epoch: 1701400, elapsed: 1.09e+01, train loss: 1.21430e-06, val loss: 1.83572e-06, min loss: 5.98907e-07\n",
      "Epoch: 1701500, elapsed: 1.09e+01, train loss: 7.72708e-07, val loss: 1.31220e-06, min loss: 5.98907e-07\n",
      "Epoch: 1701600, elapsed: 1.12e+01, train loss: 5.98559e-07, val loss: 1.20034e-06, min loss: 5.98559e-07\n",
      "Epoch: 1701700, elapsed: 1.15e+01, train loss: 1.31668e-06, val loss: 2.29124e-06, min loss: 5.98559e-07\n",
      "Epoch: 1701800, elapsed: 1.67e+01, train loss: 5.97998e-07, val loss: 1.20760e-06, min loss: 5.97998e-07\n",
      "Epoch: 1701900, elapsed: 1.17e+01, train loss: 7.01739e-07, val loss: 1.30367e-06, min loss: 5.97998e-07\n",
      "Epoch: 1702000, elapsed: 1.16e+01, train loss: 6.83630e-07, val loss: 1.37661e-06, min loss: 5.97998e-07\n",
      "Epoch: 1702100, elapsed: 1.19e+01, train loss: 8.07038e-07, val loss: 1.35675e-06, min loss: 5.97998e-07\n",
      "Epoch: 1702200, elapsed: 1.18e+01, train loss: 6.30560e-07, val loss: 1.26193e-06, min loss: 5.97998e-07\n",
      "Epoch: 1702300, elapsed: 1.16e+01, train loss: 6.71055e-07, val loss: 1.27611e-06, min loss: 5.97998e-07\n",
      "Epoch: 1702400, elapsed: 1.17e+01, train loss: 6.09268e-07, val loss: 1.21424e-06, min loss: 5.97998e-07\n",
      "Epoch: 1702500, elapsed: 1.16e+01, train loss: 6.58248e-07, val loss: 1.28168e-06, min loss: 5.97998e-07\n",
      "Epoch: 1702600, elapsed: 1.15e+01, train loss: 6.14954e-07, val loss: 1.23802e-06, min loss: 5.97998e-07\n",
      "Epoch: 1702700, elapsed: 1.13e+01, train loss: 6.03672e-07, val loss: 1.22560e-06, min loss: 5.97998e-07\n",
      "Epoch: 1702800, elapsed: 1.16e+01, train loss: 5.98430e-07, val loss: 1.20997e-06, min loss: 5.97998e-07\n",
      "Epoch: 1702900, elapsed: 1.13e+01, train loss: 5.98746e-07, val loss: 1.21489e-06, min loss: 5.97998e-07\n",
      "Epoch: 1703000, elapsed: 1.14e+01, train loss: 6.84220e-07, val loss: 1.36319e-06, min loss: 5.97998e-07\n",
      "Epoch: 1703100, elapsed: 1.14e+01, train loss: 6.38388e-07, val loss: 1.21671e-06, min loss: 5.97998e-07\n",
      "Epoch: 1703200, elapsed: 1.14e+01, train loss: 8.90123e-07, val loss: 1.50797e-06, min loss: 5.97998e-07\n",
      "Epoch: 1703300, elapsed: 1.16e+01, train loss: 7.41553e-07, val loss: 1.31838e-06, min loss: 5.97998e-07\n",
      "Epoch: 1703400, elapsed: 1.12e+01, train loss: 3.06368e-06, val loss: 3.14324e-06, min loss: 5.97998e-07\n",
      "Epoch: 1703500, elapsed: 1.15e+01, train loss: 6.50651e-07, val loss: 1.25679e-06, min loss: 5.97998e-07\n",
      "Epoch: 1703600, elapsed: 1.14e+01, train loss: 5.97601e-07, val loss: 1.21319e-06, min loss: 5.97601e-07\n",
      "Epoch: 1703700, elapsed: 1.14e+01, train loss: 6.00239e-07, val loss: 1.21915e-06, min loss: 5.97601e-07\n",
      "Epoch: 1703800, elapsed: 1.09e+01, train loss: 6.69983e-07, val loss: 1.33164e-06, min loss: 5.97601e-07\n",
      "Epoch: 1703900, elapsed: 1.14e+01, train loss: 7.51319e-07, val loss: 1.28504e-06, min loss: 5.97601e-07\n",
      "Epoch: 1704000, elapsed: 1.14e+01, train loss: 1.72369e-06, val loss: 1.85082e-06, min loss: 5.97601e-07\n",
      "Epoch: 1704100, elapsed: 1.12e+01, train loss: 1.12946e-06, val loss: 1.89804e-06, min loss: 5.97601e-07\n",
      "Epoch: 1704200, elapsed: 1.13e+01, train loss: 9.44975e-07, val loss: 1.63569e-06, min loss: 5.97601e-07\n",
      "Epoch: 1704300, elapsed: 1.14e+01, train loss: 6.27684e-07, val loss: 1.29542e-06, min loss: 5.97601e-07\n",
      "Epoch: 1704400, elapsed: 1.14e+01, train loss: 6.22964e-07, val loss: 1.30046e-06, min loss: 5.97601e-07\n",
      "Epoch: 1704500, elapsed: 1.11e+01, train loss: 1.51764e-06, val loss: 2.38996e-06, min loss: 5.97601e-07\n",
      "Epoch: 1704600, elapsed: 1.14e+01, train loss: 6.04354e-07, val loss: 1.22417e-06, min loss: 5.97601e-07\n",
      "Epoch: 1704700, elapsed: 1.12e+01, train loss: 8.61196e-07, val loss: 1.56811e-06, min loss: 5.97601e-07\n",
      "Epoch: 1704800, elapsed: 1.14e+01, train loss: 6.48123e-07, val loss: 1.31430e-06, min loss: 5.97601e-07\n",
      "Epoch: 1704900, elapsed: 1.63e+01, train loss: 8.04264e-07, val loss: 1.40138e-06, min loss: 5.97601e-07\n",
      "Epoch: 1705000, elapsed: 1.16e+01, train loss: 1.60021e-06, val loss: 1.80531e-06, min loss: 5.97601e-07\n",
      "Epoch: 1705100, elapsed: 1.35e+01, train loss: 9.01432e-07, val loss: 1.48019e-06, min loss: 5.97601e-07\n",
      "Epoch: 1705200, elapsed: 1.16e+01, train loss: 9.05079e-07, val loss: 1.50452e-06, min loss: 5.97601e-07\n",
      "Epoch: 1705300, elapsed: 1.14e+01, train loss: 2.83832e-06, val loss: 3.93700e-06, min loss: 5.97601e-07\n",
      "Epoch: 1705400, elapsed: 1.17e+01, train loss: 5.96225e-07, val loss: 1.20686e-06, min loss: 5.96225e-07\n",
      "Epoch: 1705500, elapsed: 1.13e+01, train loss: 7.07679e-07, val loss: 1.30179e-06, min loss: 5.96225e-07\n",
      "Epoch: 1705600, elapsed: 1.13e+01, train loss: 5.95928e-07, val loss: 1.20607e-06, min loss: 5.95928e-07\n",
      "Epoch: 1705700, elapsed: 1.13e+01, train loss: 7.21569e-07, val loss: 1.29906e-06, min loss: 5.95928e-07\n",
      "Epoch: 1705800, elapsed: 1.09e+01, train loss: 5.95779e-07, val loss: 1.20656e-06, min loss: 5.95779e-07\n",
      "Epoch: 1705900, elapsed: 1.09e+01, train loss: 6.00120e-07, val loss: 1.20882e-06, min loss: 5.95779e-07\n",
      "Epoch: 1706000, elapsed: 1.08e+01, train loss: 6.69823e-07, val loss: 1.25634e-06, min loss: 5.95779e-07\n",
      "Epoch: 1706100, elapsed: 1.07e+01, train loss: 6.06295e-07, val loss: 1.20164e-06, min loss: 5.95779e-07\n",
      "Epoch: 1706200, elapsed: 1.09e+01, train loss: 6.11973e-07, val loss: 1.23486e-06, min loss: 5.95779e-07\n",
      "Epoch: 1706300, elapsed: 1.08e+01, train loss: 1.27223e-06, val loss: 2.22577e-06, min loss: 5.95779e-07\n",
      "Epoch: 1706400, elapsed: 1.10e+01, train loss: 5.95829e-07, val loss: 1.20874e-06, min loss: 5.95779e-07\n",
      "Epoch: 1706500, elapsed: 1.10e+01, train loss: 7.52257e-07, val loss: 1.43872e-06, min loss: 5.95779e-07\n",
      "Epoch: 1706600, elapsed: 1.08e+01, train loss: 8.10810e-07, val loss: 1.43414e-06, min loss: 5.95779e-07\n",
      "Epoch: 1706700, elapsed: 1.08e+01, train loss: 5.95374e-07, val loss: 1.20380e-06, min loss: 5.95374e-07\n",
      "Epoch: 1706800, elapsed: 1.09e+01, train loss: 6.72520e-07, val loss: 1.28195e-06, min loss: 5.95374e-07\n",
      "Epoch: 1706900, elapsed: 1.11e+01, train loss: 5.95228e-07, val loss: 1.20700e-06, min loss: 5.95228e-07\n",
      "Epoch: 1707000, elapsed: 1.07e+01, train loss: 5.99924e-07, val loss: 1.20627e-06, min loss: 5.95228e-07\n",
      "Epoch: 1707100, elapsed: 1.08e+01, train loss: 6.72665e-07, val loss: 1.24476e-06, min loss: 5.95228e-07\n",
      "Epoch: 1707200, elapsed: 1.07e+01, train loss: 5.95018e-07, val loss: 1.20569e-06, min loss: 5.95018e-07\n",
      "Epoch: 1707300, elapsed: 1.09e+01, train loss: 5.99317e-07, val loss: 1.20104e-06, min loss: 5.95018e-07\n",
      "Epoch: 1707400, elapsed: 1.09e+01, train loss: 5.97443e-07, val loss: 1.20850e-06, min loss: 5.95018e-07\n",
      "Epoch: 1707500, elapsed: 1.08e+01, train loss: 6.44444e-07, val loss: 1.22526e-06, min loss: 5.95018e-07\n",
      "Epoch: 1707600, elapsed: 1.06e+01, train loss: 7.77586e-07, val loss: 1.43230e-06, min loss: 5.95018e-07\n",
      "Epoch: 1707700, elapsed: 1.08e+01, train loss: 7.47284e-07, val loss: 1.32933e-06, min loss: 5.95018e-07\n",
      "Epoch: 1707800, elapsed: 1.09e+01, train loss: 6.61141e-07, val loss: 1.27061e-06, min loss: 5.95018e-07\n",
      "Epoch: 1707900, elapsed: 1.56e+01, train loss: 6.11741e-07, val loss: 1.22747e-06, min loss: 5.95018e-07\n",
      "Epoch: 1708000, elapsed: 1.13e+01, train loss: 6.18098e-07, val loss: 1.25694e-06, min loss: 5.95018e-07\n",
      "Epoch: 1708100, elapsed: 1.10e+01, train loss: 5.99733e-07, val loss: 1.21772e-06, min loss: 5.95018e-07\n",
      "Epoch: 1708200, elapsed: 1.12e+01, train loss: 5.94997e-07, val loss: 1.20080e-06, min loss: 5.94997e-07\n",
      "Epoch: 1708300, elapsed: 1.10e+01, train loss: 6.20956e-07, val loss: 1.20495e-06, min loss: 5.94997e-07\n",
      "Epoch: 1708400, elapsed: 1.11e+01, train loss: 2.69779e-06, val loss: 2.39649e-06, min loss: 5.94997e-07\n",
      "Epoch: 1708500, elapsed: 1.10e+01, train loss: 7.07088e-07, val loss: 1.35161e-06, min loss: 5.94997e-07\n",
      "Epoch: 1708600, elapsed: 1.10e+01, train loss: 8.69875e-07, val loss: 1.53617e-06, min loss: 5.94997e-07\n",
      "Epoch: 1708700, elapsed: 1.09e+01, train loss: 5.94347e-07, val loss: 1.20415e-06, min loss: 5.94347e-07\n",
      "Epoch: 1708800, elapsed: 1.11e+01, train loss: 6.48448e-07, val loss: 1.23627e-06, min loss: 5.94347e-07\n",
      "Epoch: 1708900, elapsed: 1.10e+01, train loss: 6.02327e-07, val loss: 1.20456e-06, min loss: 5.94347e-07\n",
      "Epoch: 1709000, elapsed: 1.10e+01, train loss: 7.08187e-07, val loss: 1.40851e-06, min loss: 5.94347e-07\n",
      "Epoch: 1709100, elapsed: 1.08e+01, train loss: 1.12305e-06, val loss: 1.51834e-06, min loss: 5.94347e-07\n",
      "Epoch: 1709200, elapsed: 1.11e+01, train loss: 1.29903e-06, val loss: 2.00587e-06, min loss: 5.94347e-07\n",
      "Epoch: 1709300, elapsed: 1.09e+01, train loss: 7.69822e-07, val loss: 1.28042e-06, min loss: 5.94347e-07\n",
      "Epoch: 1709400, elapsed: 1.10e+01, train loss: 5.98486e-07, val loss: 1.22394e-06, min loss: 5.94347e-07\n",
      "Epoch: 1709500, elapsed: 1.10e+01, train loss: 6.78400e-07, val loss: 1.21057e-06, min loss: 5.94347e-07\n",
      "Epoch: 1709600, elapsed: 1.09e+01, train loss: 6.47225e-07, val loss: 1.19085e-06, min loss: 5.94347e-07\n",
      "Epoch: 1709700, elapsed: 1.10e+01, train loss: 6.26178e-07, val loss: 1.23005e-06, min loss: 5.94347e-07\n",
      "Epoch: 1709800, elapsed: 1.09e+01, train loss: 7.35375e-07, val loss: 1.38655e-06, min loss: 5.94347e-07\n",
      "Epoch: 1709900, elapsed: 1.09e+01, train loss: 6.36513e-07, val loss: 1.24545e-06, min loss: 5.94347e-07\n",
      "Epoch: 1710000, elapsed: 1.09e+01, train loss: 9.38369e-07, val loss: 1.54005e-06, min loss: 5.94347e-07\n",
      "Epoch: 1710100, elapsed: 1.29e+01, train loss: 1.29815e-06, val loss: 1.93416e-06, min loss: 5.94347e-07\n",
      "Epoch: 1710200, elapsed: 1.07e+01, train loss: 1.74002e-06, val loss: 2.40900e-06, min loss: 5.94347e-07\n",
      "Epoch: 1710300, elapsed: 1.08e+01, train loss: 1.00769e-06, val loss: 1.59457e-06, min loss: 5.94347e-07\n",
      "Epoch: 1710400, elapsed: 1.08e+01, train loss: 6.03512e-07, val loss: 1.22245e-06, min loss: 5.94347e-07\n",
      "Epoch: 1710500, elapsed: 1.08e+01, train loss: 6.24766e-07, val loss: 1.25267e-06, min loss: 5.94347e-07\n",
      "Epoch: 1710600, elapsed: 1.07e+01, train loss: 5.96697e-07, val loss: 1.21667e-06, min loss: 5.94347e-07\n",
      "Epoch: 1710700, elapsed: 1.10e+01, train loss: 6.17364e-07, val loss: 1.25471e-06, min loss: 5.94347e-07\n",
      "Epoch: 1710800, elapsed: 1.08e+01, train loss: 6.26935e-07, val loss: 1.28548e-06, min loss: 5.94347e-07\n",
      "Epoch: 1710900, elapsed: 1.07e+01, train loss: 7.59936e-07, val loss: 1.53733e-06, min loss: 5.94347e-07\n",
      "Epoch: 1711000, elapsed: 1.57e+01, train loss: 5.93169e-07, val loss: 1.20285e-06, min loss: 5.93169e-07\n",
      "Epoch: 1711100, elapsed: 1.10e+01, train loss: 6.28770e-07, val loss: 1.26250e-06, min loss: 5.93169e-07\n",
      "Epoch: 1711200, elapsed: 1.11e+01, train loss: 5.95642e-07, val loss: 1.23151e-06, min loss: 5.93169e-07\n",
      "Epoch: 1711300, elapsed: 1.10e+01, train loss: 6.31436e-07, val loss: 1.30255e-06, min loss: 5.93169e-07\n",
      "Epoch: 1711400, elapsed: 1.09e+01, train loss: 1.45431e-06, val loss: 1.92336e-06, min loss: 5.93169e-07\n",
      "Epoch: 1711500, elapsed: 1.08e+01, train loss: 5.98966e-07, val loss: 1.20114e-06, min loss: 5.93169e-07\n",
      "Epoch: 1711600, elapsed: 1.12e+01, train loss: 5.93440e-07, val loss: 1.20409e-06, min loss: 5.93169e-07\n",
      "Epoch: 1711700, elapsed: 1.09e+01, train loss: 5.94578e-07, val loss: 1.20782e-06, min loss: 5.93169e-07\n",
      "Epoch: 1711800, elapsed: 1.09e+01, train loss: 7.49382e-07, val loss: 1.35823e-06, min loss: 5.93169e-07\n",
      "Epoch: 1711900, elapsed: 1.10e+01, train loss: 5.97497e-07, val loss: 1.19952e-06, min loss: 5.93169e-07\n",
      "Epoch: 1712000, elapsed: 1.10e+01, train loss: 6.45507e-07, val loss: 1.23092e-06, min loss: 5.93169e-07\n",
      "Epoch: 1712100, elapsed: 1.09e+01, train loss: 6.16063e-07, val loss: 1.20839e-06, min loss: 5.93169e-07\n",
      "Epoch: 1712200, elapsed: 1.08e+01, train loss: 5.99368e-07, val loss: 1.19548e-06, min loss: 5.93169e-07\n",
      "Epoch: 1712300, elapsed: 1.11e+01, train loss: 6.04996e-07, val loss: 1.20298e-06, min loss: 5.93169e-07\n",
      "Epoch: 1712400, elapsed: 1.07e+01, train loss: 5.99664e-07, val loss: 1.19268e-06, min loss: 5.93169e-07\n",
      "Epoch: 1712500, elapsed: 1.08e+01, train loss: 6.06402e-07, val loss: 1.21620e-06, min loss: 5.93169e-07\n",
      "Epoch: 1712600, elapsed: 1.09e+01, train loss: 7.92189e-07, val loss: 1.32967e-06, min loss: 5.93169e-07\n",
      "Epoch: 1712700, elapsed: 1.06e+01, train loss: 1.02653e-06, val loss: 1.33357e-06, min loss: 5.93169e-07\n",
      "Epoch: 1712800, elapsed: 1.10e+01, train loss: 5.93033e-07, val loss: 1.20736e-06, min loss: 5.93033e-07\n",
      "Epoch: 1712900, elapsed: 1.09e+01, train loss: 5.93619e-07, val loss: 1.19829e-06, min loss: 5.93033e-07\n",
      "Epoch: 1713000, elapsed: 1.11e+01, train loss: 6.02106e-07, val loss: 1.22129e-06, min loss: 5.93033e-07\n",
      "Epoch: 1713100, elapsed: 1.09e+01, train loss: 9.18067e-07, val loss: 1.60347e-06, min loss: 5.93033e-07\n",
      "Epoch: 1713200, elapsed: 1.08e+01, train loss: 5.92242e-07, val loss: 1.19993e-06, min loss: 5.92242e-07\n",
      "Epoch: 1713300, elapsed: 1.10e+01, train loss: 7.29959e-07, val loss: 1.35506e-06, min loss: 5.92242e-07\n",
      "Epoch: 1713400, elapsed: 1.08e+01, train loss: 5.92436e-07, val loss: 1.20672e-06, min loss: 5.92242e-07\n",
      "Epoch: 1713500, elapsed: 1.10e+01, train loss: 5.93766e-07, val loss: 1.20409e-06, min loss: 5.92242e-07\n",
      "Epoch: 1713600, elapsed: 1.08e+01, train loss: 6.00583e-07, val loss: 1.22083e-06, min loss: 5.92242e-07\n",
      "Epoch: 1713700, elapsed: 1.08e+01, train loss: 6.15231e-07, val loss: 1.24738e-06, min loss: 5.92242e-07\n",
      "Epoch: 1713800, elapsed: 1.08e+01, train loss: 6.27452e-07, val loss: 1.20486e-06, min loss: 5.92242e-07\n",
      "Epoch: 1713900, elapsed: 1.10e+01, train loss: 7.53586e-07, val loss: 1.36662e-06, min loss: 5.92242e-07\n",
      "Epoch: 1714000, elapsed: 1.09e+01, train loss: 6.06623e-07, val loss: 1.19886e-06, min loss: 5.92242e-07\n",
      "Epoch: 1714100, elapsed: 1.59e+01, train loss: 6.00769e-07, val loss: 1.21805e-06, min loss: 5.92242e-07\n",
      "Epoch: 1714200, elapsed: 1.11e+01, train loss: 6.03654e-07, val loss: 1.21418e-06, min loss: 5.92242e-07\n",
      "Epoch: 1714300, elapsed: 1.10e+01, train loss: 1.15416e-06, val loss: 1.53406e-06, min loss: 5.92242e-07\n",
      "Epoch: 1714400, elapsed: 1.08e+01, train loss: 9.72671e-07, val loss: 1.42758e-06, min loss: 5.92242e-07\n",
      "Epoch: 1714500, elapsed: 1.11e+01, train loss: 7.87103e-07, val loss: 1.31892e-06, min loss: 5.92242e-07\n",
      "Epoch: 1714600, elapsed: 1.09e+01, train loss: 5.93075e-07, val loss: 1.19521e-06, min loss: 5.92242e-07\n",
      "Epoch: 1714700, elapsed: 1.11e+01, train loss: 5.91750e-07, val loss: 1.20479e-06, min loss: 5.91750e-07\n",
      "Epoch: 1714800, elapsed: 1.09e+01, train loss: 9.99384e-07, val loss: 2.30009e-06, min loss: 5.91750e-07\n",
      "Epoch: 1714900, elapsed: 1.10e+01, train loss: 5.91065e-07, val loss: 1.20048e-06, min loss: 5.91065e-07\n",
      "Epoch: 1715000, elapsed: 1.10e+01, train loss: 8.66400e-07, val loss: 1.37665e-06, min loss: 5.91065e-07\n",
      "Epoch: 1715100, elapsed: 1.29e+01, train loss: 7.81364e-07, val loss: 1.33698e-06, min loss: 5.91065e-07\n",
      "Epoch: 1715200, elapsed: 1.09e+01, train loss: 5.90917e-07, val loss: 1.20257e-06, min loss: 5.90917e-07\n",
      "Epoch: 1715300, elapsed: 1.09e+01, train loss: 5.92314e-07, val loss: 1.20223e-06, min loss: 5.90917e-07\n",
      "Epoch: 1715400, elapsed: 1.10e+01, train loss: 1.62255e-06, val loss: 2.37743e-06, min loss: 5.90917e-07\n",
      "Epoch: 1715500, elapsed: 1.09e+01, train loss: 5.90871e-07, val loss: 1.19936e-06, min loss: 5.90871e-07\n",
      "Epoch: 1715600, elapsed: 1.11e+01, train loss: 5.96410e-07, val loss: 1.21825e-06, min loss: 5.90871e-07\n",
      "Epoch: 1715700, elapsed: 1.09e+01, train loss: 2.24439e-06, val loss: 2.28856e-06, min loss: 5.90871e-07\n",
      "Epoch: 1715800, elapsed: 1.08e+01, train loss: 5.90766e-07, val loss: 1.20237e-06, min loss: 5.90766e-07\n",
      "Epoch: 1715900, elapsed: 1.09e+01, train loss: 5.94744e-07, val loss: 1.20778e-06, min loss: 5.90766e-07\n",
      "Epoch: 1716000, elapsed: 1.09e+01, train loss: 1.42966e-06, val loss: 2.06836e-06, min loss: 5.90766e-07\n",
      "Epoch: 1716100, elapsed: 1.10e+01, train loss: 1.43862e-06, val loss: 2.38519e-06, min loss: 5.90766e-07\n",
      "Epoch: 1716200, elapsed: 1.07e+01, train loss: 5.90659e-07, val loss: 1.19975e-06, min loss: 5.90659e-07\n",
      "Epoch: 1716300, elapsed: 1.08e+01, train loss: 6.03405e-07, val loss: 1.21619e-06, min loss: 5.90659e-07\n",
      "Epoch: 1716400, elapsed: 1.09e+01, train loss: 6.32547e-07, val loss: 1.23054e-06, min loss: 5.90659e-07\n",
      "Epoch: 1716500, elapsed: 1.09e+01, train loss: 5.90744e-07, val loss: 1.20156e-06, min loss: 5.90659e-07\n",
      "Epoch: 1716600, elapsed: 1.07e+01, train loss: 6.09891e-07, val loss: 1.28627e-06, min loss: 5.90659e-07\n",
      "Epoch: 1716700, elapsed: 1.09e+01, train loss: 6.50645e-07, val loss: 1.22443e-06, min loss: 5.90659e-07\n",
      "Epoch: 1716800, elapsed: 1.09e+01, train loss: 5.93646e-07, val loss: 1.19279e-06, min loss: 5.90659e-07\n",
      "Epoch: 1716900, elapsed: 1.09e+01, train loss: 5.95300e-07, val loss: 1.20690e-06, min loss: 5.90659e-07\n",
      "Epoch: 1717000, elapsed: 1.09e+01, train loss: 7.55472e-07, val loss: 1.42038e-06, min loss: 5.90659e-07\n",
      "Epoch: 1717100, elapsed: 1.53e+01, train loss: 5.98948e-07, val loss: 1.21679e-06, min loss: 5.90659e-07\n",
      "Epoch: 1717200, elapsed: 1.11e+01, train loss: 5.90342e-07, val loss: 1.20660e-06, min loss: 5.90342e-07\n",
      "Epoch: 1717300, elapsed: 1.10e+01, train loss: 5.94601e-07, val loss: 1.18872e-06, min loss: 5.90342e-07\n",
      "Epoch: 1717400, elapsed: 1.11e+01, train loss: 2.33699e-06, val loss: 2.29337e-06, min loss: 5.90342e-07\n",
      "Epoch: 1717500, elapsed: 1.08e+01, train loss: 5.89700e-07, val loss: 1.19974e-06, min loss: 5.89700e-07\n",
      "Epoch: 1717600, elapsed: 1.09e+01, train loss: 6.69012e-07, val loss: 1.24159e-06, min loss: 5.89700e-07\n",
      "Epoch: 1717700, elapsed: 1.11e+01, train loss: 6.03478e-07, val loss: 1.20433e-06, min loss: 5.89700e-07\n",
      "Epoch: 1717800, elapsed: 1.09e+01, train loss: 5.94749e-07, val loss: 1.21241e-06, min loss: 5.89700e-07\n",
      "Epoch: 1717900, elapsed: 1.11e+01, train loss: 1.62395e-06, val loss: 1.57228e-06, min loss: 5.89700e-07\n",
      "Epoch: 1718000, elapsed: 1.09e+01, train loss: 5.90578e-07, val loss: 1.19820e-06, min loss: 5.89700e-07\n",
      "Epoch: 1718100, elapsed: 1.10e+01, train loss: 6.11592e-07, val loss: 1.17667e-06, min loss: 5.89700e-07\n",
      "Epoch: 1718200, elapsed: 1.09e+01, train loss: 6.23336e-07, val loss: 1.32170e-06, min loss: 5.89700e-07\n",
      "Epoch: 1718300, elapsed: 1.09e+01, train loss: 9.73076e-07, val loss: 1.63065e-06, min loss: 5.89700e-07\n",
      "Epoch: 1718400, elapsed: 1.08e+01, train loss: 6.77582e-07, val loss: 1.21610e-06, min loss: 5.89700e-07\n",
      "Epoch: 1718500, elapsed: 1.09e+01, train loss: 7.89669e-07, val loss: 1.39863e-06, min loss: 5.89700e-07\n",
      "Epoch: 1718600, elapsed: 1.08e+01, train loss: 6.50618e-07, val loss: 1.22428e-06, min loss: 5.89700e-07\n",
      "Epoch: 1718700, elapsed: 1.10e+01, train loss: 8.84550e-07, val loss: 1.59332e-06, min loss: 5.89700e-07\n",
      "Epoch: 1718800, elapsed: 1.08e+01, train loss: 6.36524e-07, val loss: 1.23911e-06, min loss: 5.89700e-07\n",
      "Epoch: 1718900, elapsed: 1.07e+01, train loss: 5.94613e-07, val loss: 1.20293e-06, min loss: 5.89700e-07\n",
      "Epoch: 1719000, elapsed: 1.10e+01, train loss: 7.82020e-07, val loss: 1.37526e-06, min loss: 5.89700e-07\n",
      "Epoch: 1719100, elapsed: 1.08e+01, train loss: 5.96785e-07, val loss: 1.23181e-06, min loss: 5.89700e-07\n",
      "Epoch: 1719200, elapsed: 1.07e+01, train loss: 6.65206e-07, val loss: 1.31042e-06, min loss: 5.89700e-07\n",
      "Epoch: 1719300, elapsed: 1.05e+01, train loss: 6.09881e-07, val loss: 1.23066e-06, min loss: 5.89700e-07\n",
      "Epoch: 1719400, elapsed: 1.08e+01, train loss: 1.01560e-06, val loss: 1.61204e-06, min loss: 5.89700e-07\n",
      "Epoch: 1719500, elapsed: 1.07e+01, train loss: 6.61806e-07, val loss: 1.30770e-06, min loss: 5.89700e-07\n",
      "Epoch: 1719600, elapsed: 1.07e+01, train loss: 6.01945e-07, val loss: 1.21982e-06, min loss: 5.89700e-07\n",
      "Epoch: 1719700, elapsed: 1.07e+01, train loss: 6.01828e-07, val loss: 1.18742e-06, min loss: 5.89700e-07\n",
      "Epoch: 1719800, elapsed: 1.08e+01, train loss: 6.08645e-07, val loss: 1.23272e-06, min loss: 5.89700e-07\n",
      "Epoch: 1719900, elapsed: 1.07e+01, train loss: 6.32322e-07, val loss: 1.25440e-06, min loss: 5.89700e-07\n",
      "Epoch: 1720000, elapsed: 1.08e+01, train loss: 8.67623e-07, val loss: 1.42508e-06, min loss: 5.89700e-07\n",
      "Epoch: 1720100, elapsed: 1.25e+01, train loss: 5.94532e-07, val loss: 1.22993e-06, min loss: 5.89700e-07\n",
      "Epoch: 1720200, elapsed: 1.58e+01, train loss: 5.88555e-07, val loss: 1.19540e-06, min loss: 5.88555e-07\n",
      "Epoch: 1720300, elapsed: 1.12e+01, train loss: 7.03160e-07, val loss: 1.28556e-06, min loss: 5.88555e-07\n",
      "Epoch: 1720400, elapsed: 1.10e+01, train loss: 6.56160e-07, val loss: 1.21468e-06, min loss: 5.88555e-07\n",
      "Epoch: 1720500, elapsed: 1.11e+01, train loss: 8.77610e-07, val loss: 1.65689e-06, min loss: 5.88555e-07\n",
      "Epoch: 1720600, elapsed: 1.10e+01, train loss: 6.07443e-07, val loss: 1.20902e-06, min loss: 5.88555e-07\n",
      "Epoch: 1720700, elapsed: 1.09e+01, train loss: 5.92195e-07, val loss: 1.19369e-06, min loss: 5.88555e-07\n",
      "Epoch: 1720800, elapsed: 1.09e+01, train loss: 5.89227e-07, val loss: 1.20994e-06, min loss: 5.88555e-07\n",
      "Epoch: 1720900, elapsed: 1.09e+01, train loss: 1.15471e-06, val loss: 1.87729e-06, min loss: 5.88555e-07\n",
      "Epoch: 1721000, elapsed: 1.09e+01, train loss: 5.87973e-07, val loss: 1.19826e-06, min loss: 5.87973e-07\n",
      "Epoch: 1721100, elapsed: 1.08e+01, train loss: 6.50743e-07, val loss: 1.35066e-06, min loss: 5.87973e-07\n",
      "Epoch: 1721200, elapsed: 1.10e+01, train loss: 5.87905e-07, val loss: 1.19867e-06, min loss: 5.87905e-07\n",
      "Epoch: 1721300, elapsed: 1.08e+01, train loss: 6.28431e-07, val loss: 1.25419e-06, min loss: 5.87905e-07\n",
      "Epoch: 1721400, elapsed: 1.11e+01, train loss: 6.01133e-07, val loss: 1.19486e-06, min loss: 5.87905e-07\n",
      "Epoch: 1721500, elapsed: 1.09e+01, train loss: 5.90601e-07, val loss: 1.19672e-06, min loss: 5.87905e-07\n",
      "Epoch: 1721600, elapsed: 1.11e+01, train loss: 5.94636e-07, val loss: 1.21681e-06, min loss: 5.87905e-07\n",
      "Epoch: 1721700, elapsed: 1.11e+01, train loss: 1.77566e-06, val loss: 2.07804e-06, min loss: 5.87905e-07\n",
      "Epoch: 1721800, elapsed: 1.11e+01, train loss: 5.87698e-07, val loss: 1.19628e-06, min loss: 5.87698e-07\n",
      "Epoch: 1721900, elapsed: 1.12e+01, train loss: 5.90047e-07, val loss: 1.20438e-06, min loss: 5.87698e-07\n",
      "Epoch: 1722000, elapsed: 1.13e+01, train loss: 6.60034e-07, val loss: 1.30125e-06, min loss: 5.87698e-07\n",
      "Epoch: 1722100, elapsed: 1.10e+01, train loss: 5.88277e-07, val loss: 1.19669e-06, min loss: 5.87698e-07\n",
      "Epoch: 1722200, elapsed: 1.08e+01, train loss: 6.95583e-07, val loss: 1.28356e-06, min loss: 5.87698e-07\n",
      "Epoch: 1722300, elapsed: 1.09e+01, train loss: 5.87769e-07, val loss: 1.20186e-06, min loss: 5.87698e-07\n",
      "Epoch: 1722400, elapsed: 1.09e+01, train loss: 6.80023e-07, val loss: 1.38304e-06, min loss: 5.87698e-07\n",
      "Epoch: 1722500, elapsed: 1.08e+01, train loss: 5.88127e-07, val loss: 1.19860e-06, min loss: 5.87698e-07\n",
      "Epoch: 1722600, elapsed: 1.07e+01, train loss: 5.99600e-07, val loss: 1.20679e-06, min loss: 5.87698e-07\n",
      "Epoch: 1722700, elapsed: 1.09e+01, train loss: 5.87016e-07, val loss: 1.19882e-06, min loss: 5.87016e-07\n",
      "Epoch: 1722800, elapsed: 1.09e+01, train loss: 5.90296e-07, val loss: 1.21293e-06, min loss: 5.87016e-07\n",
      "Epoch: 1722900, elapsed: 1.10e+01, train loss: 5.94330e-07, val loss: 1.18379e-06, min loss: 5.87016e-07\n",
      "Epoch: 1723000, elapsed: 1.06e+01, train loss: 6.25412e-07, val loss: 1.18990e-06, min loss: 5.87016e-07\n",
      "Epoch: 1723100, elapsed: 1.08e+01, train loss: 7.77801e-07, val loss: 1.32225e-06, min loss: 5.87016e-07\n",
      "Epoch: 1723200, elapsed: 1.08e+01, train loss: 6.92572e-07, val loss: 1.44492e-06, min loss: 5.87016e-07\n",
      "Epoch: 1723300, elapsed: 1.55e+01, train loss: 8.80168e-07, val loss: 1.49565e-06, min loss: 5.87016e-07\n",
      "Epoch: 1723400, elapsed: 1.10e+01, train loss: 6.13425e-07, val loss: 1.20863e-06, min loss: 5.87016e-07\n",
      "Epoch: 1723500, elapsed: 1.11e+01, train loss: 5.96128e-07, val loss: 1.21193e-06, min loss: 5.87016e-07\n",
      "Epoch: 1723600, elapsed: 1.09e+01, train loss: 6.82367e-07, val loss: 1.28034e-06, min loss: 5.87016e-07\n",
      "Epoch: 1723700, elapsed: 1.07e+01, train loss: 6.78217e-07, val loss: 1.55265e-06, min loss: 5.87016e-07\n",
      "Epoch: 1723800, elapsed: 1.09e+01, train loss: 6.87887e-07, val loss: 1.24879e-06, min loss: 5.87016e-07\n",
      "Epoch: 1723900, elapsed: 1.09e+01, train loss: 5.93283e-07, val loss: 1.18686e-06, min loss: 5.87016e-07\n",
      "Epoch: 1724000, elapsed: 1.09e+01, train loss: 5.92685e-07, val loss: 1.20893e-06, min loss: 5.87016e-07\n",
      "Epoch: 1724100, elapsed: 1.10e+01, train loss: 6.10421e-07, val loss: 1.19514e-06, min loss: 5.87016e-07\n",
      "Epoch: 1724200, elapsed: 1.10e+01, train loss: 5.86274e-07, val loss: 1.19694e-06, min loss: 5.86274e-07\n",
      "Epoch: 1724300, elapsed: 1.08e+01, train loss: 5.90053e-07, val loss: 1.18577e-06, min loss: 5.86274e-07\n",
      "Epoch: 1724400, elapsed: 1.10e+01, train loss: 5.90901e-07, val loss: 1.22040e-06, min loss: 5.86274e-07\n",
      "Epoch: 1724500, elapsed: 1.08e+01, train loss: 5.87610e-07, val loss: 1.20984e-06, min loss: 5.86274e-07\n",
      "Epoch: 1724600, elapsed: 1.08e+01, train loss: 7.18970e-07, val loss: 1.23983e-06, min loss: 5.86274e-07\n",
      "Epoch: 1724700, elapsed: 1.09e+01, train loss: 5.86568e-07, val loss: 1.19452e-06, min loss: 5.86274e-07\n",
      "Epoch: 1724800, elapsed: 1.09e+01, train loss: 5.88342e-07, val loss: 1.20594e-06, min loss: 5.86274e-07\n",
      "Epoch: 1724900, elapsed: 1.09e+01, train loss: 4.05095e-06, val loss: 4.76156e-06, min loss: 5.86274e-07\n",
      "Epoch: 1725000, elapsed: 1.08e+01, train loss: 5.86010e-07, val loss: 1.19752e-06, min loss: 5.86010e-07\n",
      "Epoch: 1725100, elapsed: 1.28e+01, train loss: 6.03642e-07, val loss: 1.19645e-06, min loss: 5.86010e-07\n",
      "Epoch: 1725200, elapsed: 1.08e+01, train loss: 5.95148e-07, val loss: 1.18384e-06, min loss: 5.86010e-07\n",
      "Epoch: 1725300, elapsed: 1.07e+01, train loss: 6.18118e-07, val loss: 1.22459e-06, min loss: 5.86010e-07\n",
      "Epoch: 1725400, elapsed: 1.09e+01, train loss: 8.20703e-07, val loss: 1.44713e-06, min loss: 5.86010e-07\n",
      "Epoch: 1725500, elapsed: 1.08e+01, train loss: 5.85636e-07, val loss: 1.19812e-06, min loss: 5.85636e-07\n",
      "Epoch: 1725600, elapsed: 1.07e+01, train loss: 5.95027e-07, val loss: 1.21113e-06, min loss: 5.85636e-07\n",
      "Epoch: 1725700, elapsed: 1.06e+01, train loss: 6.38473e-07, val loss: 1.25616e-06, min loss: 5.85636e-07\n",
      "Epoch: 1725800, elapsed: 1.08e+01, train loss: 1.37762e-06, val loss: 1.75109e-06, min loss: 5.85636e-07\n",
      "Epoch: 1725900, elapsed: 1.07e+01, train loss: 3.08014e-06, val loss: 3.40615e-06, min loss: 5.85636e-07\n",
      "Epoch: 1726000, elapsed: 1.09e+01, train loss: 6.11881e-07, val loss: 1.26962e-06, min loss: 5.85636e-07\n",
      "Epoch: 1726100, elapsed: 1.07e+01, train loss: 5.87659e-07, val loss: 1.19073e-06, min loss: 5.85636e-07\n",
      "Epoch: 1726200, elapsed: 1.06e+01, train loss: 5.87850e-07, val loss: 1.20313e-06, min loss: 5.85636e-07\n",
      "Epoch: 1726300, elapsed: 1.09e+01, train loss: 5.92759e-07, val loss: 1.18935e-06, min loss: 5.85636e-07\n",
      "Epoch: 1726400, elapsed: 1.55e+01, train loss: 1.49862e-06, val loss: 1.80106e-06, min loss: 5.85636e-07\n",
      "Epoch: 1726500, elapsed: 1.09e+01, train loss: 5.94510e-07, val loss: 1.22693e-06, min loss: 5.85636e-07\n",
      "Epoch: 1726600, elapsed: 1.09e+01, train loss: 5.85380e-07, val loss: 1.19501e-06, min loss: 5.85380e-07\n",
      "Epoch: 1726700, elapsed: 1.09e+01, train loss: 6.07782e-07, val loss: 1.20073e-06, min loss: 5.85380e-07\n",
      "Epoch: 1726800, elapsed: 1.08e+01, train loss: 5.97772e-07, val loss: 1.22190e-06, min loss: 5.85380e-07\n",
      "Epoch: 1726900, elapsed: 1.09e+01, train loss: 8.67520e-07, val loss: 1.50511e-06, min loss: 5.85380e-07\n",
      "Epoch: 1727000, elapsed: 1.11e+01, train loss: 3.01461e-06, val loss: 3.23162e-06, min loss: 5.85380e-07\n",
      "Epoch: 1727100, elapsed: 1.07e+01, train loss: 6.32324e-07, val loss: 1.29627e-06, min loss: 5.85380e-07\n",
      "Epoch: 1727200, elapsed: 1.08e+01, train loss: 5.85359e-07, val loss: 1.20062e-06, min loss: 5.85359e-07\n",
      "Epoch: 1727300, elapsed: 1.08e+01, train loss: 6.10674e-07, val loss: 1.19059e-06, min loss: 5.85359e-07\n",
      "Epoch: 1727400, elapsed: 1.10e+01, train loss: 5.94788e-07, val loss: 1.21433e-06, min loss: 5.85359e-07\n",
      "Epoch: 1727500, elapsed: 1.08e+01, train loss: 5.85905e-07, val loss: 1.20055e-06, min loss: 5.85359e-07\n",
      "Epoch: 1727600, elapsed: 1.09e+01, train loss: 7.29105e-07, val loss: 1.35237e-06, min loss: 5.85359e-07\n",
      "Epoch: 1727700, elapsed: 1.08e+01, train loss: 6.17194e-07, val loss: 1.21849e-06, min loss: 5.85359e-07\n",
      "Epoch: 1727800, elapsed: 1.08e+01, train loss: 5.84646e-07, val loss: 1.19886e-06, min loss: 5.84646e-07\n",
      "Epoch: 1727900, elapsed: 1.08e+01, train loss: 5.99155e-07, val loss: 1.22365e-06, min loss: 5.84646e-07\n",
      "Epoch: 1728000, elapsed: 1.09e+01, train loss: 1.74867e-06, val loss: 1.63947e-06, min loss: 5.84646e-07\n",
      "Epoch: 1728100, elapsed: 1.09e+01, train loss: 5.84447e-07, val loss: 1.19662e-06, min loss: 5.84447e-07\n",
      "Epoch: 1728200, elapsed: 1.07e+01, train loss: 5.99191e-07, val loss: 1.22731e-06, min loss: 5.84447e-07\n",
      "Epoch: 1728300, elapsed: 1.07e+01, train loss: 5.99243e-07, val loss: 1.20486e-06, min loss: 5.84447e-07\n",
      "Epoch: 1728400, elapsed: 1.09e+01, train loss: 5.84824e-07, val loss: 1.19553e-06, min loss: 5.84447e-07\n",
      "Epoch: 1728500, elapsed: 1.08e+01, train loss: 1.31411e-06, val loss: 1.59647e-06, min loss: 5.84447e-07\n",
      "Epoch: 1728600, elapsed: 1.06e+01, train loss: 5.84193e-07, val loss: 1.19757e-06, min loss: 5.84193e-07\n",
      "Epoch: 1728700, elapsed: 1.08e+01, train loss: 9.87572e-07, val loss: 1.47132e-06, min loss: 5.84193e-07\n",
      "Epoch: 1728800, elapsed: 1.06e+01, train loss: 1.55965e-06, val loss: 2.17381e-06, min loss: 5.84193e-07\n",
      "Epoch: 1728900, elapsed: 1.08e+01, train loss: 5.88699e-07, val loss: 1.19408e-06, min loss: 5.84193e-07\n",
      "Epoch: 1729000, elapsed: 1.08e+01, train loss: 5.84717e-07, val loss: 1.19028e-06, min loss: 5.84193e-07\n",
      "Epoch: 1729100, elapsed: 1.07e+01, train loss: 6.62036e-07, val loss: 1.25189e-06, min loss: 5.84193e-07\n",
      "Epoch: 1729200, elapsed: 1.06e+01, train loss: 9.12115e-07, val loss: 1.57034e-06, min loss: 5.84193e-07\n",
      "Epoch: 1729300, elapsed: 1.07e+01, train loss: 3.43171e-06, val loss: 3.75046e-06, min loss: 5.84193e-07\n",
      "Epoch: 1729400, elapsed: 1.06e+01, train loss: 6.01285e-07, val loss: 1.17044e-06, min loss: 5.84193e-07\n",
      "Epoch: 1729500, elapsed: 1.55e+01, train loss: 5.84199e-07, val loss: 1.19150e-06, min loss: 5.84193e-07\n",
      "Epoch: 1729600, elapsed: 1.11e+01, train loss: 8.81199e-07, val loss: 1.46821e-06, min loss: 5.84193e-07\n",
      "Epoch: 1729700, elapsed: 1.10e+01, train loss: 8.79949e-07, val loss: 1.42243e-06, min loss: 5.84193e-07\n",
      "Epoch: 1729800, elapsed: 1.11e+01, train loss: 6.87826e-07, val loss: 1.23119e-06, min loss: 5.84193e-07\n",
      "Epoch: 1729900, elapsed: 1.09e+01, train loss: 7.86634e-07, val loss: 1.32835e-06, min loss: 5.84193e-07\n",
      "Epoch: 1730000, elapsed: 1.09e+01, train loss: 5.95100e-07, val loss: 1.17566e-06, min loss: 5.84193e-07\n",
      "Epoch: 1730100, elapsed: 1.31e+01, train loss: 5.84030e-07, val loss: 1.19277e-06, min loss: 5.84030e-07\n",
      "Epoch: 1730200, elapsed: 1.08e+01, train loss: 6.69803e-07, val loss: 1.27364e-06, min loss: 5.84030e-07\n",
      "Epoch: 1730300, elapsed: 1.10e+01, train loss: 5.83788e-07, val loss: 1.19646e-06, min loss: 5.83788e-07\n",
      "Epoch: 1730400, elapsed: 1.09e+01, train loss: 7.35156e-07, val loss: 1.38032e-06, min loss: 5.83788e-07\n",
      "Epoch: 1730500, elapsed: 1.09e+01, train loss: 6.65049e-07, val loss: 1.19290e-06, min loss: 5.83788e-07\n",
      "Epoch: 1730600, elapsed: 1.10e+01, train loss: 6.21873e-07, val loss: 1.20914e-06, min loss: 5.83788e-07\n",
      "Epoch: 1730700, elapsed: 1.10e+01, train loss: 5.91857e-07, val loss: 1.20924e-06, min loss: 5.83788e-07\n",
      "Epoch: 1730800, elapsed: 1.09e+01, train loss: 6.57312e-07, val loss: 1.28476e-06, min loss: 5.83788e-07\n",
      "Epoch: 1730900, elapsed: 1.09e+01, train loss: 1.10794e-06, val loss: 1.87260e-06, min loss: 5.83788e-07\n",
      "Epoch: 1731000, elapsed: 1.09e+01, train loss: 1.83221e-06, val loss: 2.61887e-06, min loss: 5.83788e-07\n",
      "Epoch: 1731100, elapsed: 1.09e+01, train loss: 7.78406e-07, val loss: 1.39291e-06, min loss: 5.83788e-07\n",
      "Epoch: 1731200, elapsed: 1.11e+01, train loss: 1.06365e-06, val loss: 1.40507e-06, min loss: 5.83788e-07\n",
      "Epoch: 1731300, elapsed: 1.09e+01, train loss: 2.97235e-06, val loss: 3.29100e-06, min loss: 5.83788e-07\n",
      "Epoch: 1731400, elapsed: 1.09e+01, train loss: 1.09848e-06, val loss: 1.65359e-06, min loss: 5.83788e-07\n",
      "Epoch: 1731500, elapsed: 1.09e+01, train loss: 6.19693e-07, val loss: 1.22376e-06, min loss: 5.83788e-07\n",
      "Epoch: 1731600, elapsed: 1.09e+01, train loss: 6.03199e-07, val loss: 1.22971e-06, min loss: 5.83788e-07\n",
      "Epoch: 1731700, elapsed: 1.08e+01, train loss: 5.94740e-07, val loss: 1.19971e-06, min loss: 5.83788e-07\n",
      "Epoch: 1731800, elapsed: 1.09e+01, train loss: 7.68635e-07, val loss: 1.44790e-06, min loss: 5.83788e-07\n",
      "Epoch: 1731900, elapsed: 1.06e+01, train loss: 5.88674e-07, val loss: 1.21585e-06, min loss: 5.83788e-07\n",
      "Epoch: 1732000, elapsed: 1.07e+01, train loss: 1.16287e-06, val loss: 1.52044e-06, min loss: 5.83788e-07\n",
      "Epoch: 1732100, elapsed: 1.07e+01, train loss: 6.21063e-07, val loss: 1.23905e-06, min loss: 5.83788e-07\n",
      "Epoch: 1732200, elapsed: 1.09e+01, train loss: 6.20544e-07, val loss: 1.23178e-06, min loss: 5.83788e-07\n",
      "Epoch: 1732300, elapsed: 1.10e+01, train loss: 9.11729e-07, val loss: 1.53972e-06, min loss: 5.83788e-07\n",
      "Epoch: 1732400, elapsed: 1.09e+01, train loss: 5.97789e-07, val loss: 1.18090e-06, min loss: 5.83788e-07\n",
      "Epoch: 1732500, elapsed: 1.11e+01, train loss: 3.04845e-06, val loss: 3.71627e-06, min loss: 5.83788e-07\n",
      "Epoch: 1732600, elapsed: 1.61e+01, train loss: 6.03861e-07, val loss: 1.23064e-06, min loss: 5.83788e-07\n",
      "Epoch: 1732700, elapsed: 1.10e+01, train loss: 5.82320e-07, val loss: 1.19435e-06, min loss: 5.82320e-07\n",
      "Epoch: 1732800, elapsed: 1.10e+01, train loss: 5.99795e-07, val loss: 1.21769e-06, min loss: 5.82320e-07\n",
      "Epoch: 1732900, elapsed: 1.10e+01, train loss: 9.15602e-07, val loss: 1.50598e-06, min loss: 5.82320e-07\n",
      "Epoch: 1733000, elapsed: 1.08e+01, train loss: 7.39995e-07, val loss: 1.40119e-06, min loss: 5.82320e-07\n",
      "Epoch: 1733100, elapsed: 1.10e+01, train loss: 5.89383e-07, val loss: 1.20297e-06, min loss: 5.82320e-07\n",
      "Epoch: 1733200, elapsed: 1.09e+01, train loss: 5.89738e-07, val loss: 1.18535e-06, min loss: 5.82320e-07\n",
      "Epoch: 1733300, elapsed: 1.10e+01, train loss: 6.06514e-07, val loss: 1.20732e-06, min loss: 5.82320e-07\n",
      "Epoch: 1733400, elapsed: 1.10e+01, train loss: 6.38694e-07, val loss: 1.24219e-06, min loss: 5.82320e-07\n",
      "Epoch: 1733500, elapsed: 1.09e+01, train loss: 5.91154e-07, val loss: 1.19532e-06, min loss: 5.82320e-07\n",
      "Epoch: 1733600, elapsed: 1.08e+01, train loss: 5.83395e-07, val loss: 1.20172e-06, min loss: 5.82320e-07\n",
      "Epoch: 1733700, elapsed: 1.10e+01, train loss: 6.23217e-07, val loss: 1.32272e-06, min loss: 5.82320e-07\n",
      "Epoch: 1733800, elapsed: 1.09e+01, train loss: 1.28098e-06, val loss: 1.52157e-06, min loss: 5.82320e-07\n",
      "Epoch: 1733900, elapsed: 1.10e+01, train loss: 5.92427e-07, val loss: 1.19208e-06, min loss: 5.82320e-07\n",
      "Epoch: 1734000, elapsed: 1.09e+01, train loss: 5.85544e-07, val loss: 1.18957e-06, min loss: 5.82320e-07\n",
      "Epoch: 1734100, elapsed: 1.09e+01, train loss: 5.99219e-07, val loss: 1.18388e-06, min loss: 5.82320e-07\n",
      "Epoch: 1734200, elapsed: 1.08e+01, train loss: 5.88118e-07, val loss: 1.18770e-06, min loss: 5.82320e-07\n",
      "Epoch: 1734300, elapsed: 1.08e+01, train loss: 5.92832e-07, val loss: 1.21607e-06, min loss: 5.82320e-07\n",
      "Epoch: 1734400, elapsed: 1.08e+01, train loss: 5.81533e-07, val loss: 1.19299e-06, min loss: 5.81533e-07\n",
      "Epoch: 1734500, elapsed: 1.07e+01, train loss: 5.82721e-07, val loss: 1.19894e-06, min loss: 5.81533e-07\n",
      "Epoch: 1734600, elapsed: 1.07e+01, train loss: 6.01328e-07, val loss: 1.22032e-06, min loss: 5.81533e-07\n",
      "Epoch: 1734700, elapsed: 1.08e+01, train loss: 6.01758e-07, val loss: 1.21832e-06, min loss: 5.81533e-07\n",
      "Epoch: 1734800, elapsed: 1.09e+01, train loss: 3.83297e-06, val loss: 4.95585e-06, min loss: 5.81533e-07\n",
      "Epoch: 1734900, elapsed: 1.09e+01, train loss: 5.81191e-07, val loss: 1.19155e-06, min loss: 5.81191e-07\n",
      "Epoch: 1735000, elapsed: 1.06e+01, train loss: 5.93590e-07, val loss: 1.19180e-06, min loss: 5.81191e-07\n",
      "Epoch: 1735100, elapsed: 1.29e+01, train loss: 4.91359e-06, val loss: 5.57554e-06, min loss: 5.81191e-07\n",
      "Epoch: 1735200, elapsed: 1.10e+01, train loss: 5.80994e-07, val loss: 1.19349e-06, min loss: 5.80994e-07\n",
      "Epoch: 1735300, elapsed: 1.09e+01, train loss: 5.85264e-07, val loss: 1.18116e-06, min loss: 5.80994e-07\n",
      "Epoch: 1735400, elapsed: 1.08e+01, train loss: 6.43967e-07, val loss: 1.24335e-06, min loss: 5.80994e-07\n",
      "Epoch: 1735500, elapsed: 1.08e+01, train loss: 1.62383e-06, val loss: 2.61193e-06, min loss: 5.80994e-07\n",
      "Epoch: 1735600, elapsed: 1.09e+01, train loss: 5.84315e-07, val loss: 1.19444e-06, min loss: 5.80994e-07\n",
      "Epoch: 1735700, elapsed: 1.56e+01, train loss: 5.84788e-07, val loss: 1.20394e-06, min loss: 5.80994e-07\n",
      "Epoch: 1735800, elapsed: 1.11e+01, train loss: 6.52660e-07, val loss: 1.26526e-06, min loss: 5.80994e-07\n",
      "Epoch: 1735900, elapsed: 1.11e+01, train loss: 2.31613e-06, val loss: 2.77933e-06, min loss: 5.80994e-07\n",
      "Epoch: 1736000, elapsed: 1.10e+01, train loss: 5.80569e-07, val loss: 1.19265e-06, min loss: 5.80569e-07\n",
      "Epoch: 1736100, elapsed: 1.09e+01, train loss: 5.81664e-07, val loss: 1.19410e-06, min loss: 5.80569e-07\n",
      "Epoch: 1736200, elapsed: 1.10e+01, train loss: 5.82781e-07, val loss: 1.20271e-06, min loss: 5.80569e-07\n",
      "Epoch: 1736300, elapsed: 1.10e+01, train loss: 6.19830e-07, val loss: 1.24943e-06, min loss: 5.80569e-07\n",
      "Epoch: 1736400, elapsed: 1.09e+01, train loss: 9.30245e-07, val loss: 1.48045e-06, min loss: 5.80569e-07\n",
      "Epoch: 1736500, elapsed: 1.10e+01, train loss: 1.68986e-06, val loss: 2.46152e-06, min loss: 5.80569e-07\n",
      "Epoch: 1736600, elapsed: 1.09e+01, train loss: 5.80434e-07, val loss: 1.19374e-06, min loss: 5.80434e-07\n",
      "Epoch: 1736700, elapsed: 1.09e+01, train loss: 5.83475e-07, val loss: 1.19438e-06, min loss: 5.80434e-07\n",
      "Epoch: 1736800, elapsed: 1.10e+01, train loss: 5.86150e-07, val loss: 1.19566e-06, min loss: 5.80434e-07\n",
      "Epoch: 1736900, elapsed: 1.10e+01, train loss: 1.63477e-06, val loss: 2.09290e-06, min loss: 5.80434e-07\n",
      "Epoch: 1737000, elapsed: 1.10e+01, train loss: 5.81094e-07, val loss: 1.20022e-06, min loss: 5.80434e-07\n",
      "Epoch: 1737100, elapsed: 1.10e+01, train loss: 5.80728e-07, val loss: 1.19340e-06, min loss: 5.80434e-07\n",
      "Epoch: 1737200, elapsed: 1.12e+01, train loss: 9.09256e-07, val loss: 1.56689e-06, min loss: 5.80434e-07\n",
      "Epoch: 1737300, elapsed: 1.09e+01, train loss: 1.64749e-06, val loss: 2.40223e-06, min loss: 5.80434e-07\n",
      "Epoch: 1737400, elapsed: 1.09e+01, train loss: 5.80527e-07, val loss: 1.19074e-06, min loss: 5.80434e-07\n",
      "Epoch: 1737500, elapsed: 1.08e+01, train loss: 5.86850e-07, val loss: 1.19790e-06, min loss: 5.80434e-07\n",
      "Epoch: 1737600, elapsed: 1.07e+01, train loss: 5.88499e-07, val loss: 1.21762e-06, min loss: 5.80434e-07\n",
      "Epoch: 1737700, elapsed: 1.08e+01, train loss: 8.64454e-07, val loss: 1.37102e-06, min loss: 5.80434e-07\n",
      "Epoch: 1737800, elapsed: 1.11e+01, train loss: 7.37614e-07, val loss: 1.28760e-06, min loss: 5.80434e-07\n",
      "Epoch: 1737900, elapsed: 1.09e+01, train loss: 5.83850e-07, val loss: 1.18274e-06, min loss: 5.80434e-07\n",
      "Epoch: 1738000, elapsed: 1.09e+01, train loss: 5.86551e-07, val loss: 1.18441e-06, min loss: 5.80434e-07\n",
      "Epoch: 1738100, elapsed: 1.08e+01, train loss: 5.93997e-07, val loss: 1.21786e-06, min loss: 5.80434e-07\n",
      "Epoch: 1738200, elapsed: 1.09e+01, train loss: 5.96220e-07, val loss: 1.17670e-06, min loss: 5.80434e-07\n",
      "Epoch: 1738300, elapsed: 1.07e+01, train loss: 6.56100e-07, val loss: 1.38478e-06, min loss: 5.80434e-07\n",
      "Epoch: 1738400, elapsed: 1.10e+01, train loss: 6.03249e-07, val loss: 1.24523e-06, min loss: 5.80434e-07\n",
      "Epoch: 1738500, elapsed: 1.07e+01, train loss: 5.84664e-07, val loss: 1.20526e-06, min loss: 5.80434e-07\n",
      "Epoch: 1738600, elapsed: 1.09e+01, train loss: 6.13921e-07, val loss: 1.24121e-06, min loss: 5.80434e-07\n",
      "Epoch: 1738700, elapsed: 1.09e+01, train loss: 6.78235e-07, val loss: 1.27331e-06, min loss: 5.80434e-07\n",
      "Epoch: 1738800, elapsed: 1.57e+01, train loss: 6.50296e-07, val loss: 1.20564e-06, min loss: 5.80434e-07\n",
      "Epoch: 1738900, elapsed: 1.12e+01, train loss: 9.13249e-07, val loss: 1.28005e-06, min loss: 5.80434e-07\n",
      "Epoch: 1739000, elapsed: 1.12e+01, train loss: 3.99723e-06, val loss: 4.47899e-06, min loss: 5.80434e-07\n",
      "Epoch: 1739100, elapsed: 1.09e+01, train loss: 5.79351e-07, val loss: 1.19217e-06, min loss: 5.79351e-07\n",
      "Epoch: 1739200, elapsed: 1.09e+01, train loss: 6.10027e-07, val loss: 1.28561e-06, min loss: 5.79351e-07\n",
      "Epoch: 1739300, elapsed: 1.09e+01, train loss: 5.89761e-07, val loss: 1.22122e-06, min loss: 5.79351e-07\n",
      "Epoch: 1739400, elapsed: 1.10e+01, train loss: 5.79868e-07, val loss: 1.19321e-06, min loss: 5.79351e-07\n",
      "Epoch: 1739500, elapsed: 1.10e+01, train loss: 5.78848e-07, val loss: 1.19193e-06, min loss: 5.78848e-07\n",
      "Epoch: 1739600, elapsed: 1.10e+01, train loss: 5.80205e-07, val loss: 1.19086e-06, min loss: 5.78848e-07\n",
      "Epoch: 1739700, elapsed: 1.09e+01, train loss: 6.75579e-07, val loss: 1.28524e-06, min loss: 5.78848e-07\n",
      "Epoch: 1739800, elapsed: 1.09e+01, train loss: 8.57348e-07, val loss: 1.55179e-06, min loss: 5.78848e-07\n",
      "Epoch: 1739900, elapsed: 1.08e+01, train loss: 5.90565e-07, val loss: 1.17913e-06, min loss: 5.78848e-07\n",
      "Epoch: 1740000, elapsed: 1.10e+01, train loss: 9.89082e-07, val loss: 1.77044e-06, min loss: 5.78848e-07\n",
      "Epoch: 1740100, elapsed: 1.27e+01, train loss: 8.15103e-07, val loss: 1.56651e-06, min loss: 5.78848e-07\n",
      "Epoch: 1740200, elapsed: 1.09e+01, train loss: 2.06916e-06, val loss: 2.87394e-06, min loss: 5.78848e-07\n",
      "Epoch: 1740300, elapsed: 1.09e+01, train loss: 6.81936e-07, val loss: 1.36198e-06, min loss: 5.78848e-07\n",
      "Epoch: 1740400, elapsed: 1.09e+01, train loss: 6.48650e-07, val loss: 1.24408e-06, min loss: 5.78848e-07\n",
      "Epoch: 1740500, elapsed: 1.10e+01, train loss: 8.05199e-07, val loss: 1.63443e-06, min loss: 5.78848e-07\n",
      "Epoch: 1740600, elapsed: 1.10e+01, train loss: 6.24609e-07, val loss: 1.21754e-06, min loss: 5.78848e-07\n",
      "Epoch: 1740700, elapsed: 1.08e+01, train loss: 5.83651e-07, val loss: 1.20669e-06, min loss: 5.78848e-07\n",
      "Epoch: 1740800, elapsed: 1.08e+01, train loss: 5.78492e-07, val loss: 1.19470e-06, min loss: 5.78492e-07\n",
      "Epoch: 1740900, elapsed: 1.10e+01, train loss: 4.01981e-06, val loss: 4.60112e-06, min loss: 5.78492e-07\n",
      "Epoch: 1741000, elapsed: 1.10e+01, train loss: 1.16979e-06, val loss: 1.82052e-06, min loss: 5.78492e-07\n",
      "Epoch: 1741100, elapsed: 1.07e+01, train loss: 5.96672e-07, val loss: 1.23135e-06, min loss: 5.78492e-07\n",
      "Epoch: 1741200, elapsed: 1.06e+01, train loss: 7.61214e-07, val loss: 1.45376e-06, min loss: 5.78492e-07\n",
      "Epoch: 1741300, elapsed: 1.09e+01, train loss: 8.14863e-07, val loss: 1.23581e-06, min loss: 5.78492e-07\n",
      "Epoch: 1741400, elapsed: 1.07e+01, train loss: 6.74748e-07, val loss: 1.30267e-06, min loss: 5.78492e-07\n",
      "Epoch: 1741500, elapsed: 1.09e+01, train loss: 6.50776e-07, val loss: 1.28303e-06, min loss: 5.78492e-07\n",
      "Epoch: 1741600, elapsed: 1.09e+01, train loss: 5.93867e-07, val loss: 1.17128e-06, min loss: 5.78492e-07\n",
      "Epoch: 1741700, elapsed: 1.09e+01, train loss: 6.38280e-07, val loss: 1.23044e-06, min loss: 5.78492e-07\n",
      "Epoch: 1741800, elapsed: 1.10e+01, train loss: 7.21055e-07, val loss: 1.32537e-06, min loss: 5.78492e-07\n",
      "Epoch: 1741900, elapsed: 1.54e+01, train loss: 6.08517e-07, val loss: 1.20626e-06, min loss: 5.78492e-07\n",
      "Epoch: 1742000, elapsed: 1.11e+01, train loss: 5.94082e-07, val loss: 1.22355e-06, min loss: 5.78492e-07\n",
      "Epoch: 1742100, elapsed: 1.09e+01, train loss: 1.51919e-06, val loss: 2.05224e-06, min loss: 5.78492e-07\n",
      "Epoch: 1742200, elapsed: 1.10e+01, train loss: 5.81811e-07, val loss: 1.18840e-06, min loss: 5.78492e-07\n",
      "Epoch: 1742300, elapsed: 1.10e+01, train loss: 6.13913e-07, val loss: 1.25179e-06, min loss: 5.78492e-07\n",
      "Epoch: 1742400, elapsed: 1.11e+01, train loss: 6.55021e-07, val loss: 1.24416e-06, min loss: 5.78492e-07\n",
      "Epoch: 1742500, elapsed: 1.11e+01, train loss: 4.24195e-06, val loss: 4.36146e-06, min loss: 5.78492e-07\n",
      "Epoch: 1742600, elapsed: 1.10e+01, train loss: 5.98908e-07, val loss: 1.24325e-06, min loss: 5.78492e-07\n",
      "Epoch: 1742700, elapsed: 1.10e+01, train loss: 5.95974e-07, val loss: 1.24446e-06, min loss: 5.78492e-07\n",
      "Epoch: 1742800, elapsed: 1.11e+01, train loss: 5.84914e-07, val loss: 1.18374e-06, min loss: 5.78492e-07\n",
      "Epoch: 1742900, elapsed: 1.09e+01, train loss: 5.80290e-07, val loss: 1.20459e-06, min loss: 5.78492e-07\n",
      "Epoch: 1743000, elapsed: 1.11e+01, train loss: 5.79576e-07, val loss: 1.19218e-06, min loss: 5.78492e-07\n",
      "Epoch: 1743100, elapsed: 1.10e+01, train loss: 6.02982e-07, val loss: 1.27452e-06, min loss: 5.78492e-07\n",
      "Epoch: 1743200, elapsed: 1.08e+01, train loss: 1.48399e-06, val loss: 1.63622e-06, min loss: 5.78492e-07\n",
      "Epoch: 1743300, elapsed: 1.09e+01, train loss: 5.77322e-07, val loss: 1.19279e-06, min loss: 5.77322e-07\n",
      "Epoch: 1743400, elapsed: 1.09e+01, train loss: 6.09294e-07, val loss: 1.19611e-06, min loss: 5.77322e-07\n",
      "Epoch: 1743500, elapsed: 1.08e+01, train loss: 6.92371e-07, val loss: 1.30728e-06, min loss: 5.77322e-07\n",
      "Epoch: 1743600, elapsed: 1.10e+01, train loss: 8.22899e-07, val loss: 1.38180e-06, min loss: 5.77322e-07\n",
      "Epoch: 1743700, elapsed: 1.10e+01, train loss: 1.13264e-06, val loss: 1.21782e-06, min loss: 5.77322e-07\n",
      "Epoch: 1743800, elapsed: 1.09e+01, train loss: 6.30589e-07, val loss: 1.25767e-06, min loss: 5.77322e-07\n",
      "Epoch: 1743900, elapsed: 1.09e+01, train loss: 1.36518e-06, val loss: 1.99443e-06, min loss: 5.77322e-07\n",
      "Epoch: 1744000, elapsed: 1.08e+01, train loss: 2.20883e-06, val loss: 3.11802e-06, min loss: 5.77322e-07\n",
      "Epoch: 1744100, elapsed: 1.08e+01, train loss: 5.76766e-07, val loss: 1.19131e-06, min loss: 5.76766e-07\n",
      "Epoch: 1744200, elapsed: 1.09e+01, train loss: 5.96684e-07, val loss: 1.25189e-06, min loss: 5.76766e-07\n",
      "Epoch: 1744300, elapsed: 1.10e+01, train loss: 5.83270e-07, val loss: 1.18699e-06, min loss: 5.76766e-07\n",
      "Epoch: 1744400, elapsed: 1.08e+01, train loss: 1.18224e-06, val loss: 1.57563e-06, min loss: 5.76766e-07\n",
      "Epoch: 1744500, elapsed: 1.08e+01, train loss: 9.29009e-07, val loss: 1.45678e-06, min loss: 5.76766e-07\n",
      "Epoch: 1744600, elapsed: 1.08e+01, train loss: 6.32902e-07, val loss: 1.25259e-06, min loss: 5.76766e-07\n",
      "Epoch: 1744700, elapsed: 1.08e+01, train loss: 9.76209e-07, val loss: 1.45184e-06, min loss: 5.76766e-07\n",
      "Epoch: 1744800, elapsed: 1.09e+01, train loss: 1.91984e-06, val loss: 2.57836e-06, min loss: 5.76766e-07\n",
      "Epoch: 1744900, elapsed: 1.07e+01, train loss: 1.21820e-06, val loss: 1.85441e-06, min loss: 5.76766e-07\n",
      "Epoch: 1745000, elapsed: 1.04e+01, train loss: 5.81727e-07, val loss: 1.21473e-06, min loss: 5.76766e-07\n",
      "Epoch: 1745100, elapsed: 1.77e+01, train loss: 5.79258e-07, val loss: 1.20482e-06, min loss: 5.76766e-07\n",
      "Epoch: 1745200, elapsed: 1.11e+01, train loss: 5.81034e-07, val loss: 1.18638e-06, min loss: 5.76766e-07\n",
      "Epoch: 1745300, elapsed: 1.10e+01, train loss: 5.94662e-07, val loss: 1.21386e-06, min loss: 5.76766e-07\n",
      "Epoch: 1745400, elapsed: 1.09e+01, train loss: 5.93945e-07, val loss: 1.21511e-06, min loss: 5.76766e-07\n",
      "Epoch: 1745500, elapsed: 1.10e+01, train loss: 5.82096e-07, val loss: 1.19884e-06, min loss: 5.76766e-07\n",
      "Epoch: 1745600, elapsed: 1.09e+01, train loss: 5.79745e-07, val loss: 1.20027e-06, min loss: 5.76766e-07\n",
      "Epoch: 1745700, elapsed: 1.10e+01, train loss: 5.81262e-07, val loss: 1.19038e-06, min loss: 5.76766e-07\n",
      "Epoch: 1745800, elapsed: 1.10e+01, train loss: 5.98077e-07, val loss: 1.20392e-06, min loss: 5.76766e-07\n",
      "Epoch: 1745900, elapsed: 1.09e+01, train loss: 5.80475e-07, val loss: 1.21691e-06, min loss: 5.76766e-07\n",
      "Epoch: 1746000, elapsed: 1.09e+01, train loss: 5.77637e-07, val loss: 1.18917e-06, min loss: 5.76766e-07\n",
      "Epoch: 1746100, elapsed: 1.09e+01, train loss: 5.89038e-07, val loss: 1.22638e-06, min loss: 5.76766e-07\n",
      "Epoch: 1746200, elapsed: 1.08e+01, train loss: 5.76044e-07, val loss: 1.18931e-06, min loss: 5.76044e-07\n",
      "Epoch: 1746300, elapsed: 1.10e+01, train loss: 5.89779e-07, val loss: 1.17150e-06, min loss: 5.76044e-07\n",
      "Epoch: 1746400, elapsed: 1.09e+01, train loss: 5.75674e-07, val loss: 1.19130e-06, min loss: 5.75674e-07\n",
      "Epoch: 1746500, elapsed: 1.10e+01, train loss: 8.29086e-07, val loss: 1.26161e-06, min loss: 5.75674e-07\n",
      "Epoch: 1746600, elapsed: 1.08e+01, train loss: 5.75546e-07, val loss: 1.19312e-06, min loss: 5.75546e-07\n",
      "Epoch: 1746700, elapsed: 1.09e+01, train loss: 6.29701e-07, val loss: 1.25500e-06, min loss: 5.75546e-07\n",
      "Epoch: 1746800, elapsed: 1.10e+01, train loss: 8.61148e-07, val loss: 1.43352e-06, min loss: 5.75546e-07\n",
      "Epoch: 1746900, elapsed: 1.09e+01, train loss: 6.48083e-07, val loss: 1.18834e-06, min loss: 5.75546e-07\n",
      "Epoch: 1747000, elapsed: 1.10e+01, train loss: 1.74647e-06, val loss: 2.08825e-06, min loss: 5.75546e-07\n",
      "Epoch: 1747100, elapsed: 1.07e+01, train loss: 6.26505e-07, val loss: 1.19404e-06, min loss: 5.75546e-07\n",
      "Epoch: 1747200, elapsed: 1.10e+01, train loss: 5.82754e-07, val loss: 1.17986e-06, min loss: 5.75546e-07\n",
      "Epoch: 1747300, elapsed: 1.09e+01, train loss: 7.15456e-07, val loss: 1.37025e-06, min loss: 5.75546e-07\n",
      "Epoch: 1747400, elapsed: 1.07e+01, train loss: 9.32078e-07, val loss: 1.53385e-06, min loss: 5.75546e-07\n",
      "Epoch: 1747500, elapsed: 1.09e+01, train loss: 5.75483e-07, val loss: 1.19099e-06, min loss: 5.75483e-07\n",
      "Epoch: 1747600, elapsed: 1.08e+01, train loss: 5.79755e-07, val loss: 1.23487e-06, min loss: 5.75483e-07\n",
      "Epoch: 1747700, elapsed: 1.09e+01, train loss: 5.75101e-07, val loss: 1.19256e-06, min loss: 5.75101e-07\n",
      "Epoch: 1747800, elapsed: 1.09e+01, train loss: 5.77601e-07, val loss: 1.19223e-06, min loss: 5.75101e-07\n",
      "Epoch: 1747900, elapsed: 1.10e+01, train loss: 5.87181e-07, val loss: 1.23382e-06, min loss: 5.75101e-07\n",
      "Epoch: 1748000, elapsed: 1.07e+01, train loss: 5.76935e-07, val loss: 1.18764e-06, min loss: 5.75101e-07\n",
      "Epoch: 1748100, elapsed: 1.07e+01, train loss: 6.21025e-07, val loss: 1.31862e-06, min loss: 5.75101e-07\n",
      "Epoch: 1748200, elapsed: 1.57e+01, train loss: 1.09582e-06, val loss: 1.71545e-06, min loss: 5.75101e-07\n",
      "Epoch: 1748300, elapsed: 1.11e+01, train loss: 1.25292e-06, val loss: 1.84553e-06, min loss: 5.75101e-07\n",
      "Epoch: 1748400, elapsed: 1.10e+01, train loss: 3.63505e-06, val loss: 4.01411e-06, min loss: 5.75101e-07\n",
      "Epoch: 1748500, elapsed: 1.10e+01, train loss: 7.26017e-07, val loss: 1.21400e-06, min loss: 5.75101e-07\n",
      "Epoch: 1748600, elapsed: 1.12e+01, train loss: 7.32258e-07, val loss: 1.30039e-06, min loss: 5.75101e-07\n",
      "Epoch: 1748700, elapsed: 1.11e+01, train loss: 6.07881e-07, val loss: 1.27374e-06, min loss: 5.75101e-07\n",
      "Epoch: 1748800, elapsed: 1.10e+01, train loss: 5.80699e-07, val loss: 1.20168e-06, min loss: 5.75101e-07\n",
      "Epoch: 1748900, elapsed: 1.11e+01, train loss: 5.81864e-07, val loss: 1.19926e-06, min loss: 5.75101e-07\n",
      "Epoch: 1749000, elapsed: 1.10e+01, train loss: 5.77474e-07, val loss: 1.20761e-06, min loss: 5.75101e-07\n",
      "Epoch: 1749100, elapsed: 1.10e+01, train loss: 5.74950e-07, val loss: 1.18654e-06, min loss: 5.74950e-07\n",
      "Epoch: 1749200, elapsed: 1.09e+01, train loss: 5.81299e-07, val loss: 1.20905e-06, min loss: 5.74950e-07\n",
      "Epoch: 1749300, elapsed: 1.08e+01, train loss: 2.10951e-06, val loss: 2.50242e-06, min loss: 5.74950e-07\n",
      "Epoch: 1749400, elapsed: 1.10e+01, train loss: 1.29231e-06, val loss: 1.72596e-06, min loss: 5.74950e-07\n",
      "Epoch: 1749500, elapsed: 1.09e+01, train loss: 3.43216e-06, val loss: 3.33423e-06, min loss: 5.74950e-07\n",
      "Epoch: 1749600, elapsed: 1.08e+01, train loss: 5.77496e-07, val loss: 1.20303e-06, min loss: 5.74950e-07\n",
      "Epoch: 1749700, elapsed: 1.07e+01, train loss: 5.74736e-07, val loss: 1.19330e-06, min loss: 5.74736e-07\n",
      "Epoch: 1749800, elapsed: 1.08e+01, train loss: 6.29754e-07, val loss: 1.38689e-06, min loss: 5.74736e-07\n",
      "Epoch: 1749900, elapsed: 1.09e+01, train loss: 5.74104e-07, val loss: 1.19224e-06, min loss: 5.74104e-07\n",
      "Epoch: 1750000, elapsed: 1.10e+01, train loss: 6.25021e-07, val loss: 1.23280e-06, min loss: 5.74104e-07\n",
      "Epoch: 1750100, elapsed: 1.29e+01, train loss: 5.79847e-07, val loss: 1.18930e-06, min loss: 5.74104e-07\n",
      "Epoch: 1750200, elapsed: 1.10e+01, train loss: 6.23556e-07, val loss: 1.28971e-06, min loss: 5.74104e-07\n",
      "Epoch: 1750300, elapsed: 1.09e+01, train loss: 5.81654e-07, val loss: 1.22129e-06, min loss: 5.74104e-07\n",
      "Epoch: 1750400, elapsed: 1.08e+01, train loss: 1.41223e-06, val loss: 1.70120e-06, min loss: 5.74104e-07\n",
      "Epoch: 1750500, elapsed: 1.06e+01, train loss: 7.35194e-07, val loss: 1.35688e-06, min loss: 5.74104e-07\n",
      "Epoch: 1750600, elapsed: 1.09e+01, train loss: 3.21145e-06, val loss: 3.68176e-06, min loss: 5.74104e-07\n",
      "Epoch: 1750700, elapsed: 1.08e+01, train loss: 5.73788e-07, val loss: 1.19198e-06, min loss: 5.73788e-07\n",
      "Epoch: 1750800, elapsed: 1.07e+01, train loss: 7.64514e-07, val loss: 1.56054e-06, min loss: 5.73788e-07\n",
      "Epoch: 1750900, elapsed: 1.08e+01, train loss: 5.74338e-07, val loss: 1.18787e-06, min loss: 5.73788e-07\n",
      "Epoch: 1751000, elapsed: 1.08e+01, train loss: 2.33291e-06, val loss: 2.86571e-06, min loss: 5.73788e-07\n",
      "Epoch: 1751100, elapsed: 1.09e+01, train loss: 6.03578e-07, val loss: 1.29152e-06, min loss: 5.73788e-07\n",
      "Epoch: 1751200, elapsed: 1.08e+01, train loss: 7.02068e-07, val loss: 1.36960e-06, min loss: 5.73788e-07\n",
      "Epoch: 1751300, elapsed: 1.56e+01, train loss: 1.78174e-06, val loss: 2.03300e-06, min loss: 5.73788e-07\n",
      "Epoch: 1751400, elapsed: 1.12e+01, train loss: 1.07951e-06, val loss: 1.50152e-06, min loss: 5.73788e-07\n",
      "Epoch: 1751500, elapsed: 1.11e+01, train loss: 5.74634e-07, val loss: 1.19481e-06, min loss: 5.73788e-07\n",
      "Epoch: 1751600, elapsed: 1.10e+01, train loss: 5.93408e-07, val loss: 1.20541e-06, min loss: 5.73788e-07\n",
      "Epoch: 1751700, elapsed: 1.09e+01, train loss: 5.73216e-07, val loss: 1.19108e-06, min loss: 5.73216e-07\n",
      "Epoch: 1751800, elapsed: 1.10e+01, train loss: 9.28952e-07, val loss: 1.62379e-06, min loss: 5.73216e-07\n",
      "Epoch: 1751900, elapsed: 1.09e+01, train loss: 5.76107e-07, val loss: 1.21327e-06, min loss: 5.73216e-07\n",
      "Epoch: 1752000, elapsed: 1.10e+01, train loss: 6.28523e-07, val loss: 1.21892e-06, min loss: 5.73216e-07\n",
      "Epoch: 1752100, elapsed: 1.09e+01, train loss: 2.01692e-06, val loss: 2.78048e-06, min loss: 5.73216e-07\n",
      "Epoch: 1752200, elapsed: 1.09e+01, train loss: 6.06209e-07, val loss: 1.23162e-06, min loss: 5.73216e-07\n",
      "Epoch: 1752300, elapsed: 1.07e+01, train loss: 6.68212e-07, val loss: 1.32427e-06, min loss: 5.73216e-07\n",
      "Epoch: 1752400, elapsed: 1.09e+01, train loss: 6.49603e-07, val loss: 1.23017e-06, min loss: 5.73216e-07\n",
      "Epoch: 1752500, elapsed: 1.09e+01, train loss: 7.58713e-07, val loss: 1.38943e-06, min loss: 5.73216e-07\n",
      "Epoch: 1752600, elapsed: 1.09e+01, train loss: 5.86390e-07, val loss: 1.20707e-06, min loss: 5.73216e-07\n",
      "Epoch: 1752700, elapsed: 1.09e+01, train loss: 5.82145e-07, val loss: 1.18978e-06, min loss: 5.73216e-07\n",
      "Epoch: 1752800, elapsed: 1.08e+01, train loss: 6.56926e-07, val loss: 1.33773e-06, min loss: 5.73216e-07\n",
      "Epoch: 1752900, elapsed: 1.09e+01, train loss: 5.74927e-07, val loss: 1.18675e-06, min loss: 5.73216e-07\n",
      "Epoch: 1753000, elapsed: 1.09e+01, train loss: 5.74835e-07, val loss: 1.18657e-06, min loss: 5.73216e-07\n",
      "Epoch: 1753100, elapsed: 1.11e+01, train loss: 1.67911e-06, val loss: 2.51919e-06, min loss: 5.73216e-07\n",
      "Epoch: 1753200, elapsed: 1.09e+01, train loss: 3.60193e-06, val loss: 3.70949e-06, min loss: 5.73216e-07\n",
      "Epoch: 1753300, elapsed: 1.12e+01, train loss: 6.82712e-07, val loss: 1.41157e-06, min loss: 5.73216e-07\n",
      "Epoch: 1753400, elapsed: 1.08e+01, train loss: 5.82093e-07, val loss: 1.19880e-06, min loss: 5.73216e-07\n",
      "Epoch: 1753500, elapsed: 1.09e+01, train loss: 5.72675e-07, val loss: 1.19043e-06, min loss: 5.72675e-07\n",
      "Epoch: 1753600, elapsed: 1.08e+01, train loss: 5.78251e-07, val loss: 1.19235e-06, min loss: 5.72675e-07\n",
      "Epoch: 1753700, elapsed: 1.08e+01, train loss: 6.95292e-07, val loss: 1.35369e-06, min loss: 5.72675e-07\n",
      "Epoch: 1753800, elapsed: 1.11e+01, train loss: 5.78281e-07, val loss: 1.19760e-06, min loss: 5.72675e-07\n",
      "Epoch: 1753900, elapsed: 1.10e+01, train loss: 5.85752e-07, val loss: 1.22129e-06, min loss: 5.72675e-07\n",
      "Epoch: 1754000, elapsed: 1.09e+01, train loss: 6.17340e-07, val loss: 1.19067e-06, min loss: 5.72675e-07\n",
      "Epoch: 1754100, elapsed: 1.08e+01, train loss: 1.42114e-06, val loss: 1.72282e-06, min loss: 5.72675e-07\n",
      "Epoch: 1754200, elapsed: 1.08e+01, train loss: 6.92106e-07, val loss: 1.24445e-06, min loss: 5.72675e-07\n",
      "Epoch: 1754300, elapsed: 1.10e+01, train loss: 6.18995e-07, val loss: 1.21865e-06, min loss: 5.72675e-07\n",
      "Epoch: 1754400, elapsed: 1.07e+01, train loss: 8.14374e-07, val loss: 1.33840e-06, min loss: 5.72675e-07\n",
      "Epoch: 1754500, elapsed: 1.60e+01, train loss: 3.64074e-06, val loss: 3.11222e-06, min loss: 5.72675e-07\n",
      "Epoch: 1754600, elapsed: 1.11e+01, train loss: 5.72126e-07, val loss: 1.19331e-06, min loss: 5.72126e-07\n",
      "Epoch: 1754700, elapsed: 1.12e+01, train loss: 5.72369e-07, val loss: 1.19062e-06, min loss: 5.72126e-07\n",
      "Epoch: 1754800, elapsed: 1.09e+01, train loss: 5.74904e-07, val loss: 1.19254e-06, min loss: 5.72126e-07\n",
      "Epoch: 1754900, elapsed: 1.08e+01, train loss: 8.13185e-07, val loss: 1.44006e-06, min loss: 5.72126e-07\n",
      "Epoch: 1755000, elapsed: 1.09e+01, train loss: 5.86629e-07, val loss: 1.21096e-06, min loss: 5.72126e-07\n",
      "Epoch: 1755100, elapsed: 1.29e+01, train loss: 5.73753e-07, val loss: 1.18834e-06, min loss: 5.72126e-07\n",
      "Epoch: 1755200, elapsed: 1.09e+01, train loss: 6.14195e-07, val loss: 1.19161e-06, min loss: 5.72126e-07\n",
      "Epoch: 1755300, elapsed: 1.10e+01, train loss: 7.75183e-07, val loss: 1.47351e-06, min loss: 5.72126e-07\n",
      "Epoch: 1755400, elapsed: 1.10e+01, train loss: 6.59412e-07, val loss: 1.35501e-06, min loss: 5.72126e-07\n",
      "Epoch: 1755500, elapsed: 1.09e+01, train loss: 5.72072e-07, val loss: 1.19438e-06, min loss: 5.72072e-07\n",
      "Epoch: 1755600, elapsed: 1.09e+01, train loss: 6.88709e-07, val loss: 1.31406e-06, min loss: 5.72072e-07\n",
      "Epoch: 1755700, elapsed: 1.08e+01, train loss: 1.00876e-06, val loss: 1.54505e-06, min loss: 5.72072e-07\n",
      "Epoch: 1755800, elapsed: 1.10e+01, train loss: 5.85297e-07, val loss: 1.23826e-06, min loss: 5.72072e-07\n",
      "Epoch: 1755900, elapsed: 1.10e+01, train loss: 6.06595e-07, val loss: 1.18823e-06, min loss: 5.72072e-07\n",
      "Epoch: 1756000, elapsed: 1.10e+01, train loss: 7.76574e-07, val loss: 1.43813e-06, min loss: 5.72072e-07\n",
      "Epoch: 1756100, elapsed: 1.09e+01, train loss: 5.77914e-07, val loss: 1.18536e-06, min loss: 5.72072e-07\n",
      "Epoch: 1756200, elapsed: 1.10e+01, train loss: 5.71842e-07, val loss: 1.19430e-06, min loss: 5.71842e-07\n",
      "Epoch: 1756300, elapsed: 1.07e+01, train loss: 9.25768e-07, val loss: 1.34501e-06, min loss: 5.71842e-07\n",
      "Epoch: 1756400, elapsed: 1.10e+01, train loss: 5.71144e-07, val loss: 1.19057e-06, min loss: 5.71144e-07\n",
      "Epoch: 1756500, elapsed: 1.08e+01, train loss: 5.78241e-07, val loss: 1.20428e-06, min loss: 5.71144e-07\n",
      "Epoch: 1756600, elapsed: 1.08e+01, train loss: 5.71053e-07, val loss: 1.19023e-06, min loss: 5.71053e-07\n",
      "Epoch: 1756700, elapsed: 1.08e+01, train loss: 5.71542e-07, val loss: 1.19263e-06, min loss: 5.71053e-07\n",
      "Epoch: 1756800, elapsed: 1.07e+01, train loss: 1.14393e-06, val loss: 2.12492e-06, min loss: 5.71053e-07\n",
      "Epoch: 1756900, elapsed: 1.09e+01, train loss: 6.45641e-07, val loss: 1.21967e-06, min loss: 5.71053e-07\n",
      "Epoch: 1757000, elapsed: 1.08e+01, train loss: 6.08060e-07, val loss: 1.25827e-06, min loss: 5.71053e-07\n",
      "Epoch: 1757100, elapsed: 1.10e+01, train loss: 7.10056e-07, val loss: 1.42207e-06, min loss: 5.71053e-07\n",
      "Epoch: 1757200, elapsed: 1.08e+01, train loss: 1.71857e-06, val loss: 2.70097e-06, min loss: 5.71053e-07\n",
      "Epoch: 1757300, elapsed: 1.09e+01, train loss: 6.03601e-07, val loss: 1.26086e-06, min loss: 5.71053e-07\n",
      "Epoch: 1757400, elapsed: 1.09e+01, train loss: 5.70965e-07, val loss: 1.19317e-06, min loss: 5.70965e-07\n",
      "Epoch: 1757500, elapsed: 1.09e+01, train loss: 8.32848e-07, val loss: 1.53260e-06, min loss: 5.70965e-07\n",
      "Epoch: 1757600, elapsed: 1.59e+01, train loss: 2.28027e-06, val loss: 3.07669e-06, min loss: 5.70965e-07\n",
      "Epoch: 1757700, elapsed: 1.10e+01, train loss: 5.80009e-07, val loss: 1.20422e-06, min loss: 5.70965e-07\n",
      "Epoch: 1757800, elapsed: 1.10e+01, train loss: 6.51771e-07, val loss: 1.21805e-06, min loss: 5.70965e-07\n",
      "Epoch: 1757900, elapsed: 1.08e+01, train loss: 5.70672e-07, val loss: 1.19282e-06, min loss: 5.70672e-07\n",
      "Epoch: 1758000, elapsed: 1.08e+01, train loss: 5.88579e-07, val loss: 1.22746e-06, min loss: 5.70672e-07\n",
      "Epoch: 1758100, elapsed: 1.10e+01, train loss: 5.86872e-07, val loss: 1.21106e-06, min loss: 5.70672e-07\n",
      "Epoch: 1758200, elapsed: 1.07e+01, train loss: 5.70561e-07, val loss: 1.19010e-06, min loss: 5.70561e-07\n",
      "Epoch: 1758300, elapsed: 1.09e+01, train loss: 5.75450e-07, val loss: 1.19844e-06, min loss: 5.70561e-07\n",
      "Epoch: 1758400, elapsed: 1.10e+01, train loss: 5.82034e-07, val loss: 1.21993e-06, min loss: 5.70561e-07\n",
      "Epoch: 1758500, elapsed: 1.08e+01, train loss: 5.71160e-07, val loss: 1.19132e-06, min loss: 5.70561e-07\n",
      "Epoch: 1758600, elapsed: 1.09e+01, train loss: 7.31765e-07, val loss: 1.32331e-06, min loss: 5.70561e-07\n",
      "Epoch: 1758700, elapsed: 1.09e+01, train loss: 5.95995e-07, val loss: 1.21819e-06, min loss: 5.70561e-07\n",
      "Epoch: 1758800, elapsed: 1.06e+01, train loss: 6.97090e-07, val loss: 1.37872e-06, min loss: 5.70561e-07\n",
      "Epoch: 1758900, elapsed: 1.08e+01, train loss: 6.24578e-07, val loss: 1.29170e-06, min loss: 5.70561e-07\n",
      "Epoch: 1759000, elapsed: 1.08e+01, train loss: 7.31897e-07, val loss: 1.30717e-06, min loss: 5.70561e-07\n",
      "Epoch: 1759100, elapsed: 1.09e+01, train loss: 1.10186e-06, val loss: 1.92836e-06, min loss: 5.70561e-07\n",
      "Epoch: 1759200, elapsed: 1.07e+01, train loss: 6.04026e-07, val loss: 1.20589e-06, min loss: 5.70561e-07\n",
      "Epoch: 1759300, elapsed: 1.08e+01, train loss: 5.71069e-07, val loss: 1.19451e-06, min loss: 5.70561e-07\n",
      "Epoch: 1759400, elapsed: 1.07e+01, train loss: 6.00703e-07, val loss: 1.22703e-06, min loss: 5.70561e-07\n",
      "Epoch: 1759500, elapsed: 1.09e+01, train loss: 6.70041e-07, val loss: 1.24125e-06, min loss: 5.70561e-07\n",
      "Epoch: 1759600, elapsed: 1.06e+01, train loss: 5.71608e-07, val loss: 1.20683e-06, min loss: 5.70561e-07\n",
      "Epoch: 1759700, elapsed: 1.06e+01, train loss: 5.96072e-07, val loss: 1.19814e-06, min loss: 5.70561e-07\n",
      "Epoch: 1759800, elapsed: 1.07e+01, train loss: 1.56262e-06, val loss: 2.20053e-06, min loss: 5.70561e-07\n",
      "Epoch: 1759900, elapsed: 1.08e+01, train loss: 2.28852e-06, val loss: 2.73900e-06, min loss: 5.70561e-07\n",
      "Epoch: 1760000, elapsed: 1.09e+01, train loss: 1.81479e-06, val loss: 1.81943e-06, min loss: 5.70561e-07\n",
      "Epoch: 1760100, elapsed: 1.26e+01, train loss: 3.12728e-06, val loss: 2.70124e-06, min loss: 5.70561e-07\n",
      "Epoch: 1760200, elapsed: 1.08e+01, train loss: 6.98029e-07, val loss: 1.23083e-06, min loss: 5.70561e-07\n",
      "Epoch: 1760300, elapsed: 1.08e+01, train loss: 8.18223e-07, val loss: 1.51670e-06, min loss: 5.70561e-07\n",
      "Epoch: 1760400, elapsed: 1.07e+01, train loss: 7.56421e-07, val loss: 1.40571e-06, min loss: 5.70561e-07\n",
      "Epoch: 1760500, elapsed: 1.07e+01, train loss: 8.72206e-07, val loss: 1.64860e-06, min loss: 5.70561e-07\n",
      "Epoch: 1760600, elapsed: 1.09e+01, train loss: 8.63677e-07, val loss: 1.25707e-06, min loss: 5.70561e-07\n",
      "Epoch: 1760700, elapsed: 1.57e+01, train loss: 6.21714e-07, val loss: 1.29225e-06, min loss: 5.70561e-07\n",
      "Epoch: 1760800, elapsed: 1.11e+01, train loss: 5.70449e-07, val loss: 1.18068e-06, min loss: 5.70449e-07\n",
      "Epoch: 1760900, elapsed: 1.08e+01, train loss: 4.06127e-06, val loss: 4.82268e-06, min loss: 5.70449e-07\n",
      "Epoch: 1761000, elapsed: 1.10e+01, train loss: 5.69295e-07, val loss: 1.19242e-06, min loss: 5.69295e-07\n",
      "Epoch: 1761100, elapsed: 1.11e+01, train loss: 5.73287e-07, val loss: 1.19423e-06, min loss: 5.69295e-07\n",
      "Epoch: 1761200, elapsed: 1.10e+01, train loss: 4.42774e-06, val loss: 3.95282e-06, min loss: 5.69295e-07\n",
      "Epoch: 1761300, elapsed: 1.11e+01, train loss: 9.18453e-07, val loss: 1.59206e-06, min loss: 5.69295e-07\n",
      "Epoch: 1761400, elapsed: 1.09e+01, train loss: 5.87992e-07, val loss: 1.19037e-06, min loss: 5.69295e-07\n",
      "Epoch: 1761500, elapsed: 1.09e+01, train loss: 6.04073e-07, val loss: 1.23826e-06, min loss: 5.69295e-07\n",
      "Epoch: 1761600, elapsed: 1.10e+01, train loss: 5.85965e-07, val loss: 1.21262e-06, min loss: 5.69295e-07\n",
      "Epoch: 1761700, elapsed: 1.11e+01, train loss: 5.74467e-07, val loss: 1.21538e-06, min loss: 5.69295e-07\n",
      "Epoch: 1761800, elapsed: 1.09e+01, train loss: 6.26829e-07, val loss: 1.26478e-06, min loss: 5.69295e-07\n",
      "Epoch: 1761900, elapsed: 1.11e+01, train loss: 1.31359e-06, val loss: 2.32834e-06, min loss: 5.69295e-07\n",
      "Epoch: 1762000, elapsed: 1.10e+01, train loss: 5.74485e-07, val loss: 1.18604e-06, min loss: 5.69295e-07\n",
      "Epoch: 1762100, elapsed: 1.11e+01, train loss: 5.75408e-07, val loss: 1.21434e-06, min loss: 5.69295e-07\n",
      "Epoch: 1762200, elapsed: 1.08e+01, train loss: 6.14382e-07, val loss: 1.24846e-06, min loss: 5.69295e-07\n",
      "Epoch: 1762300, elapsed: 1.08e+01, train loss: 5.78981e-07, val loss: 1.19137e-06, min loss: 5.69295e-07\n",
      "Epoch: 1762400, elapsed: 1.08e+01, train loss: 8.09159e-07, val loss: 1.43591e-06, min loss: 5.69295e-07\n",
      "Epoch: 1762500, elapsed: 1.09e+01, train loss: 5.89813e-07, val loss: 1.21770e-06, min loss: 5.69295e-07\n",
      "Epoch: 1762600, elapsed: 1.08e+01, train loss: 1.07529e-06, val loss: 1.51792e-06, min loss: 5.69295e-07\n",
      "Epoch: 1762700, elapsed: 1.08e+01, train loss: 6.73209e-07, val loss: 1.25701e-06, min loss: 5.69295e-07\n",
      "Epoch: 1762800, elapsed: 1.07e+01, train loss: 5.83072e-07, val loss: 1.22230e-06, min loss: 5.69295e-07\n",
      "Epoch: 1762900, elapsed: 1.08e+01, train loss: 6.74623e-07, val loss: 1.22914e-06, min loss: 5.69295e-07\n",
      "Epoch: 1763000, elapsed: 1.09e+01, train loss: 7.59625e-07, val loss: 1.42477e-06, min loss: 5.69295e-07\n",
      "Epoch: 1763100, elapsed: 1.08e+01, train loss: 6.81216e-07, val loss: 1.34882e-06, min loss: 5.69295e-07\n",
      "Epoch: 1763200, elapsed: 1.10e+01, train loss: 6.29366e-07, val loss: 1.23478e-06, min loss: 5.69295e-07\n",
      "Epoch: 1763300, elapsed: 1.07e+01, train loss: 3.05696e-06, val loss: 2.82771e-06, min loss: 5.69295e-07\n",
      "Epoch: 1763400, elapsed: 1.10e+01, train loss: 5.68369e-07, val loss: 1.19566e-06, min loss: 5.68369e-07\n",
      "Epoch: 1763500, elapsed: 1.07e+01, train loss: 5.70288e-07, val loss: 1.19348e-06, min loss: 5.68369e-07\n",
      "Epoch: 1763600, elapsed: 1.08e+01, train loss: 5.87147e-07, val loss: 1.21921e-06, min loss: 5.68369e-07\n",
      "Epoch: 1763700, elapsed: 1.09e+01, train loss: 5.68848e-07, val loss: 1.19535e-06, min loss: 5.68369e-07\n",
      "Epoch: 1763800, elapsed: 1.09e+01, train loss: 9.14115e-07, val loss: 1.47217e-06, min loss: 5.68369e-07\n",
      "Epoch: 1763900, elapsed: 1.57e+01, train loss: 5.67894e-07, val loss: 1.18972e-06, min loss: 5.67894e-07\n",
      "Epoch: 1764000, elapsed: 1.11e+01, train loss: 5.85315e-07, val loss: 1.22060e-06, min loss: 5.67894e-07\n",
      "Epoch: 1764100, elapsed: 1.10e+01, train loss: 9.26283e-07, val loss: 2.41156e-06, min loss: 5.67894e-07\n",
      "Epoch: 1764200, elapsed: 1.13e+01, train loss: 5.67914e-07, val loss: 1.18983e-06, min loss: 5.67894e-07\n",
      "Epoch: 1764300, elapsed: 1.10e+01, train loss: 9.50453e-07, val loss: 1.61855e-06, min loss: 5.67894e-07\n",
      "Epoch: 1764400, elapsed: 1.10e+01, train loss: 6.06523e-07, val loss: 1.24010e-06, min loss: 5.67894e-07\n",
      "Epoch: 1764500, elapsed: 1.10e+01, train loss: 1.12086e-06, val loss: 1.61316e-06, min loss: 5.67894e-07\n",
      "Epoch: 1764600, elapsed: 1.10e+01, train loss: 6.73173e-07, val loss: 1.27040e-06, min loss: 5.67894e-07\n",
      "Epoch: 1764700, elapsed: 1.10e+01, train loss: 5.96853e-07, val loss: 1.18772e-06, min loss: 5.67894e-07\n",
      "Epoch: 1764800, elapsed: 1.10e+01, train loss: 6.09618e-07, val loss: 1.24284e-06, min loss: 5.67894e-07\n",
      "Epoch: 1764900, elapsed: 1.10e+01, train loss: 6.09616e-07, val loss: 1.22142e-06, min loss: 5.67894e-07\n",
      "Epoch: 1765000, elapsed: 1.11e+01, train loss: 1.83762e-06, val loss: 2.20965e-06, min loss: 5.67894e-07\n",
      "Epoch: 1765100, elapsed: 1.29e+01, train loss: 8.31252e-07, val loss: 1.39602e-06, min loss: 5.67894e-07\n",
      "Epoch: 1765200, elapsed: 1.09e+01, train loss: 5.80561e-07, val loss: 1.23219e-06, min loss: 5.67894e-07\n",
      "Epoch: 1765300, elapsed: 1.11e+01, train loss: 6.46767e-07, val loss: 1.28860e-06, min loss: 5.67894e-07\n",
      "Epoch: 1765400, elapsed: 1.10e+01, train loss: 5.74747e-07, val loss: 1.23143e-06, min loss: 5.67894e-07\n",
      "Epoch: 1765500, elapsed: 1.08e+01, train loss: 7.59559e-07, val loss: 1.45660e-06, min loss: 5.67894e-07\n",
      "Epoch: 1765600, elapsed: 1.09e+01, train loss: 8.45796e-07, val loss: 1.95701e-06, min loss: 5.67894e-07\n",
      "Epoch: 1765700, elapsed: 1.08e+01, train loss: 5.70670e-07, val loss: 1.18914e-06, min loss: 5.67894e-07\n",
      "Epoch: 1765800, elapsed: 1.09e+01, train loss: 5.68739e-07, val loss: 1.19631e-06, min loss: 5.67894e-07\n",
      "Epoch: 1765900, elapsed: 1.08e+01, train loss: 6.82428e-07, val loss: 1.18220e-06, min loss: 5.67894e-07\n",
      "Epoch: 1766000, elapsed: 1.09e+01, train loss: 5.69572e-07, val loss: 1.20397e-06, min loss: 5.67894e-07\n",
      "Epoch: 1766100, elapsed: 1.08e+01, train loss: 8.67049e-07, val loss: 1.61338e-06, min loss: 5.67894e-07\n",
      "Epoch: 1766200, elapsed: 1.09e+01, train loss: 9.54713e-07, val loss: 1.35234e-06, min loss: 5.67894e-07\n",
      "Epoch: 1766300, elapsed: 1.07e+01, train loss: 8.78769e-07, val loss: 1.25700e-06, min loss: 5.67894e-07\n",
      "Epoch: 1766400, elapsed: 1.10e+01, train loss: 8.50468e-07, val loss: 1.34643e-06, min loss: 5.67894e-07\n",
      "Epoch: 1766500, elapsed: 1.11e+01, train loss: 8.70707e-07, val loss: 1.49402e-06, min loss: 5.67894e-07\n",
      "Epoch: 1766600, elapsed: 1.08e+01, train loss: 6.23847e-07, val loss: 1.25780e-06, min loss: 5.67894e-07\n",
      "Epoch: 1766700, elapsed: 1.08e+01, train loss: 5.93032e-07, val loss: 1.22425e-06, min loss: 5.67894e-07\n",
      "Epoch: 1766800, elapsed: 1.08e+01, train loss: 5.67678e-07, val loss: 1.20047e-06, min loss: 5.67678e-07\n",
      "Epoch: 1766900, elapsed: 1.09e+01, train loss: 1.29780e-06, val loss: 2.45443e-06, min loss: 5.67678e-07\n",
      "Epoch: 1767000, elapsed: 1.08e+01, train loss: 5.67273e-07, val loss: 1.19295e-06, min loss: 5.67273e-07\n",
      "Epoch: 1767100, elapsed: 1.59e+01, train loss: 6.08764e-07, val loss: 1.25340e-06, min loss: 5.67273e-07\n",
      "Epoch: 1767200, elapsed: 1.10e+01, train loss: 5.66746e-07, val loss: 1.18892e-06, min loss: 5.66746e-07\n",
      "Epoch: 1767300, elapsed: 1.09e+01, train loss: 5.98547e-07, val loss: 1.22605e-06, min loss: 5.66746e-07\n",
      "Epoch: 1767400, elapsed: 1.10e+01, train loss: 5.69812e-07, val loss: 1.20483e-06, min loss: 5.66746e-07\n",
      "Epoch: 1767500, elapsed: 1.09e+01, train loss: 5.66745e-07, val loss: 1.19431e-06, min loss: 5.66745e-07\n",
      "Epoch: 1767600, elapsed: 1.09e+01, train loss: 5.99627e-07, val loss: 1.21988e-06, min loss: 5.66745e-07\n",
      "Epoch: 1767700, elapsed: 1.09e+01, train loss: 5.72305e-07, val loss: 1.20158e-06, min loss: 5.66745e-07\n",
      "Epoch: 1767800, elapsed: 1.10e+01, train loss: 8.18305e-07, val loss: 1.43570e-06, min loss: 5.66745e-07\n",
      "Epoch: 1767900, elapsed: 1.08e+01, train loss: 8.57370e-07, val loss: 1.53475e-06, min loss: 5.66745e-07\n",
      "Epoch: 1768000, elapsed: 1.08e+01, train loss: 2.62665e-06, val loss: 2.70662e-06, min loss: 5.66745e-07\n",
      "Epoch: 1768100, elapsed: 1.11e+01, train loss: 2.16901e-06, val loss: 3.11515e-06, min loss: 5.66745e-07\n",
      "Epoch: 1768200, elapsed: 1.10e+01, train loss: 6.35106e-07, val loss: 1.38747e-06, min loss: 5.66745e-07\n",
      "Epoch: 1768300, elapsed: 1.09e+01, train loss: 5.69288e-07, val loss: 1.20003e-06, min loss: 5.66745e-07\n",
      "Epoch: 1768400, elapsed: 1.10e+01, train loss: 5.66444e-07, val loss: 1.18478e-06, min loss: 5.66444e-07\n",
      "Epoch: 1768500, elapsed: 1.08e+01, train loss: 5.97431e-07, val loss: 1.21799e-06, min loss: 5.66444e-07\n",
      "Epoch: 1768600, elapsed: 1.08e+01, train loss: 6.85049e-07, val loss: 1.19775e-06, min loss: 5.66444e-07\n",
      "Epoch: 1768700, elapsed: 1.07e+01, train loss: 9.43951e-07, val loss: 1.74044e-06, min loss: 5.66444e-07\n",
      "Epoch: 1768800, elapsed: 1.09e+01, train loss: 7.99110e-07, val loss: 1.36387e-06, min loss: 5.66444e-07\n",
      "Epoch: 1768900, elapsed: 1.07e+01, train loss: 2.56797e-06, val loss: 2.00232e-06, min loss: 5.66444e-07\n",
      "Epoch: 1769000, elapsed: 1.05e+01, train loss: 5.89628e-07, val loss: 1.18268e-06, min loss: 5.66444e-07\n",
      "Epoch: 1769100, elapsed: 1.06e+01, train loss: 3.47332e-06, val loss: 3.64134e-06, min loss: 5.66444e-07\n",
      "Epoch: 1769200, elapsed: 1.06e+01, train loss: 5.82531e-07, val loss: 1.16704e-06, min loss: 5.66444e-07\n",
      "Epoch: 1769300, elapsed: 1.09e+01, train loss: 5.79476e-07, val loss: 1.19353e-06, min loss: 5.66444e-07\n",
      "Epoch: 1769400, elapsed: 1.08e+01, train loss: 6.12617e-07, val loss: 1.25485e-06, min loss: 5.66444e-07\n",
      "Epoch: 1769500, elapsed: 1.07e+01, train loss: 5.98897e-07, val loss: 1.19651e-06, min loss: 5.66444e-07\n",
      "Epoch: 1769600, elapsed: 1.07e+01, train loss: 1.72012e-06, val loss: 2.25032e-06, min loss: 5.66444e-07\n",
      "Epoch: 1769700, elapsed: 1.07e+01, train loss: 6.61379e-07, val loss: 1.31785e-06, min loss: 5.66444e-07\n",
      "Epoch: 1769800, elapsed: 1.08e+01, train loss: 5.68406e-07, val loss: 1.18578e-06, min loss: 5.66444e-07\n",
      "Epoch: 1769900, elapsed: 1.08e+01, train loss: 6.28296e-07, val loss: 1.31450e-06, min loss: 5.66444e-07\n",
      "Epoch: 1770000, elapsed: 1.08e+01, train loss: 5.95729e-07, val loss: 1.19375e-06, min loss: 5.66444e-07\n",
      "Epoch: 1770100, elapsed: 1.29e+01, train loss: 5.67312e-07, val loss: 1.18830e-06, min loss: 5.66444e-07\n",
      "Epoch: 1770200, elapsed: 1.56e+01, train loss: 1.12119e-06, val loss: 1.81610e-06, min loss: 5.66444e-07\n",
      "Epoch: 1770300, elapsed: 1.11e+01, train loss: 9.43270e-07, val loss: 1.61555e-06, min loss: 5.66444e-07\n",
      "Epoch: 1770400, elapsed: 1.12e+01, train loss: 1.27146e-06, val loss: 1.95977e-06, min loss: 5.66444e-07\n",
      "Epoch: 1770500, elapsed: 1.09e+01, train loss: 6.15053e-07, val loss: 1.21173e-06, min loss: 5.66444e-07\n",
      "Epoch: 1770600, elapsed: 1.11e+01, train loss: 5.85443e-07, val loss: 1.21369e-06, min loss: 5.66444e-07\n",
      "Epoch: 1770700, elapsed: 1.09e+01, train loss: 5.69002e-07, val loss: 1.18920e-06, min loss: 5.66444e-07\n",
      "Epoch: 1770800, elapsed: 1.09e+01, train loss: 5.75065e-07, val loss: 1.21774e-06, min loss: 5.66444e-07\n",
      "Epoch: 1770900, elapsed: 1.09e+01, train loss: 9.68408e-07, val loss: 2.06869e-06, min loss: 5.66444e-07\n",
      "Epoch: 1771000, elapsed: 1.09e+01, train loss: 5.64951e-07, val loss: 1.19049e-06, min loss: 5.64951e-07\n",
      "Epoch: 1771100, elapsed: 1.10e+01, train loss: 8.68278e-07, val loss: 1.37383e-06, min loss: 5.64951e-07\n",
      "Epoch: 1771200, elapsed: 1.09e+01, train loss: 5.67865e-07, val loss: 1.17842e-06, min loss: 5.64951e-07\n",
      "Epoch: 1771300, elapsed: 1.10e+01, train loss: 5.79491e-07, val loss: 1.21672e-06, min loss: 5.64951e-07\n",
      "Epoch: 1771400, elapsed: 1.10e+01, train loss: 5.83910e-07, val loss: 1.21559e-06, min loss: 5.64951e-07\n",
      "Epoch: 1771500, elapsed: 1.10e+01, train loss: 5.65390e-07, val loss: 1.18743e-06, min loss: 5.64951e-07\n",
      "Epoch: 1771600, elapsed: 1.08e+01, train loss: 5.67104e-07, val loss: 1.18446e-06, min loss: 5.64951e-07\n",
      "Epoch: 1771700, elapsed: 1.09e+01, train loss: 5.70056e-07, val loss: 1.18275e-06, min loss: 5.64951e-07\n",
      "Epoch: 1771800, elapsed: 1.08e+01, train loss: 8.44244e-07, val loss: 1.46393e-06, min loss: 5.64951e-07\n",
      "Epoch: 1771900, elapsed: 1.08e+01, train loss: 7.72956e-07, val loss: 1.30591e-06, min loss: 5.64951e-07\n",
      "Epoch: 1772000, elapsed: 1.09e+01, train loss: 5.87053e-07, val loss: 1.26140e-06, min loss: 5.64951e-07\n",
      "Epoch: 1772100, elapsed: 1.10e+01, train loss: 7.86574e-07, val loss: 1.35549e-06, min loss: 5.64951e-07\n",
      "Epoch: 1772200, elapsed: 1.08e+01, train loss: 5.77059e-07, val loss: 1.18627e-06, min loss: 5.64951e-07\n",
      "Epoch: 1772300, elapsed: 1.09e+01, train loss: 6.58519e-07, val loss: 1.22566e-06, min loss: 5.64951e-07\n",
      "Epoch: 1772400, elapsed: 1.08e+01, train loss: 5.85943e-07, val loss: 1.20216e-06, min loss: 5.64951e-07\n",
      "Epoch: 1772500, elapsed: 1.08e+01, train loss: 5.93279e-07, val loss: 1.19921e-06, min loss: 5.64951e-07\n",
      "Epoch: 1772600, elapsed: 1.09e+01, train loss: 5.67413e-07, val loss: 1.17875e-06, min loss: 5.64951e-07\n",
      "Epoch: 1772700, elapsed: 1.08e+01, train loss: 5.65026e-07, val loss: 1.18840e-06, min loss: 5.64951e-07\n",
      "Epoch: 1772800, elapsed: 1.08e+01, train loss: 6.83394e-07, val loss: 1.26194e-06, min loss: 5.64951e-07\n",
      "Epoch: 1772900, elapsed: 1.09e+01, train loss: 5.82343e-07, val loss: 1.23205e-06, min loss: 5.64951e-07\n",
      "Epoch: 1773000, elapsed: 1.10e+01, train loss: 5.82903e-07, val loss: 1.19001e-06, min loss: 5.64951e-07\n",
      "Epoch: 1773100, elapsed: 1.09e+01, train loss: 6.70169e-07, val loss: 1.36498e-06, min loss: 5.64951e-07\n",
      "Epoch: 1773200, elapsed: 1.08e+01, train loss: 9.07155e-07, val loss: 1.50388e-06, min loss: 5.64951e-07\n",
      "Epoch: 1773300, elapsed: 1.07e+01, train loss: 5.66325e-07, val loss: 1.20354e-06, min loss: 5.64951e-07\n",
      "Epoch: 1773400, elapsed: 1.59e+01, train loss: 5.65512e-07, val loss: 1.18585e-06, min loss: 5.64951e-07\n",
      "Epoch: 1773500, elapsed: 1.11e+01, train loss: 6.07838e-07, val loss: 1.26941e-06, min loss: 5.64951e-07\n",
      "Epoch: 1773600, elapsed: 1.10e+01, train loss: 8.27753e-07, val loss: 1.32945e-06, min loss: 5.64951e-07\n",
      "Epoch: 1773700, elapsed: 1.09e+01, train loss: 5.64033e-07, val loss: 1.19152e-06, min loss: 5.64033e-07\n",
      "Epoch: 1773800, elapsed: 1.09e+01, train loss: 5.65229e-07, val loss: 1.19518e-06, min loss: 5.64033e-07\n",
      "Epoch: 1773900, elapsed: 1.09e+01, train loss: 6.81552e-07, val loss: 1.38281e-06, min loss: 5.64033e-07\n",
      "Epoch: 1774000, elapsed: 1.11e+01, train loss: 6.31715e-07, val loss: 1.20810e-06, min loss: 5.64033e-07\n",
      "Epoch: 1774100, elapsed: 1.09e+01, train loss: 9.97120e-07, val loss: 1.69225e-06, min loss: 5.64033e-07\n",
      "Epoch: 1774200, elapsed: 1.09e+01, train loss: 5.71377e-07, val loss: 1.18098e-06, min loss: 5.64033e-07\n",
      "Epoch: 1774300, elapsed: 1.11e+01, train loss: 5.63658e-07, val loss: 1.18967e-06, min loss: 5.63658e-07\n",
      "Epoch: 1774400, elapsed: 1.07e+01, train loss: 6.36341e-07, val loss: 1.22744e-06, min loss: 5.63658e-07\n",
      "Epoch: 1774500, elapsed: 1.09e+01, train loss: 2.38468e-06, val loss: 3.68905e-06, min loss: 5.63658e-07\n",
      "Epoch: 1774600, elapsed: 1.09e+01, train loss: 6.90569e-07, val loss: 1.38739e-06, min loss: 5.63658e-07\n",
      "Epoch: 1774700, elapsed: 1.09e+01, train loss: 5.77504e-07, val loss: 1.21585e-06, min loss: 5.63658e-07\n",
      "Epoch: 1774800, elapsed: 1.10e+01, train loss: 6.30509e-07, val loss: 1.29348e-06, min loss: 5.63658e-07\n",
      "Epoch: 1774900, elapsed: 1.07e+01, train loss: 5.69256e-07, val loss: 1.19152e-06, min loss: 5.63658e-07\n",
      "Epoch: 1775000, elapsed: 1.11e+01, train loss: 5.72554e-07, val loss: 1.20013e-06, min loss: 5.63658e-07\n",
      "Epoch: 1775100, elapsed: 1.28e+01, train loss: 6.84159e-07, val loss: 1.38761e-06, min loss: 5.63658e-07\n",
      "Epoch: 1775200, elapsed: 1.09e+01, train loss: 1.74738e-06, val loss: 2.26403e-06, min loss: 5.63658e-07\n",
      "Epoch: 1775300, elapsed: 1.09e+01, train loss: 6.90998e-07, val loss: 1.40801e-06, min loss: 5.63658e-07\n",
      "Epoch: 1775400, elapsed: 1.08e+01, train loss: 5.70161e-07, val loss: 1.18840e-06, min loss: 5.63658e-07\n",
      "Epoch: 1775500, elapsed: 1.07e+01, train loss: 5.68543e-07, val loss: 1.18851e-06, min loss: 5.63658e-07\n",
      "Epoch: 1775600, elapsed: 1.09e+01, train loss: 5.65488e-07, val loss: 1.19822e-06, min loss: 5.63658e-07\n",
      "Epoch: 1775700, elapsed: 1.06e+01, train loss: 5.66408e-07, val loss: 1.19302e-06, min loss: 5.63658e-07\n",
      "Epoch: 1775800, elapsed: 1.10e+01, train loss: 6.03183e-07, val loss: 1.20107e-06, min loss: 5.63658e-07\n",
      "Epoch: 1775900, elapsed: 1.07e+01, train loss: 1.36748e-06, val loss: 1.50484e-06, min loss: 5.63658e-07\n",
      "Epoch: 1776000, elapsed: 1.09e+01, train loss: 5.63748e-07, val loss: 1.18560e-06, min loss: 5.63658e-07\n",
      "Epoch: 1776100, elapsed: 1.10e+01, train loss: 5.64729e-07, val loss: 1.19989e-06, min loss: 5.63658e-07\n",
      "Epoch: 1776200, elapsed: 1.07e+01, train loss: 5.83349e-07, val loss: 1.17711e-06, min loss: 5.63658e-07\n",
      "Epoch: 1776300, elapsed: 1.06e+01, train loss: 1.13161e-06, val loss: 1.65399e-06, min loss: 5.63658e-07\n",
      "Epoch: 1776400, elapsed: 1.09e+01, train loss: 5.63017e-07, val loss: 1.19704e-06, min loss: 5.63017e-07\n",
      "Epoch: 1776500, elapsed: 1.10e+01, train loss: 6.32059e-07, val loss: 1.23513e-06, min loss: 5.63017e-07\n",
      "Epoch: 1776600, elapsed: 1.59e+01, train loss: 6.02692e-07, val loss: 1.24046e-06, min loss: 5.63017e-07\n",
      "Epoch: 1776700, elapsed: 1.09e+01, train loss: 5.64565e-07, val loss: 1.19705e-06, min loss: 5.63017e-07\n",
      "Epoch: 1776800, elapsed: 1.07e+01, train loss: 5.69700e-07, val loss: 1.21184e-06, min loss: 5.63017e-07\n",
      "Epoch: 1776900, elapsed: 1.10e+01, train loss: 5.68733e-07, val loss: 1.18743e-06, min loss: 5.63017e-07\n",
      "Epoch: 1777000, elapsed: 1.09e+01, train loss: 5.63142e-07, val loss: 1.18554e-06, min loss: 5.63017e-07\n",
      "Epoch: 1777100, elapsed: 1.08e+01, train loss: 6.40676e-07, val loss: 1.27116e-06, min loss: 5.63017e-07\n",
      "Epoch: 1777200, elapsed: 1.09e+01, train loss: 7.41578e-07, val loss: 1.37804e-06, min loss: 5.63017e-07\n",
      "Epoch: 1777300, elapsed: 1.08e+01, train loss: 5.74998e-07, val loss: 1.19523e-06, min loss: 5.63017e-07\n",
      "Epoch: 1777400, elapsed: 1.11e+01, train loss: 5.69507e-07, val loss: 1.20169e-06, min loss: 5.63017e-07\n",
      "Epoch: 1777500, elapsed: 1.08e+01, train loss: 5.72477e-07, val loss: 1.18384e-06, min loss: 5.63017e-07\n",
      "Epoch: 1777600, elapsed: 1.08e+01, train loss: 5.91837e-07, val loss: 1.22394e-06, min loss: 5.63017e-07\n",
      "Epoch: 1777700, elapsed: 1.08e+01, train loss: 5.81654e-07, val loss: 1.21312e-06, min loss: 5.63017e-07\n",
      "Epoch: 1777800, elapsed: 1.10e+01, train loss: 6.12534e-07, val loss: 1.35696e-06, min loss: 5.63017e-07\n",
      "Epoch: 1777900, elapsed: 1.07e+01, train loss: 6.45921e-07, val loss: 1.27893e-06, min loss: 5.63017e-07\n",
      "Epoch: 1778000, elapsed: 1.11e+01, train loss: 5.85646e-07, val loss: 1.19017e-06, min loss: 5.63017e-07\n",
      "Epoch: 1778100, elapsed: 1.08e+01, train loss: 6.13798e-07, val loss: 1.28277e-06, min loss: 5.63017e-07\n",
      "Epoch: 1778200, elapsed: 1.11e+01, train loss: 2.19426e-06, val loss: 2.78542e-06, min loss: 5.63017e-07\n",
      "Epoch: 1778300, elapsed: 1.07e+01, train loss: 1.54291e-06, val loss: 2.30366e-06, min loss: 5.63017e-07\n",
      "Epoch: 1778400, elapsed: 1.06e+01, train loss: 7.00856e-07, val loss: 1.31026e-06, min loss: 5.63017e-07\n",
      "Epoch: 1778500, elapsed: 1.08e+01, train loss: 6.51668e-07, val loss: 1.27670e-06, min loss: 5.63017e-07\n",
      "Epoch: 1778600, elapsed: 1.09e+01, train loss: 6.13020e-07, val loss: 1.21650e-06, min loss: 5.63017e-07\n",
      "Epoch: 1778700, elapsed: 1.08e+01, train loss: 5.62077e-07, val loss: 1.19283e-06, min loss: 5.62077e-07\n",
      "Epoch: 1778800, elapsed: 1.08e+01, train loss: 6.29277e-07, val loss: 1.26381e-06, min loss: 5.62077e-07\n",
      "Epoch: 1778900, elapsed: 1.08e+01, train loss: 2.61821e-06, val loss: 3.51549e-06, min loss: 5.62077e-07\n",
      "Epoch: 1779000, elapsed: 1.10e+01, train loss: 7.48961e-07, val loss: 1.44954e-06, min loss: 5.62077e-07\n",
      "Epoch: 1779100, elapsed: 1.07e+01, train loss: 5.61718e-07, val loss: 1.18941e-06, min loss: 5.61718e-07\n",
      "Epoch: 1779200, elapsed: 1.08e+01, train loss: 5.89228e-07, val loss: 1.18212e-06, min loss: 5.61718e-07\n",
      "Epoch: 1779300, elapsed: 1.10e+01, train loss: 3.00430e-06, val loss: 3.98018e-06, min loss: 5.61718e-07\n",
      "Epoch: 1779400, elapsed: 1.06e+01, train loss: 5.61600e-07, val loss: 1.19164e-06, min loss: 5.61600e-07\n",
      "Epoch: 1779500, elapsed: 1.07e+01, train loss: 5.79630e-07, val loss: 1.18170e-06, min loss: 5.61600e-07\n",
      "Epoch: 1779600, elapsed: 1.09e+01, train loss: 7.25752e-07, val loss: 1.36691e-06, min loss: 5.61600e-07\n",
      "Epoch: 1779700, elapsed: 1.06e+01, train loss: 7.69570e-07, val loss: 1.40999e-06, min loss: 5.61600e-07\n",
      "Epoch: 1779800, elapsed: 1.62e+01, train loss: 6.37299e-07, val loss: 1.24027e-06, min loss: 5.61600e-07\n",
      "Epoch: 1779900, elapsed: 1.10e+01, train loss: 5.65514e-07, val loss: 1.18436e-06, min loss: 5.61600e-07\n",
      "Epoch: 1780000, elapsed: 1.11e+01, train loss: 5.62922e-07, val loss: 1.19394e-06, min loss: 5.61600e-07\n",
      "Epoch: 1780100, elapsed: 1.31e+01, train loss: 5.65160e-07, val loss: 1.20779e-06, min loss: 5.61600e-07\n",
      "Epoch: 1780200, elapsed: 1.11e+01, train loss: 7.86121e-07, val loss: 1.31031e-06, min loss: 5.61600e-07\n",
      "Epoch: 1780300, elapsed: 1.09e+01, train loss: 9.75177e-07, val loss: 1.59988e-06, min loss: 5.61600e-07\n",
      "Epoch: 1780400, elapsed: 1.10e+01, train loss: 1.62139e-06, val loss: 2.33715e-06, min loss: 5.61600e-07\n",
      "Epoch: 1780500, elapsed: 1.10e+01, train loss: 1.77994e-06, val loss: 2.41170e-06, min loss: 5.61600e-07\n",
      "Epoch: 1780600, elapsed: 1.09e+01, train loss: 5.99706e-07, val loss: 1.31355e-06, min loss: 5.61600e-07\n",
      "Epoch: 1780700, elapsed: 1.09e+01, train loss: 8.95239e-07, val loss: 1.57168e-06, min loss: 5.61600e-07\n",
      "Epoch: 1780800, elapsed: 1.09e+01, train loss: 1.24878e-06, val loss: 1.90749e-06, min loss: 5.61600e-07\n",
      "Epoch: 1780900, elapsed: 1.08e+01, train loss: 8.09861e-07, val loss: 1.21201e-06, min loss: 5.61600e-07\n",
      "Epoch: 1781000, elapsed: 1.09e+01, train loss: 2.12350e-06, val loss: 2.75651e-06, min loss: 5.61600e-07\n",
      "Epoch: 1781100, elapsed: 1.11e+01, train loss: 1.35181e-06, val loss: 2.01133e-06, min loss: 5.61600e-07\n",
      "Epoch: 1781200, elapsed: 1.08e+01, train loss: 6.03440e-07, val loss: 1.22619e-06, min loss: 5.61600e-07\n",
      "Epoch: 1781300, elapsed: 1.09e+01, train loss: 5.61052e-07, val loss: 1.19095e-06, min loss: 5.61052e-07\n",
      "Epoch: 1781400, elapsed: 1.09e+01, train loss: 7.37734e-07, val loss: 1.38386e-06, min loss: 5.61052e-07\n",
      "Epoch: 1781500, elapsed: 1.09e+01, train loss: 5.60536e-07, val loss: 1.19192e-06, min loss: 5.60536e-07\n",
      "Epoch: 1781600, elapsed: 1.08e+01, train loss: 6.95003e-07, val loss: 1.45876e-06, min loss: 5.60536e-07\n",
      "Epoch: 1781700, elapsed: 1.09e+01, train loss: 5.60450e-07, val loss: 1.19197e-06, min loss: 5.60450e-07\n",
      "Epoch: 1781800, elapsed: 1.08e+01, train loss: 6.41663e-07, val loss: 1.25801e-06, min loss: 5.60450e-07\n",
      "Epoch: 1781900, elapsed: 1.08e+01, train loss: 5.81552e-07, val loss: 1.24676e-06, min loss: 5.60450e-07\n",
      "Epoch: 1782000, elapsed: 1.09e+01, train loss: 5.73081e-07, val loss: 1.25040e-06, min loss: 5.60450e-07\n",
      "Epoch: 1782100, elapsed: 1.08e+01, train loss: 2.62952e-06, val loss: 4.00193e-06, min loss: 5.60450e-07\n",
      "Epoch: 1782200, elapsed: 1.06e+01, train loss: 5.60244e-07, val loss: 1.19162e-06, min loss: 5.60244e-07\n",
      "Epoch: 1782300, elapsed: 1.08e+01, train loss: 5.70051e-07, val loss: 1.20163e-06, min loss: 5.60244e-07\n",
      "Epoch: 1782400, elapsed: 1.08e+01, train loss: 6.07517e-07, val loss: 1.23872e-06, min loss: 5.60244e-07\n",
      "Epoch: 1782500, elapsed: 1.08e+01, train loss: 6.00406e-07, val loss: 1.25289e-06, min loss: 5.60244e-07\n",
      "Epoch: 1782600, elapsed: 1.08e+01, train loss: 1.65953e-06, val loss: 2.93765e-06, min loss: 5.60244e-07\n",
      "Epoch: 1782700, elapsed: 1.07e+01, train loss: 5.60348e-07, val loss: 1.19012e-06, min loss: 5.60244e-07\n",
      "Epoch: 1782800, elapsed: 1.09e+01, train loss: 5.71936e-07, val loss: 1.18395e-06, min loss: 5.60244e-07\n",
      "Epoch: 1782900, elapsed: 1.56e+01, train loss: 5.69704e-07, val loss: 1.22385e-06, min loss: 5.60244e-07\n",
      "Epoch: 1783000, elapsed: 1.12e+01, train loss: 5.60284e-07, val loss: 1.18806e-06, min loss: 5.60244e-07\n",
      "Epoch: 1783100, elapsed: 1.09e+01, train loss: 3.03762e-06, val loss: 2.29166e-06, min loss: 5.60244e-07\n",
      "Epoch: 1783200, elapsed: 1.11e+01, train loss: 5.59921e-07, val loss: 1.18903e-06, min loss: 5.59921e-07\n",
      "Epoch: 1783300, elapsed: 1.10e+01, train loss: 6.02157e-07, val loss: 1.23465e-06, min loss: 5.59921e-07\n",
      "Epoch: 1783400, elapsed: 1.10e+01, train loss: 5.59727e-07, val loss: 1.19344e-06, min loss: 5.59727e-07\n",
      "Epoch: 1783500, elapsed: 1.11e+01, train loss: 5.74583e-07, val loss: 1.17971e-06, min loss: 5.59727e-07\n",
      "Epoch: 1783600, elapsed: 1.08e+01, train loss: 1.68659e-06, val loss: 1.35111e-06, min loss: 5.59727e-07\n",
      "Epoch: 1783700, elapsed: 1.10e+01, train loss: 5.59656e-07, val loss: 1.19185e-06, min loss: 5.59656e-07\n",
      "Epoch: 1783800, elapsed: 1.09e+01, train loss: 5.70514e-07, val loss: 1.21537e-06, min loss: 5.59656e-07\n",
      "Epoch: 1783900, elapsed: 1.09e+01, train loss: 6.00974e-07, val loss: 1.22591e-06, min loss: 5.59656e-07\n",
      "Epoch: 1784000, elapsed: 1.09e+01, train loss: 2.22768e-06, val loss: 3.01124e-06, min loss: 5.59656e-07\n",
      "Epoch: 1784100, elapsed: 1.11e+01, train loss: 5.60674e-07, val loss: 1.18283e-06, min loss: 5.59656e-07\n",
      "Epoch: 1784200, elapsed: 1.10e+01, train loss: 5.60600e-07, val loss: 1.20225e-06, min loss: 5.59656e-07\n",
      "Epoch: 1784300, elapsed: 1.09e+01, train loss: 5.59603e-07, val loss: 1.18946e-06, min loss: 5.59603e-07\n",
      "Epoch: 1784400, elapsed: 1.09e+01, train loss: 5.96875e-07, val loss: 1.26579e-06, min loss: 5.59603e-07\n",
      "Epoch: 1784500, elapsed: 1.10e+01, train loss: 1.37984e-06, val loss: 2.23671e-06, min loss: 5.59603e-07\n",
      "Epoch: 1784600, elapsed: 1.09e+01, train loss: 5.60668e-07, val loss: 1.18516e-06, min loss: 5.59603e-07\n",
      "Epoch: 1784700, elapsed: 1.09e+01, train loss: 5.61346e-07, val loss: 1.18279e-06, min loss: 5.59603e-07\n",
      "Epoch: 1784800, elapsed: 1.10e+01, train loss: 5.78828e-07, val loss: 1.22558e-06, min loss: 5.59603e-07\n",
      "Epoch: 1784900, elapsed: 1.08e+01, train loss: 4.36106e-06, val loss: 3.78063e-06, min loss: 5.59603e-07\n",
      "Epoch: 1785000, elapsed: 1.12e+01, train loss: 5.59253e-07, val loss: 1.19097e-06, min loss: 5.59253e-07\n",
      "Epoch: 1785100, elapsed: 1.27e+01, train loss: 6.58102e-07, val loss: 1.46090e-06, min loss: 5.59253e-07\n",
      "Epoch: 1785200, elapsed: 1.09e+01, train loss: 5.72261e-07, val loss: 1.16272e-06, min loss: 5.59253e-07\n",
      "Epoch: 1785300, elapsed: 1.08e+01, train loss: 6.82374e-07, val loss: 1.30333e-06, min loss: 5.59253e-07\n",
      "Epoch: 1785400, elapsed: 1.08e+01, train loss: 1.47398e-06, val loss: 1.62372e-06, min loss: 5.59253e-07\n",
      "Epoch: 1785500, elapsed: 1.09e+01, train loss: 1.46073e-06, val loss: 1.60881e-06, min loss: 5.59253e-07\n",
      "Epoch: 1785600, elapsed: 1.10e+01, train loss: 6.16367e-07, val loss: 1.23911e-06, min loss: 5.59253e-07\n",
      "Epoch: 1785700, elapsed: 1.09e+01, train loss: 5.65167e-07, val loss: 1.20103e-06, min loss: 5.59253e-07\n",
      "Epoch: 1785800, elapsed: 1.09e+01, train loss: 5.76008e-07, val loss: 1.18333e-06, min loss: 5.59253e-07\n",
      "Epoch: 1785900, elapsed: 1.06e+01, train loss: 5.67414e-07, val loss: 1.21590e-06, min loss: 5.59253e-07\n",
      "Epoch: 1786000, elapsed: 1.08e+01, train loss: 5.62255e-07, val loss: 1.20169e-06, min loss: 5.59253e-07\n",
      "Epoch: 1786100, elapsed: 1.59e+01, train loss: 5.85815e-07, val loss: 1.21497e-06, min loss: 5.59253e-07\n",
      "Epoch: 1786200, elapsed: 1.09e+01, train loss: 6.79791e-07, val loss: 1.25034e-06, min loss: 5.59253e-07\n",
      "Epoch: 1786300, elapsed: 1.10e+01, train loss: 6.47781e-07, val loss: 1.19231e-06, min loss: 5.59253e-07\n",
      "Epoch: 1786400, elapsed: 1.11e+01, train loss: 6.00317e-07, val loss: 1.24548e-06, min loss: 5.59253e-07\n",
      "Epoch: 1786500, elapsed: 1.10e+01, train loss: 5.58541e-07, val loss: 1.19325e-06, min loss: 5.58541e-07\n",
      "Epoch: 1786600, elapsed: 1.11e+01, train loss: 5.61039e-07, val loss: 1.18602e-06, min loss: 5.58541e-07\n",
      "Epoch: 1786700, elapsed: 1.12e+01, train loss: 6.31918e-07, val loss: 1.20380e-06, min loss: 5.58541e-07\n",
      "Epoch: 1786800, elapsed: 1.10e+01, train loss: 1.42363e-06, val loss: 2.47228e-06, min loss: 5.58541e-07\n",
      "Epoch: 1786900, elapsed: 1.09e+01, train loss: 6.23228e-07, val loss: 1.19773e-06, min loss: 5.58541e-07\n",
      "Epoch: 1787000, elapsed: 1.11e+01, train loss: 5.59549e-07, val loss: 1.18166e-06, min loss: 5.58541e-07\n",
      "Epoch: 1787100, elapsed: 1.09e+01, train loss: 5.79038e-07, val loss: 1.22387e-06, min loss: 5.58541e-07\n",
      "Epoch: 1787200, elapsed: 1.09e+01, train loss: 6.10388e-07, val loss: 1.28196e-06, min loss: 5.58541e-07\n",
      "Epoch: 1787300, elapsed: 1.09e+01, train loss: 6.38215e-07, val loss: 1.23160e-06, min loss: 5.58541e-07\n",
      "Epoch: 1787400, elapsed: 1.08e+01, train loss: 6.39404e-07, val loss: 1.32861e-06, min loss: 5.58541e-07\n",
      "Epoch: 1787500, elapsed: 1.10e+01, train loss: 2.38888e-06, val loss: 2.89070e-06, min loss: 5.58541e-07\n",
      "Epoch: 1787600, elapsed: 1.09e+01, train loss: 2.62005e-06, val loss: 2.96176e-06, min loss: 5.58541e-07\n",
      "Epoch: 1787700, elapsed: 1.09e+01, train loss: 5.59609e-07, val loss: 1.18506e-06, min loss: 5.58541e-07\n",
      "Epoch: 1787800, elapsed: 1.10e+01, train loss: 5.58882e-07, val loss: 1.20061e-06, min loss: 5.58541e-07\n",
      "Epoch: 1787900, elapsed: 1.08e+01, train loss: 2.98654e-06, val loss: 3.64201e-06, min loss: 5.58541e-07\n",
      "Epoch: 1788000, elapsed: 1.08e+01, train loss: 6.65205e-07, val loss: 1.30391e-06, min loss: 5.58541e-07\n",
      "Epoch: 1788100, elapsed: 1.09e+01, train loss: 5.57905e-07, val loss: 1.19332e-06, min loss: 5.57905e-07\n",
      "Epoch: 1788200, elapsed: 1.06e+01, train loss: 5.58617e-07, val loss: 1.19526e-06, min loss: 5.57905e-07\n",
      "Epoch: 1788300, elapsed: 1.09e+01, train loss: 1.23038e-06, val loss: 2.01502e-06, min loss: 5.57905e-07\n",
      "Epoch: 1788400, elapsed: 1.07e+01, train loss: 5.57809e-07, val loss: 1.19575e-06, min loss: 5.57809e-07\n",
      "Epoch: 1788500, elapsed: 1.09e+01, train loss: 5.59887e-07, val loss: 1.19135e-06, min loss: 5.57809e-07\n",
      "Epoch: 1788600, elapsed: 1.09e+01, train loss: 5.61704e-07, val loss: 1.20444e-06, min loss: 5.57809e-07\n",
      "Epoch: 1788700, elapsed: 1.09e+01, train loss: 5.63179e-07, val loss: 1.19898e-06, min loss: 5.57809e-07\n",
      "Epoch: 1788800, elapsed: 1.07e+01, train loss: 5.69652e-07, val loss: 1.19356e-06, min loss: 5.57809e-07\n",
      "Epoch: 1788900, elapsed: 1.08e+01, train loss: 5.62921e-07, val loss: 1.21474e-06, min loss: 5.57809e-07\n",
      "Epoch: 1789000, elapsed: 1.08e+01, train loss: 7.38097e-07, val loss: 1.16567e-06, min loss: 5.57809e-07\n",
      "Epoch: 1789100, elapsed: 1.10e+01, train loss: 9.40035e-07, val loss: 1.60700e-06, min loss: 5.57809e-07\n",
      "Epoch: 1789200, elapsed: 1.08e+01, train loss: 5.57959e-07, val loss: 1.19802e-06, min loss: 5.57809e-07\n",
      "Epoch: 1789300, elapsed: 1.58e+01, train loss: 5.58442e-07, val loss: 1.19593e-06, min loss: 5.57809e-07\n",
      "Epoch: 1789400, elapsed: 1.12e+01, train loss: 5.58493e-07, val loss: 1.18998e-06, min loss: 5.57809e-07\n",
      "Epoch: 1789500, elapsed: 1.11e+01, train loss: 5.76372e-07, val loss: 1.22213e-06, min loss: 5.57809e-07\n",
      "Epoch: 1789600, elapsed: 1.10e+01, train loss: 5.84472e-07, val loss: 1.20279e-06, min loss: 5.57809e-07\n",
      "Epoch: 1789700, elapsed: 1.09e+01, train loss: 6.71023e-07, val loss: 1.29275e-06, min loss: 5.57809e-07\n",
      "Epoch: 1789800, elapsed: 1.10e+01, train loss: 9.21389e-07, val loss: 1.93948e-06, min loss: 5.57809e-07\n",
      "Epoch: 1789900, elapsed: 1.09e+01, train loss: 6.67526e-07, val loss: 1.33633e-06, min loss: 5.57809e-07\n",
      "Epoch: 1790000, elapsed: 1.10e+01, train loss: 5.58593e-07, val loss: 1.19626e-06, min loss: 5.57809e-07\n",
      "Epoch: 1790100, elapsed: 1.29e+01, train loss: 6.11340e-07, val loss: 1.27863e-06, min loss: 5.57809e-07\n",
      "Epoch: 1790200, elapsed: 1.11e+01, train loss: 1.29169e-06, val loss: 2.07894e-06, min loss: 5.57809e-07\n",
      "Epoch: 1790300, elapsed: 1.09e+01, train loss: 6.51527e-07, val loss: 1.23154e-06, min loss: 5.57809e-07\n",
      "Epoch: 1790400, elapsed: 1.10e+01, train loss: 5.57130e-07, val loss: 1.19518e-06, min loss: 5.57130e-07\n",
      "Epoch: 1790500, elapsed: 1.09e+01, train loss: 5.65737e-07, val loss: 1.19359e-06, min loss: 5.57130e-07\n",
      "Epoch: 1790600, elapsed: 1.09e+01, train loss: 5.56892e-07, val loss: 1.19381e-06, min loss: 5.56892e-07\n",
      "Epoch: 1790700, elapsed: 1.10e+01, train loss: 5.60122e-07, val loss: 1.20647e-06, min loss: 5.56892e-07\n",
      "Epoch: 1790800, elapsed: 1.10e+01, train loss: 1.52985e-06, val loss: 2.53634e-06, min loss: 5.56892e-07\n",
      "Epoch: 1790900, elapsed: 1.07e+01, train loss: 1.60903e-06, val loss: 1.86551e-06, min loss: 5.56892e-07\n",
      "Epoch: 1791000, elapsed: 1.09e+01, train loss: 8.06943e-07, val loss: 1.33287e-06, min loss: 5.56892e-07\n",
      "Epoch: 1791100, elapsed: 1.08e+01, train loss: 4.14894e-06, val loss: 4.13417e-06, min loss: 5.56892e-07\n",
      "Epoch: 1791200, elapsed: 1.06e+01, train loss: 5.56657e-07, val loss: 1.19550e-06, min loss: 5.56657e-07\n",
      "Epoch: 1791300, elapsed: 1.08e+01, train loss: 5.70360e-07, val loss: 1.19109e-06, min loss: 5.56657e-07\n",
      "Epoch: 1791400, elapsed: 1.10e+01, train loss: 1.01289e-06, val loss: 1.84750e-06, min loss: 5.56657e-07\n",
      "Epoch: 1791500, elapsed: 1.07e+01, train loss: 5.58375e-07, val loss: 1.20357e-06, min loss: 5.56657e-07\n",
      "Epoch: 1791600, elapsed: 1.10e+01, train loss: 5.59916e-07, val loss: 1.18917e-06, min loss: 5.56657e-07\n",
      "Epoch: 1791700, elapsed: 1.10e+01, train loss: 5.81099e-07, val loss: 1.22746e-06, min loss: 5.56657e-07\n",
      "Epoch: 1791800, elapsed: 1.08e+01, train loss: 5.59018e-07, val loss: 1.19022e-06, min loss: 5.56657e-07\n",
      "Epoch: 1791900, elapsed: 1.09e+01, train loss: 8.38639e-07, val loss: 1.51712e-06, min loss: 5.56657e-07\n",
      "Epoch: 1792000, elapsed: 1.11e+01, train loss: 5.57727e-07, val loss: 1.18736e-06, min loss: 5.56657e-07\n",
      "Epoch: 1792100, elapsed: 1.09e+01, train loss: 5.69047e-07, val loss: 1.21253e-06, min loss: 5.56657e-07\n",
      "Epoch: 1792200, elapsed: 1.10e+01, train loss: 5.56965e-07, val loss: 1.19219e-06, min loss: 5.56657e-07\n",
      "Epoch: 1792300, elapsed: 1.09e+01, train loss: 5.70999e-07, val loss: 1.23382e-06, min loss: 5.56657e-07\n",
      "Epoch: 1792400, elapsed: 1.09e+01, train loss: 9.43599e-07, val loss: 1.60752e-06, min loss: 5.56657e-07\n",
      "Epoch: 1792500, elapsed: 1.57e+01, train loss: 7.92409e-07, val loss: 1.37262e-06, min loss: 5.56657e-07\n",
      "Epoch: 1792600, elapsed: 1.10e+01, train loss: 5.56219e-07, val loss: 1.19479e-06, min loss: 5.56219e-07\n",
      "Epoch: 1792700, elapsed: 1.10e+01, train loss: 5.58022e-07, val loss: 1.19758e-06, min loss: 5.56219e-07\n",
      "Epoch: 1792800, elapsed: 1.11e+01, train loss: 7.47290e-07, val loss: 1.25550e-06, min loss: 5.56219e-07\n",
      "Epoch: 1792900, elapsed: 1.10e+01, train loss: 6.14596e-07, val loss: 1.17222e-06, min loss: 5.56219e-07\n",
      "Epoch: 1793000, elapsed: 1.10e+01, train loss: 6.52553e-07, val loss: 1.25490e-06, min loss: 5.56219e-07\n",
      "Epoch: 1793100, elapsed: 1.09e+01, train loss: 8.93303e-07, val loss: 1.41972e-06, min loss: 5.56219e-07\n",
      "Epoch: 1793200, elapsed: 1.11e+01, train loss: 5.60223e-07, val loss: 1.22759e-06, min loss: 5.56219e-07\n",
      "Epoch: 1793300, elapsed: 1.10e+01, train loss: 7.35493e-07, val loss: 1.42480e-06, min loss: 5.56219e-07\n",
      "Epoch: 1793400, elapsed: 1.10e+01, train loss: 5.55829e-07, val loss: 1.19572e-06, min loss: 5.55829e-07\n",
      "Epoch: 1793500, elapsed: 1.09e+01, train loss: 5.73032e-07, val loss: 1.20337e-06, min loss: 5.55829e-07\n",
      "Epoch: 1793600, elapsed: 1.08e+01, train loss: 5.93900e-07, val loss: 1.30314e-06, min loss: 5.55829e-07\n",
      "Epoch: 1793700, elapsed: 1.10e+01, train loss: 7.77125e-07, val loss: 1.41132e-06, min loss: 5.55829e-07\n",
      "Epoch: 1793800, elapsed: 1.10e+01, train loss: 6.62724e-07, val loss: 1.33388e-06, min loss: 5.55829e-07\n",
      "Epoch: 1793900, elapsed: 1.10e+01, train loss: 7.46957e-07, val loss: 1.45569e-06, min loss: 5.55829e-07\n",
      "Epoch: 1794000, elapsed: 1.09e+01, train loss: 1.21641e-06, val loss: 1.64060e-06, min loss: 5.55829e-07\n",
      "Epoch: 1794100, elapsed: 1.10e+01, train loss: 6.91951e-07, val loss: 1.23249e-06, min loss: 5.55829e-07\n",
      "Epoch: 1794200, elapsed: 1.09e+01, train loss: 8.46591e-07, val loss: 1.44817e-06, min loss: 5.55829e-07\n",
      "Epoch: 1794300, elapsed: 1.09e+01, train loss: 5.59881e-07, val loss: 1.21269e-06, min loss: 5.55829e-07\n",
      "Epoch: 1794400, elapsed: 1.09e+01, train loss: 9.73127e-07, val loss: 1.79374e-06, min loss: 5.55829e-07\n",
      "Epoch: 1794500, elapsed: 1.08e+01, train loss: 7.93476e-07, val loss: 1.63338e-06, min loss: 5.55829e-07\n",
      "Epoch: 1794600, elapsed: 1.09e+01, train loss: 5.91551e-07, val loss: 1.21504e-06, min loss: 5.55829e-07\n",
      "Epoch: 1794700, elapsed: 1.09e+01, train loss: 5.77352e-07, val loss: 1.25131e-06, min loss: 5.55829e-07\n",
      "Epoch: 1794800, elapsed: 1.09e+01, train loss: 5.57338e-07, val loss: 1.19357e-06, min loss: 5.55829e-07\n",
      "Epoch: 1794900, elapsed: 1.10e+01, train loss: 5.56540e-07, val loss: 1.19370e-06, min loss: 5.55829e-07\n",
      "Epoch: 1795000, elapsed: 1.08e+01, train loss: 5.58626e-07, val loss: 1.20759e-06, min loss: 5.55829e-07\n",
      "Epoch: 1795100, elapsed: 1.28e+01, train loss: 1.10291e-06, val loss: 1.51023e-06, min loss: 5.55829e-07\n",
      "Epoch: 1795200, elapsed: 1.09e+01, train loss: 5.56277e-07, val loss: 1.20046e-06, min loss: 5.55829e-07\n",
      "Epoch: 1795300, elapsed: 1.08e+01, train loss: 5.58859e-07, val loss: 1.20424e-06, min loss: 5.55829e-07\n",
      "Epoch: 1795400, elapsed: 1.10e+01, train loss: 5.63660e-07, val loss: 1.20364e-06, min loss: 5.55829e-07\n",
      "Epoch: 1795500, elapsed: 1.07e+01, train loss: 8.82509e-07, val loss: 1.43150e-06, min loss: 5.55829e-07\n",
      "Epoch: 1795600, elapsed: 1.08e+01, train loss: 6.68469e-07, val loss: 1.32259e-06, min loss: 5.55829e-07\n",
      "Epoch: 1795700, elapsed: 1.57e+01, train loss: 1.22052e-06, val loss: 1.84271e-06, min loss: 5.55829e-07\n",
      "Epoch: 1795800, elapsed: 1.12e+01, train loss: 2.28437e-06, val loss: 2.09775e-06, min loss: 5.55829e-07\n",
      "Epoch: 1795900, elapsed: 1.09e+01, train loss: 6.63795e-07, val loss: 1.42709e-06, min loss: 5.55829e-07\n",
      "Epoch: 1796000, elapsed: 1.11e+01, train loss: 5.54877e-07, val loss: 1.19665e-06, min loss: 5.54877e-07\n",
      "Epoch: 1796100, elapsed: 1.09e+01, train loss: 5.59339e-07, val loss: 1.20108e-06, min loss: 5.54877e-07\n",
      "Epoch: 1796200, elapsed: 1.09e+01, train loss: 2.97551e-06, val loss: 3.11376e-06, min loss: 5.54877e-07\n",
      "Epoch: 1796300, elapsed: 1.09e+01, train loss: 5.87286e-07, val loss: 1.24061e-06, min loss: 5.54877e-07\n",
      "Epoch: 1796400, elapsed: 1.11e+01, train loss: 5.55247e-07, val loss: 1.19637e-06, min loss: 5.54877e-07\n",
      "Epoch: 1796500, elapsed: 1.10e+01, train loss: 5.64384e-07, val loss: 1.17232e-06, min loss: 5.54877e-07\n",
      "Epoch: 1796600, elapsed: 1.10e+01, train loss: 5.74972e-07, val loss: 1.21714e-06, min loss: 5.54877e-07\n",
      "Epoch: 1796700, elapsed: 1.08e+01, train loss: 7.19636e-07, val loss: 1.24041e-06, min loss: 5.54877e-07\n",
      "Epoch: 1796800, elapsed: 1.11e+01, train loss: 8.61275e-07, val loss: 1.48426e-06, min loss: 5.54877e-07\n",
      "Epoch: 1796900, elapsed: 1.10e+01, train loss: 6.23347e-07, val loss: 1.32542e-06, min loss: 5.54877e-07\n",
      "Epoch: 1797000, elapsed: 1.09e+01, train loss: 5.55175e-07, val loss: 1.20327e-06, min loss: 5.54877e-07\n",
      "Epoch: 1797100, elapsed: 1.09e+01, train loss: 6.66627e-07, val loss: 1.30159e-06, min loss: 5.54877e-07\n",
      "Epoch: 1797200, elapsed: 1.09e+01, train loss: 2.78956e-06, val loss: 3.59048e-06, min loss: 5.54877e-07\n",
      "Epoch: 1797300, elapsed: 1.08e+01, train loss: 5.54616e-07, val loss: 1.19674e-06, min loss: 5.54616e-07\n",
      "Epoch: 1797400, elapsed: 1.09e+01, train loss: 5.55316e-07, val loss: 1.19651e-06, min loss: 5.54616e-07\n",
      "Epoch: 1797500, elapsed: 1.07e+01, train loss: 7.51010e-07, val loss: 1.40296e-06, min loss: 5.54616e-07\n",
      "Epoch: 1797600, elapsed: 1.09e+01, train loss: 5.56476e-07, val loss: 1.20200e-06, min loss: 5.54616e-07\n",
      "Epoch: 1797700, elapsed: 1.10e+01, train loss: 5.61159e-07, val loss: 1.17403e-06, min loss: 5.54616e-07\n",
      "Epoch: 1797800, elapsed: 1.08e+01, train loss: 8.11031e-07, val loss: 1.34768e-06, min loss: 5.54616e-07\n",
      "Epoch: 1797900, elapsed: 1.08e+01, train loss: 1.98543e-06, val loss: 1.79705e-06, min loss: 5.54616e-07\n",
      "Epoch: 1798000, elapsed: 1.10e+01, train loss: 5.55525e-07, val loss: 1.20005e-06, min loss: 5.54616e-07\n",
      "Epoch: 1798100, elapsed: 1.09e+01, train loss: 5.56384e-07, val loss: 1.20764e-06, min loss: 5.54616e-07\n",
      "Epoch: 1798200, elapsed: 1.08e+01, train loss: 5.59944e-07, val loss: 1.21172e-06, min loss: 5.54616e-07\n",
      "Epoch: 1798300, elapsed: 1.09e+01, train loss: 6.25290e-07, val loss: 1.32313e-06, min loss: 5.54616e-07\n",
      "Epoch: 1798400, elapsed: 1.07e+01, train loss: 5.71469e-07, val loss: 1.29453e-06, min loss: 5.54616e-07\n",
      "Epoch: 1798500, elapsed: 1.10e+01, train loss: 6.81694e-07, val loss: 1.30317e-06, min loss: 5.54616e-07\n",
      "Epoch: 1798600, elapsed: 1.07e+01, train loss: 1.06638e-06, val loss: 1.67266e-06, min loss: 5.54616e-07\n",
      "Epoch: 1798700, elapsed: 1.10e+01, train loss: 5.70985e-07, val loss: 1.18243e-06, min loss: 5.54616e-07\n",
      "Epoch: 1798800, elapsed: 1.09e+01, train loss: 5.73341e-07, val loss: 1.22917e-06, min loss: 5.54616e-07\n",
      "Epoch: 1798900, elapsed: 1.09e+01, train loss: 5.56299e-07, val loss: 1.18742e-06, min loss: 5.54616e-07\n",
      "Epoch: 1799000, elapsed: 1.58e+01, train loss: 5.59606e-07, val loss: 1.22269e-06, min loss: 5.54616e-07\n",
      "Epoch: 1799100, elapsed: 1.09e+01, train loss: 5.63912e-07, val loss: 1.24582e-06, min loss: 5.54616e-07\n",
      "Epoch: 1799200, elapsed: 1.08e+01, train loss: 6.08451e-07, val loss: 1.26672e-06, min loss: 5.54616e-07\n",
      "Epoch: 1799300, elapsed: 1.09e+01, train loss: 5.73540e-07, val loss: 1.27311e-06, min loss: 5.54616e-07\n",
      "Epoch: 1799400, elapsed: 1.08e+01, train loss: 5.55835e-07, val loss: 1.20426e-06, min loss: 5.54616e-07\n",
      "Epoch: 1799500, elapsed: 1.10e+01, train loss: 6.09272e-07, val loss: 1.22970e-06, min loss: 5.54616e-07\n",
      "Epoch: 1799600, elapsed: 1.10e+01, train loss: 5.60836e-07, val loss: 1.28519e-06, min loss: 5.54616e-07\n",
      "Epoch: 1799700, elapsed: 1.10e+01, train loss: 5.80132e-07, val loss: 1.21017e-06, min loss: 5.54616e-07\n",
      "Epoch: 1799800, elapsed: 1.10e+01, train loss: 5.53796e-07, val loss: 1.20260e-06, min loss: 5.53796e-07\n",
      "Epoch: 1799900, elapsed: 1.12e+01, train loss: 5.69462e-07, val loss: 1.22311e-06, min loss: 5.53796e-07\n",
      "Epoch: 1800000, elapsed: 1.09e+01, train loss: 5.66126e-07, val loss: 1.20924e-06, min loss: 5.53796e-07\n",
      "Epoch: 1800100, elapsed: 1.29e+01, train loss: 5.54164e-07, val loss: 1.19485e-06, min loss: 5.53796e-07\n",
      "Epoch: 1800200, elapsed: 1.10e+01, train loss: 7.81317e-07, val loss: 1.46859e-06, min loss: 5.53796e-07\n",
      "Epoch: 1800300, elapsed: 1.10e+01, train loss: 5.59202e-07, val loss: 1.21223e-06, min loss: 5.53796e-07\n",
      "Epoch: 1800400, elapsed: 1.10e+01, train loss: 1.56237e-06, val loss: 2.01340e-06, min loss: 5.53796e-07\n",
      "Epoch: 1800500, elapsed: 1.09e+01, train loss: 6.24891e-07, val loss: 1.23948e-06, min loss: 5.53796e-07\n",
      "Epoch: 1800600, elapsed: 1.09e+01, train loss: 5.74671e-07, val loss: 1.21506e-06, min loss: 5.53796e-07\n",
      "Epoch: 1800700, elapsed: 1.09e+01, train loss: 5.81050e-07, val loss: 1.23526e-06, min loss: 5.53796e-07\n",
      "Epoch: 1800800, elapsed: 1.12e+01, train loss: 5.55182e-07, val loss: 1.19506e-06, min loss: 5.53796e-07\n",
      "Epoch: 1800900, elapsed: 1.09e+01, train loss: 5.64761e-07, val loss: 1.21292e-06, min loss: 5.53796e-07\n",
      "Epoch: 1801000, elapsed: 1.09e+01, train loss: 5.71909e-07, val loss: 1.22569e-06, min loss: 5.53796e-07\n",
      "Epoch: 1801100, elapsed: 1.08e+01, train loss: 5.56207e-07, val loss: 1.21413e-06, min loss: 5.53796e-07\n",
      "Epoch: 1801200, elapsed: 1.09e+01, train loss: 7.40748e-07, val loss: 1.46051e-06, min loss: 5.53796e-07\n",
      "Epoch: 1801300, elapsed: 1.07e+01, train loss: 5.53075e-07, val loss: 1.20147e-06, min loss: 5.53075e-07\n",
      "Epoch: 1801400, elapsed: 1.09e+01, train loss: 5.53045e-07, val loss: 1.19814e-06, min loss: 5.53045e-07\n",
      "Epoch: 1801500, elapsed: 1.07e+01, train loss: 6.90198e-07, val loss: 1.36475e-06, min loss: 5.53045e-07\n",
      "Epoch: 1801600, elapsed: 1.06e+01, train loss: 5.71562e-07, val loss: 1.20001e-06, min loss: 5.53045e-07\n",
      "Epoch: 1801700, elapsed: 1.07e+01, train loss: 5.77289e-07, val loss: 1.21904e-06, min loss: 5.53045e-07\n",
      "Epoch: 1801800, elapsed: 1.09e+01, train loss: 5.54214e-07, val loss: 1.19311e-06, min loss: 5.53045e-07\n",
      "Epoch: 1801900, elapsed: 1.08e+01, train loss: 5.54066e-07, val loss: 1.20549e-06, min loss: 5.53045e-07\n",
      "Epoch: 1802000, elapsed: 1.09e+01, train loss: 5.54954e-07, val loss: 1.19973e-06, min loss: 5.53045e-07\n",
      "Epoch: 1802100, elapsed: 1.07e+01, train loss: 5.64390e-07, val loss: 1.21636e-06, min loss: 5.53045e-07\n",
      "Epoch: 1802200, elapsed: 1.61e+01, train loss: 5.68669e-07, val loss: 1.26200e-06, min loss: 5.53045e-07\n",
      "Epoch: 1802300, elapsed: 1.12e+01, train loss: 5.70956e-07, val loss: 1.18392e-06, min loss: 5.53045e-07\n",
      "Epoch: 1802400, elapsed: 1.09e+01, train loss: 7.15148e-07, val loss: 2.16244e-06, min loss: 5.53045e-07\n",
      "Epoch: 1802500, elapsed: 1.10e+01, train loss: 5.52353e-07, val loss: 1.19754e-06, min loss: 5.52353e-07\n",
      "Epoch: 1802600, elapsed: 1.09e+01, train loss: 6.12371e-07, val loss: 1.31028e-06, min loss: 5.52353e-07\n",
      "Epoch: 1802700, elapsed: 1.12e+01, train loss: 6.15430e-07, val loss: 1.23455e-06, min loss: 5.52353e-07\n",
      "Epoch: 1802800, elapsed: 1.08e+01, train loss: 7.34802e-07, val loss: 1.28670e-06, min loss: 5.52353e-07\n",
      "Epoch: 1802900, elapsed: 1.12e+01, train loss: 8.68755e-07, val loss: 1.46643e-06, min loss: 5.52353e-07\n",
      "Epoch: 1803000, elapsed: 1.08e+01, train loss: 5.68450e-07, val loss: 1.19667e-06, min loss: 5.52353e-07\n",
      "Epoch: 1803100, elapsed: 1.09e+01, train loss: 5.66609e-07, val loss: 1.25099e-06, min loss: 5.52353e-07\n",
      "Epoch: 1803200, elapsed: 1.10e+01, train loss: 5.53607e-07, val loss: 1.19716e-06, min loss: 5.52353e-07\n",
      "Epoch: 1803300, elapsed: 1.09e+01, train loss: 5.57862e-07, val loss: 1.19249e-06, min loss: 5.52353e-07\n",
      "Epoch: 1803400, elapsed: 1.10e+01, train loss: 5.66101e-07, val loss: 1.22606e-06, min loss: 5.52353e-07\n",
      "Epoch: 1803500, elapsed: 1.09e+01, train loss: 6.30027e-07, val loss: 1.25712e-06, min loss: 5.52353e-07\n",
      "Epoch: 1803600, elapsed: 1.10e+01, train loss: 6.69534e-07, val loss: 1.46260e-06, min loss: 5.52353e-07\n",
      "Epoch: 1803700, elapsed: 1.08e+01, train loss: 5.52501e-07, val loss: 1.20046e-06, min loss: 5.52353e-07\n",
      "Epoch: 1803800, elapsed: 1.10e+01, train loss: 6.06919e-07, val loss: 1.22928e-06, min loss: 5.52353e-07\n",
      "Epoch: 1803900, elapsed: 1.10e+01, train loss: 5.51720e-07, val loss: 1.19822e-06, min loss: 5.51720e-07\n",
      "Epoch: 1804000, elapsed: 1.10e+01, train loss: 5.53803e-07, val loss: 1.18412e-06, min loss: 5.51720e-07\n",
      "Epoch: 1804100, elapsed: 1.08e+01, train loss: 2.58976e-06, val loss: 3.75589e-06, min loss: 5.51720e-07\n",
      "Epoch: 1804200, elapsed: 1.10e+01, train loss: 5.51758e-07, val loss: 1.20441e-06, min loss: 5.51720e-07\n",
      "Epoch: 1804300, elapsed: 1.08e+01, train loss: 6.32367e-07, val loss: 1.27549e-06, min loss: 5.51720e-07\n",
      "Epoch: 1804400, elapsed: 1.09e+01, train loss: 5.54386e-07, val loss: 1.20888e-06, min loss: 5.51720e-07\n",
      "Epoch: 1804500, elapsed: 1.10e+01, train loss: 5.53002e-07, val loss: 1.20894e-06, min loss: 5.51720e-07\n",
      "Epoch: 1804600, elapsed: 1.09e+01, train loss: 5.60187e-07, val loss: 1.21427e-06, min loss: 5.51720e-07\n",
      "Epoch: 1804700, elapsed: 1.10e+01, train loss: 6.39700e-07, val loss: 1.35374e-06, min loss: 5.51720e-07\n",
      "Epoch: 1804800, elapsed: 1.07e+01, train loss: 5.69293e-07, val loss: 1.22558e-06, min loss: 5.51720e-07\n",
      "Epoch: 1804900, elapsed: 1.10e+01, train loss: 6.13200e-07, val loss: 1.28852e-06, min loss: 5.51720e-07\n",
      "Epoch: 1805000, elapsed: 1.08e+01, train loss: 7.90076e-07, val loss: 1.75457e-06, min loss: 5.51720e-07\n",
      "Epoch: 1805100, elapsed: 1.31e+01, train loss: 7.58661e-07, val loss: 1.53640e-06, min loss: 5.51720e-07\n",
      "Epoch: 1805200, elapsed: 1.08e+01, train loss: 5.51828e-07, val loss: 1.19841e-06, min loss: 5.51720e-07\n",
      "Epoch: 1805300, elapsed: 1.08e+01, train loss: 5.80721e-07, val loss: 1.30506e-06, min loss: 5.51720e-07\n",
      "Epoch: 1805400, elapsed: 1.61e+01, train loss: 6.83481e-07, val loss: 1.24208e-06, min loss: 5.51720e-07\n",
      "Epoch: 1805500, elapsed: 1.10e+01, train loss: 1.66093e-06, val loss: 2.30822e-06, min loss: 5.51720e-07\n",
      "Epoch: 1805600, elapsed: 1.10e+01, train loss: 1.11678e-06, val loss: 2.00550e-06, min loss: 5.51720e-07\n",
      "Epoch: 1805700, elapsed: 1.14e+01, train loss: 5.58100e-07, val loss: 1.24001e-06, min loss: 5.51720e-07\n",
      "Epoch: 1805800, elapsed: 1.11e+01, train loss: 5.52990e-07, val loss: 1.21195e-06, min loss: 5.51720e-07\n",
      "Epoch: 1805900, elapsed: 1.10e+01, train loss: 5.56165e-07, val loss: 1.19753e-06, min loss: 5.51720e-07\n",
      "Epoch: 1806000, elapsed: 1.12e+01, train loss: 3.55943e-06, val loss: 4.62268e-06, min loss: 5.51720e-07\n",
      "Epoch: 1806100, elapsed: 1.09e+01, train loss: 5.50990e-07, val loss: 1.19964e-06, min loss: 5.50990e-07\n",
      "Epoch: 1806200, elapsed: 1.09e+01, train loss: 8.14073e-07, val loss: 1.29840e-06, min loss: 5.50990e-07\n",
      "Epoch: 1806300, elapsed: 1.08e+01, train loss: 5.50864e-07, val loss: 1.19930e-06, min loss: 5.50864e-07\n",
      "Epoch: 1806400, elapsed: 1.09e+01, train loss: 1.07678e-06, val loss: 1.79126e-06, min loss: 5.50864e-07\n",
      "Epoch: 1806500, elapsed: 1.10e+01, train loss: 1.01014e-06, val loss: 1.76304e-06, min loss: 5.50864e-07\n",
      "Epoch: 1806600, elapsed: 1.14e+01, train loss: 6.07320e-07, val loss: 1.21201e-06, min loss: 5.50864e-07\n",
      "Epoch: 1806700, elapsed: 1.08e+01, train loss: 6.55931e-07, val loss: 1.23563e-06, min loss: 5.50864e-07\n",
      "Epoch: 1806800, elapsed: 1.09e+01, train loss: 1.10671e-06, val loss: 1.79864e-06, min loss: 5.50864e-07\n",
      "Epoch: 1806900, elapsed: 1.08e+01, train loss: 5.82260e-07, val loss: 1.22782e-06, min loss: 5.50864e-07\n",
      "Epoch: 1807000, elapsed: 1.09e+01, train loss: 5.55178e-07, val loss: 1.19887e-06, min loss: 5.50864e-07\n",
      "Epoch: 1807100, elapsed: 1.06e+01, train loss: 5.51177e-07, val loss: 1.19686e-06, min loss: 5.50864e-07\n",
      "Epoch: 1807200, elapsed: 1.08e+01, train loss: 5.51101e-07, val loss: 1.20181e-06, min loss: 5.50864e-07\n",
      "Epoch: 1807300, elapsed: 1.09e+01, train loss: 8.55481e-07, val loss: 1.64097e-06, min loss: 5.50864e-07\n",
      "Epoch: 1807400, elapsed: 1.07e+01, train loss: 5.85699e-07, val loss: 1.26839e-06, min loss: 5.50864e-07\n",
      "Epoch: 1807500, elapsed: 1.08e+01, train loss: 9.96144e-07, val loss: 1.71897e-06, min loss: 5.50864e-07\n",
      "Epoch: 1807600, elapsed: 1.07e+01, train loss: 6.09909e-07, val loss: 1.29652e-06, min loss: 5.50864e-07\n",
      "Epoch: 1807700, elapsed: 1.10e+01, train loss: 5.56805e-07, val loss: 1.18691e-06, min loss: 5.50864e-07\n",
      "Epoch: 1807800, elapsed: 1.10e+01, train loss: 5.54376e-07, val loss: 1.20959e-06, min loss: 5.50864e-07\n",
      "Epoch: 1807900, elapsed: 1.09e+01, train loss: 5.54067e-07, val loss: 1.20648e-06, min loss: 5.50864e-07\n",
      "Epoch: 1808000, elapsed: 1.11e+01, train loss: 5.54329e-07, val loss: 1.21821e-06, min loss: 5.50864e-07\n",
      "Epoch: 1808100, elapsed: 1.07e+01, train loss: 6.75040e-07, val loss: 1.34161e-06, min loss: 5.50864e-07\n",
      "Epoch: 1808200, elapsed: 1.08e+01, train loss: 1.49986e-06, val loss: 2.28531e-06, min loss: 5.50864e-07\n",
      "Epoch: 1808300, elapsed: 1.08e+01, train loss: 5.51533e-07, val loss: 1.19957e-06, min loss: 5.50864e-07\n",
      "Epoch: 1808400, elapsed: 1.07e+01, train loss: 5.58087e-07, val loss: 1.21222e-06, min loss: 5.50864e-07\n",
      "Epoch: 1808500, elapsed: 1.07e+01, train loss: 5.54859e-07, val loss: 1.21034e-06, min loss: 5.50864e-07\n",
      "Epoch: 1808600, elapsed: 1.58e+01, train loss: 5.50667e-07, val loss: 1.19385e-06, min loss: 5.50667e-07\n",
      "Epoch: 1808700, elapsed: 1.16e+01, train loss: 1.26807e-06, val loss: 1.52949e-06, min loss: 5.50667e-07\n",
      "Epoch: 1808800, elapsed: 1.10e+01, train loss: 5.50035e-07, val loss: 1.19776e-06, min loss: 5.50035e-07\n",
      "Epoch: 1808900, elapsed: 1.10e+01, train loss: 7.27402e-07, val loss: 1.36532e-06, min loss: 5.50035e-07\n",
      "Epoch: 1809000, elapsed: 1.09e+01, train loss: 7.47092e-07, val loss: 1.33925e-06, min loss: 5.50035e-07\n",
      "Epoch: 1809100, elapsed: 1.09e+01, train loss: 5.54517e-07, val loss: 1.19793e-06, min loss: 5.50035e-07\n",
      "Epoch: 1809200, elapsed: 1.11e+01, train loss: 7.98784e-07, val loss: 1.45106e-06, min loss: 5.50035e-07\n",
      "Epoch: 1809300, elapsed: 1.10e+01, train loss: 1.00876e-06, val loss: 1.77675e-06, min loss: 5.50035e-07\n",
      "Epoch: 1809400, elapsed: 1.09e+01, train loss: 7.26667e-07, val loss: 1.35971e-06, min loss: 5.50035e-07\n",
      "Epoch: 1809500, elapsed: 1.09e+01, train loss: 5.85467e-07, val loss: 1.22992e-06, min loss: 5.50035e-07\n",
      "Epoch: 1809600, elapsed: 1.10e+01, train loss: 5.83767e-07, val loss: 1.23699e-06, min loss: 5.50035e-07\n",
      "Epoch: 1809700, elapsed: 1.10e+01, train loss: 5.93115e-07, val loss: 1.26280e-06, min loss: 5.50035e-07\n",
      "Epoch: 1809800, elapsed: 1.09e+01, train loss: 6.73661e-07, val loss: 1.41665e-06, min loss: 5.50035e-07\n",
      "Epoch: 1809900, elapsed: 1.10e+01, train loss: 6.62713e-07, val loss: 1.33373e-06, min loss: 5.50035e-07\n",
      "Epoch: 1810000, elapsed: 1.10e+01, train loss: 5.58416e-07, val loss: 1.18764e-06, min loss: 5.50035e-07\n",
      "Epoch: 1810100, elapsed: 1.30e+01, train loss: 5.50370e-07, val loss: 1.19782e-06, min loss: 5.50035e-07\n",
      "Epoch: 1810200, elapsed: 1.08e+01, train loss: 7.34432e-07, val loss: 1.41052e-06, min loss: 5.50035e-07\n",
      "Epoch: 1810300, elapsed: 1.10e+01, train loss: 5.50158e-07, val loss: 1.19912e-06, min loss: 5.50035e-07\n",
      "Epoch: 1810400, elapsed: 1.08e+01, train loss: 5.50796e-07, val loss: 1.20134e-06, min loss: 5.50035e-07\n",
      "Epoch: 1810500, elapsed: 1.08e+01, train loss: 1.56108e-06, val loss: 1.40648e-06, min loss: 5.50035e-07\n",
      "Epoch: 1810600, elapsed: 1.09e+01, train loss: 5.49322e-07, val loss: 1.20130e-06, min loss: 5.49322e-07\n",
      "Epoch: 1810700, elapsed: 1.11e+01, train loss: 5.61436e-07, val loss: 1.27400e-06, min loss: 5.49322e-07\n",
      "Epoch: 1810800, elapsed: 1.08e+01, train loss: 5.49120e-07, val loss: 1.20215e-06, min loss: 5.49120e-07\n",
      "Epoch: 1810900, elapsed: 1.09e+01, train loss: 5.60482e-07, val loss: 1.20980e-06, min loss: 5.49120e-07\n",
      "Epoch: 1811000, elapsed: 1.09e+01, train loss: 5.61219e-07, val loss: 1.17726e-06, min loss: 5.49120e-07\n",
      "Epoch: 1811100, elapsed: 1.09e+01, train loss: 1.38102e-06, val loss: 2.23986e-06, min loss: 5.49120e-07\n",
      "Epoch: 1811200, elapsed: 1.09e+01, train loss: 2.18162e-06, val loss: 3.08928e-06, min loss: 5.49120e-07\n",
      "Epoch: 1811300, elapsed: 1.09e+01, train loss: 5.49595e-07, val loss: 1.19852e-06, min loss: 5.49120e-07\n",
      "Epoch: 1811400, elapsed: 1.07e+01, train loss: 5.51414e-07, val loss: 1.19673e-06, min loss: 5.49120e-07\n",
      "Epoch: 1811500, elapsed: 1.08e+01, train loss: 5.52442e-07, val loss: 1.19966e-06, min loss: 5.49120e-07\n",
      "Epoch: 1811600, elapsed: 1.08e+01, train loss: 5.65731e-07, val loss: 1.22610e-06, min loss: 5.49120e-07\n",
      "Epoch: 1811700, elapsed: 1.07e+01, train loss: 1.09198e-06, val loss: 1.56282e-06, min loss: 5.49120e-07\n",
      "Epoch: 1811800, elapsed: 1.08e+01, train loss: 1.54690e-06, val loss: 2.26606e-06, min loss: 5.49120e-07\n",
      "Epoch: 1811900, elapsed: 1.60e+01, train loss: 7.87480e-07, val loss: 1.35323e-06, min loss: 5.49120e-07\n",
      "Epoch: 1812000, elapsed: 1.09e+01, train loss: 6.84393e-07, val loss: 1.45042e-06, min loss: 5.49120e-07\n",
      "Epoch: 1812100, elapsed: 1.11e+01, train loss: 6.06166e-07, val loss: 1.38838e-06, min loss: 5.49120e-07\n",
      "Epoch: 1812200, elapsed: 1.08e+01, train loss: 4.74872e-06, val loss: 5.69172e-06, min loss: 5.49120e-07\n",
      "Epoch: 1812300, elapsed: 1.10e+01, train loss: 5.48701e-07, val loss: 1.20084e-06, min loss: 5.48701e-07\n",
      "Epoch: 1812400, elapsed: 1.11e+01, train loss: 1.29055e-06, val loss: 2.42998e-06, min loss: 5.48701e-07\n",
      "Epoch: 1812500, elapsed: 1.11e+01, train loss: 5.48588e-07, val loss: 1.20329e-06, min loss: 5.48588e-07\n",
      "Epoch: 1812600, elapsed: 1.09e+01, train loss: 5.78926e-07, val loss: 1.22051e-06, min loss: 5.48588e-07\n",
      "Epoch: 1812700, elapsed: 1.10e+01, train loss: 6.03113e-07, val loss: 1.23486e-06, min loss: 5.48588e-07\n",
      "Epoch: 1812800, elapsed: 1.10e+01, train loss: 4.06016e-06, val loss: 4.15457e-06, min loss: 5.48588e-07\n",
      "Epoch: 1812900, elapsed: 1.09e+01, train loss: 5.48822e-07, val loss: 1.19957e-06, min loss: 5.48588e-07\n",
      "Epoch: 1813000, elapsed: 1.09e+01, train loss: 5.53111e-07, val loss: 1.22257e-06, min loss: 5.48588e-07\n",
      "Epoch: 1813100, elapsed: 1.09e+01, train loss: 6.57617e-07, val loss: 1.24868e-06, min loss: 5.48588e-07\n",
      "Epoch: 1813200, elapsed: 1.07e+01, train loss: 5.49103e-07, val loss: 1.20032e-06, min loss: 5.48588e-07\n",
      "Epoch: 1813300, elapsed: 1.09e+01, train loss: 5.48959e-07, val loss: 1.20159e-06, min loss: 5.48588e-07\n",
      "Epoch: 1813400, elapsed: 1.08e+01, train loss: 7.45841e-07, val loss: 1.52571e-06, min loss: 5.48588e-07\n",
      "Epoch: 1813500, elapsed: 1.06e+01, train loss: 5.51794e-07, val loss: 1.20775e-06, min loss: 5.48588e-07\n",
      "Epoch: 1813600, elapsed: 1.10e+01, train loss: 5.99385e-07, val loss: 1.28954e-06, min loss: 5.48588e-07\n",
      "Epoch: 1813700, elapsed: 1.08e+01, train loss: 7.62799e-07, val loss: 1.46283e-06, min loss: 5.48588e-07\n",
      "Epoch: 1813800, elapsed: 1.08e+01, train loss: 5.49293e-07, val loss: 1.21635e-06, min loss: 5.48588e-07\n",
      "Epoch: 1813900, elapsed: 1.11e+01, train loss: 5.51759e-07, val loss: 1.19618e-06, min loss: 5.48588e-07\n",
      "Epoch: 1814000, elapsed: 1.09e+01, train loss: 5.51144e-07, val loss: 1.20439e-06, min loss: 5.48588e-07\n",
      "Epoch: 1814100, elapsed: 1.09e+01, train loss: 5.76755e-07, val loss: 1.18745e-06, min loss: 5.48588e-07\n",
      "Epoch: 1814200, elapsed: 1.08e+01, train loss: 1.18726e-06, val loss: 1.77501e-06, min loss: 5.48588e-07\n",
      "Epoch: 1814300, elapsed: 1.09e+01, train loss: 2.34667e-06, val loss: 2.87409e-06, min loss: 5.48588e-07\n",
      "Epoch: 1814400, elapsed: 1.10e+01, train loss: 2.07113e-06, val loss: 3.01135e-06, min loss: 5.48588e-07\n",
      "Epoch: 1814500, elapsed: 1.09e+01, train loss: 1.46388e-06, val loss: 2.21307e-06, min loss: 5.48588e-07\n",
      "Epoch: 1814600, elapsed: 1.08e+01, train loss: 5.61901e-07, val loss: 1.20463e-06, min loss: 5.48588e-07\n",
      "Epoch: 1814700, elapsed: 1.07e+01, train loss: 5.49376e-07, val loss: 1.20219e-06, min loss: 5.48588e-07\n",
      "Epoch: 1814800, elapsed: 1.09e+01, train loss: 5.50942e-07, val loss: 1.20010e-06, min loss: 5.48588e-07\n",
      "Epoch: 1814900, elapsed: 1.07e+01, train loss: 5.51679e-07, val loss: 1.19158e-06, min loss: 5.48588e-07\n",
      "Epoch: 1815000, elapsed: 1.06e+01, train loss: 5.68413e-07, val loss: 1.18367e-06, min loss: 5.48588e-07\n",
      "Epoch: 1815100, elapsed: 1.79e+01, train loss: 3.04503e-06, val loss: 3.40259e-06, min loss: 5.48588e-07\n",
      "Epoch: 1815200, elapsed: 1.13e+01, train loss: 5.59609e-07, val loss: 1.19849e-06, min loss: 5.48588e-07\n",
      "Epoch: 1815300, elapsed: 1.08e+01, train loss: 5.48586e-07, val loss: 1.20302e-06, min loss: 5.48586e-07\n",
      "Epoch: 1815400, elapsed: 1.09e+01, train loss: 5.71438e-07, val loss: 1.21456e-06, min loss: 5.48586e-07\n",
      "Epoch: 1815500, elapsed: 1.10e+01, train loss: 6.07139e-07, val loss: 1.29403e-06, min loss: 5.48586e-07\n",
      "Epoch: 1815600, elapsed: 1.12e+01, train loss: 5.49270e-07, val loss: 1.20253e-06, min loss: 5.48586e-07\n",
      "Epoch: 1815700, elapsed: 1.11e+01, train loss: 6.78912e-07, val loss: 1.37695e-06, min loss: 5.48586e-07\n",
      "Epoch: 1815800, elapsed: 1.12e+01, train loss: 5.56473e-07, val loss: 1.19067e-06, min loss: 5.48586e-07\n",
      "Epoch: 1815900, elapsed: 1.10e+01, train loss: 5.86318e-07, val loss: 1.19615e-06, min loss: 5.48586e-07\n",
      "Epoch: 1816000, elapsed: 1.09e+01, train loss: 8.59920e-07, val loss: 1.32227e-06, min loss: 5.48586e-07\n",
      "Epoch: 1816100, elapsed: 1.11e+01, train loss: 5.47227e-07, val loss: 1.20430e-06, min loss: 5.47227e-07\n",
      "Epoch: 1816200, elapsed: 1.11e+01, train loss: 8.94868e-07, val loss: 1.40310e-06, min loss: 5.47227e-07\n",
      "Epoch: 1816300, elapsed: 1.11e+01, train loss: 9.06733e-07, val loss: 1.79157e-06, min loss: 5.47227e-07\n",
      "Epoch: 1816400, elapsed: 1.07e+01, train loss: 6.08672e-07, val loss: 1.25562e-06, min loss: 5.47227e-07\n",
      "Epoch: 1816500, elapsed: 1.10e+01, train loss: 6.27251e-07, val loss: 1.23404e-06, min loss: 5.47227e-07\n",
      "Epoch: 1816600, elapsed: 1.10e+01, train loss: 5.46992e-07, val loss: 1.20335e-06, min loss: 5.46992e-07\n",
      "Epoch: 1816700, elapsed: 1.07e+01, train loss: 5.67108e-07, val loss: 1.22285e-06, min loss: 5.46992e-07\n",
      "Epoch: 1816800, elapsed: 1.09e+01, train loss: 5.46891e-07, val loss: 1.20454e-06, min loss: 5.46891e-07\n",
      "Epoch: 1816900, elapsed: 1.09e+01, train loss: 5.58442e-07, val loss: 1.17955e-06, min loss: 5.46891e-07\n",
      "Epoch: 1817000, elapsed: 1.09e+01, train loss: 5.46907e-07, val loss: 1.20425e-06, min loss: 5.46891e-07\n",
      "Epoch: 1817100, elapsed: 1.10e+01, train loss: 5.95113e-07, val loss: 1.32841e-06, min loss: 5.46891e-07\n",
      "Epoch: 1817200, elapsed: 1.12e+01, train loss: 5.46777e-07, val loss: 1.20525e-06, min loss: 5.46777e-07\n",
      "Epoch: 1817300, elapsed: 1.09e+01, train loss: 5.51050e-07, val loss: 1.19847e-06, min loss: 5.46777e-07\n",
      "Epoch: 1817400, elapsed: 1.09e+01, train loss: 5.50372e-07, val loss: 1.22096e-06, min loss: 5.46777e-07\n",
      "Epoch: 1817500, elapsed: 1.08e+01, train loss: 6.30920e-07, val loss: 1.21396e-06, min loss: 5.46777e-07\n",
      "Epoch: 1817600, elapsed: 1.07e+01, train loss: 5.93495e-07, val loss: 1.28400e-06, min loss: 5.46777e-07\n",
      "Epoch: 1817700, elapsed: 1.07e+01, train loss: 5.53590e-07, val loss: 1.21063e-06, min loss: 5.46777e-07\n",
      "Epoch: 1817800, elapsed: 1.10e+01, train loss: 5.63064e-07, val loss: 1.24335e-06, min loss: 5.46777e-07\n",
      "Epoch: 1817900, elapsed: 1.07e+01, train loss: 5.56015e-07, val loss: 1.20996e-06, min loss: 5.46777e-07\n",
      "Epoch: 1818000, elapsed: 1.09e+01, train loss: 6.72598e-07, val loss: 1.23098e-06, min loss: 5.46777e-07\n",
      "Epoch: 1818100, elapsed: 1.07e+01, train loss: 5.46804e-07, val loss: 1.20902e-06, min loss: 5.46777e-07\n",
      "Epoch: 1818200, elapsed: 1.08e+01, train loss: 5.49031e-07, val loss: 1.20319e-06, min loss: 5.46777e-07\n",
      "Epoch: 1818300, elapsed: 1.05e+01, train loss: 5.73203e-07, val loss: 1.19965e-06, min loss: 5.46777e-07\n",
      "Epoch: 1818400, elapsed: 1.59e+01, train loss: 5.52084e-07, val loss: 1.20172e-06, min loss: 5.46777e-07\n",
      "Epoch: 1818500, elapsed: 1.11e+01, train loss: 5.46912e-07, val loss: 1.20681e-06, min loss: 5.46777e-07\n",
      "Epoch: 1818600, elapsed: 1.10e+01, train loss: 5.51310e-07, val loss: 1.20447e-06, min loss: 5.46777e-07\n",
      "Epoch: 1818700, elapsed: 1.11e+01, train loss: 5.73999e-07, val loss: 1.19848e-06, min loss: 5.46777e-07\n",
      "Epoch: 1818800, elapsed: 1.09e+01, train loss: 7.24952e-07, val loss: 1.42102e-06, min loss: 5.46777e-07\n",
      "Epoch: 1818900, elapsed: 1.09e+01, train loss: 5.46333e-07, val loss: 1.20252e-06, min loss: 5.46333e-07\n",
      "Epoch: 1819000, elapsed: 1.09e+01, train loss: 5.52670e-07, val loss: 1.22440e-06, min loss: 5.46333e-07\n",
      "Epoch: 1819100, elapsed: 1.11e+01, train loss: 5.49168e-07, val loss: 1.22584e-06, min loss: 5.46333e-07\n",
      "Epoch: 1819200, elapsed: 1.10e+01, train loss: 6.36783e-07, val loss: 1.19710e-06, min loss: 5.46333e-07\n",
      "Epoch: 1819300, elapsed: 1.10e+01, train loss: 2.39367e-06, val loss: 2.50356e-06, min loss: 5.46333e-07\n",
      "Epoch: 1819400, elapsed: 1.11e+01, train loss: 2.72174e-06, val loss: 3.16364e-06, min loss: 5.46333e-07\n",
      "Epoch: 1819500, elapsed: 1.10e+01, train loss: 6.15680e-07, val loss: 1.26437e-06, min loss: 5.46333e-07\n",
      "Epoch: 1819600, elapsed: 1.09e+01, train loss: 5.46907e-07, val loss: 1.20130e-06, min loss: 5.46333e-07\n",
      "Epoch: 1819700, elapsed: 1.08e+01, train loss: 5.52382e-07, val loss: 1.20126e-06, min loss: 5.46333e-07\n",
      "Epoch: 1819800, elapsed: 1.08e+01, train loss: 5.54060e-07, val loss: 1.23477e-06, min loss: 5.46333e-07\n",
      "Epoch: 1819900, elapsed: 1.08e+01, train loss: 5.53029e-07, val loss: 1.20270e-06, min loss: 5.46333e-07\n",
      "Epoch: 1820000, elapsed: 1.09e+01, train loss: 6.64593e-07, val loss: 1.29656e-06, min loss: 5.46333e-07\n",
      "Epoch: 1820100, elapsed: 1.29e+01, train loss: 5.53560e-07, val loss: 1.22009e-06, min loss: 5.46333e-07\n",
      "Epoch: 1820200, elapsed: 1.10e+01, train loss: 5.64373e-07, val loss: 1.19168e-06, min loss: 5.46333e-07\n",
      "Epoch: 1820300, elapsed: 1.08e+01, train loss: 1.03023e-06, val loss: 1.30732e-06, min loss: 5.46333e-07\n",
      "Epoch: 1820400, elapsed: 1.07e+01, train loss: 5.45680e-07, val loss: 1.20420e-06, min loss: 5.45680e-07\n",
      "Epoch: 1820500, elapsed: 1.11e+01, train loss: 5.48183e-07, val loss: 1.20728e-06, min loss: 5.45680e-07\n",
      "Epoch: 1820600, elapsed: 1.07e+01, train loss: 5.64721e-07, val loss: 1.21958e-06, min loss: 5.45680e-07\n",
      "Epoch: 1820700, elapsed: 1.08e+01, train loss: 1.68751e-06, val loss: 2.22234e-06, min loss: 5.45680e-07\n",
      "Epoch: 1820800, elapsed: 1.08e+01, train loss: 1.14576e-06, val loss: 1.70468e-06, min loss: 5.45680e-07\n",
      "Epoch: 1820900, elapsed: 1.09e+01, train loss: 6.10152e-07, val loss: 1.19692e-06, min loss: 5.45680e-07\n",
      "Epoch: 1821000, elapsed: 1.09e+01, train loss: 5.63712e-07, val loss: 1.21785e-06, min loss: 5.45680e-07\n",
      "Epoch: 1821100, elapsed: 1.08e+01, train loss: 7.25437e-07, val loss: 1.33153e-06, min loss: 5.45680e-07\n",
      "Epoch: 1821200, elapsed: 1.06e+01, train loss: 3.75698e-06, val loss: 4.54425e-06, min loss: 5.45680e-07\n",
      "Epoch: 1821300, elapsed: 1.09e+01, train loss: 8.33920e-07, val loss: 1.71350e-06, min loss: 5.45680e-07\n",
      "Epoch: 1821400, elapsed: 1.07e+01, train loss: 7.00172e-07, val loss: 1.33864e-06, min loss: 5.45680e-07\n",
      "Epoch: 1821500, elapsed: 1.08e+01, train loss: 5.83900e-07, val loss: 1.26656e-06, min loss: 5.45680e-07\n",
      "Epoch: 1821600, elapsed: 1.60e+01, train loss: 5.49930e-07, val loss: 1.21063e-06, min loss: 5.45680e-07\n",
      "Epoch: 1821700, elapsed: 1.12e+01, train loss: 5.53730e-07, val loss: 1.22293e-06, min loss: 5.45680e-07\n",
      "Epoch: 1821800, elapsed: 1.09e+01, train loss: 5.84261e-07, val loss: 1.18908e-06, min loss: 5.45680e-07\n",
      "Epoch: 1821900, elapsed: 1.10e+01, train loss: 6.92438e-07, val loss: 1.19010e-06, min loss: 5.45680e-07\n",
      "Epoch: 1822000, elapsed: 1.09e+01, train loss: 8.58276e-07, val loss: 1.44209e-06, min loss: 5.45680e-07\n",
      "Epoch: 1822100, elapsed: 1.10e+01, train loss: 5.59236e-07, val loss: 1.22657e-06, min loss: 5.45680e-07\n",
      "Epoch: 1822200, elapsed: 1.09e+01, train loss: 5.46892e-07, val loss: 1.21953e-06, min loss: 5.45680e-07\n",
      "Epoch: 1822300, elapsed: 1.10e+01, train loss: 5.46880e-07, val loss: 1.21893e-06, min loss: 5.45680e-07\n",
      "Epoch: 1822400, elapsed: 1.09e+01, train loss: 5.74935e-07, val loss: 1.24466e-06, min loss: 5.45680e-07\n",
      "Epoch: 1822500, elapsed: 1.11e+01, train loss: 5.48739e-07, val loss: 1.23473e-06, min loss: 5.45680e-07\n",
      "Epoch: 1822600, elapsed: 1.07e+01, train loss: 5.44947e-07, val loss: 1.20630e-06, min loss: 5.44947e-07\n",
      "Epoch: 1822700, elapsed: 1.10e+01, train loss: 1.27865e-06, val loss: 1.87306e-06, min loss: 5.44947e-07\n",
      "Epoch: 1822800, elapsed: 1.09e+01, train loss: 5.44757e-07, val loss: 1.20707e-06, min loss: 5.44757e-07\n",
      "Epoch: 1822900, elapsed: 1.08e+01, train loss: 5.49514e-07, val loss: 1.22623e-06, min loss: 5.44757e-07\n",
      "Epoch: 1823000, elapsed: 1.10e+01, train loss: 5.75226e-07, val loss: 1.31746e-06, min loss: 5.44757e-07\n",
      "Epoch: 1823100, elapsed: 1.08e+01, train loss: 5.45109e-07, val loss: 1.20974e-06, min loss: 5.44757e-07\n",
      "Epoch: 1823200, elapsed: 1.08e+01, train loss: 6.16090e-07, val loss: 1.34922e-06, min loss: 5.44757e-07\n",
      "Epoch: 1823300, elapsed: 1.08e+01, train loss: 5.44984e-07, val loss: 1.20284e-06, min loss: 5.44757e-07\n",
      "Epoch: 1823400, elapsed: 1.07e+01, train loss: 5.80914e-07, val loss: 1.24806e-06, min loss: 5.44757e-07\n",
      "Epoch: 1823500, elapsed: 1.10e+01, train loss: 5.96236e-07, val loss: 1.27704e-06, min loss: 5.44757e-07\n",
      "Epoch: 1823600, elapsed: 1.10e+01, train loss: 5.56913e-07, val loss: 1.22524e-06, min loss: 5.44757e-07\n",
      "Epoch: 1823700, elapsed: 1.08e+01, train loss: 5.47837e-07, val loss: 1.21979e-06, min loss: 5.44757e-07\n",
      "Epoch: 1823800, elapsed: 1.09e+01, train loss: 5.45161e-07, val loss: 1.20926e-06, min loss: 5.44757e-07\n",
      "Epoch: 1823900, elapsed: 1.09e+01, train loss: 5.51691e-07, val loss: 1.21756e-06, min loss: 5.44757e-07\n",
      "Epoch: 1824000, elapsed: 1.07e+01, train loss: 6.10353e-07, val loss: 1.23746e-06, min loss: 5.44757e-07\n",
      "Epoch: 1824100, elapsed: 1.09e+01, train loss: 7.79805e-07, val loss: 1.72568e-06, min loss: 5.44757e-07\n",
      "Epoch: 1824200, elapsed: 1.11e+01, train loss: 5.52754e-07, val loss: 1.23163e-06, min loss: 5.44757e-07\n",
      "Epoch: 1824300, elapsed: 1.08e+01, train loss: 5.44258e-07, val loss: 1.20685e-06, min loss: 5.44258e-07\n",
      "Epoch: 1824400, elapsed: 1.08e+01, train loss: 5.59471e-07, val loss: 1.22408e-06, min loss: 5.44258e-07\n",
      "Epoch: 1824500, elapsed: 1.08e+01, train loss: 6.83391e-07, val loss: 1.30708e-06, min loss: 5.44258e-07\n",
      "Epoch: 1824600, elapsed: 1.07e+01, train loss: 1.78245e-06, val loss: 2.51835e-06, min loss: 5.44258e-07\n",
      "Epoch: 1824700, elapsed: 1.09e+01, train loss: 2.93746e-06, val loss: 3.56776e-06, min loss: 5.44258e-07\n",
      "Epoch: 1824800, elapsed: 1.08e+01, train loss: 5.75884e-07, val loss: 1.27491e-06, min loss: 5.44258e-07\n",
      "Epoch: 1824900, elapsed: 1.62e+01, train loss: 5.44155e-07, val loss: 1.20958e-06, min loss: 5.44155e-07\n",
      "Epoch: 1825000, elapsed: 1.10e+01, train loss: 5.47284e-07, val loss: 1.20312e-06, min loss: 5.44155e-07\n",
      "Epoch: 1825100, elapsed: 1.30e+01, train loss: 6.63346e-07, val loss: 1.38627e-06, min loss: 5.44155e-07\n",
      "Epoch: 1825200, elapsed: 1.11e+01, train loss: 1.73541e-06, val loss: 1.74840e-06, min loss: 5.44155e-07\n",
      "Epoch: 1825300, elapsed: 1.09e+01, train loss: 6.02466e-07, val loss: 1.27201e-06, min loss: 5.44155e-07\n",
      "Epoch: 1825400, elapsed: 1.11e+01, train loss: 5.48351e-07, val loss: 1.21851e-06, min loss: 5.44155e-07\n",
      "Epoch: 1825500, elapsed: 1.10e+01, train loss: 5.58110e-07, val loss: 1.24110e-06, min loss: 5.44155e-07\n",
      "Epoch: 1825600, elapsed: 1.10e+01, train loss: 5.85271e-07, val loss: 1.20941e-06, min loss: 5.44155e-07\n",
      "Epoch: 1825700, elapsed: 1.10e+01, train loss: 5.43687e-07, val loss: 1.21028e-06, min loss: 5.43687e-07\n",
      "Epoch: 1825800, elapsed: 1.09e+01, train loss: 5.70252e-07, val loss: 1.19930e-06, min loss: 5.43687e-07\n",
      "Epoch: 1825900, elapsed: 1.10e+01, train loss: 9.31209e-07, val loss: 1.52318e-06, min loss: 5.43687e-07\n",
      "Epoch: 1826000, elapsed: 1.09e+01, train loss: 5.46200e-07, val loss: 1.20658e-06, min loss: 5.43687e-07\n",
      "Epoch: 1826100, elapsed: 1.08e+01, train loss: 1.38561e-06, val loss: 1.98954e-06, min loss: 5.43687e-07\n",
      "Epoch: 1826200, elapsed: 1.09e+01, train loss: 5.54207e-07, val loss: 1.21776e-06, min loss: 5.43687e-07\n",
      "Epoch: 1826300, elapsed: 1.09e+01, train loss: 6.66685e-07, val loss: 1.32508e-06, min loss: 5.43687e-07\n",
      "Epoch: 1826400, elapsed: 1.08e+01, train loss: 6.58540e-07, val loss: 1.32284e-06, min loss: 5.43687e-07\n",
      "Epoch: 1826500, elapsed: 1.10e+01, train loss: 5.48605e-07, val loss: 1.20853e-06, min loss: 5.43687e-07\n",
      "Epoch: 1826600, elapsed: 1.08e+01, train loss: 5.43460e-07, val loss: 1.20785e-06, min loss: 5.43460e-07\n",
      "Epoch: 1826700, elapsed: 1.08e+01, train loss: 5.57849e-07, val loss: 1.21273e-06, min loss: 5.43460e-07\n",
      "Epoch: 1826800, elapsed: 1.09e+01, train loss: 2.18547e-06, val loss: 3.10218e-06, min loss: 5.43460e-07\n",
      "Epoch: 1826900, elapsed: 1.08e+01, train loss: 5.75009e-07, val loss: 1.29248e-06, min loss: 5.43460e-07\n",
      "Epoch: 1827000, elapsed: 1.08e+01, train loss: 5.54193e-07, val loss: 1.22116e-06, min loss: 5.43460e-07\n",
      "Epoch: 1827100, elapsed: 1.08e+01, train loss: 5.68607e-07, val loss: 1.21715e-06, min loss: 5.43460e-07\n",
      "Epoch: 1827200, elapsed: 1.08e+01, train loss: 5.80366e-07, val loss: 1.32555e-06, min loss: 5.43460e-07\n",
      "Epoch: 1827300, elapsed: 1.08e+01, train loss: 9.99354e-07, val loss: 1.89494e-06, min loss: 5.43460e-07\n",
      "Epoch: 1827400, elapsed: 1.07e+01, train loss: 6.72551e-07, val loss: 1.25168e-06, min loss: 5.43460e-07\n",
      "Epoch: 1827500, elapsed: 1.08e+01, train loss: 5.62329e-07, val loss: 1.20235e-06, min loss: 5.43460e-07\n",
      "Epoch: 1827600, elapsed: 1.05e+01, train loss: 5.43240e-07, val loss: 1.21126e-06, min loss: 5.43240e-07\n",
      "Epoch: 1827700, elapsed: 1.08e+01, train loss: 5.64195e-07, val loss: 1.21170e-06, min loss: 5.43240e-07\n",
      "Epoch: 1827800, elapsed: 1.07e+01, train loss: 3.76126e-06, val loss: 3.45065e-06, min loss: 5.43240e-07\n",
      "Epoch: 1827900, elapsed: 1.07e+01, train loss: 5.42983e-07, val loss: 1.21223e-06, min loss: 5.42983e-07\n",
      "Epoch: 1828000, elapsed: 1.07e+01, train loss: 5.61549e-07, val loss: 1.24632e-06, min loss: 5.42983e-07\n",
      "Epoch: 1828100, elapsed: 1.59e+01, train loss: 5.42808e-07, val loss: 1.21126e-06, min loss: 5.42808e-07\n",
      "Epoch: 1828200, elapsed: 1.12e+01, train loss: 5.44166e-07, val loss: 1.21680e-06, min loss: 5.42808e-07\n",
      "Epoch: 1828300, elapsed: 1.11e+01, train loss: 5.82620e-07, val loss: 1.21762e-06, min loss: 5.42808e-07\n",
      "Epoch: 1828400, elapsed: 1.11e+01, train loss: 1.05779e-06, val loss: 1.92202e-06, min loss: 5.42808e-07\n",
      "Epoch: 1828500, elapsed: 1.08e+01, train loss: 9.51073e-07, val loss: 1.77166e-06, min loss: 5.42808e-07\n",
      "Epoch: 1828600, elapsed: 1.10e+01, train loss: 2.41697e-06, val loss: 2.00915e-06, min loss: 5.42808e-07\n",
      "Epoch: 1828700, elapsed: 1.11e+01, train loss: 5.43195e-07, val loss: 1.20835e-06, min loss: 5.42808e-07\n",
      "Epoch: 1828800, elapsed: 1.11e+01, train loss: 5.44078e-07, val loss: 1.20561e-06, min loss: 5.42808e-07\n",
      "Epoch: 1828900, elapsed: 1.11e+01, train loss: 5.55976e-07, val loss: 1.24270e-06, min loss: 5.42808e-07\n",
      "Epoch: 1829000, elapsed: 1.10e+01, train loss: 1.44350e-06, val loss: 1.97601e-06, min loss: 5.42808e-07\n",
      "Epoch: 1829100, elapsed: 1.08e+01, train loss: 5.51431e-07, val loss: 1.23395e-06, min loss: 5.42808e-07\n",
      "Epoch: 1829200, elapsed: 1.08e+01, train loss: 5.76042e-07, val loss: 1.25458e-06, min loss: 5.42808e-07\n",
      "Epoch: 1829300, elapsed: 1.10e+01, train loss: 5.71267e-07, val loss: 1.22685e-06, min loss: 5.42808e-07\n",
      "Epoch: 1829400, elapsed: 1.10e+01, train loss: 5.86622e-07, val loss: 1.30083e-06, min loss: 5.42808e-07\n",
      "Epoch: 1829500, elapsed: 1.08e+01, train loss: 6.59289e-07, val loss: 1.59842e-06, min loss: 5.42808e-07\n",
      "Epoch: 1829600, elapsed: 1.09e+01, train loss: 5.42278e-07, val loss: 1.21061e-06, min loss: 5.42278e-07\n",
      "Epoch: 1829700, elapsed: 1.08e+01, train loss: 5.59593e-07, val loss: 1.24194e-06, min loss: 5.42278e-07\n",
      "Epoch: 1829800, elapsed: 1.09e+01, train loss: 5.65305e-07, val loss: 1.21198e-06, min loss: 5.42278e-07\n",
      "Epoch: 1829900, elapsed: 1.08e+01, train loss: 5.54480e-07, val loss: 1.25516e-06, min loss: 5.42278e-07\n",
      "Epoch: 1830000, elapsed: 1.09e+01, train loss: 7.19119e-07, val loss: 1.27840e-06, min loss: 5.42278e-07\n",
      "Epoch: 1830100, elapsed: 1.28e+01, train loss: 2.09407e-06, val loss: 1.95455e-06, min loss: 5.42278e-07\n",
      "Epoch: 1830200, elapsed: 1.07e+01, train loss: 9.11690e-07, val loss: 1.54411e-06, min loss: 5.42278e-07\n",
      "Epoch: 1830300, elapsed: 1.07e+01, train loss: 5.43139e-07, val loss: 1.21118e-06, min loss: 5.42278e-07\n",
      "Epoch: 1830400, elapsed: 1.07e+01, train loss: 5.47052e-07, val loss: 1.22104e-06, min loss: 5.42278e-07\n",
      "Epoch: 1830500, elapsed: 1.07e+01, train loss: 5.50676e-07, val loss: 1.21874e-06, min loss: 5.42278e-07\n",
      "Epoch: 1830600, elapsed: 1.08e+01, train loss: 1.11281e-06, val loss: 1.80058e-06, min loss: 5.42278e-07\n",
      "Epoch: 1830700, elapsed: 1.06e+01, train loss: 8.48479e-07, val loss: 1.63997e-06, min loss: 5.42278e-07\n",
      "Epoch: 1830800, elapsed: 1.08e+01, train loss: 6.26988e-07, val loss: 1.28942e-06, min loss: 5.42278e-07\n",
      "Epoch: 1830900, elapsed: 1.06e+01, train loss: 6.49177e-07, val loss: 1.32428e-06, min loss: 5.42278e-07\n",
      "Epoch: 1831000, elapsed: 1.08e+01, train loss: 5.47889e-07, val loss: 1.20103e-06, min loss: 5.42278e-07\n",
      "Epoch: 1831100, elapsed: 1.07e+01, train loss: 5.49429e-07, val loss: 1.23309e-06, min loss: 5.42278e-07\n",
      "Epoch: 1831200, elapsed: 1.09e+01, train loss: 5.86450e-07, val loss: 1.26438e-06, min loss: 5.42278e-07\n",
      "Epoch: 1831300, elapsed: 1.08e+01, train loss: 2.39333e-06, val loss: 2.94727e-06, min loss: 5.42278e-07\n",
      "Epoch: 1831400, elapsed: 1.60e+01, train loss: 1.91831e-06, val loss: 2.64297e-06, min loss: 5.42278e-07\n",
      "Epoch: 1831500, elapsed: 1.09e+01, train loss: 5.44431e-07, val loss: 1.22308e-06, min loss: 5.42278e-07\n",
      "Epoch: 1831600, elapsed: 1.12e+01, train loss: 5.41917e-07, val loss: 1.21341e-06, min loss: 5.41917e-07\n",
      "Epoch: 1831700, elapsed: 1.09e+01, train loss: 5.60560e-07, val loss: 1.26117e-06, min loss: 5.41917e-07\n",
      "Epoch: 1831800, elapsed: 1.10e+01, train loss: 5.83992e-07, val loss: 1.29248e-06, min loss: 5.41917e-07\n",
      "Epoch: 1831900, elapsed: 1.10e+01, train loss: 6.25385e-07, val loss: 1.25107e-06, min loss: 5.41917e-07\n",
      "Epoch: 1832000, elapsed: 1.10e+01, train loss: 5.45761e-07, val loss: 1.23114e-06, min loss: 5.41917e-07\n",
      "Epoch: 1832100, elapsed: 1.09e+01, train loss: 7.97536e-07, val loss: 1.23485e-06, min loss: 5.41917e-07\n",
      "Epoch: 1832200, elapsed: 1.09e+01, train loss: 5.41493e-07, val loss: 1.21372e-06, min loss: 5.41493e-07\n",
      "Epoch: 1832300, elapsed: 1.08e+01, train loss: 5.86667e-07, val loss: 1.22838e-06, min loss: 5.41493e-07\n",
      "Epoch: 1832400, elapsed: 1.12e+01, train loss: 8.81514e-07, val loss: 1.50173e-06, min loss: 5.41493e-07\n",
      "Epoch: 1832500, elapsed: 1.08e+01, train loss: 5.41777e-07, val loss: 1.21090e-06, min loss: 5.41493e-07\n",
      "Epoch: 1832600, elapsed: 1.10e+01, train loss: 5.50954e-07, val loss: 1.20931e-06, min loss: 5.41493e-07\n",
      "Epoch: 1832700, elapsed: 1.10e+01, train loss: 1.18013e-06, val loss: 1.41159e-06, min loss: 5.41493e-07\n",
      "Epoch: 1832800, elapsed: 1.09e+01, train loss: 5.52032e-07, val loss: 1.23940e-06, min loss: 5.41493e-07\n",
      "Epoch: 1832900, elapsed: 1.09e+01, train loss: 5.41357e-07, val loss: 1.21430e-06, min loss: 5.41357e-07\n",
      "Epoch: 1833000, elapsed: 1.09e+01, train loss: 5.60771e-07, val loss: 1.24338e-06, min loss: 5.41357e-07\n",
      "Epoch: 1833100, elapsed: 1.10e+01, train loss: 5.41828e-07, val loss: 1.21962e-06, min loss: 5.41357e-07\n",
      "Epoch: 1833200, elapsed: 1.07e+01, train loss: 6.55988e-07, val loss: 1.26359e-06, min loss: 5.41357e-07\n",
      "Epoch: 1833300, elapsed: 1.10e+01, train loss: 8.02285e-07, val loss: 1.65012e-06, min loss: 5.41357e-07\n",
      "Epoch: 1833400, elapsed: 1.08e+01, train loss: 6.24710e-07, val loss: 1.32200e-06, min loss: 5.41357e-07\n",
      "Epoch: 1833500, elapsed: 1.09e+01, train loss: 5.41545e-07, val loss: 1.21589e-06, min loss: 5.41357e-07\n",
      "Epoch: 1833600, elapsed: 1.08e+01, train loss: 8.30221e-07, val loss: 1.60442e-06, min loss: 5.41357e-07\n",
      "Epoch: 1833700, elapsed: 1.06e+01, train loss: 5.59876e-07, val loss: 1.22871e-06, min loss: 5.41357e-07\n",
      "Epoch: 1833800, elapsed: 1.08e+01, train loss: 5.45314e-07, val loss: 1.21350e-06, min loss: 5.41357e-07\n",
      "Epoch: 1833900, elapsed: 1.09e+01, train loss: 5.43020e-07, val loss: 1.22032e-06, min loss: 5.41357e-07\n",
      "Epoch: 1834000, elapsed: 1.10e+01, train loss: 1.35215e-06, val loss: 2.18867e-06, min loss: 5.41357e-07\n",
      "Epoch: 1834100, elapsed: 1.08e+01, train loss: 5.93260e-07, val loss: 1.33416e-06, min loss: 5.41357e-07\n",
      "Epoch: 1834200, elapsed: 1.08e+01, train loss: 5.42861e-07, val loss: 1.20497e-06, min loss: 5.41357e-07\n",
      "Epoch: 1834300, elapsed: 1.08e+01, train loss: 5.53036e-07, val loss: 1.21395e-06, min loss: 5.41357e-07\n",
      "Epoch: 1834400, elapsed: 1.10e+01, train loss: 5.48665e-07, val loss: 1.23327e-06, min loss: 5.41357e-07\n",
      "Epoch: 1834500, elapsed: 1.09e+01, train loss: 5.60137e-07, val loss: 1.25764e-06, min loss: 5.41357e-07\n",
      "Epoch: 1834600, elapsed: 1.06e+01, train loss: 5.42692e-07, val loss: 1.21804e-06, min loss: 5.41357e-07\n",
      "Epoch: 1834700, elapsed: 1.59e+01, train loss: 5.41842e-07, val loss: 1.21501e-06, min loss: 5.41357e-07\n",
      "Epoch: 1834800, elapsed: 1.08e+01, train loss: 5.53100e-07, val loss: 1.24645e-06, min loss: 5.41357e-07\n",
      "Epoch: 1834900, elapsed: 1.09e+01, train loss: 7.82887e-07, val loss: 1.56623e-06, min loss: 5.41357e-07\n",
      "Epoch: 1835000, elapsed: 1.10e+01, train loss: 5.43510e-07, val loss: 1.20952e-06, min loss: 5.41357e-07\n",
      "Epoch: 1835100, elapsed: 1.28e+01, train loss: 5.40448e-07, val loss: 1.21585e-06, min loss: 5.40448e-07\n",
      "Epoch: 1835200, elapsed: 1.07e+01, train loss: 7.39608e-07, val loss: 1.34249e-06, min loss: 5.40448e-07\n",
      "Epoch: 1835300, elapsed: 1.10e+01, train loss: 5.41631e-07, val loss: 1.22026e-06, min loss: 5.40448e-07\n",
      "Epoch: 1835400, elapsed: 1.08e+01, train loss: 5.40408e-07, val loss: 1.22047e-06, min loss: 5.40408e-07\n",
      "Epoch: 1835500, elapsed: 1.11e+01, train loss: 6.16297e-07, val loss: 1.76251e-06, min loss: 5.40408e-07\n",
      "Epoch: 1835600, elapsed: 1.10e+01, train loss: 5.40244e-07, val loss: 1.21759e-06, min loss: 5.40244e-07\n",
      "Epoch: 1835700, elapsed: 1.07e+01, train loss: 5.52633e-07, val loss: 1.21610e-06, min loss: 5.40244e-07\n",
      "Epoch: 1835800, elapsed: 1.07e+01, train loss: 5.45107e-07, val loss: 1.22334e-06, min loss: 5.40244e-07\n",
      "Epoch: 1835900, elapsed: 1.09e+01, train loss: 6.24300e-07, val loss: 1.35650e-06, min loss: 5.40244e-07\n",
      "Epoch: 1836000, elapsed: 1.08e+01, train loss: 1.35901e-06, val loss: 2.27098e-06, min loss: 5.40244e-07\n",
      "Epoch: 1836100, elapsed: 1.10e+01, train loss: 5.45943e-07, val loss: 1.21508e-06, min loss: 5.40244e-07\n",
      "Epoch: 1836200, elapsed: 1.07e+01, train loss: 5.43613e-07, val loss: 1.21323e-06, min loss: 5.40244e-07\n",
      "Epoch: 1836300, elapsed: 1.09e+01, train loss: 5.49826e-07, val loss: 1.23924e-06, min loss: 5.40244e-07\n",
      "Epoch: 1836400, elapsed: 1.07e+01, train loss: 7.76552e-07, val loss: 1.38685e-06, min loss: 5.40244e-07\n",
      "Epoch: 1836500, elapsed: 1.08e+01, train loss: 1.58847e-06, val loss: 2.32926e-06, min loss: 5.40244e-07\n",
      "Epoch: 1836600, elapsed: 1.07e+01, train loss: 8.18740e-07, val loss: 1.57805e-06, min loss: 5.40244e-07\n",
      "Epoch: 1836700, elapsed: 1.08e+01, train loss: 9.41930e-07, val loss: 1.71568e-06, min loss: 5.40244e-07\n",
      "Epoch: 1836800, elapsed: 1.08e+01, train loss: 5.92567e-07, val loss: 1.34010e-06, min loss: 5.40244e-07\n",
      "Epoch: 1836900, elapsed: 1.07e+01, train loss: 6.05111e-07, val loss: 1.34109e-06, min loss: 5.40244e-07\n",
      "Epoch: 1837000, elapsed: 1.07e+01, train loss: 5.92529e-07, val loss: 1.19401e-06, min loss: 5.40244e-07\n",
      "Epoch: 1837100, elapsed: 1.07e+01, train loss: 1.55127e-06, val loss: 2.03902e-06, min loss: 5.40244e-07\n",
      "Epoch: 1837200, elapsed: 1.08e+01, train loss: 8.06346e-07, val loss: 1.53393e-06, min loss: 5.40244e-07\n",
      "Epoch: 1837300, elapsed: 1.06e+01, train loss: 8.34417e-07, val loss: 1.66677e-06, min loss: 5.40244e-07\n",
      "Epoch: 1837400, elapsed: 1.07e+01, train loss: 5.87157e-07, val loss: 1.27319e-06, min loss: 5.40244e-07\n",
      "Epoch: 1837500, elapsed: 1.08e+01, train loss: 6.52786e-07, val loss: 1.24381e-06, min loss: 5.40244e-07\n",
      "Epoch: 1837600, elapsed: 1.08e+01, train loss: 2.47318e-06, val loss: 2.57266e-06, min loss: 5.40244e-07\n",
      "Epoch: 1837700, elapsed: 1.06e+01, train loss: 5.39919e-07, val loss: 1.22460e-06, min loss: 5.39919e-07\n",
      "Epoch: 1837800, elapsed: 1.08e+01, train loss: 5.40056e-07, val loss: 1.21408e-06, min loss: 5.39919e-07\n",
      "Epoch: 1837900, elapsed: 1.56e+01, train loss: 5.39659e-07, val loss: 1.21487e-06, min loss: 5.39659e-07\n",
      "Epoch: 1838000, elapsed: 1.11e+01, train loss: 5.40797e-07, val loss: 1.21454e-06, min loss: 5.39659e-07\n",
      "Epoch: 1838100, elapsed: 1.09e+01, train loss: 8.01669e-07, val loss: 1.17830e-06, min loss: 5.39659e-07\n",
      "Epoch: 1838200, elapsed: 1.09e+01, train loss: 8.95946e-07, val loss: 1.77102e-06, min loss: 5.39659e-07\n",
      "Epoch: 1838300, elapsed: 1.08e+01, train loss: 5.42360e-07, val loss: 1.22640e-06, min loss: 5.39659e-07\n",
      "Epoch: 1838400, elapsed: 1.09e+01, train loss: 5.56381e-07, val loss: 1.21215e-06, min loss: 5.39659e-07\n",
      "Epoch: 1838500, elapsed: 1.09e+01, train loss: 6.29098e-07, val loss: 1.36832e-06, min loss: 5.39659e-07\n",
      "Epoch: 1838600, elapsed: 1.10e+01, train loss: 5.55902e-07, val loss: 1.23301e-06, min loss: 5.39659e-07\n",
      "Epoch: 1838700, elapsed: 1.10e+01, train loss: 5.53382e-07, val loss: 1.21042e-06, min loss: 5.39659e-07\n",
      "Epoch: 1838800, elapsed: 1.09e+01, train loss: 7.66404e-07, val loss: 1.52972e-06, min loss: 5.39659e-07\n",
      "Epoch: 1838900, elapsed: 1.11e+01, train loss: 5.40525e-07, val loss: 1.23037e-06, min loss: 5.39659e-07\n",
      "Epoch: 1839000, elapsed: 1.09e+01, train loss: 5.75343e-07, val loss: 1.27186e-06, min loss: 5.39659e-07\n",
      "Epoch: 1839100, elapsed: 1.09e+01, train loss: 5.50378e-07, val loss: 1.20064e-06, min loss: 5.39659e-07\n",
      "Epoch: 1839200, elapsed: 1.08e+01, train loss: 5.42258e-07, val loss: 1.21927e-06, min loss: 5.39659e-07\n",
      "Epoch: 1839300, elapsed: 1.08e+01, train loss: 6.52716e-07, val loss: 1.20631e-06, min loss: 5.39659e-07\n",
      "Epoch: 1839400, elapsed: 1.09e+01, train loss: 3.44742e-06, val loss: 3.51148e-06, min loss: 5.39659e-07\n",
      "Epoch: 1839500, elapsed: 1.08e+01, train loss: 6.33642e-07, val loss: 1.42477e-06, min loss: 5.39659e-07\n",
      "Epoch: 1839600, elapsed: 1.07e+01, train loss: 5.63530e-07, val loss: 1.22006e-06, min loss: 5.39659e-07\n",
      "Epoch: 1839700, elapsed: 1.10e+01, train loss: 9.40816e-07, val loss: 1.73151e-06, min loss: 5.39659e-07\n",
      "Epoch: 1839800, elapsed: 1.08e+01, train loss: 5.39945e-07, val loss: 1.22172e-06, min loss: 5.39659e-07\n",
      "Epoch: 1839900, elapsed: 1.08e+01, train loss: 5.44860e-07, val loss: 1.22622e-06, min loss: 5.39659e-07\n",
      "Epoch: 1840000, elapsed: 1.09e+01, train loss: 8.76405e-07, val loss: 1.61908e-06, min loss: 5.39659e-07\n",
      "Epoch: 1840100, elapsed: 1.26e+01, train loss: 1.11393e-06, val loss: 1.94624e-06, min loss: 5.39659e-07\n",
      "Epoch: 1840200, elapsed: 1.09e+01, train loss: 5.51764e-07, val loss: 1.25511e-06, min loss: 5.39659e-07\n",
      "Epoch: 1840300, elapsed: 1.08e+01, train loss: 5.39447e-07, val loss: 1.21619e-06, min loss: 5.39447e-07\n",
      "Epoch: 1840400, elapsed: 1.08e+01, train loss: 5.45700e-07, val loss: 1.24552e-06, min loss: 5.39447e-07\n",
      "Epoch: 1840500, elapsed: 1.09e+01, train loss: 5.58839e-07, val loss: 1.22408e-06, min loss: 5.39447e-07\n",
      "Epoch: 1840600, elapsed: 1.06e+01, train loss: 6.16418e-07, val loss: 1.39788e-06, min loss: 5.39447e-07\n",
      "Epoch: 1840700, elapsed: 1.09e+01, train loss: 8.11006e-07, val loss: 1.41463e-06, min loss: 5.39447e-07\n",
      "Epoch: 1840800, elapsed: 1.08e+01, train loss: 5.54260e-07, val loss: 1.18607e-06, min loss: 5.39447e-07\n",
      "Epoch: 1840900, elapsed: 1.10e+01, train loss: 5.44060e-07, val loss: 1.22635e-06, min loss: 5.39447e-07\n",
      "Epoch: 1841000, elapsed: 1.08e+01, train loss: 7.22008e-07, val loss: 1.47635e-06, min loss: 5.39447e-07\n",
      "Epoch: 1841100, elapsed: 1.09e+01, train loss: 6.55745e-07, val loss: 1.32091e-06, min loss: 5.39447e-07\n",
      "Epoch: 1841200, elapsed: 1.61e+01, train loss: 7.27526e-07, val loss: 1.31430e-06, min loss: 5.39447e-07\n",
      "Epoch: 1841300, elapsed: 1.14e+01, train loss: 5.38917e-07, val loss: 1.21986e-06, min loss: 5.38917e-07\n",
      "Epoch: 1841400, elapsed: 1.12e+01, train loss: 5.50606e-07, val loss: 1.24779e-06, min loss: 5.38917e-07\n",
      "Epoch: 1841500, elapsed: 1.11e+01, train loss: 5.94144e-07, val loss: 1.25621e-06, min loss: 5.38917e-07\n",
      "Epoch: 1841600, elapsed: 1.11e+01, train loss: 5.48227e-07, val loss: 1.19965e-06, min loss: 5.38917e-07\n",
      "Epoch: 1841700, elapsed: 1.11e+01, train loss: 6.06890e-07, val loss: 1.29090e-06, min loss: 5.38917e-07\n",
      "Epoch: 1841800, elapsed: 1.09e+01, train loss: 1.60114e-06, val loss: 2.40046e-06, min loss: 5.38917e-07\n",
      "Epoch: 1841900, elapsed: 1.09e+01, train loss: 5.38239e-07, val loss: 1.22579e-06, min loss: 5.38239e-07\n",
      "Epoch: 1842000, elapsed: 1.09e+01, train loss: 5.39245e-07, val loss: 1.21309e-06, min loss: 5.38239e-07\n",
      "Epoch: 1842100, elapsed: 1.11e+01, train loss: 1.01311e-06, val loss: 1.76237e-06, min loss: 5.38239e-07\n",
      "Epoch: 1842200, elapsed: 1.09e+01, train loss: 6.78767e-07, val loss: 1.37531e-06, min loss: 5.38239e-07\n",
      "Epoch: 1842300, elapsed: 1.09e+01, train loss: 1.89237e-06, val loss: 3.18397e-06, min loss: 5.38239e-07\n",
      "Epoch: 1842400, elapsed: 1.10e+01, train loss: 5.65653e-07, val loss: 1.24087e-06, min loss: 5.38239e-07\n",
      "Epoch: 1842500, elapsed: 1.10e+01, train loss: 5.61212e-07, val loss: 1.23509e-06, min loss: 5.38239e-07\n",
      "Epoch: 1842600, elapsed: 1.11e+01, train loss: 5.54509e-07, val loss: 1.24205e-06, min loss: 5.38239e-07\n",
      "Epoch: 1842700, elapsed: 1.07e+01, train loss: 5.65825e-07, val loss: 1.25491e-06, min loss: 5.38239e-07\n",
      "Epoch: 1842800, elapsed: 1.08e+01, train loss: 6.17233e-07, val loss: 1.24159e-06, min loss: 5.38239e-07\n",
      "Epoch: 1842900, elapsed: 1.09e+01, train loss: 1.56326e-06, val loss: 1.97023e-06, min loss: 5.38239e-07\n",
      "Epoch: 1843000, elapsed: 1.07e+01, train loss: 6.31538e-07, val loss: 1.32379e-06, min loss: 5.38239e-07\n",
      "Epoch: 1843100, elapsed: 1.08e+01, train loss: 8.26499e-07, val loss: 1.46570e-06, min loss: 5.38239e-07\n",
      "Epoch: 1843200, elapsed: 1.08e+01, train loss: 2.49565e-06, val loss: 3.04119e-06, min loss: 5.38239e-07\n",
      "Epoch: 1843300, elapsed: 1.09e+01, train loss: 5.43080e-07, val loss: 1.19876e-06, min loss: 5.38239e-07\n",
      "Epoch: 1843400, elapsed: 1.08e+01, train loss: 5.38045e-07, val loss: 1.21680e-06, min loss: 5.38045e-07\n",
      "Epoch: 1843500, elapsed: 1.09e+01, train loss: 1.26462e-06, val loss: 1.96476e-06, min loss: 5.38045e-07\n",
      "Epoch: 1843600, elapsed: 1.10e+01, train loss: 5.37352e-07, val loss: 1.22137e-06, min loss: 5.37352e-07\n",
      "Epoch: 1843700, elapsed: 1.08e+01, train loss: 5.58738e-07, val loss: 1.25855e-06, min loss: 5.37352e-07\n",
      "Epoch: 1843800, elapsed: 1.10e+01, train loss: 8.46548e-07, val loss: 1.85648e-06, min loss: 5.37352e-07\n",
      "Epoch: 1843900, elapsed: 1.07e+01, train loss: 2.13646e-06, val loss: 3.15241e-06, min loss: 5.37352e-07\n",
      "Epoch: 1844000, elapsed: 1.10e+01, train loss: 1.24658e-06, val loss: 2.00031e-06, min loss: 5.37352e-07\n",
      "Epoch: 1844100, elapsed: 1.07e+01, train loss: 7.65685e-07, val loss: 1.62049e-06, min loss: 5.37352e-07\n",
      "Epoch: 1844200, elapsed: 1.08e+01, train loss: 5.37254e-07, val loss: 1.22235e-06, min loss: 5.37254e-07\n",
      "Epoch: 1844300, elapsed: 1.07e+01, train loss: 5.39609e-07, val loss: 1.23979e-06, min loss: 5.37254e-07\n",
      "Epoch: 1844400, elapsed: 1.09e+01, train loss: 5.38358e-07, val loss: 1.21769e-06, min loss: 5.37254e-07\n",
      "Epoch: 1844500, elapsed: 1.57e+01, train loss: 5.39238e-07, val loss: 1.22998e-06, min loss: 5.37254e-07\n",
      "Epoch: 1844600, elapsed: 1.12e+01, train loss: 5.54265e-07, val loss: 1.23542e-06, min loss: 5.37254e-07\n",
      "Epoch: 1844700, elapsed: 1.12e+01, train loss: 8.16160e-07, val loss: 1.43105e-06, min loss: 5.37254e-07\n",
      "Epoch: 1844800, elapsed: 1.11e+01, train loss: 8.87020e-07, val loss: 1.46730e-06, min loss: 5.37254e-07\n",
      "Epoch: 1844900, elapsed: 1.10e+01, train loss: 5.51621e-07, val loss: 1.23057e-06, min loss: 5.37254e-07\n",
      "Epoch: 1845000, elapsed: 1.10e+01, train loss: 5.40606e-07, val loss: 1.23359e-06, min loss: 5.37254e-07\n",
      "Epoch: 1845100, elapsed: 1.29e+01, train loss: 5.97987e-07, val loss: 1.23001e-06, min loss: 5.37254e-07\n",
      "Epoch: 1845200, elapsed: 1.10e+01, train loss: 1.68192e-06, val loss: 2.11841e-06, min loss: 5.37254e-07\n",
      "Epoch: 1845300, elapsed: 1.11e+01, train loss: 6.77137e-07, val loss: 1.29421e-06, min loss: 5.37254e-07\n",
      "Epoch: 1845400, elapsed: 1.10e+01, train loss: 5.41187e-07, val loss: 1.20941e-06, min loss: 5.37254e-07\n",
      "Epoch: 1845500, elapsed: 1.09e+01, train loss: 5.39636e-07, val loss: 1.23796e-06, min loss: 5.37254e-07\n",
      "Epoch: 1845600, elapsed: 1.10e+01, train loss: 6.01036e-07, val loss: 1.29109e-06, min loss: 5.37254e-07\n",
      "Epoch: 1845700, elapsed: 1.09e+01, train loss: 5.37166e-07, val loss: 1.22142e-06, min loss: 5.37166e-07\n",
      "Epoch: 1845800, elapsed: 1.10e+01, train loss: 5.45544e-07, val loss: 1.20668e-06, min loss: 5.37166e-07\n",
      "Epoch: 1845900, elapsed: 1.10e+01, train loss: 2.03038e-06, val loss: 2.29256e-06, min loss: 5.37166e-07\n",
      "Epoch: 1846000, elapsed: 1.09e+01, train loss: 5.71437e-07, val loss: 1.26683e-06, min loss: 5.37166e-07\n",
      "Epoch: 1846100, elapsed: 1.07e+01, train loss: 1.37842e-06, val loss: 1.64857e-06, min loss: 5.37166e-07\n",
      "Epoch: 1846200, elapsed: 1.09e+01, train loss: 6.71745e-07, val loss: 1.43839e-06, min loss: 5.37166e-07\n",
      "Epoch: 1846300, elapsed: 1.10e+01, train loss: 5.47577e-07, val loss: 1.23795e-06, min loss: 5.37166e-07\n",
      "Epoch: 1846400, elapsed: 1.09e+01, train loss: 6.10876e-07, val loss: 1.43780e-06, min loss: 5.37166e-07\n",
      "Epoch: 1846500, elapsed: 1.08e+01, train loss: 5.36348e-07, val loss: 1.22354e-06, min loss: 5.36348e-07\n",
      "Epoch: 1846600, elapsed: 1.09e+01, train loss: 5.40994e-07, val loss: 1.23311e-06, min loss: 5.36348e-07\n",
      "Epoch: 1846700, elapsed: 1.06e+01, train loss: 7.09880e-07, val loss: 1.66485e-06, min loss: 5.36348e-07\n",
      "Epoch: 1846800, elapsed: 1.09e+01, train loss: 7.80432e-07, val loss: 1.41978e-06, min loss: 5.36348e-07\n",
      "Epoch: 1846900, elapsed: 1.08e+01, train loss: 5.42320e-07, val loss: 1.22105e-06, min loss: 5.36348e-07\n",
      "Epoch: 1847000, elapsed: 1.07e+01, train loss: 5.36454e-07, val loss: 1.21960e-06, min loss: 5.36348e-07\n",
      "Epoch: 1847100, elapsed: 1.07e+01, train loss: 5.77011e-07, val loss: 1.24850e-06, min loss: 5.36348e-07\n",
      "Epoch: 1847200, elapsed: 1.07e+01, train loss: 5.41589e-07, val loss: 1.23982e-06, min loss: 5.36348e-07\n",
      "Epoch: 1847300, elapsed: 1.09e+01, train loss: 5.38663e-07, val loss: 1.24087e-06, min loss: 5.36348e-07\n",
      "Epoch: 1847400, elapsed: 1.07e+01, train loss: 5.40254e-07, val loss: 1.22194e-06, min loss: 5.36348e-07\n",
      "Epoch: 1847500, elapsed: 1.08e+01, train loss: 7.75950e-07, val loss: 1.48337e-06, min loss: 5.36348e-07\n",
      "Epoch: 1847600, elapsed: 1.06e+01, train loss: 5.44582e-07, val loss: 1.23783e-06, min loss: 5.36348e-07\n",
      "Epoch: 1847700, elapsed: 1.07e+01, train loss: 6.49322e-07, val loss: 1.30572e-06, min loss: 5.36348e-07\n",
      "Epoch: 1847800, elapsed: 1.59e+01, train loss: 5.52379e-07, val loss: 1.41275e-06, min loss: 5.36348e-07\n",
      "Epoch: 1847900, elapsed: 1.11e+01, train loss: 1.23871e-06, val loss: 1.99178e-06, min loss: 5.36348e-07\n",
      "Epoch: 1848000, elapsed: 1.10e+01, train loss: 5.42619e-07, val loss: 1.21709e-06, min loss: 5.36348e-07\n",
      "Epoch: 1848100, elapsed: 1.12e+01, train loss: 5.36916e-07, val loss: 1.22238e-06, min loss: 5.36348e-07\n",
      "Epoch: 1848200, elapsed: 1.09e+01, train loss: 5.62411e-07, val loss: 1.28350e-06, min loss: 5.36348e-07\n",
      "Epoch: 1848300, elapsed: 1.12e+01, train loss: 9.06516e-07, val loss: 1.92145e-06, min loss: 5.36348e-07\n",
      "Epoch: 1848400, elapsed: 1.10e+01, train loss: 5.35718e-07, val loss: 1.22463e-06, min loss: 5.35718e-07\n",
      "Epoch: 1848500, elapsed: 1.10e+01, train loss: 5.40240e-07, val loss: 1.21951e-06, min loss: 5.35718e-07\n",
      "Epoch: 1848600, elapsed: 1.10e+01, train loss: 7.26349e-07, val loss: 1.60795e-06, min loss: 5.35718e-07\n",
      "Epoch: 1848700, elapsed: 1.09e+01, train loss: 5.36123e-07, val loss: 1.22307e-06, min loss: 5.35718e-07\n",
      "Epoch: 1848800, elapsed: 1.08e+01, train loss: 5.45714e-07, val loss: 1.23340e-06, min loss: 5.35718e-07\n",
      "Epoch: 1848900, elapsed: 1.10e+01, train loss: 7.18081e-07, val loss: 1.22710e-06, min loss: 5.35718e-07\n",
      "Epoch: 1849000, elapsed: 1.09e+01, train loss: 5.35493e-07, val loss: 1.22479e-06, min loss: 5.35493e-07\n",
      "Epoch: 1849100, elapsed: 1.09e+01, train loss: 5.40794e-07, val loss: 1.24841e-06, min loss: 5.35493e-07\n",
      "Epoch: 1849200, elapsed: 1.07e+01, train loss: 1.04927e-06, val loss: 1.74859e-06, min loss: 5.35493e-07\n",
      "Epoch: 1849300, elapsed: 1.07e+01, train loss: 7.90807e-07, val loss: 1.75385e-06, min loss: 5.35493e-07\n",
      "Epoch: 1849400, elapsed: 1.09e+01, train loss: 9.40602e-07, val loss: 1.42842e-06, min loss: 5.35493e-07\n",
      "Epoch: 1849500, elapsed: 1.07e+01, train loss: 5.43124e-07, val loss: 1.25169e-06, min loss: 5.35493e-07\n",
      "Epoch: 1849600, elapsed: 1.09e+01, train loss: 5.95299e-07, val loss: 1.33504e-06, min loss: 5.35493e-07\n",
      "Epoch: 1849700, elapsed: 1.08e+01, train loss: 7.82972e-07, val loss: 1.63859e-06, min loss: 5.35493e-07\n",
      "Epoch: 1849800, elapsed: 1.09e+01, train loss: 5.49942e-07, val loss: 1.26333e-06, min loss: 5.35493e-07\n",
      "Epoch: 1849900, elapsed: 1.09e+01, train loss: 1.00589e-06, val loss: 1.66789e-06, min loss: 5.35493e-07\n",
      "Epoch: 1850000, elapsed: 1.08e+01, train loss: 5.49063e-07, val loss: 1.25542e-06, min loss: 5.35493e-07\n",
      "Epoch: 1850100, elapsed: 1.27e+01, train loss: 5.36630e-07, val loss: 1.22780e-06, min loss: 5.35493e-07\n",
      "Epoch: 1850200, elapsed: 1.08e+01, train loss: 5.39957e-07, val loss: 1.22353e-06, min loss: 5.35493e-07\n",
      "Epoch: 1850300, elapsed: 1.07e+01, train loss: 5.39349e-07, val loss: 1.24725e-06, min loss: 5.35493e-07\n",
      "Epoch: 1850400, elapsed: 1.07e+01, train loss: 5.72839e-07, val loss: 1.23241e-06, min loss: 5.35493e-07\n",
      "Epoch: 1850500, elapsed: 1.07e+01, train loss: 5.50157e-07, val loss: 1.30371e-06, min loss: 5.35493e-07\n",
      "Epoch: 1850600, elapsed: 1.07e+01, train loss: 5.44517e-07, val loss: 1.23031e-06, min loss: 5.35493e-07\n",
      "Epoch: 1850700, elapsed: 1.08e+01, train loss: 7.71365e-07, val loss: 1.36098e-06, min loss: 5.35493e-07\n",
      "Epoch: 1850800, elapsed: 1.07e+01, train loss: 5.35220e-07, val loss: 1.22828e-06, min loss: 5.35220e-07\n",
      "Epoch: 1850900, elapsed: 1.08e+01, train loss: 8.44074e-07, val loss: 1.31734e-06, min loss: 5.35220e-07\n",
      "Epoch: 1851000, elapsed: 1.09e+01, train loss: 7.23018e-07, val loss: 1.51992e-06, min loss: 5.35220e-07\n",
      "Epoch: 1851100, elapsed: 1.63e+01, train loss: 9.70702e-07, val loss: 1.83365e-06, min loss: 5.35220e-07\n",
      "Epoch: 1851200, elapsed: 1.11e+01, train loss: 5.82790e-07, val loss: 1.33142e-06, min loss: 5.35220e-07\n",
      "Epoch: 1851300, elapsed: 1.11e+01, train loss: 1.34084e-06, val loss: 2.29136e-06, min loss: 5.35220e-07\n",
      "Epoch: 1851400, elapsed: 1.09e+01, train loss: 5.34993e-07, val loss: 1.23039e-06, min loss: 5.34993e-07\n",
      "Epoch: 1851500, elapsed: 1.09e+01, train loss: 5.35156e-07, val loss: 1.23827e-06, min loss: 5.34993e-07\n",
      "Epoch: 1851600, elapsed: 1.11e+01, train loss: 5.42976e-07, val loss: 1.22753e-06, min loss: 5.34993e-07\n",
      "Epoch: 1851700, elapsed: 1.09e+01, train loss: 5.44732e-07, val loss: 1.24315e-06, min loss: 5.34993e-07\n",
      "Epoch: 1851800, elapsed: 1.11e+01, train loss: 5.39508e-07, val loss: 1.23739e-06, min loss: 5.34993e-07\n",
      "Epoch: 1851900, elapsed: 1.09e+01, train loss: 7.18653e-07, val loss: 1.52581e-06, min loss: 5.34993e-07\n",
      "Epoch: 1852000, elapsed: 1.09e+01, train loss: 5.42022e-07, val loss: 1.24658e-06, min loss: 5.34993e-07\n",
      "Epoch: 1852100, elapsed: 1.09e+01, train loss: 5.37339e-07, val loss: 1.22253e-06, min loss: 5.34993e-07\n",
      "Epoch: 1852200, elapsed: 1.08e+01, train loss: 8.42682e-07, val loss: 1.49210e-06, min loss: 5.34993e-07\n",
      "Epoch: 1852300, elapsed: 1.09e+01, train loss: 6.40204e-07, val loss: 1.34957e-06, min loss: 5.34993e-07\n",
      "Epoch: 1852400, elapsed: 1.10e+01, train loss: 1.17783e-06, val loss: 1.28487e-06, min loss: 5.34993e-07\n",
      "Epoch: 1852500, elapsed: 1.06e+01, train loss: 5.34302e-07, val loss: 1.22830e-06, min loss: 5.34302e-07\n",
      "Epoch: 1852600, elapsed: 1.09e+01, train loss: 6.28005e-07, val loss: 1.30175e-06, min loss: 5.34302e-07\n",
      "Epoch: 1852700, elapsed: 1.08e+01, train loss: 5.34607e-07, val loss: 1.22360e-06, min loss: 5.34302e-07\n",
      "Epoch: 1852800, elapsed: 1.08e+01, train loss: 7.55742e-07, val loss: 1.30621e-06, min loss: 5.34302e-07\n",
      "Epoch: 1852900, elapsed: 1.09e+01, train loss: 5.34261e-07, val loss: 1.22650e-06, min loss: 5.34261e-07\n",
      "Epoch: 1853000, elapsed: 1.09e+01, train loss: 5.34214e-07, val loss: 1.23532e-06, min loss: 5.34214e-07\n",
      "Epoch: 1853100, elapsed: 1.07e+01, train loss: 1.15185e-06, val loss: 1.94205e-06, min loss: 5.34214e-07\n",
      "Epoch: 1853200, elapsed: 1.09e+01, train loss: 5.34086e-07, val loss: 1.22667e-06, min loss: 5.34086e-07\n",
      "Epoch: 1853300, elapsed: 1.07e+01, train loss: 5.40868e-07, val loss: 1.21657e-06, min loss: 5.34086e-07\n",
      "Epoch: 1853400, elapsed: 1.07e+01, train loss: 9.80589e-07, val loss: 1.67196e-06, min loss: 5.34086e-07\n",
      "Epoch: 1853500, elapsed: 1.09e+01, train loss: 7.55429e-07, val loss: 1.36903e-06, min loss: 5.34086e-07\n",
      "Epoch: 1853600, elapsed: 1.09e+01, train loss: 5.58336e-07, val loss: 1.25439e-06, min loss: 5.34086e-07\n",
      "Epoch: 1853700, elapsed: 1.07e+01, train loss: 5.36423e-07, val loss: 1.24119e-06, min loss: 5.34086e-07\n",
      "Epoch: 1853800, elapsed: 1.07e+01, train loss: 1.33900e-06, val loss: 1.55691e-06, min loss: 5.34086e-07\n",
      "Epoch: 1853900, elapsed: 1.05e+01, train loss: 5.33823e-07, val loss: 1.22898e-06, min loss: 5.33823e-07\n",
      "Epoch: 1854000, elapsed: 1.07e+01, train loss: 5.51706e-07, val loss: 1.26514e-06, min loss: 5.33823e-07\n",
      "Epoch: 1854100, elapsed: 1.09e+01, train loss: 9.72294e-07, val loss: 1.90308e-06, min loss: 5.33823e-07\n",
      "Epoch: 1854200, elapsed: 1.09e+01, train loss: 5.77524e-07, val loss: 1.26100e-06, min loss: 5.33823e-07\n",
      "Epoch: 1854300, elapsed: 1.09e+01, train loss: 5.55372e-07, val loss: 1.26024e-06, min loss: 5.33823e-07\n",
      "Epoch: 1854400, elapsed: 1.59e+01, train loss: 6.87970e-07, val loss: 1.36884e-06, min loss: 5.33823e-07\n",
      "Epoch: 1854500, elapsed: 1.12e+01, train loss: 9.28103e-07, val loss: 1.67554e-06, min loss: 5.33823e-07\n",
      "Epoch: 1854600, elapsed: 1.11e+01, train loss: 5.94789e-07, val loss: 1.24513e-06, min loss: 5.33823e-07\n",
      "Epoch: 1854700, elapsed: 1.08e+01, train loss: 5.36668e-07, val loss: 1.24065e-06, min loss: 5.33823e-07\n",
      "Epoch: 1854800, elapsed: 1.10e+01, train loss: 7.25298e-07, val loss: 1.28981e-06, min loss: 5.33823e-07\n",
      "Epoch: 1854900, elapsed: 1.09e+01, train loss: 5.52787e-07, val loss: 1.28263e-06, min loss: 5.33823e-07\n",
      "Epoch: 1855000, elapsed: 1.10e+01, train loss: 5.34345e-07, val loss: 1.22651e-06, min loss: 5.33823e-07\n",
      "Epoch: 1855100, elapsed: 1.28e+01, train loss: 5.36108e-07, val loss: 1.24593e-06, min loss: 5.33823e-07\n",
      "Epoch: 1855200, elapsed: 1.09e+01, train loss: 8.61994e-07, val loss: 1.72437e-06, min loss: 5.33823e-07\n",
      "Epoch: 1855300, elapsed: 1.12e+01, train loss: 5.33389e-07, val loss: 1.23051e-06, min loss: 5.33389e-07\n",
      "Epoch: 1855400, elapsed: 1.08e+01, train loss: 9.97704e-07, val loss: 1.60718e-06, min loss: 5.33389e-07\n",
      "Epoch: 1855500, elapsed: 1.11e+01, train loss: 5.79806e-07, val loss: 1.30938e-06, min loss: 5.33389e-07\n",
      "Epoch: 1855600, elapsed: 1.09e+01, train loss: 9.45086e-07, val loss: 1.67997e-06, min loss: 5.33389e-07\n",
      "Epoch: 1855700, elapsed: 1.10e+01, train loss: 6.88158e-07, val loss: 1.45033e-06, min loss: 5.33389e-07\n",
      "Epoch: 1855800, elapsed: 1.07e+01, train loss: 5.77776e-07, val loss: 1.25495e-06, min loss: 5.33389e-07\n",
      "Epoch: 1855900, elapsed: 1.10e+01, train loss: 1.26299e-06, val loss: 2.02427e-06, min loss: 5.33389e-07\n",
      "Epoch: 1856000, elapsed: 1.09e+01, train loss: 5.71639e-07, val loss: 1.26905e-06, min loss: 5.33389e-07\n",
      "Epoch: 1856100, elapsed: 1.09e+01, train loss: 5.52056e-07, val loss: 1.24181e-06, min loss: 5.33389e-07\n",
      "Epoch: 1856200, elapsed: 1.07e+01, train loss: 1.63956e-06, val loss: 2.90682e-06, min loss: 5.33389e-07\n",
      "Epoch: 1856300, elapsed: 1.09e+01, train loss: 5.50865e-07, val loss: 1.28570e-06, min loss: 5.33389e-07\n",
      "Epoch: 1856400, elapsed: 1.09e+01, train loss: 5.42803e-07, val loss: 1.23430e-06, min loss: 5.33389e-07\n",
      "Epoch: 1856500, elapsed: 1.09e+01, train loss: 5.43936e-07, val loss: 1.26185e-06, min loss: 5.33389e-07\n",
      "Epoch: 1856600, elapsed: 1.08e+01, train loss: 5.33542e-07, val loss: 1.23675e-06, min loss: 5.33389e-07\n",
      "Epoch: 1856700, elapsed: 1.08e+01, train loss: 1.50349e-06, val loss: 1.73886e-06, min loss: 5.33389e-07\n",
      "Epoch: 1856800, elapsed: 1.08e+01, train loss: 5.32923e-07, val loss: 1.23304e-06, min loss: 5.32923e-07\n",
      "Epoch: 1856900, elapsed: 1.09e+01, train loss: 5.33411e-07, val loss: 1.24085e-06, min loss: 5.32923e-07\n",
      "Epoch: 1857000, elapsed: 1.08e+01, train loss: 6.11304e-07, val loss: 1.22713e-06, min loss: 5.32923e-07\n",
      "Epoch: 1857100, elapsed: 1.08e+01, train loss: 5.40235e-07, val loss: 1.24099e-06, min loss: 5.32923e-07\n",
      "Epoch: 1857200, elapsed: 1.09e+01, train loss: 5.70576e-07, val loss: 1.22627e-06, min loss: 5.32923e-07\n",
      "Epoch: 1857300, elapsed: 1.08e+01, train loss: 5.33197e-07, val loss: 1.23346e-06, min loss: 5.32923e-07\n",
      "Epoch: 1857400, elapsed: 1.07e+01, train loss: 5.35904e-07, val loss: 1.23329e-06, min loss: 5.32923e-07\n",
      "Epoch: 1857500, elapsed: 1.08e+01, train loss: 1.01667e-06, val loss: 1.84541e-06, min loss: 5.32923e-07\n",
      "Epoch: 1857600, elapsed: 1.09e+01, train loss: 5.33244e-07, val loss: 1.23051e-06, min loss: 5.32923e-07\n",
      "Epoch: 1857700, elapsed: 1.61e+01, train loss: 5.41617e-07, val loss: 1.24756e-06, min loss: 5.32923e-07\n",
      "Epoch: 1857800, elapsed: 1.12e+01, train loss: 6.21508e-07, val loss: 1.26170e-06, min loss: 5.32923e-07\n",
      "Epoch: 1857900, elapsed: 1.12e+01, train loss: 8.69762e-07, val loss: 1.45386e-06, min loss: 5.32923e-07\n",
      "Epoch: 1858000, elapsed: 1.11e+01, train loss: 5.44156e-07, val loss: 1.21716e-06, min loss: 5.32923e-07\n",
      "Epoch: 1858100, elapsed: 1.08e+01, train loss: 5.72623e-07, val loss: 1.28100e-06, min loss: 5.32923e-07\n",
      "Epoch: 1858200, elapsed: 1.12e+01, train loss: 1.38560e-06, val loss: 2.63831e-06, min loss: 5.32923e-07\n",
      "Epoch: 1858300, elapsed: 1.11e+01, train loss: 6.49376e-07, val loss: 1.27929e-06, min loss: 5.32923e-07\n",
      "Epoch: 1858400, elapsed: 1.11e+01, train loss: 5.67715e-07, val loss: 1.27847e-06, min loss: 5.32923e-07\n",
      "Epoch: 1858500, elapsed: 1.09e+01, train loss: 5.48339e-07, val loss: 1.24648e-06, min loss: 5.32923e-07\n",
      "Epoch: 1858600, elapsed: 1.12e+01, train loss: 5.32694e-07, val loss: 1.23294e-06, min loss: 5.32694e-07\n",
      "Epoch: 1858700, elapsed: 1.08e+01, train loss: 7.94032e-07, val loss: 1.64306e-06, min loss: 5.32694e-07\n",
      "Epoch: 1858800, elapsed: 1.08e+01, train loss: 5.39107e-07, val loss: 1.22896e-06, min loss: 5.32694e-07\n",
      "Epoch: 1858900, elapsed: 1.09e+01, train loss: 1.47319e-06, val loss: 1.76376e-06, min loss: 5.32694e-07\n",
      "Epoch: 1859000, elapsed: 1.09e+01, train loss: 8.02962e-07, val loss: 1.57102e-06, min loss: 5.32694e-07\n",
      "Epoch: 1859100, elapsed: 1.09e+01, train loss: 5.41265e-07, val loss: 1.26303e-06, min loss: 5.32694e-07\n",
      "Epoch: 1859200, elapsed: 1.10e+01, train loss: 5.41191e-07, val loss: 1.26812e-06, min loss: 5.32694e-07\n",
      "Epoch: 1859300, elapsed: 1.09e+01, train loss: 5.35832e-07, val loss: 1.25621e-06, min loss: 5.32694e-07\n",
      "Epoch: 1859400, elapsed: 1.07e+01, train loss: 5.92789e-07, val loss: 1.36739e-06, min loss: 5.32694e-07\n",
      "Epoch: 1859500, elapsed: 1.12e+01, train loss: 5.56234e-07, val loss: 1.22927e-06, min loss: 5.32694e-07\n",
      "Epoch: 1859600, elapsed: 1.12e+01, train loss: 7.83802e-07, val loss: 1.50921e-06, min loss: 5.32694e-07\n",
      "Epoch: 1859700, elapsed: 1.07e+01, train loss: 2.06173e-06, val loss: 1.50087e-06, min loss: 5.32694e-07\n",
      "Epoch: 1859800, elapsed: 1.11e+01, train loss: 5.31874e-07, val loss: 1.23275e-06, min loss: 5.31874e-07\n",
      "Epoch: 1859900, elapsed: 1.10e+01, train loss: 8.05573e-07, val loss: 1.54389e-06, min loss: 5.31874e-07\n",
      "Epoch: 1860000, elapsed: 1.10e+01, train loss: 5.31723e-07, val loss: 1.23438e-06, min loss: 5.31723e-07\n",
      "Epoch: 1860100, elapsed: 1.30e+01, train loss: 5.51245e-07, val loss: 1.28353e-06, min loss: 5.31723e-07\n",
      "Epoch: 1860200, elapsed: 1.13e+01, train loss: 1.47539e-06, val loss: 2.28465e-06, min loss: 5.31723e-07\n",
      "Epoch: 1860300, elapsed: 1.15e+01, train loss: 6.02938e-07, val loss: 1.29729e-06, min loss: 5.31723e-07\n",
      "Epoch: 1860400, elapsed: 1.14e+01, train loss: 5.44744e-07, val loss: 1.27280e-06, min loss: 5.31723e-07\n",
      "Epoch: 1860500, elapsed: 1.14e+01, train loss: 7.16828e-07, val loss: 1.37576e-06, min loss: 5.31723e-07\n",
      "Epoch: 1860600, elapsed: 1.13e+01, train loss: 5.32802e-07, val loss: 1.22301e-06, min loss: 5.31723e-07\n",
      "Epoch: 1860700, elapsed: 1.15e+01, train loss: 8.91079e-07, val loss: 1.72990e-06, min loss: 5.31723e-07\n",
      "Epoch: 1860800, elapsed: 1.14e+01, train loss: 1.04594e-06, val loss: 2.16615e-06, min loss: 5.31723e-07\n",
      "Epoch: 1860900, elapsed: 1.13e+01, train loss: 5.31711e-07, val loss: 1.23768e-06, min loss: 5.31711e-07\n",
      "Epoch: 1861000, elapsed: 1.62e+01, train loss: 5.35647e-07, val loss: 1.25082e-06, min loss: 5.31711e-07\n",
      "Epoch: 1861100, elapsed: 1.12e+01, train loss: 5.47411e-07, val loss: 1.28774e-06, min loss: 5.31711e-07\n",
      "Epoch: 1861200, elapsed: 1.13e+01, train loss: 1.10482e-06, val loss: 1.75397e-06, min loss: 5.31711e-07\n",
      "Epoch: 1861300, elapsed: 1.11e+01, train loss: 7.29626e-07, val loss: 1.40492e-06, min loss: 5.31711e-07\n",
      "Epoch: 1861400, elapsed: 1.12e+01, train loss: 6.07245e-07, val loss: 1.28842e-06, min loss: 5.31711e-07\n",
      "Epoch: 1861500, elapsed: 1.11e+01, train loss: 5.31693e-07, val loss: 1.23697e-06, min loss: 5.31693e-07\n",
      "Epoch: 1861600, elapsed: 1.10e+01, train loss: 9.53133e-07, val loss: 1.49264e-06, min loss: 5.31693e-07\n",
      "Epoch: 1861700, elapsed: 1.11e+01, train loss: 5.32831e-07, val loss: 1.24345e-06, min loss: 5.31693e-07\n",
      "Epoch: 1861800, elapsed: 1.10e+01, train loss: 5.46965e-07, val loss: 1.25206e-06, min loss: 5.31693e-07\n",
      "Epoch: 1861900, elapsed: 1.13e+01, train loss: 6.23869e-07, val loss: 1.29664e-06, min loss: 5.31693e-07\n",
      "Epoch: 1862000, elapsed: 1.10e+01, train loss: 5.49273e-07, val loss: 1.28758e-06, min loss: 5.31693e-07\n",
      "Epoch: 1862100, elapsed: 1.12e+01, train loss: 5.31975e-07, val loss: 1.23395e-06, min loss: 5.31693e-07\n",
      "Epoch: 1862200, elapsed: 1.10e+01, train loss: 5.53209e-07, val loss: 1.23794e-06, min loss: 5.31693e-07\n",
      "Epoch: 1862300, elapsed: 1.08e+01, train loss: 6.14041e-07, val loss: 1.28888e-06, min loss: 5.31693e-07\n",
      "Epoch: 1862400, elapsed: 1.08e+01, train loss: 6.71188e-07, val loss: 1.23619e-06, min loss: 5.31693e-07\n",
      "Epoch: 1862500, elapsed: 1.07e+01, train loss: 6.06523e-06, val loss: 4.81671e-06, min loss: 5.31693e-07\n",
      "Epoch: 1862600, elapsed: 1.09e+01, train loss: 5.30925e-07, val loss: 1.23513e-06, min loss: 5.30925e-07\n",
      "Epoch: 1862700, elapsed: 1.08e+01, train loss: 1.60024e-06, val loss: 2.25313e-06, min loss: 5.30925e-07\n",
      "Epoch: 1862800, elapsed: 1.10e+01, train loss: 5.70027e-07, val loss: 1.32663e-06, min loss: 5.30925e-07\n",
      "Epoch: 1862900, elapsed: 1.08e+01, train loss: 5.30910e-07, val loss: 1.23683e-06, min loss: 5.30910e-07\n",
      "Epoch: 1863000, elapsed: 1.10e+01, train loss: 2.29023e-06, val loss: 1.64449e-06, min loss: 5.30910e-07\n",
      "Epoch: 1863100, elapsed: 1.12e+01, train loss: 5.30778e-07, val loss: 1.23362e-06, min loss: 5.30778e-07\n",
      "Epoch: 1863200, elapsed: 1.09e+01, train loss: 5.32060e-07, val loss: 1.23097e-06, min loss: 5.30778e-07\n",
      "Epoch: 1863300, elapsed: 1.08e+01, train loss: 5.52342e-07, val loss: 1.29154e-06, min loss: 5.30778e-07\n",
      "Epoch: 1863400, elapsed: 1.11e+01, train loss: 6.45525e-07, val loss: 1.36679e-06, min loss: 5.30778e-07\n",
      "Epoch: 1863500, elapsed: 1.08e+01, train loss: 3.01427e-06, val loss: 3.05739e-06, min loss: 5.30778e-07\n",
      "Epoch: 1863600, elapsed: 1.09e+01, train loss: 5.30633e-07, val loss: 1.23439e-06, min loss: 5.30633e-07\n",
      "Epoch: 1863700, elapsed: 1.07e+01, train loss: 5.31095e-07, val loss: 1.24584e-06, min loss: 5.30633e-07\n",
      "Epoch: 1863800, elapsed: 1.09e+01, train loss: 1.93842e-06, val loss: 2.18739e-06, min loss: 5.30633e-07\n",
      "Epoch: 1863900, elapsed: 1.09e+01, train loss: 5.59265e-07, val loss: 1.29212e-06, min loss: 5.30633e-07\n",
      "Epoch: 1864000, elapsed: 1.09e+01, train loss: 7.83211e-07, val loss: 1.40483e-06, min loss: 5.30633e-07\n",
      "Epoch: 1864100, elapsed: 1.09e+01, train loss: 5.81346e-07, val loss: 1.31069e-06, min loss: 5.30633e-07\n",
      "Epoch: 1864200, elapsed: 1.12e+01, train loss: 5.39535e-07, val loss: 1.21868e-06, min loss: 5.30633e-07\n",
      "Epoch: 1864300, elapsed: 1.13e+01, train loss: 5.80766e-07, val loss: 1.26437e-06, min loss: 5.30633e-07\n",
      "Epoch: 1864400, elapsed: 1.67e+01, train loss: 5.48998e-07, val loss: 1.28694e-06, min loss: 5.30633e-07\n",
      "Epoch: 1864500, elapsed: 1.14e+01, train loss: 5.63100e-07, val loss: 1.32402e-06, min loss: 5.30633e-07\n",
      "Epoch: 1864600, elapsed: 1.15e+01, train loss: 5.33848e-07, val loss: 1.24715e-06, min loss: 5.30633e-07\n",
      "Epoch: 1864700, elapsed: 1.15e+01, train loss: 5.31493e-07, val loss: 1.22652e-06, min loss: 5.30633e-07\n",
      "Epoch: 1864800, elapsed: 1.15e+01, train loss: 5.90237e-07, val loss: 1.29101e-06, min loss: 5.30633e-07\n",
      "Epoch: 1864900, elapsed: 1.11e+01, train loss: 4.01696e-06, val loss: 5.17377e-06, min loss: 5.30633e-07\n",
      "Epoch: 1865000, elapsed: 1.17e+01, train loss: 5.37847e-07, val loss: 1.21855e-06, min loss: 5.30633e-07\n",
      "Epoch: 1865100, elapsed: 1.38e+01, train loss: 7.32328e-07, val loss: 1.52705e-06, min loss: 5.30633e-07\n",
      "Epoch: 1865200, elapsed: 1.13e+01, train loss: 6.47713e-07, val loss: 1.40691e-06, min loss: 5.30633e-07\n",
      "Epoch: 1865300, elapsed: 1.15e+01, train loss: 1.22804e-06, val loss: 2.28203e-06, min loss: 5.30633e-07\n",
      "Epoch: 1865400, elapsed: 1.15e+01, train loss: 5.30130e-07, val loss: 1.23491e-06, min loss: 5.30130e-07\n",
      "Epoch: 1865500, elapsed: 1.15e+01, train loss: 5.66062e-07, val loss: 1.26694e-06, min loss: 5.30130e-07\n",
      "Epoch: 1865600, elapsed: 1.14e+01, train loss: 5.29819e-07, val loss: 1.24115e-06, min loss: 5.29819e-07\n",
      "Epoch: 1865700, elapsed: 1.12e+01, train loss: 5.30722e-07, val loss: 1.23618e-06, min loss: 5.29819e-07\n",
      "Epoch: 1865800, elapsed: 1.13e+01, train loss: 7.70506e-07, val loss: 1.48280e-06, min loss: 5.29819e-07\n",
      "Epoch: 1865900, elapsed: 1.15e+01, train loss: 5.29942e-07, val loss: 1.24193e-06, min loss: 5.29819e-07\n",
      "Epoch: 1866000, elapsed: 1.13e+01, train loss: 5.32611e-07, val loss: 1.24571e-06, min loss: 5.29819e-07\n",
      "Epoch: 1866100, elapsed: 1.13e+01, train loss: 3.27380e-06, val loss: 4.44936e-06, min loss: 5.29819e-07\n",
      "Epoch: 1866200, elapsed: 1.14e+01, train loss: 5.29720e-07, val loss: 1.23785e-06, min loss: 5.29720e-07\n",
      "Epoch: 1866300, elapsed: 1.14e+01, train loss: 5.39440e-07, val loss: 1.24038e-06, min loss: 5.29720e-07\n",
      "Epoch: 1866400, elapsed: 1.14e+01, train loss: 5.29657e-07, val loss: 1.24109e-06, min loss: 5.29657e-07\n",
      "Epoch: 1866500, elapsed: 1.11e+01, train loss: 5.30524e-07, val loss: 1.23874e-06, min loss: 5.29657e-07\n",
      "Epoch: 1866600, elapsed: 1.13e+01, train loss: 5.29490e-07, val loss: 1.23884e-06, min loss: 5.29490e-07\n",
      "Epoch: 1866700, elapsed: 1.12e+01, train loss: 5.32093e-07, val loss: 1.24060e-06, min loss: 5.29490e-07\n",
      "Epoch: 1866800, elapsed: 1.12e+01, train loss: 5.29388e-07, val loss: 1.24134e-06, min loss: 5.29388e-07\n",
      "Epoch: 1866900, elapsed: 1.12e+01, train loss: 5.34446e-07, val loss: 1.25946e-06, min loss: 5.29388e-07\n",
      "Epoch: 1867000, elapsed: 1.13e+01, train loss: 6.19236e-07, val loss: 1.41869e-06, min loss: 5.29388e-07\n",
      "Epoch: 1867100, elapsed: 1.11e+01, train loss: 5.29520e-07, val loss: 1.23517e-06, min loss: 5.29388e-07\n",
      "Epoch: 1867200, elapsed: 1.12e+01, train loss: 1.14226e-06, val loss: 1.72430e-06, min loss: 5.29388e-07\n",
      "Epoch: 1867300, elapsed: 1.12e+01, train loss: 5.29333e-07, val loss: 1.24187e-06, min loss: 5.29333e-07\n",
      "Epoch: 1867400, elapsed: 1.15e+01, train loss: 6.76367e-07, val loss: 1.27440e-06, min loss: 5.29333e-07\n",
      "Epoch: 1867500, elapsed: 1.12e+01, train loss: 5.29173e-07, val loss: 1.24139e-06, min loss: 5.29173e-07\n",
      "Epoch: 1867600, elapsed: 1.12e+01, train loss: 5.39977e-07, val loss: 1.24495e-06, min loss: 5.29173e-07\n",
      "Epoch: 1867700, elapsed: 1.67e+01, train loss: 5.29372e-07, val loss: 1.23460e-06, min loss: 5.29173e-07\n",
      "Epoch: 1867800, elapsed: 1.16e+01, train loss: 5.36033e-07, val loss: 1.22604e-06, min loss: 5.29173e-07\n",
      "Epoch: 1867900, elapsed: 1.16e+01, train loss: 9.33099e-07, val loss: 1.67660e-06, min loss: 5.29173e-07\n",
      "Epoch: 1868000, elapsed: 1.15e+01, train loss: 6.78118e-07, val loss: 1.28319e-06, min loss: 5.29173e-07\n",
      "Epoch: 1868100, elapsed: 1.17e+01, train loss: 8.01760e-07, val loss: 1.47697e-06, min loss: 5.29173e-07\n",
      "Epoch: 1868200, elapsed: 1.12e+01, train loss: 5.76426e-07, val loss: 1.24159e-06, min loss: 5.29173e-07\n",
      "Epoch: 1868300, elapsed: 1.12e+01, train loss: 5.62476e-07, val loss: 1.30253e-06, min loss: 5.29173e-07\n",
      "Epoch: 1868400, elapsed: 1.09e+01, train loss: 6.52026e-07, val loss: 1.34119e-06, min loss: 5.29173e-07\n",
      "Epoch: 1868500, elapsed: 1.11e+01, train loss: 3.13313e-06, val loss: 3.58620e-06, min loss: 5.29173e-07\n",
      "Epoch: 1868600, elapsed: 1.12e+01, train loss: 6.18791e-07, val loss: 1.25787e-06, min loss: 5.29173e-07\n",
      "Epoch: 1868700, elapsed: 1.10e+01, train loss: 5.33334e-07, val loss: 1.25262e-06, min loss: 5.29173e-07\n",
      "Epoch: 1868800, elapsed: 1.09e+01, train loss: 5.30344e-07, val loss: 1.24150e-06, min loss: 5.29173e-07\n",
      "Epoch: 1868900, elapsed: 1.10e+01, train loss: 5.40211e-07, val loss: 1.25938e-06, min loss: 5.29173e-07\n",
      "Epoch: 1869000, elapsed: 1.10e+01, train loss: 6.58512e-07, val loss: 1.31868e-06, min loss: 5.29173e-07\n",
      "Epoch: 1869100, elapsed: 1.10e+01, train loss: 3.08101e-06, val loss: 3.24101e-06, min loss: 5.29173e-07\n",
      "Epoch: 1869200, elapsed: 1.11e+01, train loss: 7.30859e-07, val loss: 1.58291e-06, min loss: 5.29173e-07\n",
      "Epoch: 1869300, elapsed: 1.10e+01, train loss: 3.14570e-06, val loss: 3.38096e-06, min loss: 5.29173e-07\n",
      "Epoch: 1869400, elapsed: 1.10e+01, train loss: 5.34156e-07, val loss: 1.26924e-06, min loss: 5.29173e-07\n",
      "Epoch: 1869500, elapsed: 1.09e+01, train loss: 5.48474e-07, val loss: 1.22722e-06, min loss: 5.29173e-07\n",
      "Epoch: 1869600, elapsed: 1.12e+01, train loss: 5.94458e-07, val loss: 1.25633e-06, min loss: 5.29173e-07\n",
      "Epoch: 1869700, elapsed: 1.10e+01, train loss: 6.74679e-07, val loss: 1.43316e-06, min loss: 5.29173e-07\n",
      "Epoch: 1869800, elapsed: 1.11e+01, train loss: 5.54911e-07, val loss: 1.28141e-06, min loss: 5.29173e-07\n",
      "Epoch: 1869900, elapsed: 1.08e+01, train loss: 5.29688e-07, val loss: 1.24015e-06, min loss: 5.29173e-07\n",
      "Epoch: 1870000, elapsed: 1.08e+01, train loss: 5.29872e-07, val loss: 1.24816e-06, min loss: 5.29173e-07\n",
      "Epoch: 1870100, elapsed: 1.27e+01, train loss: 5.43460e-07, val loss: 1.25845e-06, min loss: 5.29173e-07\n",
      "Epoch: 1870200, elapsed: 1.08e+01, train loss: 5.38104e-07, val loss: 1.34240e-06, min loss: 5.29173e-07\n",
      "Epoch: 1870300, elapsed: 1.10e+01, train loss: 5.28324e-07, val loss: 1.24331e-06, min loss: 5.28324e-07\n",
      "Epoch: 1870400, elapsed: 1.09e+01, train loss: 5.37702e-07, val loss: 1.27136e-06, min loss: 5.28324e-07\n",
      "Epoch: 1870500, elapsed: 1.09e+01, train loss: 1.52692e-06, val loss: 2.03740e-06, min loss: 5.28324e-07\n",
      "Epoch: 1870600, elapsed: 1.10e+01, train loss: 5.28530e-07, val loss: 1.24142e-06, min loss: 5.28324e-07\n",
      "Epoch: 1870700, elapsed: 1.08e+01, train loss: 5.29459e-07, val loss: 1.22468e-06, min loss: 5.28324e-07\n",
      "Epoch: 1870800, elapsed: 1.09e+01, train loss: 2.93281e-06, val loss: 3.24943e-06, min loss: 5.28324e-07\n",
      "Epoch: 1870900, elapsed: 1.08e+01, train loss: 5.28138e-07, val loss: 1.24266e-06, min loss: 5.28138e-07\n",
      "Epoch: 1871000, elapsed: 1.62e+01, train loss: 9.77415e-07, val loss: 1.82822e-06, min loss: 5.28138e-07\n",
      "Epoch: 1871100, elapsed: 1.15e+01, train loss: 5.28142e-07, val loss: 1.24239e-06, min loss: 5.28138e-07\n",
      "Epoch: 1871200, elapsed: 1.12e+01, train loss: 5.30422e-07, val loss: 1.24676e-06, min loss: 5.28138e-07\n",
      "Epoch: 1871300, elapsed: 1.11e+01, train loss: 5.31631e-07, val loss: 1.22305e-06, min loss: 5.28138e-07\n",
      "Epoch: 1871400, elapsed: 1.12e+01, train loss: 5.27996e-07, val loss: 1.24008e-06, min loss: 5.27996e-07\n",
      "Epoch: 1871500, elapsed: 1.09e+01, train loss: 5.32073e-07, val loss: 1.25966e-06, min loss: 5.27996e-07\n",
      "Epoch: 1871600, elapsed: 1.12e+01, train loss: 5.38716e-07, val loss: 1.27068e-06, min loss: 5.27996e-07\n",
      "Epoch: 1871700, elapsed: 1.10e+01, train loss: 7.27676e-07, val loss: 1.45300e-06, min loss: 5.27996e-07\n",
      "Epoch: 1871800, elapsed: 1.10e+01, train loss: 5.45700e-07, val loss: 1.28702e-06, min loss: 5.27996e-07\n",
      "Epoch: 1871900, elapsed: 1.10e+01, train loss: 5.28604e-07, val loss: 1.23466e-06, min loss: 5.27996e-07\n",
      "Epoch: 1872000, elapsed: 1.10e+01, train loss: 5.28988e-07, val loss: 1.26077e-06, min loss: 5.27996e-07\n",
      "Epoch: 1872100, elapsed: 1.09e+01, train loss: 5.77955e-07, val loss: 1.26718e-06, min loss: 5.27996e-07\n",
      "Epoch: 1872200, elapsed: 1.10e+01, train loss: 4.26384e-06, val loss: 3.80076e-06, min loss: 5.27996e-07\n",
      "Epoch: 1872300, elapsed: 1.12e+01, train loss: 5.29872e-07, val loss: 1.24565e-06, min loss: 5.27996e-07\n",
      "Epoch: 1872400, elapsed: 1.07e+01, train loss: 5.48377e-07, val loss: 1.26304e-06, min loss: 5.27996e-07\n",
      "Epoch: 1872500, elapsed: 1.07e+01, train loss: 5.60566e-07, val loss: 1.33158e-06, min loss: 5.27996e-07\n",
      "Epoch: 1872600, elapsed: 1.09e+01, train loss: 5.72778e-07, val loss: 1.29801e-06, min loss: 5.27996e-07\n",
      "Epoch: 1872700, elapsed: 1.09e+01, train loss: 5.46103e-07, val loss: 1.32182e-06, min loss: 5.27996e-07\n",
      "Epoch: 1872800, elapsed: 1.08e+01, train loss: 5.64125e-07, val loss: 1.33583e-06, min loss: 5.27996e-07\n",
      "Epoch: 1872900, elapsed: 1.07e+01, train loss: 6.31333e-07, val loss: 1.40367e-06, min loss: 5.27996e-07\n",
      "Epoch: 1873000, elapsed: 1.07e+01, train loss: 1.06665e-06, val loss: 1.97244e-06, min loss: 5.27996e-07\n",
      "Epoch: 1873100, elapsed: 1.10e+01, train loss: 5.30833e-07, val loss: 1.26197e-06, min loss: 5.27996e-07\n",
      "Epoch: 1873200, elapsed: 1.11e+01, train loss: 9.47577e-07, val loss: 1.76026e-06, min loss: 5.27996e-07\n",
      "Epoch: 1873300, elapsed: 1.06e+01, train loss: 5.68054e-07, val loss: 1.31739e-06, min loss: 5.27996e-07\n",
      "Epoch: 1873400, elapsed: 1.06e+01, train loss: 6.10529e-07, val loss: 1.27132e-06, min loss: 5.27996e-07\n",
      "Epoch: 1873500, elapsed: 1.06e+01, train loss: 5.30968e-07, val loss: 1.26172e-06, min loss: 5.27996e-07\n",
      "Epoch: 1873600, elapsed: 1.09e+01, train loss: 1.72434e-06, val loss: 2.38974e-06, min loss: 5.27996e-07\n",
      "Epoch: 1873700, elapsed: 1.07e+01, train loss: 5.27249e-07, val loss: 1.24523e-06, min loss: 5.27249e-07\n",
      "Epoch: 1873800, elapsed: 1.08e+01, train loss: 5.31557e-07, val loss: 1.25561e-06, min loss: 5.27249e-07\n",
      "Epoch: 1873900, elapsed: 1.06e+01, train loss: 5.35259e-07, val loss: 1.26500e-06, min loss: 5.27249e-07\n",
      "Epoch: 1874000, elapsed: 1.08e+01, train loss: 1.02956e-06, val loss: 1.67154e-06, min loss: 5.27249e-07\n",
      "Epoch: 1874100, elapsed: 1.09e+01, train loss: 1.22077e-06, val loss: 1.93537e-06, min loss: 5.27249e-07\n",
      "Epoch: 1874200, elapsed: 1.07e+01, train loss: 5.27350e-07, val loss: 1.24687e-06, min loss: 5.27249e-07\n",
      "Epoch: 1874300, elapsed: 1.07e+01, train loss: 5.51439e-07, val loss: 1.29465e-06, min loss: 5.27249e-07\n",
      "Epoch: 1874400, elapsed: 1.64e+01, train loss: 8.16155e-07, val loss: 1.44571e-06, min loss: 5.27249e-07\n",
      "Epoch: 1874500, elapsed: 1.11e+01, train loss: 5.29811e-07, val loss: 1.22728e-06, min loss: 5.27249e-07\n",
      "Epoch: 1874600, elapsed: 1.11e+01, train loss: 6.46213e-07, val loss: 1.38432e-06, min loss: 5.27249e-07\n",
      "Epoch: 1874700, elapsed: 1.11e+01, train loss: 5.28205e-07, val loss: 1.25367e-06, min loss: 5.27249e-07\n",
      "Epoch: 1874800, elapsed: 1.10e+01, train loss: 5.28432e-07, val loss: 1.25276e-06, min loss: 5.27249e-07\n",
      "Epoch: 1874900, elapsed: 1.12e+01, train loss: 5.38728e-07, val loss: 1.26533e-06, min loss: 5.27249e-07\n",
      "Epoch: 1875000, elapsed: 1.10e+01, train loss: 5.28466e-07, val loss: 1.23376e-06, min loss: 5.27249e-07\n",
      "Epoch: 1875100, elapsed: 1.28e+01, train loss: 7.89938e-07, val loss: 1.34895e-06, min loss: 5.27249e-07\n",
      "Epoch: 1875200, elapsed: 1.09e+01, train loss: 5.40640e-07, val loss: 1.28802e-06, min loss: 5.27249e-07\n",
      "Epoch: 1875300, elapsed: 1.08e+01, train loss: 5.46136e-07, val loss: 1.20171e-06, min loss: 5.27249e-07\n",
      "Epoch: 1875400, elapsed: 1.10e+01, train loss: 5.40553e-07, val loss: 1.25038e-06, min loss: 5.27249e-07\n",
      "Epoch: 1875500, elapsed: 1.11e+01, train loss: 5.59130e-07, val loss: 1.31556e-06, min loss: 5.27249e-07\n",
      "Epoch: 1875600, elapsed: 1.11e+01, train loss: 7.54785e-07, val loss: 1.46970e-06, min loss: 5.27249e-07\n",
      "Epoch: 1875700, elapsed: 1.08e+01, train loss: 5.56927e-07, val loss: 1.28213e-06, min loss: 5.27249e-07\n",
      "Epoch: 1875800, elapsed: 1.08e+01, train loss: 6.41990e-07, val loss: 1.31386e-06, min loss: 5.27249e-07\n",
      "Epoch: 1875900, elapsed: 1.08e+01, train loss: 5.93769e-07, val loss: 1.25949e-06, min loss: 5.27249e-07\n",
      "Epoch: 1876000, elapsed: 1.08e+01, train loss: 5.94936e-07, val loss: 1.37449e-06, min loss: 5.27249e-07\n",
      "Epoch: 1876100, elapsed: 1.08e+01, train loss: 9.62974e-07, val loss: 1.75709e-06, min loss: 5.27249e-07\n",
      "Epoch: 1876200, elapsed: 1.10e+01, train loss: 5.27164e-07, val loss: 1.25139e-06, min loss: 5.27164e-07\n",
      "Epoch: 1876300, elapsed: 1.06e+01, train loss: 5.33718e-07, val loss: 1.24005e-06, min loss: 5.27164e-07\n",
      "Epoch: 1876400, elapsed: 1.06e+01, train loss: 8.11664e-07, val loss: 1.66024e-06, min loss: 5.27164e-07\n",
      "Epoch: 1876500, elapsed: 1.08e+01, train loss: 5.31766e-07, val loss: 1.26702e-06, min loss: 5.27164e-07\n",
      "Epoch: 1876600, elapsed: 1.08e+01, train loss: 5.42869e-07, val loss: 1.23509e-06, min loss: 5.27164e-07\n",
      "Epoch: 1876700, elapsed: 1.07e+01, train loss: 1.62493e-06, val loss: 1.48029e-06, min loss: 5.27164e-07\n",
      "Epoch: 1876800, elapsed: 1.06e+01, train loss: 5.26116e-07, val loss: 1.24825e-06, min loss: 5.26116e-07\n",
      "Epoch: 1876900, elapsed: 1.09e+01, train loss: 5.46958e-07, val loss: 1.26284e-06, min loss: 5.26116e-07\n",
      "Epoch: 1877000, elapsed: 1.08e+01, train loss: 5.29024e-07, val loss: 1.27048e-06, min loss: 5.26116e-07\n",
      "Epoch: 1877100, elapsed: 1.07e+01, train loss: 5.26569e-07, val loss: 1.25851e-06, min loss: 5.26116e-07\n",
      "Epoch: 1877200, elapsed: 1.07e+01, train loss: 6.50389e-07, val loss: 1.33723e-06, min loss: 5.26116e-07\n",
      "Epoch: 1877300, elapsed: 1.09e+01, train loss: 5.76873e-07, val loss: 1.24491e-06, min loss: 5.26116e-07\n",
      "Epoch: 1877400, elapsed: 1.08e+01, train loss: 5.26594e-07, val loss: 1.24887e-06, min loss: 5.26116e-07\n",
      "Epoch: 1877500, elapsed: 1.08e+01, train loss: 5.49401e-07, val loss: 1.22740e-06, min loss: 5.26116e-07\n",
      "Epoch: 1877600, elapsed: 1.08e+01, train loss: 1.28614e-06, val loss: 2.31234e-06, min loss: 5.26116e-07\n",
      "Epoch: 1877700, elapsed: 1.59e+01, train loss: 1.21550e-06, val loss: 1.80233e-06, min loss: 5.26116e-07\n",
      "Epoch: 1877800, elapsed: 1.10e+01, train loss: 8.55972e-07, val loss: 1.92055e-06, min loss: 5.26116e-07\n",
      "Epoch: 1877900, elapsed: 1.09e+01, train loss: 2.19626e-06, val loss: 2.00597e-06, min loss: 5.26116e-07\n",
      "Epoch: 1878000, elapsed: 1.08e+01, train loss: 7.44215e-07, val loss: 1.42750e-06, min loss: 5.26116e-07\n",
      "Epoch: 1878100, elapsed: 1.12e+01, train loss: 5.76642e-07, val loss: 1.39447e-06, min loss: 5.26116e-07\n",
      "Epoch: 1878200, elapsed: 1.10e+01, train loss: 5.29070e-07, val loss: 1.26835e-06, min loss: 5.26116e-07\n",
      "Epoch: 1878300, elapsed: 1.10e+01, train loss: 5.64601e-07, val loss: 1.35037e-06, min loss: 5.26116e-07\n",
      "Epoch: 1878400, elapsed: 1.09e+01, train loss: 6.39288e-07, val loss: 1.29149e-06, min loss: 5.26116e-07\n",
      "Epoch: 1878500, elapsed: 1.10e+01, train loss: 9.74742e-07, val loss: 1.50052e-06, min loss: 5.26116e-07\n",
      "Epoch: 1878600, elapsed: 1.08e+01, train loss: 7.29781e-07, val loss: 1.35330e-06, min loss: 5.26116e-07\n",
      "Epoch: 1878700, elapsed: 1.10e+01, train loss: 6.60378e-07, val loss: 1.31633e-06, min loss: 5.26116e-07\n",
      "Epoch: 1878800, elapsed: 1.12e+01, train loss: 6.27263e-07, val loss: 1.26055e-06, min loss: 5.26116e-07\n",
      "Epoch: 1878900, elapsed: 1.10e+01, train loss: 7.73006e-07, val loss: 1.50567e-06, min loss: 5.26116e-07\n",
      "Epoch: 1879000, elapsed: 1.09e+01, train loss: 3.06638e-06, val loss: 2.98464e-06, min loss: 5.26116e-07\n",
      "Epoch: 1879100, elapsed: 1.07e+01, train loss: 5.28016e-07, val loss: 1.23230e-06, min loss: 5.26116e-07\n",
      "Epoch: 1879200, elapsed: 1.08e+01, train loss: 5.26328e-07, val loss: 1.24886e-06, min loss: 5.26116e-07\n",
      "Epoch: 1879300, elapsed: 1.09e+01, train loss: 5.51816e-07, val loss: 1.29351e-06, min loss: 5.26116e-07\n",
      "Epoch: 1879400, elapsed: 1.08e+01, train loss: 3.68063e-06, val loss: 3.91531e-06, min loss: 5.26116e-07\n",
      "Epoch: 1879500, elapsed: 1.08e+01, train loss: 5.25668e-07, val loss: 1.25970e-06, min loss: 5.25668e-07\n",
      "Epoch: 1879600, elapsed: 1.08e+01, train loss: 5.30317e-07, val loss: 1.24535e-06, min loss: 5.25668e-07\n",
      "Epoch: 1879700, elapsed: 1.10e+01, train loss: 5.30567e-07, val loss: 1.25563e-06, min loss: 5.25668e-07\n",
      "Epoch: 1879800, elapsed: 1.09e+01, train loss: 5.26585e-07, val loss: 1.25973e-06, min loss: 5.25668e-07\n",
      "Epoch: 1879900, elapsed: 1.09e+01, train loss: 5.30077e-07, val loss: 1.24002e-06, min loss: 5.25668e-07\n",
      "Epoch: 1880000, elapsed: 1.07e+01, train loss: 5.34165e-07, val loss: 1.24560e-06, min loss: 5.25668e-07\n",
      "Epoch: 1880100, elapsed: 1.29e+01, train loss: 5.32302e-07, val loss: 1.27012e-06, min loss: 5.25668e-07\n",
      "Epoch: 1880200, elapsed: 1.09e+01, train loss: 5.27052e-07, val loss: 1.25094e-06, min loss: 5.25668e-07\n",
      "Epoch: 1880300, elapsed: 1.06e+01, train loss: 5.26269e-07, val loss: 1.25329e-06, min loss: 5.25668e-07\n",
      "Epoch: 1880400, elapsed: 1.08e+01, train loss: 7.89793e-07, val loss: 1.48057e-06, min loss: 5.25668e-07\n",
      "Epoch: 1880500, elapsed: 1.08e+01, train loss: 5.25490e-07, val loss: 1.26118e-06, min loss: 5.25490e-07\n",
      "Epoch: 1880600, elapsed: 1.09e+01, train loss: 5.26849e-07, val loss: 1.26623e-06, min loss: 5.25490e-07\n",
      "Epoch: 1880700, elapsed: 1.10e+01, train loss: 5.40661e-07, val loss: 1.28580e-06, min loss: 5.25490e-07\n",
      "Epoch: 1880800, elapsed: 1.06e+01, train loss: 1.80501e-06, val loss: 3.04604e-06, min loss: 5.25490e-07\n",
      "Epoch: 1880900, elapsed: 1.08e+01, train loss: 5.24757e-07, val loss: 1.25281e-06, min loss: 5.24757e-07\n",
      "Epoch: 1881000, elapsed: 1.08e+01, train loss: 5.62718e-07, val loss: 1.29101e-06, min loss: 5.24757e-07\n",
      "Epoch: 1881100, elapsed: 1.66e+01, train loss: 8.08996e-07, val loss: 1.54066e-06, min loss: 5.24757e-07\n",
      "Epoch: 1881200, elapsed: 1.14e+01, train loss: 5.28091e-07, val loss: 1.26977e-06, min loss: 5.24757e-07\n",
      "Epoch: 1881300, elapsed: 1.12e+01, train loss: 5.24898e-07, val loss: 1.25624e-06, min loss: 5.24757e-07\n",
      "Epoch: 1881400, elapsed: 1.12e+01, train loss: 5.92645e-07, val loss: 1.29644e-06, min loss: 5.24757e-07\n",
      "Epoch: 1881500, elapsed: 1.09e+01, train loss: 5.72333e-07, val loss: 1.22640e-06, min loss: 5.24757e-07\n",
      "Epoch: 1881600, elapsed: 1.10e+01, train loss: 5.24658e-07, val loss: 1.25381e-06, min loss: 5.24658e-07\n",
      "Epoch: 1881700, elapsed: 1.09e+01, train loss: 5.31024e-07, val loss: 1.25757e-06, min loss: 5.24658e-07\n",
      "Epoch: 1881800, elapsed: 1.14e+01, train loss: 5.86828e-07, val loss: 1.27402e-06, min loss: 5.24658e-07\n",
      "Epoch: 1881900, elapsed: 1.09e+01, train loss: 2.25286e-06, val loss: 3.26897e-06, min loss: 5.24658e-07\n",
      "Epoch: 1882000, elapsed: 1.10e+01, train loss: 5.24528e-07, val loss: 1.25543e-06, min loss: 5.24528e-07\n",
      "Epoch: 1882100, elapsed: 1.09e+01, train loss: 5.80104e-07, val loss: 1.30269e-06, min loss: 5.24528e-07\n",
      "Epoch: 1882200, elapsed: 1.08e+01, train loss: 5.24764e-07, val loss: 1.26458e-06, min loss: 5.24528e-07\n",
      "Epoch: 1882300, elapsed: 1.09e+01, train loss: 5.24999e-07, val loss: 1.25838e-06, min loss: 5.24528e-07\n",
      "Epoch: 1882400, elapsed: 1.10e+01, train loss: 7.21348e-07, val loss: 1.82441e-06, min loss: 5.24528e-07\n",
      "Epoch: 1882500, elapsed: 1.08e+01, train loss: 5.24258e-07, val loss: 1.25463e-06, min loss: 5.24258e-07\n",
      "Epoch: 1882600, elapsed: 1.09e+01, train loss: 5.43634e-07, val loss: 1.31312e-06, min loss: 5.24258e-07\n",
      "Epoch: 1882700, elapsed: 1.08e+01, train loss: 5.63717e-07, val loss: 1.31553e-06, min loss: 5.24258e-07\n",
      "Epoch: 1882800, elapsed: 1.09e+01, train loss: 5.25782e-07, val loss: 1.27020e-06, min loss: 5.24258e-07\n",
      "Epoch: 1882900, elapsed: 1.07e+01, train loss: 5.34029e-07, val loss: 1.30538e-06, min loss: 5.24258e-07\n",
      "Epoch: 1883000, elapsed: 1.08e+01, train loss: 5.30792e-07, val loss: 1.26754e-06, min loss: 5.24258e-07\n",
      "Epoch: 1883100, elapsed: 1.09e+01, train loss: 1.99133e-06, val loss: 3.42272e-06, min loss: 5.24258e-07\n",
      "Epoch: 1883200, elapsed: 1.07e+01, train loss: 5.24070e-07, val loss: 1.25919e-06, min loss: 5.24070e-07\n",
      "Epoch: 1883300, elapsed: 1.10e+01, train loss: 5.45217e-07, val loss: 1.22345e-06, min loss: 5.24070e-07\n",
      "Epoch: 1883400, elapsed: 1.08e+01, train loss: 5.32149e-07, val loss: 1.24841e-06, min loss: 5.24070e-07\n",
      "Epoch: 1883500, elapsed: 1.08e+01, train loss: 5.30207e-07, val loss: 1.27403e-06, min loss: 5.24070e-07\n",
      "Epoch: 1883600, elapsed: 1.10e+01, train loss: 7.74892e-07, val loss: 1.44387e-06, min loss: 5.24070e-07\n",
      "Epoch: 1883700, elapsed: 1.10e+01, train loss: 6.24777e-07, val loss: 1.31193e-06, min loss: 5.24070e-07\n",
      "Epoch: 1883800, elapsed: 1.09e+01, train loss: 5.26762e-07, val loss: 1.26951e-06, min loss: 5.24070e-07\n",
      "Epoch: 1883900, elapsed: 1.06e+01, train loss: 5.30223e-07, val loss: 1.25883e-06, min loss: 5.24070e-07\n",
      "Epoch: 1884000, elapsed: 1.07e+01, train loss: 5.69957e-07, val loss: 1.24792e-06, min loss: 5.24070e-07\n",
      "Epoch: 1884100, elapsed: 1.06e+01, train loss: 1.58718e-06, val loss: 2.50260e-06, min loss: 5.24070e-07\n",
      "Epoch: 1884200, elapsed: 1.08e+01, train loss: 1.78404e-06, val loss: 2.10972e-06, min loss: 5.24070e-07\n",
      "Epoch: 1884300, elapsed: 1.08e+01, train loss: 2.38954e-06, val loss: 2.84279e-06, min loss: 5.24070e-07\n",
      "Epoch: 1884400, elapsed: 1.61e+01, train loss: 1.94345e-06, val loss: 1.86629e-06, min loss: 5.24070e-07\n",
      "Epoch: 1884500, elapsed: 1.14e+01, train loss: 1.10878e-06, val loss: 1.94557e-06, min loss: 5.24070e-07\n",
      "Epoch: 1884600, elapsed: 1.11e+01, train loss: 6.45267e-07, val loss: 1.49673e-06, min loss: 5.24070e-07\n",
      "Epoch: 1884700, elapsed: 1.11e+01, train loss: 1.07349e-06, val loss: 1.72216e-06, min loss: 5.24070e-07\n",
      "Epoch: 1884800, elapsed: 1.08e+01, train loss: 2.29446e-06, val loss: 3.43104e-06, min loss: 5.24070e-07\n",
      "Epoch: 1884900, elapsed: 1.10e+01, train loss: 5.45064e-07, val loss: 1.27449e-06, min loss: 5.24070e-07\n",
      "Epoch: 1885000, elapsed: 1.12e+01, train loss: 5.34732e-07, val loss: 1.28706e-06, min loss: 5.24070e-07\n",
      "Epoch: 1885100, elapsed: 1.30e+01, train loss: 5.23595e-07, val loss: 1.26273e-06, min loss: 5.23595e-07\n",
      "Epoch: 1885200, elapsed: 1.11e+01, train loss: 8.03251e-07, val loss: 1.49024e-06, min loss: 5.23595e-07\n",
      "Epoch: 1885300, elapsed: 1.11e+01, train loss: 5.32545e-07, val loss: 1.25690e-06, min loss: 5.23595e-07\n",
      "Epoch: 1885400, elapsed: 1.11e+01, train loss: 5.24094e-07, val loss: 1.25963e-06, min loss: 5.23595e-07\n",
      "Epoch: 1885500, elapsed: 1.12e+01, train loss: 5.59339e-07, val loss: 1.34659e-06, min loss: 5.23595e-07\n",
      "Epoch: 1885600, elapsed: 1.11e+01, train loss: 6.97726e-07, val loss: 1.50419e-06, min loss: 5.23595e-07\n",
      "Epoch: 1885700, elapsed: 1.12e+01, train loss: 1.45563e-06, val loss: 2.55674e-06, min loss: 5.23595e-07\n",
      "Epoch: 1885800, elapsed: 1.11e+01, train loss: 5.79343e-07, val loss: 1.31833e-06, min loss: 5.23595e-07\n",
      "Epoch: 1885900, elapsed: 1.12e+01, train loss: 5.24676e-07, val loss: 1.25087e-06, min loss: 5.23595e-07\n",
      "Epoch: 1886000, elapsed: 1.12e+01, train loss: 7.87595e-07, val loss: 1.33318e-06, min loss: 5.23595e-07\n",
      "Epoch: 1886100, elapsed: 1.12e+01, train loss: 5.30830e-07, val loss: 1.28995e-06, min loss: 5.23595e-07\n",
      "Epoch: 1886200, elapsed: 1.11e+01, train loss: 5.23753e-07, val loss: 1.25889e-06, min loss: 5.23595e-07\n",
      "Epoch: 1886300, elapsed: 1.09e+01, train loss: 6.06134e-07, val loss: 1.43217e-06, min loss: 5.23595e-07\n",
      "Epoch: 1886400, elapsed: 1.09e+01, train loss: 5.23091e-07, val loss: 1.25741e-06, min loss: 5.23091e-07\n",
      "Epoch: 1886500, elapsed: 1.09e+01, train loss: 6.46572e-07, val loss: 1.41389e-06, min loss: 5.23091e-07\n",
      "Epoch: 1886600, elapsed: 1.11e+01, train loss: 5.22839e-07, val loss: 1.26083e-06, min loss: 5.22839e-07\n",
      "Epoch: 1886700, elapsed: 1.08e+01, train loss: 5.28277e-07, val loss: 1.25634e-06, min loss: 5.22839e-07\n",
      "Epoch: 1886800, elapsed: 1.09e+01, train loss: 5.25414e-07, val loss: 1.25019e-06, min loss: 5.22839e-07\n",
      "Epoch: 1886900, elapsed: 1.06e+01, train loss: 5.25345e-07, val loss: 1.24628e-06, min loss: 5.22839e-07\n",
      "Epoch: 1887000, elapsed: 1.08e+01, train loss: 6.75127e-07, val loss: 1.27484e-06, min loss: 5.22839e-07\n",
      "Epoch: 1887100, elapsed: 1.09e+01, train loss: 5.98003e-07, val loss: 1.33633e-06, min loss: 5.22839e-07\n",
      "Epoch: 1887200, elapsed: 1.10e+01, train loss: 5.27489e-07, val loss: 1.29405e-06, min loss: 5.22839e-07\n",
      "Epoch: 1887300, elapsed: 1.07e+01, train loss: 1.82111e-06, val loss: 2.29938e-06, min loss: 5.22839e-07\n",
      "Epoch: 1887400, elapsed: 1.08e+01, train loss: 5.22591e-07, val loss: 1.26007e-06, min loss: 5.22591e-07\n",
      "Epoch: 1887500, elapsed: 1.08e+01, train loss: 5.31538e-07, val loss: 1.23244e-06, min loss: 5.22591e-07\n",
      "Epoch: 1887600, elapsed: 1.10e+01, train loss: 5.44746e-07, val loss: 1.34258e-06, min loss: 5.22591e-07\n",
      "Epoch: 1887700, elapsed: 1.08e+01, train loss: 5.22979e-07, val loss: 1.25872e-06, min loss: 5.22591e-07\n",
      "Epoch: 1887800, elapsed: 1.64e+01, train loss: 8.93806e-07, val loss: 1.70140e-06, min loss: 5.22591e-07\n",
      "Epoch: 1887900, elapsed: 1.12e+01, train loss: 5.22435e-07, val loss: 1.26017e-06, min loss: 5.22435e-07\n",
      "Epoch: 1888000, elapsed: 1.10e+01, train loss: 5.51980e-07, val loss: 1.25372e-06, min loss: 5.22435e-07\n",
      "Epoch: 1888100, elapsed: 1.14e+01, train loss: 5.25035e-07, val loss: 1.25241e-06, min loss: 5.22435e-07\n",
      "Epoch: 1888200, elapsed: 1.13e+01, train loss: 5.23954e-07, val loss: 1.24326e-06, min loss: 5.22435e-07\n",
      "Epoch: 1888300, elapsed: 1.10e+01, train loss: 6.24777e-07, val loss: 1.30785e-06, min loss: 5.22435e-07\n",
      "Epoch: 1888400, elapsed: 1.10e+01, train loss: 5.41707e-07, val loss: 1.24418e-06, min loss: 5.22435e-07\n",
      "Epoch: 1888500, elapsed: 1.08e+01, train loss: 5.30843e-07, val loss: 1.30642e-06, min loss: 5.22435e-07\n",
      "Epoch: 1888600, elapsed: 1.10e+01, train loss: 5.78274e-07, val loss: 1.27790e-06, min loss: 5.22435e-07\n",
      "Epoch: 1888700, elapsed: 1.10e+01, train loss: 5.40989e-07, val loss: 1.32930e-06, min loss: 5.22435e-07\n",
      "Epoch: 1888800, elapsed: 1.09e+01, train loss: 5.22552e-07, val loss: 1.26310e-06, min loss: 5.22435e-07\n",
      "Epoch: 1888900, elapsed: 1.09e+01, train loss: 5.37069e-07, val loss: 1.24084e-06, min loss: 5.22435e-07\n",
      "Epoch: 1889000, elapsed: 1.10e+01, train loss: 1.00070e-06, val loss: 1.71889e-06, min loss: 5.22435e-07\n",
      "Epoch: 1889100, elapsed: 1.10e+01, train loss: 5.76277e-07, val loss: 1.31829e-06, min loss: 5.22435e-07\n",
      "Epoch: 1889200, elapsed: 1.09e+01, train loss: 5.43864e-07, val loss: 1.29437e-06, min loss: 5.22435e-07\n",
      "Epoch: 1889300, elapsed: 1.12e+01, train loss: 5.57060e-07, val loss: 1.27341e-06, min loss: 5.22435e-07\n",
      "Epoch: 1889400, elapsed: 1.10e+01, train loss: 5.45263e-07, val loss: 1.31777e-06, min loss: 5.22435e-07\n",
      "Epoch: 1889500, elapsed: 1.10e+01, train loss: 5.60176e-07, val loss: 1.39145e-06, min loss: 5.22435e-07\n",
      "Epoch: 1889600, elapsed: 1.09e+01, train loss: 5.22530e-07, val loss: 1.26927e-06, min loss: 5.22435e-07\n",
      "Epoch: 1889700, elapsed: 1.10e+01, train loss: 5.24930e-07, val loss: 1.26934e-06, min loss: 5.22435e-07\n",
      "Epoch: 1889800, elapsed: 1.11e+01, train loss: 5.46662e-07, val loss: 1.31207e-06, min loss: 5.22435e-07\n",
      "Epoch: 1889900, elapsed: 1.11e+01, train loss: 5.24261e-07, val loss: 1.27567e-06, min loss: 5.22435e-07\n",
      "Epoch: 1890000, elapsed: 1.08e+01, train loss: 5.23755e-07, val loss: 1.25027e-06, min loss: 5.22435e-07\n",
      "Epoch: 1890100, elapsed: 1.29e+01, train loss: 5.67339e-07, val loss: 1.26313e-06, min loss: 5.22435e-07\n",
      "Epoch: 1890200, elapsed: 1.09e+01, train loss: 1.32880e-06, val loss: 2.32945e-06, min loss: 5.22435e-07\n",
      "Epoch: 1890300, elapsed: 1.10e+01, train loss: 5.22189e-07, val loss: 1.26636e-06, min loss: 5.22189e-07\n",
      "Epoch: 1890400, elapsed: 1.07e+01, train loss: 5.32463e-07, val loss: 1.26898e-06, min loss: 5.22189e-07\n",
      "Epoch: 1890500, elapsed: 1.07e+01, train loss: 7.07723e-07, val loss: 1.54941e-06, min loss: 5.22189e-07\n",
      "Epoch: 1890600, elapsed: 1.10e+01, train loss: 1.21073e-06, val loss: 1.35656e-06, min loss: 5.22189e-07\n",
      "Epoch: 1890700, elapsed: 1.08e+01, train loss: 6.25224e-07, val loss: 1.32674e-06, min loss: 5.22189e-07\n",
      "Epoch: 1890800, elapsed: 1.13e+01, train loss: 5.28925e-07, val loss: 1.26421e-06, min loss: 5.22189e-07\n",
      "Epoch: 1890900, elapsed: 1.10e+01, train loss: 6.50191e-07, val loss: 1.42168e-06, min loss: 5.22189e-07\n",
      "Epoch: 1891000, elapsed: 1.11e+01, train loss: 5.31371e-07, val loss: 1.26204e-06, min loss: 5.22189e-07\n",
      "Epoch: 1891100, elapsed: 1.61e+01, train loss: 5.38625e-07, val loss: 1.30803e-06, min loss: 5.22189e-07\n",
      "Epoch: 1891200, elapsed: 1.12e+01, train loss: 5.24562e-07, val loss: 1.27613e-06, min loss: 5.22189e-07\n",
      "Epoch: 1891300, elapsed: 1.10e+01, train loss: 5.31016e-07, val loss: 1.29113e-06, min loss: 5.22189e-07\n",
      "Epoch: 1891400, elapsed: 1.10e+01, train loss: 5.48081e-07, val loss: 1.27408e-06, min loss: 5.22189e-07\n",
      "Epoch: 1891500, elapsed: 1.13e+01, train loss: 5.95783e-07, val loss: 1.33977e-06, min loss: 5.22189e-07\n",
      "Epoch: 1891600, elapsed: 1.11e+01, train loss: 5.42629e-07, val loss: 1.30796e-06, min loss: 5.22189e-07\n",
      "Epoch: 1891700, elapsed: 1.12e+01, train loss: 5.80716e-07, val loss: 1.32802e-06, min loss: 5.22189e-07\n",
      "Epoch: 1891800, elapsed: 1.11e+01, train loss: 5.49187e-07, val loss: 1.29176e-06, min loss: 5.22189e-07\n",
      "Epoch: 1891900, elapsed: 1.10e+01, train loss: 5.34851e-07, val loss: 1.25942e-06, min loss: 5.22189e-07\n",
      "Epoch: 1892000, elapsed: 1.09e+01, train loss: 5.38794e-07, val loss: 1.29159e-06, min loss: 5.22189e-07\n",
      "Epoch: 1892100, elapsed: 1.09e+01, train loss: 1.00332e-06, val loss: 1.74551e-06, min loss: 5.22189e-07\n",
      "Epoch: 1892200, elapsed: 1.11e+01, train loss: 5.20975e-07, val loss: 1.26539e-06, min loss: 5.20975e-07\n",
      "Epoch: 1892300, elapsed: 1.12e+01, train loss: 5.39159e-07, val loss: 1.30666e-06, min loss: 5.20975e-07\n",
      "Epoch: 1892400, elapsed: 1.09e+01, train loss: 5.20906e-07, val loss: 1.26562e-06, min loss: 5.20906e-07\n",
      "Epoch: 1892500, elapsed: 1.11e+01, train loss: 5.40443e-07, val loss: 1.29210e-06, min loss: 5.20906e-07\n",
      "Epoch: 1892600, elapsed: 1.12e+01, train loss: 7.40251e-07, val loss: 1.39126e-06, min loss: 5.20906e-07\n",
      "Epoch: 1892700, elapsed: 1.10e+01, train loss: 5.27654e-07, val loss: 1.30637e-06, min loss: 5.20906e-07\n",
      "Epoch: 1892800, elapsed: 1.08e+01, train loss: 5.36806e-07, val loss: 1.28273e-06, min loss: 5.20906e-07\n",
      "Epoch: 1892900, elapsed: 1.10e+01, train loss: 5.42939e-07, val loss: 1.23698e-06, min loss: 5.20906e-07\n",
      "Epoch: 1893000, elapsed: 1.09e+01, train loss: 8.04815e-07, val loss: 1.36339e-06, min loss: 5.20906e-07\n",
      "Epoch: 1893100, elapsed: 1.10e+01, train loss: 5.57580e-07, val loss: 1.27515e-06, min loss: 5.20906e-07\n",
      "Epoch: 1893200, elapsed: 1.07e+01, train loss: 5.32826e-07, val loss: 1.31942e-06, min loss: 5.20906e-07\n",
      "Epoch: 1893300, elapsed: 1.12e+01, train loss: 1.05386e-06, val loss: 1.57874e-06, min loss: 5.20906e-07\n",
      "Epoch: 1893400, elapsed: 1.10e+01, train loss: 5.20657e-07, val loss: 1.26728e-06, min loss: 5.20657e-07\n",
      "Epoch: 1893500, elapsed: 1.09e+01, train loss: 5.27056e-07, val loss: 1.27136e-06, min loss: 5.20657e-07\n",
      "Epoch: 1893600, elapsed: 1.08e+01, train loss: 5.23115e-07, val loss: 1.26555e-06, min loss: 5.20657e-07\n",
      "Epoch: 1893700, elapsed: 1.08e+01, train loss: 5.21041e-07, val loss: 1.26670e-06, min loss: 5.20657e-07\n",
      "Epoch: 1893800, elapsed: 1.08e+01, train loss: 5.26732e-07, val loss: 1.29671e-06, min loss: 5.20657e-07\n",
      "Epoch: 1893900, elapsed: 1.07e+01, train loss: 7.98343e-07, val loss: 1.49776e-06, min loss: 5.20657e-07\n",
      "Epoch: 1894000, elapsed: 1.05e+01, train loss: 2.46801e-06, val loss: 3.10332e-06, min loss: 5.20657e-07\n",
      "Epoch: 1894100, elapsed: 1.07e+01, train loss: 7.07901e-07, val loss: 1.55347e-06, min loss: 5.20657e-07\n",
      "Epoch: 1894200, elapsed: 1.09e+01, train loss: 7.95366e-07, val loss: 1.29264e-06, min loss: 5.20657e-07\n",
      "Epoch: 1894300, elapsed: 1.08e+01, train loss: 7.14178e-07, val loss: 1.35784e-06, min loss: 5.20657e-07\n",
      "Epoch: 1894400, elapsed: 1.12e+01, train loss: 5.55170e-07, val loss: 1.33619e-06, min loss: 5.20657e-07\n",
      "Epoch: 1894500, elapsed: 1.61e+01, train loss: 5.21419e-07, val loss: 1.26808e-06, min loss: 5.20657e-07\n",
      "Epoch: 1894600, elapsed: 1.13e+01, train loss: 5.26796e-07, val loss: 1.27629e-06, min loss: 5.20657e-07\n",
      "Epoch: 1894700, elapsed: 1.07e+01, train loss: 6.84267e-07, val loss: 1.40066e-06, min loss: 5.20657e-07\n",
      "Epoch: 1894800, elapsed: 1.11e+01, train loss: 1.11225e-06, val loss: 1.63166e-06, min loss: 5.20657e-07\n",
      "Epoch: 1894900, elapsed: 1.09e+01, train loss: 2.98125e-06, val loss: 3.46066e-06, min loss: 5.20657e-07\n",
      "Epoch: 1895000, elapsed: 1.12e+01, train loss: 5.81646e-07, val loss: 1.24249e-06, min loss: 5.20657e-07\n",
      "Epoch: 1895100, elapsed: 1.29e+01, train loss: 5.30138e-07, val loss: 1.29769e-06, min loss: 5.20657e-07\n",
      "Epoch: 1895200, elapsed: 1.12e+01, train loss: 5.23797e-07, val loss: 1.25757e-06, min loss: 5.20657e-07\n",
      "Epoch: 1895300, elapsed: 1.11e+01, train loss: 5.21870e-07, val loss: 1.26657e-06, min loss: 5.20657e-07\n",
      "Epoch: 1895400, elapsed: 1.09e+01, train loss: 5.26038e-07, val loss: 1.26375e-06, min loss: 5.20657e-07\n",
      "Epoch: 1895500, elapsed: 1.06e+01, train loss: 8.47429e-07, val loss: 1.61792e-06, min loss: 5.20657e-07\n",
      "Epoch: 1895600, elapsed: 1.09e+01, train loss: 5.57199e-07, val loss: 1.40663e-06, min loss: 5.20657e-07\n",
      "Epoch: 1895700, elapsed: 1.10e+01, train loss: 5.22352e-07, val loss: 1.26263e-06, min loss: 5.20657e-07\n",
      "Epoch: 1895800, elapsed: 1.10e+01, train loss: 5.20681e-07, val loss: 1.26868e-06, min loss: 5.20657e-07\n",
      "Epoch: 1895900, elapsed: 1.09e+01, train loss: 5.51127e-07, val loss: 1.40273e-06, min loss: 5.20657e-07\n",
      "Epoch: 1896000, elapsed: 1.09e+01, train loss: 5.26240e-07, val loss: 1.29036e-06, min loss: 5.20657e-07\n",
      "Epoch: 1896100, elapsed: 1.10e+01, train loss: 5.20797e-07, val loss: 1.26554e-06, min loss: 5.20657e-07\n",
      "Epoch: 1896200, elapsed: 1.09e+01, train loss: 6.73516e-07, val loss: 1.55734e-06, min loss: 5.20657e-07\n",
      "Epoch: 1896300, elapsed: 1.10e+01, train loss: 5.36560e-07, val loss: 1.31507e-06, min loss: 5.20657e-07\n",
      "Epoch: 1896400, elapsed: 1.10e+01, train loss: 5.67279e-07, val loss: 1.33134e-06, min loss: 5.20657e-07\n",
      "Epoch: 1896500, elapsed: 1.08e+01, train loss: 7.32269e-07, val loss: 1.65911e-06, min loss: 5.20657e-07\n",
      "Epoch: 1896600, elapsed: 1.09e+01, train loss: 5.19569e-07, val loss: 1.26932e-06, min loss: 5.19569e-07\n",
      "Epoch: 1896700, elapsed: 1.08e+01, train loss: 5.27390e-07, val loss: 1.27995e-06, min loss: 5.19569e-07\n",
      "Epoch: 1896800, elapsed: 1.09e+01, train loss: 5.49669e-07, val loss: 1.31857e-06, min loss: 5.19569e-07\n",
      "Epoch: 1896900, elapsed: 1.09e+01, train loss: 5.39702e-07, val loss: 1.28003e-06, min loss: 5.19569e-07\n",
      "Epoch: 1897000, elapsed: 1.09e+01, train loss: 6.03406e-07, val loss: 1.31865e-06, min loss: 5.19569e-07\n",
      "Epoch: 1897100, elapsed: 1.11e+01, train loss: 6.32583e-07, val loss: 1.55337e-06, min loss: 5.19569e-07\n",
      "Epoch: 1897200, elapsed: 1.08e+01, train loss: 3.19490e-06, val loss: 3.44715e-06, min loss: 5.19569e-07\n",
      "Epoch: 1897300, elapsed: 1.08e+01, train loss: 6.45584e-07, val loss: 1.40657e-06, min loss: 5.19569e-07\n",
      "Epoch: 1897400, elapsed: 1.06e+01, train loss: 6.59126e-07, val loss: 1.38959e-06, min loss: 5.19569e-07\n",
      "Epoch: 1897500, elapsed: 1.06e+01, train loss: 4.31184e-06, val loss: 4.76775e-06, min loss: 5.19569e-07\n",
      "Epoch: 1897600, elapsed: 1.11e+01, train loss: 5.19279e-07, val loss: 1.27159e-06, min loss: 5.19279e-07\n",
      "Epoch: 1897700, elapsed: 1.07e+01, train loss: 5.42173e-07, val loss: 1.26457e-06, min loss: 5.19279e-07\n",
      "Epoch: 1897800, elapsed: 1.09e+01, train loss: 1.55842e-06, val loss: 2.41762e-06, min loss: 5.19279e-07\n",
      "Epoch: 1897900, elapsed: 1.59e+01, train loss: 5.19258e-07, val loss: 1.27348e-06, min loss: 5.19258e-07\n",
      "Epoch: 1898000, elapsed: 1.10e+01, train loss: 5.20219e-07, val loss: 1.27244e-06, min loss: 5.19258e-07\n",
      "Epoch: 1898100, elapsed: 1.12e+01, train loss: 2.95679e-06, val loss: 3.24482e-06, min loss: 5.19258e-07\n",
      "Epoch: 1898200, elapsed: 1.11e+01, train loss: 5.19703e-07, val loss: 1.28201e-06, min loss: 5.19258e-07\n",
      "Epoch: 1898300, elapsed: 1.11e+01, train loss: 5.19666e-07, val loss: 1.27539e-06, min loss: 5.19258e-07\n",
      "Epoch: 1898400, elapsed: 1.10e+01, train loss: 6.26033e-07, val loss: 1.32139e-06, min loss: 5.19258e-07\n",
      "Epoch: 1898500, elapsed: 1.09e+01, train loss: 6.94388e-07, val loss: 1.41187e-06, min loss: 5.19258e-07\n",
      "Epoch: 1898600, elapsed: 1.10e+01, train loss: 2.21643e-06, val loss: 2.83742e-06, min loss: 5.19258e-07\n",
      "Epoch: 1898700, elapsed: 1.10e+01, train loss: 7.17692e-07, val loss: 1.56722e-06, min loss: 5.19258e-07\n",
      "Epoch: 1898800, elapsed: 1.08e+01, train loss: 6.16929e-07, val loss: 1.32694e-06, min loss: 5.19258e-07\n",
      "Epoch: 1898900, elapsed: 1.09e+01, train loss: 1.03209e-06, val loss: 1.81718e-06, min loss: 5.19258e-07\n",
      "Epoch: 1899000, elapsed: 1.09e+01, train loss: 5.23907e-07, val loss: 1.29243e-06, min loss: 5.19258e-07\n",
      "Epoch: 1899100, elapsed: 1.11e+01, train loss: 5.19194e-07, val loss: 1.26954e-06, min loss: 5.19194e-07\n",
      "Epoch: 1899200, elapsed: 1.10e+01, train loss: 5.20654e-07, val loss: 1.27330e-06, min loss: 5.19194e-07\n",
      "Epoch: 1899300, elapsed: 1.09e+01, train loss: 5.64566e-07, val loss: 1.33558e-06, min loss: 5.19194e-07\n",
      "Epoch: 1899400, elapsed: 1.10e+01, train loss: 5.65684e-07, val loss: 1.38204e-06, min loss: 5.19194e-07\n",
      "Epoch: 1899500, elapsed: 1.10e+01, train loss: 6.59310e-07, val loss: 1.34348e-06, min loss: 5.19194e-07\n",
      "Epoch: 1899600, elapsed: 1.09e+01, train loss: 1.10247e-06, val loss: 2.01642e-06, min loss: 5.19194e-07\n",
      "Epoch: 1899700, elapsed: 1.10e+01, train loss: 5.69406e-07, val loss: 1.39711e-06, min loss: 5.19194e-07\n",
      "Epoch: 1899800, elapsed: 1.10e+01, train loss: 9.72047e-07, val loss: 1.73487e-06, min loss: 5.19194e-07\n",
      "Epoch: 1899900, elapsed: 1.11e+01, train loss: 5.24729e-07, val loss: 1.28660e-06, min loss: 5.19194e-07\n",
      "Epoch: 1900000, elapsed: 1.10e+01, train loss: 6.16659e-07, val loss: 1.41810e-06, min loss: 5.19194e-07\n",
      "Epoch: 1900100, elapsed: 1.29e+01, train loss: 6.74487e-07, val loss: 1.43692e-06, min loss: 5.19194e-07\n",
      "Epoch: 1900200, elapsed: 1.08e+01, train loss: 5.81788e-07, val loss: 1.28319e-06, min loss: 5.19194e-07\n",
      "Epoch: 1900300, elapsed: 1.08e+01, train loss: 6.02580e-07, val loss: 1.29749e-06, min loss: 5.19194e-07\n",
      "Epoch: 1900400, elapsed: 1.08e+01, train loss: 5.20060e-07, val loss: 1.29325e-06, min loss: 5.19194e-07\n",
      "Epoch: 1900500, elapsed: 1.08e+01, train loss: 5.22057e-07, val loss: 1.28475e-06, min loss: 5.19194e-07\n",
      "Epoch: 1900600, elapsed: 1.10e+01, train loss: 6.72760e-07, val loss: 1.44065e-06, min loss: 5.19194e-07\n",
      "Epoch: 1900700, elapsed: 1.09e+01, train loss: 1.37216e-06, val loss: 2.45076e-06, min loss: 5.19194e-07\n",
      "Epoch: 1900800, elapsed: 1.10e+01, train loss: 5.62712e-07, val loss: 1.44248e-06, min loss: 5.19194e-07\n",
      "Epoch: 1900900, elapsed: 1.09e+01, train loss: 5.19512e-07, val loss: 1.27868e-06, min loss: 5.19194e-07\n",
      "Epoch: 1901000, elapsed: 1.09e+01, train loss: 6.19312e-07, val loss: 1.39934e-06, min loss: 5.19194e-07\n",
      "Epoch: 1901100, elapsed: 1.09e+01, train loss: 5.19287e-07, val loss: 1.29134e-06, min loss: 5.19194e-07\n",
      "Epoch: 1901200, elapsed: 1.09e+01, train loss: 5.18835e-07, val loss: 1.27066e-06, min loss: 5.18835e-07\n",
      "Epoch: 1901300, elapsed: 1.61e+01, train loss: 8.21414e-07, val loss: 1.61768e-06, min loss: 5.18835e-07\n",
      "Epoch: 1901400, elapsed: 1.13e+01, train loss: 5.33524e-07, val loss: 1.31173e-06, min loss: 5.18835e-07\n",
      "Epoch: 1901500, elapsed: 1.12e+01, train loss: 5.23466e-07, val loss: 1.26035e-06, min loss: 5.18835e-07\n",
      "Epoch: 1901600, elapsed: 1.10e+01, train loss: 5.19122e-07, val loss: 1.27331e-06, min loss: 5.18835e-07\n",
      "Epoch: 1901700, elapsed: 1.12e+01, train loss: 5.43994e-07, val loss: 1.30498e-06, min loss: 5.18835e-07\n",
      "Epoch: 1901800, elapsed: 1.09e+01, train loss: 5.34763e-07, val loss: 1.33462e-06, min loss: 5.18835e-07\n",
      "Epoch: 1901900, elapsed: 1.09e+01, train loss: 7.73765e-07, val loss: 1.50907e-06, min loss: 5.18835e-07\n",
      "Epoch: 1902000, elapsed: 1.09e+01, train loss: 6.05562e-07, val loss: 1.34326e-06, min loss: 5.18835e-07\n",
      "Epoch: 1902100, elapsed: 1.11e+01, train loss: 5.18323e-07, val loss: 1.28322e-06, min loss: 5.18323e-07\n",
      "Epoch: 1902200, elapsed: 1.10e+01, train loss: 5.19493e-07, val loss: 1.27773e-06, min loss: 5.18323e-07\n",
      "Epoch: 1902300, elapsed: 1.10e+01, train loss: 6.84815e-07, val loss: 1.52343e-06, min loss: 5.18323e-07\n",
      "Epoch: 1902400, elapsed: 1.11e+01, train loss: 5.17873e-07, val loss: 1.27446e-06, min loss: 5.17873e-07\n",
      "Epoch: 1902500, elapsed: 1.09e+01, train loss: 5.49749e-07, val loss: 1.33462e-06, min loss: 5.17873e-07\n",
      "Epoch: 1902600, elapsed: 1.09e+01, train loss: 5.27483e-07, val loss: 1.32433e-06, min loss: 5.17873e-07\n",
      "Epoch: 1902700, elapsed: 1.10e+01, train loss: 5.19507e-07, val loss: 1.28047e-06, min loss: 5.17873e-07\n",
      "Epoch: 1902800, elapsed: 1.10e+01, train loss: 5.33105e-07, val loss: 1.30325e-06, min loss: 5.17873e-07\n",
      "Epoch: 1902900, elapsed: 1.09e+01, train loss: 5.42495e-07, val loss: 1.31608e-06, min loss: 5.17873e-07\n",
      "Epoch: 1903000, elapsed: 1.11e+01, train loss: 5.22841e-07, val loss: 1.27530e-06, min loss: 5.17873e-07\n",
      "Epoch: 1903100, elapsed: 1.10e+01, train loss: 5.28268e-07, val loss: 1.30767e-06, min loss: 5.17873e-07\n",
      "Epoch: 1903200, elapsed: 1.10e+01, train loss: 6.00388e-07, val loss: 1.74551e-06, min loss: 5.17873e-07\n",
      "Epoch: 1903300, elapsed: 1.09e+01, train loss: 5.34413e-07, val loss: 1.25784e-06, min loss: 5.17873e-07\n",
      "Epoch: 1903400, elapsed: 1.11e+01, train loss: 5.21133e-07, val loss: 1.28229e-06, min loss: 5.17873e-07\n",
      "Epoch: 1903500, elapsed: 1.09e+01, train loss: 5.27157e-07, val loss: 1.26851e-06, min loss: 5.17873e-07\n",
      "Epoch: 1903600, elapsed: 1.11e+01, train loss: 5.21769e-07, val loss: 1.29296e-06, min loss: 5.17873e-07\n",
      "Epoch: 1903700, elapsed: 1.08e+01, train loss: 5.33944e-07, val loss: 1.32248e-06, min loss: 5.17873e-07\n",
      "Epoch: 1903800, elapsed: 1.08e+01, train loss: 5.84126e-07, val loss: 1.36468e-06, min loss: 5.17873e-07\n",
      "Epoch: 1903900, elapsed: 1.09e+01, train loss: 7.21451e-07, val loss: 1.53980e-06, min loss: 5.17873e-07\n",
      "Epoch: 1904000, elapsed: 1.10e+01, train loss: 1.55585e-06, val loss: 1.94987e-06, min loss: 5.17873e-07\n",
      "Epoch: 1904100, elapsed: 1.08e+01, train loss: 1.66493e-06, val loss: 2.18508e-06, min loss: 5.17873e-07\n",
      "Epoch: 1904200, elapsed: 1.08e+01, train loss: 6.75605e-07, val loss: 1.53292e-06, min loss: 5.17873e-07\n",
      "Epoch: 1904300, elapsed: 1.07e+01, train loss: 6.09189e-07, val loss: 1.35597e-06, min loss: 5.17873e-07\n",
      "Epoch: 1904400, elapsed: 1.08e+01, train loss: 6.20986e-07, val loss: 1.33527e-06, min loss: 5.17873e-07\n",
      "Epoch: 1904500, elapsed: 1.08e+01, train loss: 1.11009e-06, val loss: 1.59317e-06, min loss: 5.17873e-07\n",
      "Epoch: 1904600, elapsed: 1.10e+01, train loss: 8.11411e-07, val loss: 1.56254e-06, min loss: 5.17873e-07\n",
      "Epoch: 1904700, elapsed: 1.61e+01, train loss: 5.17285e-07, val loss: 1.28045e-06, min loss: 5.17285e-07\n",
      "Epoch: 1904800, elapsed: 1.13e+01, train loss: 5.29001e-07, val loss: 1.31130e-06, min loss: 5.17285e-07\n",
      "Epoch: 1904900, elapsed: 1.12e+01, train loss: 5.92039e-07, val loss: 1.38148e-06, min loss: 5.17285e-07\n",
      "Epoch: 1905000, elapsed: 1.11e+01, train loss: 5.47828e-07, val loss: 1.38714e-06, min loss: 5.17285e-07\n",
      "Epoch: 1905100, elapsed: 1.30e+01, train loss: 5.20151e-07, val loss: 1.28355e-06, min loss: 5.17285e-07\n",
      "Epoch: 1905200, elapsed: 1.12e+01, train loss: 6.45081e-07, val loss: 1.49128e-06, min loss: 5.17285e-07\n",
      "Epoch: 1905300, elapsed: 1.11e+01, train loss: 5.75168e-07, val loss: 1.28477e-06, min loss: 5.17285e-07\n",
      "Epoch: 1905400, elapsed: 1.11e+01, train loss: 5.38108e-07, val loss: 1.31042e-06, min loss: 5.17285e-07\n",
      "Epoch: 1905500, elapsed: 1.10e+01, train loss: 5.19839e-07, val loss: 1.28551e-06, min loss: 5.17285e-07\n",
      "Epoch: 1905600, elapsed: 1.11e+01, train loss: 5.87826e-07, val loss: 1.30314e-06, min loss: 5.17285e-07\n",
      "Epoch: 1905700, elapsed: 1.12e+01, train loss: 5.55227e-07, val loss: 1.27007e-06, min loss: 5.17285e-07\n",
      "Epoch: 1905800, elapsed: 1.10e+01, train loss: 5.18769e-07, val loss: 1.26480e-06, min loss: 5.17285e-07\n",
      "Epoch: 1905900, elapsed: 1.10e+01, train loss: 5.18214e-07, val loss: 1.27147e-06, min loss: 5.17285e-07\n",
      "Epoch: 1906000, elapsed: 1.08e+01, train loss: 5.42848e-07, val loss: 1.31963e-06, min loss: 5.17285e-07\n",
      "Epoch: 1906100, elapsed: 1.10e+01, train loss: 5.35802e-07, val loss: 1.27636e-06, min loss: 5.17285e-07\n",
      "Epoch: 1906200, elapsed: 1.08e+01, train loss: 6.32033e-07, val loss: 1.31221e-06, min loss: 5.17285e-07\n",
      "Epoch: 1906300, elapsed: 1.09e+01, train loss: 6.29332e-07, val loss: 1.25736e-06, min loss: 5.17285e-07\n",
      "Epoch: 1906400, elapsed: 1.10e+01, train loss: 5.54269e-07, val loss: 1.31772e-06, min loss: 5.17285e-07\n",
      "Epoch: 1906500, elapsed: 1.11e+01, train loss: 5.19469e-07, val loss: 1.28902e-06, min loss: 5.17285e-07\n",
      "Epoch: 1906600, elapsed: 1.08e+01, train loss: 5.20760e-07, val loss: 1.28066e-06, min loss: 5.17285e-07\n",
      "Epoch: 1906700, elapsed: 1.10e+01, train loss: 5.19300e-07, val loss: 1.27790e-06, min loss: 5.17285e-07\n",
      "Epoch: 1906800, elapsed: 1.09e+01, train loss: 5.41412e-07, val loss: 1.28471e-06, min loss: 5.17285e-07\n",
      "Epoch: 1906900, elapsed: 1.10e+01, train loss: 8.93390e-07, val loss: 2.03027e-06, min loss: 5.17285e-07\n",
      "Epoch: 1907000, elapsed: 1.08e+01, train loss: 5.48764e-07, val loss: 1.39699e-06, min loss: 5.17285e-07\n",
      "Epoch: 1907100, elapsed: 1.07e+01, train loss: 5.33957e-07, val loss: 1.34000e-06, min loss: 5.17285e-07\n",
      "Epoch: 1907200, elapsed: 1.09e+01, train loss: 5.31546e-07, val loss: 1.26119e-06, min loss: 5.17285e-07\n",
      "Epoch: 1907300, elapsed: 1.09e+01, train loss: 1.00854e-06, val loss: 1.55544e-06, min loss: 5.17285e-07\n",
      "Epoch: 1907400, elapsed: 1.10e+01, train loss: 8.31059e-07, val loss: 1.56574e-06, min loss: 5.17285e-07\n",
      "Epoch: 1907500, elapsed: 1.08e+01, train loss: 2.06071e-06, val loss: 2.49480e-06, min loss: 5.17285e-07\n",
      "Epoch: 1907600, elapsed: 1.09e+01, train loss: 2.71161e-06, val loss: 3.49571e-06, min loss: 5.17285e-07\n",
      "Epoch: 1907700, elapsed: 1.09e+01, train loss: 5.35144e-07, val loss: 1.27312e-06, min loss: 5.17285e-07\n",
      "Epoch: 1907800, elapsed: 1.08e+01, train loss: 5.16540e-07, val loss: 1.28903e-06, min loss: 5.16540e-07\n",
      "Epoch: 1907900, elapsed: 1.09e+01, train loss: 5.50775e-07, val loss: 1.27008e-06, min loss: 5.16540e-07\n",
      "Epoch: 1908000, elapsed: 1.07e+01, train loss: 6.11625e-07, val loss: 1.31441e-06, min loss: 5.16540e-07\n",
      "Epoch: 1908100, elapsed: 1.61e+01, train loss: 8.01067e-07, val loss: 1.46859e-06, min loss: 5.16540e-07\n",
      "Epoch: 1908200, elapsed: 1.09e+01, train loss: 5.82000e-07, val loss: 1.41174e-06, min loss: 5.16540e-07\n",
      "Epoch: 1908300, elapsed: 1.09e+01, train loss: 5.93550e-07, val loss: 1.34158e-06, min loss: 5.16540e-07\n",
      "Epoch: 1908400, elapsed: 1.11e+01, train loss: 6.38801e-07, val loss: 1.44068e-06, min loss: 5.16540e-07\n",
      "Epoch: 1908500, elapsed: 1.08e+01, train loss: 5.29211e-07, val loss: 1.28784e-06, min loss: 5.16540e-07\n",
      "Epoch: 1908600, elapsed: 1.10e+01, train loss: 5.46421e-07, val loss: 1.25771e-06, min loss: 5.16540e-07\n",
      "Epoch: 1908700, elapsed: 1.08e+01, train loss: 5.45101e-07, val loss: 1.29934e-06, min loss: 5.16540e-07\n",
      "Epoch: 1908800, elapsed: 1.08e+01, train loss: 5.31757e-07, val loss: 1.26396e-06, min loss: 5.16540e-07\n",
      "Epoch: 1908900, elapsed: 1.10e+01, train loss: 5.24021e-07, val loss: 1.27577e-06, min loss: 5.16540e-07\n",
      "Epoch: 1909000, elapsed: 1.08e+01, train loss: 5.63612e-07, val loss: 1.38842e-06, min loss: 5.16540e-07\n",
      "Epoch: 1909100, elapsed: 1.08e+01, train loss: 5.15706e-07, val loss: 1.28435e-06, min loss: 5.15706e-07\n",
      "Epoch: 1909200, elapsed: 1.07e+01, train loss: 5.89615e-07, val loss: 1.35042e-06, min loss: 5.15706e-07\n",
      "Epoch: 1909300, elapsed: 1.10e+01, train loss: 5.15484e-07, val loss: 1.28692e-06, min loss: 5.15484e-07\n",
      "Epoch: 1909400, elapsed: 1.09e+01, train loss: 5.18802e-07, val loss: 1.27856e-06, min loss: 5.15484e-07\n",
      "Epoch: 1909500, elapsed: 1.10e+01, train loss: 7.20518e-07, val loss: 1.55813e-06, min loss: 5.15484e-07\n",
      "Epoch: 1909600, elapsed: 1.09e+01, train loss: 5.16782e-07, val loss: 1.28194e-06, min loss: 5.15484e-07\n",
      "Epoch: 1909700, elapsed: 1.11e+01, train loss: 5.16531e-07, val loss: 1.29946e-06, min loss: 5.15484e-07\n",
      "Epoch: 1909800, elapsed: 1.08e+01, train loss: 5.54406e-07, val loss: 1.25864e-06, min loss: 5.15484e-07\n",
      "Epoch: 1909900, elapsed: 1.10e+01, train loss: 5.59608e-07, val loss: 1.30263e-06, min loss: 5.15484e-07\n",
      "Epoch: 1910000, elapsed: 1.09e+01, train loss: 1.24854e-06, val loss: 2.42746e-06, min loss: 5.15484e-07\n",
      "Epoch: 1910100, elapsed: 1.27e+01, train loss: 5.15197e-07, val loss: 1.28499e-06, min loss: 5.15197e-07\n",
      "Epoch: 1910200, elapsed: 1.07e+01, train loss: 5.17149e-07, val loss: 1.29778e-06, min loss: 5.15197e-07\n",
      "Epoch: 1910300, elapsed: 1.09e+01, train loss: 5.58570e-07, val loss: 1.30013e-06, min loss: 5.15197e-07\n",
      "Epoch: 1910400, elapsed: 1.05e+01, train loss: 9.89701e-07, val loss: 1.91102e-06, min loss: 5.15197e-07\n",
      "Epoch: 1910500, elapsed: 1.09e+01, train loss: 5.90948e-07, val loss: 1.34046e-06, min loss: 5.15197e-07\n",
      "Epoch: 1910600, elapsed: 1.09e+01, train loss: 6.29498e-07, val loss: 1.33667e-06, min loss: 5.15197e-07\n",
      "Epoch: 1910700, elapsed: 1.08e+01, train loss: 6.62452e-07, val loss: 1.46513e-06, min loss: 5.15197e-07\n",
      "Epoch: 1910800, elapsed: 1.06e+01, train loss: 6.07452e-07, val loss: 1.49335e-06, min loss: 5.15197e-07\n",
      "Epoch: 1910900, elapsed: 1.09e+01, train loss: 6.93292e-07, val loss: 1.46918e-06, min loss: 5.15197e-07\n",
      "Epoch: 1911000, elapsed: 1.08e+01, train loss: 5.15465e-07, val loss: 1.28007e-06, min loss: 5.15197e-07\n",
      "Epoch: 1911100, elapsed: 1.08e+01, train loss: 5.53999e-07, val loss: 1.34504e-06, min loss: 5.15197e-07\n",
      "Epoch: 1911200, elapsed: 1.07e+01, train loss: 5.14963e-07, val loss: 1.29132e-06, min loss: 5.14963e-07\n",
      "Epoch: 1911300, elapsed: 1.07e+01, train loss: 5.17747e-07, val loss: 1.28794e-06, min loss: 5.14963e-07\n",
      "Epoch: 1911400, elapsed: 1.08e+01, train loss: 2.08552e-06, val loss: 2.23105e-06, min loss: 5.14963e-07\n",
      "Epoch: 1911500, elapsed: 1.62e+01, train loss: 5.16794e-07, val loss: 1.28742e-06, min loss: 5.14963e-07\n",
      "Epoch: 1911600, elapsed: 1.10e+01, train loss: 5.16134e-07, val loss: 1.28927e-06, min loss: 5.14963e-07\n",
      "Epoch: 1911700, elapsed: 1.11e+01, train loss: 5.16242e-07, val loss: 1.28115e-06, min loss: 5.14963e-07\n",
      "Epoch: 1911800, elapsed: 1.12e+01, train loss: 6.37161e-07, val loss: 1.37675e-06, min loss: 5.14963e-07\n",
      "Epoch: 1911900, elapsed: 1.10e+01, train loss: 2.10244e-06, val loss: 2.61055e-06, min loss: 5.14963e-07\n",
      "Epoch: 1912000, elapsed: 1.12e+01, train loss: 5.69836e-07, val loss: 1.50155e-06, min loss: 5.14963e-07\n",
      "Epoch: 1912100, elapsed: 1.10e+01, train loss: 2.68053e-06, val loss: 2.06799e-06, min loss: 5.14963e-07\n",
      "Epoch: 1912200, elapsed: 1.10e+01, train loss: 5.18828e-07, val loss: 1.28017e-06, min loss: 5.14963e-07\n",
      "Epoch: 1912300, elapsed: 1.11e+01, train loss: 5.23341e-07, val loss: 1.32734e-06, min loss: 5.14963e-07\n",
      "Epoch: 1912400, elapsed: 1.07e+01, train loss: 5.53069e-07, val loss: 1.26029e-06, min loss: 5.14963e-07\n",
      "Epoch: 1912500, elapsed: 1.10e+01, train loss: 5.15361e-07, val loss: 1.29166e-06, min loss: 5.14963e-07\n",
      "Epoch: 1912600, elapsed: 1.11e+01, train loss: 5.17736e-07, val loss: 1.26717e-06, min loss: 5.14963e-07\n",
      "Epoch: 1912700, elapsed: 1.10e+01, train loss: 6.55876e-07, val loss: 1.40230e-06, min loss: 5.14963e-07\n",
      "Epoch: 1912800, elapsed: 1.08e+01, train loss: 5.20861e-07, val loss: 1.28827e-06, min loss: 5.14963e-07\n",
      "Epoch: 1912900, elapsed: 1.10e+01, train loss: 5.19851e-07, val loss: 1.31595e-06, min loss: 5.14963e-07\n",
      "Epoch: 1913000, elapsed: 1.10e+01, train loss: 5.19278e-07, val loss: 1.28778e-06, min loss: 5.14963e-07\n",
      "Epoch: 1913100, elapsed: 1.11e+01, train loss: 5.35629e-07, val loss: 1.29118e-06, min loss: 5.14963e-07\n",
      "Epoch: 1913200, elapsed: 1.10e+01, train loss: 5.68364e-07, val loss: 1.46763e-06, min loss: 5.14963e-07\n",
      "Epoch: 1913300, elapsed: 1.10e+01, train loss: 5.26408e-07, val loss: 1.29350e-06, min loss: 5.14963e-07\n",
      "Epoch: 1913400, elapsed: 1.09e+01, train loss: 5.16045e-07, val loss: 1.28999e-06, min loss: 5.14963e-07\n",
      "Epoch: 1913500, elapsed: 1.09e+01, train loss: 5.93283e-07, val loss: 1.36909e-06, min loss: 5.14963e-07\n",
      "Epoch: 1913600, elapsed: 1.10e+01, train loss: 5.13984e-07, val loss: 1.29057e-06, min loss: 5.13984e-07\n",
      "Epoch: 1913700, elapsed: 1.08e+01, train loss: 5.24915e-07, val loss: 1.28397e-06, min loss: 5.13984e-07\n",
      "Epoch: 1913800, elapsed: 1.08e+01, train loss: 5.16210e-07, val loss: 1.28969e-06, min loss: 5.13984e-07\n",
      "Epoch: 1913900, elapsed: 1.10e+01, train loss: 5.85580e-07, val loss: 1.31587e-06, min loss: 5.13984e-07\n",
      "Epoch: 1914000, elapsed: 1.09e+01, train loss: 1.02740e-06, val loss: 1.91064e-06, min loss: 5.13984e-07\n",
      "Epoch: 1914100, elapsed: 1.09e+01, train loss: 5.14366e-07, val loss: 1.28910e-06, min loss: 5.13984e-07\n",
      "Epoch: 1914200, elapsed: 1.10e+01, train loss: 5.16026e-07, val loss: 1.29100e-06, min loss: 5.13984e-07\n",
      "Epoch: 1914300, elapsed: 1.08e+01, train loss: 5.45016e-07, val loss: 1.29062e-06, min loss: 5.13984e-07\n",
      "Epoch: 1914400, elapsed: 1.10e+01, train loss: 2.76551e-06, val loss: 2.87607e-06, min loss: 5.13984e-07\n",
      "Epoch: 1914500, elapsed: 1.10e+01, train loss: 8.72134e-07, val loss: 1.58119e-06, min loss: 5.13984e-07\n",
      "Epoch: 1914600, elapsed: 1.10e+01, train loss: 5.16471e-07, val loss: 1.29200e-06, min loss: 5.13984e-07\n",
      "Epoch: 1914700, elapsed: 1.09e+01, train loss: 5.16343e-07, val loss: 1.29331e-06, min loss: 5.13984e-07\n",
      "Epoch: 1914800, elapsed: 1.08e+01, train loss: 5.80474e-07, val loss: 1.30784e-06, min loss: 5.13984e-07\n",
      "Epoch: 1914900, elapsed: 1.61e+01, train loss: 5.53243e-07, val loss: 1.28019e-06, min loss: 5.13984e-07\n",
      "Epoch: 1915000, elapsed: 1.12e+01, train loss: 5.18231e-07, val loss: 1.28685e-06, min loss: 5.13984e-07\n",
      "Epoch: 1915100, elapsed: 1.31e+01, train loss: 5.29308e-07, val loss: 1.32304e-06, min loss: 5.13984e-07\n",
      "Epoch: 1915200, elapsed: 1.12e+01, train loss: 5.17122e-07, val loss: 1.28616e-06, min loss: 5.13984e-07\n",
      "Epoch: 1915300, elapsed: 1.08e+01, train loss: 5.80649e-07, val loss: 1.39063e-06, min loss: 5.13984e-07\n",
      "Epoch: 1915400, elapsed: 1.11e+01, train loss: 5.46836e-07, val loss: 1.30866e-06, min loss: 5.13984e-07\n",
      "Epoch: 1915500, elapsed: 1.11e+01, train loss: 7.87336e-07, val loss: 1.70907e-06, min loss: 5.13984e-07\n",
      "Epoch: 1915600, elapsed: 1.10e+01, train loss: 6.71408e-07, val loss: 1.40534e-06, min loss: 5.13984e-07\n",
      "Epoch: 1915700, elapsed: 1.10e+01, train loss: 5.24898e-07, val loss: 1.36147e-06, min loss: 5.13984e-07\n",
      "Epoch: 1915800, elapsed: 1.07e+01, train loss: 8.77438e-07, val loss: 1.86974e-06, min loss: 5.13984e-07\n",
      "Epoch: 1915900, elapsed: 1.11e+01, train loss: 5.13688e-07, val loss: 1.29405e-06, min loss: 5.13688e-07\n",
      "Epoch: 1916000, elapsed: 1.10e+01, train loss: 5.19008e-07, val loss: 1.28686e-06, min loss: 5.13688e-07\n",
      "Epoch: 1916100, elapsed: 1.10e+01, train loss: 1.17418e-06, val loss: 1.69410e-06, min loss: 5.13688e-07\n",
      "Epoch: 1916200, elapsed: 1.06e+01, train loss: 5.16844e-07, val loss: 1.28881e-06, min loss: 5.13688e-07\n",
      "Epoch: 1916300, elapsed: 1.08e+01, train loss: 5.14347e-07, val loss: 1.28905e-06, min loss: 5.13688e-07\n",
      "Epoch: 1916400, elapsed: 1.09e+01, train loss: 5.13802e-07, val loss: 1.28247e-06, min loss: 5.13688e-07\n",
      "Epoch: 1916500, elapsed: 1.09e+01, train loss: 5.19723e-07, val loss: 1.30563e-06, min loss: 5.13688e-07\n",
      "Epoch: 1916600, elapsed: 1.07e+01, train loss: 5.17960e-07, val loss: 1.30187e-06, min loss: 5.13688e-07\n",
      "Epoch: 1916700, elapsed: 1.11e+01, train loss: 6.64539e-07, val loss: 1.68815e-06, min loss: 5.13688e-07\n",
      "Epoch: 1916800, elapsed: 1.08e+01, train loss: 5.14563e-07, val loss: 1.29971e-06, min loss: 5.13688e-07\n",
      "Epoch: 1916900, elapsed: 1.07e+01, train loss: 5.15849e-07, val loss: 1.28000e-06, min loss: 5.13688e-07\n",
      "Epoch: 1917000, elapsed: 1.10e+01, train loss: 5.66693e-07, val loss: 1.29887e-06, min loss: 5.13688e-07\n",
      "Epoch: 1917100, elapsed: 1.07e+01, train loss: 5.14194e-07, val loss: 1.30049e-06, min loss: 5.13688e-07\n",
      "Epoch: 1917200, elapsed: 1.08e+01, train loss: 5.15365e-07, val loss: 1.28721e-06, min loss: 5.13688e-07\n",
      "Epoch: 1917300, elapsed: 1.08e+01, train loss: 6.18330e-07, val loss: 1.36543e-06, min loss: 5.13688e-07\n",
      "Epoch: 1917400, elapsed: 1.09e+01, train loss: 5.18322e-07, val loss: 1.29655e-06, min loss: 5.13688e-07\n",
      "Epoch: 1917500, elapsed: 1.09e+01, train loss: 5.13684e-07, val loss: 1.28659e-06, min loss: 5.13684e-07\n",
      "Epoch: 1917600, elapsed: 1.08e+01, train loss: 6.73419e-07, val loss: 1.38904e-06, min loss: 5.13684e-07\n",
      "Epoch: 1917700, elapsed: 1.09e+01, train loss: 5.74685e-07, val loss: 1.31535e-06, min loss: 5.13684e-07\n",
      "Epoch: 1917800, elapsed: 1.09e+01, train loss: 5.24625e-07, val loss: 1.28420e-06, min loss: 5.13684e-07\n",
      "Epoch: 1917900, elapsed: 1.07e+01, train loss: 8.06327e-07, val loss: 1.77915e-06, min loss: 5.13684e-07\n",
      "Epoch: 1918000, elapsed: 1.10e+01, train loss: 5.16796e-07, val loss: 1.31846e-06, min loss: 5.13684e-07\n",
      "Epoch: 1918100, elapsed: 1.07e+01, train loss: 5.13440e-07, val loss: 1.30489e-06, min loss: 5.13440e-07\n",
      "Epoch: 1918200, elapsed: 1.08e+01, train loss: 5.68714e-07, val loss: 1.31082e-06, min loss: 5.13440e-07\n",
      "Epoch: 1918300, elapsed: 1.64e+01, train loss: 1.49446e-06, val loss: 2.50213e-06, min loss: 5.13440e-07\n",
      "Epoch: 1918400, elapsed: 1.13e+01, train loss: 5.76352e-07, val loss: 1.38333e-06, min loss: 5.13440e-07\n",
      "Epoch: 1918500, elapsed: 1.10e+01, train loss: 5.16212e-07, val loss: 1.29589e-06, min loss: 5.13440e-07\n",
      "Epoch: 1918600, elapsed: 1.11e+01, train loss: 6.06011e-07, val loss: 1.29929e-06, min loss: 5.13440e-07\n",
      "Epoch: 1918700, elapsed: 1.13e+01, train loss: 5.31946e-07, val loss: 1.28304e-06, min loss: 5.13440e-07\n",
      "Epoch: 1918800, elapsed: 1.11e+01, train loss: 5.70152e-07, val loss: 1.35744e-06, min loss: 5.13440e-07\n",
      "Epoch: 1918900, elapsed: 1.13e+01, train loss: 5.50753e-07, val loss: 1.43101e-06, min loss: 5.13440e-07\n",
      "Epoch: 1919000, elapsed: 1.09e+01, train loss: 7.21622e-07, val loss: 1.32725e-06, min loss: 5.13440e-07\n",
      "Epoch: 1919100, elapsed: 1.10e+01, train loss: 5.16066e-07, val loss: 1.28131e-06, min loss: 5.13440e-07\n",
      "Epoch: 1919200, elapsed: 1.13e+01, train loss: 5.12416e-07, val loss: 1.29827e-06, min loss: 5.12416e-07\n",
      "Epoch: 1919300, elapsed: 1.09e+01, train loss: 8.73971e-07, val loss: 1.71566e-06, min loss: 5.12416e-07\n",
      "Epoch: 1919400, elapsed: 1.11e+01, train loss: 5.35525e-07, val loss: 1.30862e-06, min loss: 5.12416e-07\n",
      "Epoch: 1919500, elapsed: 1.12e+01, train loss: 1.18581e-06, val loss: 2.24741e-06, min loss: 5.12416e-07\n",
      "Epoch: 1919600, elapsed: 1.07e+01, train loss: 6.51822e-07, val loss: 1.63486e-06, min loss: 5.12416e-07\n",
      "Epoch: 1919700, elapsed: 1.09e+01, train loss: 6.44459e-07, val loss: 1.40410e-06, min loss: 5.12416e-07\n",
      "Epoch: 1919800, elapsed: 1.08e+01, train loss: 5.76985e-07, val loss: 1.27632e-06, min loss: 5.12416e-07\n",
      "Epoch: 1919900, elapsed: 1.10e+01, train loss: 7.59001e-07, val loss: 1.48907e-06, min loss: 5.12416e-07\n",
      "Epoch: 1920000, elapsed: 1.12e+01, train loss: 2.39288e-06, val loss: 3.57182e-06, min loss: 5.12416e-07\n",
      "Epoch: 1920100, elapsed: 1.29e+01, train loss: 8.36783e-07, val loss: 1.49091e-06, min loss: 5.12416e-07\n",
      "Epoch: 1920200, elapsed: 1.09e+01, train loss: 9.73896e-07, val loss: 1.60218e-06, min loss: 5.12416e-07\n",
      "Epoch: 1920300, elapsed: 1.07e+01, train loss: 7.96925e-07, val loss: 1.42295e-06, min loss: 5.12416e-07\n",
      "Epoch: 1920400, elapsed: 1.08e+01, train loss: 5.26353e-07, val loss: 1.26454e-06, min loss: 5.12416e-07\n",
      "Epoch: 1920500, elapsed: 1.08e+01, train loss: 5.12441e-07, val loss: 1.29416e-06, min loss: 5.12416e-07\n",
      "Epoch: 1920600, elapsed: 1.10e+01, train loss: 5.12703e-07, val loss: 1.29176e-06, min loss: 5.12416e-07\n",
      "Epoch: 1920700, elapsed: 1.09e+01, train loss: 6.83615e-07, val loss: 1.67557e-06, min loss: 5.12416e-07\n",
      "Epoch: 1920800, elapsed: 1.09e+01, train loss: 5.13094e-07, val loss: 1.30052e-06, min loss: 5.12416e-07\n",
      "Epoch: 1920900, elapsed: 1.10e+01, train loss: 7.76290e-07, val loss: 1.47977e-06, min loss: 5.12416e-07\n",
      "Epoch: 1921000, elapsed: 1.10e+01, train loss: 5.20090e-07, val loss: 1.28640e-06, min loss: 5.12416e-07\n",
      "Epoch: 1921100, elapsed: 1.07e+01, train loss: 1.07582e-06, val loss: 2.12198e-06, min loss: 5.12416e-07\n",
      "Epoch: 1921200, elapsed: 1.10e+01, train loss: 5.11757e-07, val loss: 1.30118e-06, min loss: 5.11757e-07\n",
      "Epoch: 1921300, elapsed: 1.09e+01, train loss: 9.44483e-07, val loss: 1.49413e-06, min loss: 5.11757e-07\n",
      "Epoch: 1921400, elapsed: 1.10e+01, train loss: 5.17006e-07, val loss: 1.29991e-06, min loss: 5.11757e-07\n",
      "Epoch: 1921500, elapsed: 1.08e+01, train loss: 5.38764e-07, val loss: 1.36358e-06, min loss: 5.11757e-07\n",
      "Epoch: 1921600, elapsed: 1.07e+01, train loss: 5.11728e-07, val loss: 1.30082e-06, min loss: 5.11728e-07\n",
      "Epoch: 1921700, elapsed: 1.59e+01, train loss: 5.12761e-07, val loss: 1.29248e-06, min loss: 5.11728e-07\n",
      "Epoch: 1921800, elapsed: 1.11e+01, train loss: 1.73180e-06, val loss: 2.19200e-06, min loss: 5.11728e-07\n",
      "Epoch: 1921900, elapsed: 1.11e+01, train loss: 5.11402e-07, val loss: 1.30211e-06, min loss: 5.11402e-07\n",
      "Epoch: 1922000, elapsed: 1.10e+01, train loss: 5.41247e-07, val loss: 1.30225e-06, min loss: 5.11402e-07\n",
      "Epoch: 1922100, elapsed: 1.11e+01, train loss: 5.63864e-07, val loss: 1.31545e-06, min loss: 5.11402e-07\n",
      "Epoch: 1922200, elapsed: 1.10e+01, train loss: 5.11502e-07, val loss: 1.30365e-06, min loss: 5.11402e-07\n",
      "Epoch: 1922300, elapsed: 1.10e+01, train loss: 5.25458e-07, val loss: 1.34106e-06, min loss: 5.11402e-07\n",
      "Epoch: 1922400, elapsed: 1.11e+01, train loss: 5.43164e-07, val loss: 1.31937e-06, min loss: 5.11402e-07\n",
      "Epoch: 1922500, elapsed: 1.08e+01, train loss: 5.15242e-07, val loss: 1.32188e-06, min loss: 5.11402e-07\n",
      "Epoch: 1922600, elapsed: 1.11e+01, train loss: 5.14093e-07, val loss: 1.29826e-06, min loss: 5.11402e-07\n",
      "Epoch: 1922700, elapsed: 1.08e+01, train loss: 5.26552e-07, val loss: 1.29429e-06, min loss: 5.11402e-07\n",
      "Epoch: 1922800, elapsed: 1.09e+01, train loss: 8.91460e-07, val loss: 1.81610e-06, min loss: 5.11402e-07\n",
      "Epoch: 1922900, elapsed: 1.09e+01, train loss: 5.20216e-07, val loss: 1.32545e-06, min loss: 5.11402e-07\n",
      "Epoch: 1923000, elapsed: 1.10e+01, train loss: 5.13427e-07, val loss: 1.27451e-06, min loss: 5.11402e-07\n",
      "Epoch: 1923100, elapsed: 1.09e+01, train loss: 2.65064e-06, val loss: 3.26277e-06, min loss: 5.11402e-07\n",
      "Epoch: 1923200, elapsed: 1.10e+01, train loss: 6.72532e-07, val loss: 1.32162e-06, min loss: 5.11402e-07\n",
      "Epoch: 1923300, elapsed: 1.08e+01, train loss: 5.85655e-07, val loss: 1.38850e-06, min loss: 5.11402e-07\n",
      "Epoch: 1923400, elapsed: 1.08e+01, train loss: 5.35431e-07, val loss: 1.33865e-06, min loss: 5.11402e-07\n",
      "Epoch: 1923500, elapsed: 1.10e+01, train loss: 9.25147e-07, val loss: 1.60956e-06, min loss: 5.11402e-07\n",
      "Epoch: 1923600, elapsed: 1.08e+01, train loss: 2.92394e-06, val loss: 3.45224e-06, min loss: 5.11402e-07\n",
      "Epoch: 1923700, elapsed: 1.08e+01, train loss: 9.31623e-07, val loss: 1.49122e-06, min loss: 5.11402e-07\n",
      "Epoch: 1923800, elapsed: 1.10e+01, train loss: 6.30925e-07, val loss: 1.42906e-06, min loss: 5.11402e-07\n",
      "Epoch: 1923900, elapsed: 1.08e+01, train loss: 6.01486e-07, val loss: 1.28746e-06, min loss: 5.11402e-07\n",
      "Epoch: 1924000, elapsed: 1.08e+01, train loss: 7.40933e-07, val loss: 1.42026e-06, min loss: 5.11402e-07\n",
      "Epoch: 1924100, elapsed: 1.10e+01, train loss: 5.83155e-07, val loss: 1.39169e-06, min loss: 5.11402e-07\n",
      "Epoch: 1924200, elapsed: 1.10e+01, train loss: 5.23736e-07, val loss: 1.29750e-06, min loss: 5.11402e-07\n",
      "Epoch: 1924300, elapsed: 1.08e+01, train loss: 5.33797e-07, val loss: 1.26777e-06, min loss: 5.11402e-07\n",
      "Epoch: 1924400, elapsed: 1.09e+01, train loss: 5.21236e-07, val loss: 1.34413e-06, min loss: 5.11402e-07\n",
      "Epoch: 1924500, elapsed: 1.09e+01, train loss: 6.16311e-07, val loss: 1.52841e-06, min loss: 5.11402e-07\n",
      "Epoch: 1924600, elapsed: 1.07e+01, train loss: 5.23487e-07, val loss: 1.27887e-06, min loss: 5.11402e-07\n",
      "Epoch: 1924700, elapsed: 1.08e+01, train loss: 5.10835e-07, val loss: 1.30256e-06, min loss: 5.10835e-07\n",
      "Epoch: 1924800, elapsed: 1.08e+01, train loss: 5.44544e-07, val loss: 1.31503e-06, min loss: 5.10835e-07\n",
      "Epoch: 1924900, elapsed: 1.10e+01, train loss: 5.25246e-07, val loss: 1.33116e-06, min loss: 5.10835e-07\n",
      "Epoch: 1925000, elapsed: 1.10e+01, train loss: 5.23681e-07, val loss: 1.29772e-06, min loss: 5.10835e-07\n",
      "Epoch: 1925100, elapsed: 1.82e+01, train loss: 5.37472e-07, val loss: 1.34000e-06, min loss: 5.10835e-07\n",
      "Epoch: 1925200, elapsed: 1.11e+01, train loss: 7.08831e-07, val loss: 1.48889e-06, min loss: 5.10835e-07\n",
      "Epoch: 1925300, elapsed: 1.10e+01, train loss: 5.29831e-07, val loss: 1.37287e-06, min loss: 5.10835e-07\n",
      "Epoch: 1925400, elapsed: 1.10e+01, train loss: 5.10751e-07, val loss: 1.30913e-06, min loss: 5.10751e-07\n",
      "Epoch: 1925500, elapsed: 1.10e+01, train loss: 6.01409e-07, val loss: 1.38497e-06, min loss: 5.10751e-07\n",
      "Epoch: 1925600, elapsed: 1.11e+01, train loss: 1.36428e-06, val loss: 2.58329e-06, min loss: 5.10751e-07\n",
      "Epoch: 1925700, elapsed: 1.09e+01, train loss: 5.75743e-07, val loss: 1.38163e-06, min loss: 5.10751e-07\n",
      "Epoch: 1925800, elapsed: 1.11e+01, train loss: 5.31887e-07, val loss: 1.31196e-06, min loss: 5.10751e-07\n",
      "Epoch: 1925900, elapsed: 1.10e+01, train loss: 6.78192e-07, val loss: 1.56525e-06, min loss: 5.10751e-07\n",
      "Epoch: 1926000, elapsed: 1.11e+01, train loss: 5.54911e-07, val loss: 1.33236e-06, min loss: 5.10751e-07\n",
      "Epoch: 1926100, elapsed: 1.10e+01, train loss: 5.13281e-07, val loss: 1.28708e-06, min loss: 5.10751e-07\n",
      "Epoch: 1926200, elapsed: 1.09e+01, train loss: 5.10854e-07, val loss: 1.30613e-06, min loss: 5.10751e-07\n",
      "Epoch: 1926300, elapsed: 1.09e+01, train loss: 5.18561e-07, val loss: 1.29674e-06, min loss: 5.10751e-07\n",
      "Epoch: 1926400, elapsed: 1.10e+01, train loss: 5.23823e-07, val loss: 1.37235e-06, min loss: 5.10751e-07\n",
      "Epoch: 1926500, elapsed: 1.08e+01, train loss: 5.17893e-07, val loss: 1.31542e-06, min loss: 5.10751e-07\n",
      "Epoch: 1926600, elapsed: 1.10e+01, train loss: 5.58258e-07, val loss: 1.33991e-06, min loss: 5.10751e-07\n",
      "Epoch: 1926700, elapsed: 1.10e+01, train loss: 1.17366e-06, val loss: 2.38254e-06, min loss: 5.10751e-07\n",
      "Epoch: 1926800, elapsed: 1.08e+01, train loss: 5.23329e-07, val loss: 1.27056e-06, min loss: 5.10751e-07\n",
      "Epoch: 1926900, elapsed: 1.11e+01, train loss: 1.44213e-06, val loss: 2.29681e-06, min loss: 5.10751e-07\n",
      "Epoch: 1927000, elapsed: 1.07e+01, train loss: 5.47364e-07, val loss: 1.29147e-06, min loss: 5.10751e-07\n",
      "Epoch: 1927100, elapsed: 1.07e+01, train loss: 5.27884e-07, val loss: 1.32335e-06, min loss: 5.10751e-07\n",
      "Epoch: 1927200, elapsed: 1.07e+01, train loss: 5.51268e-07, val loss: 1.35140e-06, min loss: 5.10751e-07\n",
      "Epoch: 1927300, elapsed: 1.09e+01, train loss: 5.61998e-07, val loss: 1.34987e-06, min loss: 5.10751e-07\n",
      "Epoch: 1927400, elapsed: 1.09e+01, train loss: 5.35381e-07, val loss: 1.33544e-06, min loss: 5.10751e-07\n",
      "Epoch: 1927500, elapsed: 1.06e+01, train loss: 5.41749e-07, val loss: 1.31923e-06, min loss: 5.10751e-07\n",
      "Epoch: 1927600, elapsed: 1.08e+01, train loss: 1.43583e-06, val loss: 2.41101e-06, min loss: 5.10751e-07\n",
      "Epoch: 1927700, elapsed: 1.08e+01, train loss: 7.53816e-07, val loss: 1.59077e-06, min loss: 5.10751e-07\n",
      "Epoch: 1927800, elapsed: 1.07e+01, train loss: 1.99436e-06, val loss: 2.18419e-06, min loss: 5.10751e-07\n",
      "Epoch: 1927900, elapsed: 1.09e+01, train loss: 2.18686e-06, val loss: 2.60309e-06, min loss: 5.10751e-07\n",
      "Epoch: 1928000, elapsed: 1.08e+01, train loss: 6.12397e-07, val loss: 1.35887e-06, min loss: 5.10751e-07\n",
      "Epoch: 1928100, elapsed: 1.10e+01, train loss: 1.02026e-06, val loss: 2.11374e-06, min loss: 5.10751e-07\n",
      "Epoch: 1928200, elapsed: 1.07e+01, train loss: 5.09812e-07, val loss: 1.31526e-06, min loss: 5.09812e-07\n",
      "Epoch: 1928300, elapsed: 1.08e+01, train loss: 5.13583e-07, val loss: 1.32052e-06, min loss: 5.09812e-07\n",
      "Epoch: 1928400, elapsed: 1.08e+01, train loss: 5.16888e-07, val loss: 1.29959e-06, min loss: 5.09812e-07\n",
      "Epoch: 1928500, elapsed: 1.08e+01, train loss: 6.03298e-07, val loss: 1.36913e-06, min loss: 5.09812e-07\n",
      "Epoch: 1928600, elapsed: 1.60e+01, train loss: 5.13398e-07, val loss: 1.32191e-06, min loss: 5.09812e-07\n",
      "Epoch: 1928700, elapsed: 1.10e+01, train loss: 5.34544e-07, val loss: 1.30541e-06, min loss: 5.09812e-07\n",
      "Epoch: 1928800, elapsed: 1.10e+01, train loss: 5.58550e-07, val loss: 1.38579e-06, min loss: 5.09812e-07\n",
      "Epoch: 1928900, elapsed: 1.11e+01, train loss: 5.20205e-07, val loss: 1.28319e-06, min loss: 5.09812e-07\n",
      "Epoch: 1929000, elapsed: 1.11e+01, train loss: 5.09453e-07, val loss: 1.30511e-06, min loss: 5.09453e-07\n",
      "Epoch: 1929100, elapsed: 1.11e+01, train loss: 5.11114e-07, val loss: 1.31784e-06, min loss: 5.09453e-07\n",
      "Epoch: 1929200, elapsed: 1.10e+01, train loss: 5.86678e-07, val loss: 1.39417e-06, min loss: 5.09453e-07\n",
      "Epoch: 1929300, elapsed: 1.11e+01, train loss: 8.93367e-07, val loss: 1.84986e-06, min loss: 5.09453e-07\n",
      "Epoch: 1929400, elapsed: 1.11e+01, train loss: 5.10478e-07, val loss: 1.31457e-06, min loss: 5.09453e-07\n",
      "Epoch: 1929500, elapsed: 1.09e+01, train loss: 5.09908e-07, val loss: 1.31025e-06, min loss: 5.09453e-07\n",
      "Epoch: 1929600, elapsed: 1.12e+01, train loss: 5.18680e-07, val loss: 1.28392e-06, min loss: 5.09453e-07\n",
      "Epoch: 1929700, elapsed: 1.09e+01, train loss: 5.82707e-07, val loss: 1.26190e-06, min loss: 5.09453e-07\n",
      "Epoch: 1929800, elapsed: 1.10e+01, train loss: 5.10767e-07, val loss: 1.30803e-06, min loss: 5.09453e-07\n",
      "Epoch: 1929900, elapsed: 1.07e+01, train loss: 5.09967e-07, val loss: 1.31669e-06, min loss: 5.09453e-07\n",
      "Epoch: 1930000, elapsed: 1.10e+01, train loss: 1.13379e-06, val loss: 1.73044e-06, min loss: 5.09453e-07\n",
      "Epoch: 1930100, elapsed: 1.29e+01, train loss: 5.08684e-07, val loss: 1.30951e-06, min loss: 5.08684e-07\n",
      "Epoch: 1930200, elapsed: 1.11e+01, train loss: 5.54011e-07, val loss: 1.34561e-06, min loss: 5.08684e-07\n",
      "Epoch: 1930300, elapsed: 1.11e+01, train loss: 5.08666e-07, val loss: 1.30806e-06, min loss: 5.08666e-07\n",
      "Epoch: 1930400, elapsed: 1.09e+01, train loss: 5.15743e-07, val loss: 1.30686e-06, min loss: 5.08666e-07\n",
      "Epoch: 1930500, elapsed: 1.07e+01, train loss: 5.10803e-07, val loss: 1.33274e-06, min loss: 5.08666e-07\n",
      "Epoch: 1930600, elapsed: 1.07e+01, train loss: 3.08664e-06, val loss: 2.17698e-06, min loss: 5.08666e-07\n",
      "Epoch: 1930700, elapsed: 1.08e+01, train loss: 5.08551e-07, val loss: 1.30838e-06, min loss: 5.08551e-07\n",
      "Epoch: 1930800, elapsed: 1.08e+01, train loss: 5.75846e-07, val loss: 1.28405e-06, min loss: 5.08551e-07\n",
      "Epoch: 1930900, elapsed: 1.08e+01, train loss: 5.84296e-07, val loss: 1.39111e-06, min loss: 5.08551e-07\n",
      "Epoch: 1931000, elapsed: 1.09e+01, train loss: 1.08445e-06, val loss: 2.10734e-06, min loss: 5.08551e-07\n",
      "Epoch: 1931100, elapsed: 1.11e+01, train loss: 5.08774e-07, val loss: 1.30802e-06, min loss: 5.08551e-07\n",
      "Epoch: 1931200, elapsed: 1.08e+01, train loss: 5.08898e-07, val loss: 1.30896e-06, min loss: 5.08551e-07\n",
      "Epoch: 1931300, elapsed: 1.09e+01, train loss: 7.56817e-07, val loss: 1.41113e-06, min loss: 5.08551e-07\n",
      "Epoch: 1931400, elapsed: 1.08e+01, train loss: 5.20255e-07, val loss: 1.30827e-06, min loss: 5.08551e-07\n",
      "Epoch: 1931500, elapsed: 1.10e+01, train loss: 5.10659e-07, val loss: 1.30357e-06, min loss: 5.08551e-07\n",
      "Epoch: 1931600, elapsed: 1.09e+01, train loss: 5.33620e-07, val loss: 1.29775e-06, min loss: 5.08551e-07\n",
      "Epoch: 1931700, elapsed: 1.09e+01, train loss: 1.07482e-06, val loss: 2.26503e-06, min loss: 5.08551e-07\n",
      "Epoch: 1931800, elapsed: 1.08e+01, train loss: 5.11926e-07, val loss: 1.31620e-06, min loss: 5.08551e-07\n",
      "Epoch: 1931900, elapsed: 1.10e+01, train loss: 7.67975e-07, val loss: 1.47847e-06, min loss: 5.08551e-07\n",
      "Epoch: 1932000, elapsed: 1.62e+01, train loss: 1.27748e-06, val loss: 2.41797e-06, min loss: 5.08551e-07\n",
      "Epoch: 1932100, elapsed: 1.12e+01, train loss: 6.15985e-07, val loss: 1.36398e-06, min loss: 5.08551e-07\n",
      "Epoch: 1932200, elapsed: 1.11e+01, train loss: 5.15135e-07, val loss: 1.35207e-06, min loss: 5.08551e-07\n",
      "Epoch: 1932300, elapsed: 1.11e+01, train loss: 5.86948e-07, val loss: 1.35497e-06, min loss: 5.08551e-07\n",
      "Epoch: 1932400, elapsed: 1.07e+01, train loss: 7.20512e-07, val loss: 1.46809e-06, min loss: 5.08551e-07\n",
      "Epoch: 1932500, elapsed: 1.10e+01, train loss: 5.08821e-07, val loss: 1.31875e-06, min loss: 5.08551e-07\n",
      "Epoch: 1932600, elapsed: 1.11e+01, train loss: 5.08154e-07, val loss: 1.31744e-06, min loss: 5.08154e-07\n",
      "Epoch: 1932700, elapsed: 1.10e+01, train loss: 1.43369e-06, val loss: 2.36765e-06, min loss: 5.08154e-07\n",
      "Epoch: 1932800, elapsed: 1.10e+01, train loss: 9.95919e-07, val loss: 1.86465e-06, min loss: 5.08154e-07\n",
      "Epoch: 1932900, elapsed: 1.10e+01, train loss: 1.06856e-06, val loss: 1.97152e-06, min loss: 5.08154e-07\n",
      "Epoch: 1933000, elapsed: 1.09e+01, train loss: 2.06907e-06, val loss: 2.33999e-06, min loss: 5.08154e-07\n",
      "Epoch: 1933100, elapsed: 1.12e+01, train loss: 1.19932e-06, val loss: 2.27728e-06, min loss: 5.08154e-07\n",
      "Epoch: 1933200, elapsed: 1.10e+01, train loss: 5.07765e-07, val loss: 1.31488e-06, min loss: 5.07765e-07\n",
      "Epoch: 1933300, elapsed: 1.09e+01, train loss: 5.14504e-07, val loss: 1.33985e-06, min loss: 5.07765e-07\n",
      "Epoch: 1933400, elapsed: 1.12e+01, train loss: 3.06114e-06, val loss: 2.18503e-06, min loss: 5.07765e-07\n",
      "Epoch: 1933500, elapsed: 1.08e+01, train loss: 5.07769e-07, val loss: 1.31147e-06, min loss: 5.07765e-07\n",
      "Epoch: 1933600, elapsed: 1.10e+01, train loss: 5.27714e-07, val loss: 1.29172e-06, min loss: 5.07765e-07\n",
      "Epoch: 1933700, elapsed: 1.09e+01, train loss: 5.15500e-07, val loss: 1.34370e-06, min loss: 5.07765e-07\n",
      "Epoch: 1933800, elapsed: 1.07e+01, train loss: 5.63162e-07, val loss: 1.40236e-06, min loss: 5.07765e-07\n",
      "Epoch: 1933900, elapsed: 1.10e+01, train loss: 2.36391e-06, val loss: 4.11943e-06, min loss: 5.07765e-07\n",
      "Epoch: 1934000, elapsed: 1.10e+01, train loss: 5.07591e-07, val loss: 1.31541e-06, min loss: 5.07591e-07\n",
      "Epoch: 1934100, elapsed: 1.08e+01, train loss: 8.31013e-07, val loss: 1.78043e-06, min loss: 5.07591e-07\n",
      "Epoch: 1934200, elapsed: 1.10e+01, train loss: 7.04942e-07, val loss: 1.67952e-06, min loss: 5.07591e-07\n",
      "Epoch: 1934300, elapsed: 1.08e+01, train loss: 5.23949e-07, val loss: 1.39484e-06, min loss: 5.07591e-07\n",
      "Epoch: 1934400, elapsed: 1.09e+01, train loss: 5.11700e-07, val loss: 1.34060e-06, min loss: 5.07591e-07\n",
      "Epoch: 1934500, elapsed: 1.08e+01, train loss: 5.13018e-07, val loss: 1.32056e-06, min loss: 5.07591e-07\n",
      "Epoch: 1934600, elapsed: 1.07e+01, train loss: 5.11762e-07, val loss: 1.33826e-06, min loss: 5.07591e-07\n",
      "Epoch: 1934700, elapsed: 1.08e+01, train loss: 5.56534e-07, val loss: 1.37492e-06, min loss: 5.07591e-07\n",
      "Epoch: 1934800, elapsed: 1.10e+01, train loss: 6.08593e-07, val loss: 1.47579e-06, min loss: 5.07591e-07\n",
      "Epoch: 1934900, elapsed: 1.08e+01, train loss: 5.07235e-07, val loss: 1.31294e-06, min loss: 5.07235e-07\n",
      "Epoch: 1935000, elapsed: 1.07e+01, train loss: 5.07696e-07, val loss: 1.31252e-06, min loss: 5.07235e-07\n",
      "Epoch: 1935100, elapsed: 1.27e+01, train loss: 5.12910e-07, val loss: 1.33863e-06, min loss: 5.07235e-07\n",
      "Epoch: 1935200, elapsed: 1.08e+01, train loss: 5.18416e-07, val loss: 1.32918e-06, min loss: 5.07235e-07\n",
      "Epoch: 1935300, elapsed: 1.09e+01, train loss: 8.91874e-07, val loss: 1.88277e-06, min loss: 5.07235e-07\n",
      "Epoch: 1935400, elapsed: 1.08e+01, train loss: 7.25645e-07, val loss: 1.44919e-06, min loss: 5.07235e-07\n",
      "Epoch: 1935500, elapsed: 1.65e+01, train loss: 5.45235e-07, val loss: 1.32697e-06, min loss: 5.07235e-07\n",
      "Epoch: 1935600, elapsed: 1.10e+01, train loss: 5.28119e-07, val loss: 1.33226e-06, min loss: 5.07235e-07\n",
      "Epoch: 1935700, elapsed: 1.11e+01, train loss: 5.93336e-07, val loss: 1.52285e-06, min loss: 5.07235e-07\n",
      "Epoch: 1935800, elapsed: 1.10e+01, train loss: 7.23778e-07, val loss: 1.75480e-06, min loss: 5.07235e-07\n",
      "Epoch: 1935900, elapsed: 1.10e+01, train loss: 5.29524e-07, val loss: 1.35401e-06, min loss: 5.07235e-07\n",
      "Epoch: 1936000, elapsed: 1.09e+01, train loss: 2.73400e-06, val loss: 2.37864e-06, min loss: 5.07235e-07\n",
      "Epoch: 1936100, elapsed: 1.10e+01, train loss: 5.08008e-07, val loss: 1.29970e-06, min loss: 5.07235e-07\n",
      "Epoch: 1936200, elapsed: 1.09e+01, train loss: 5.08352e-07, val loss: 1.31185e-06, min loss: 5.07235e-07\n",
      "Epoch: 1936300, elapsed: 1.10e+01, train loss: 5.12305e-07, val loss: 1.33999e-06, min loss: 5.07235e-07\n",
      "Epoch: 1936400, elapsed: 1.09e+01, train loss: 8.73554e-07, val loss: 1.33034e-06, min loss: 5.07235e-07\n",
      "Epoch: 1936500, elapsed: 1.10e+01, train loss: 5.35360e-07, val loss: 1.31903e-06, min loss: 5.07235e-07\n",
      "Epoch: 1936600, elapsed: 1.09e+01, train loss: 5.13505e-07, val loss: 1.35270e-06, min loss: 5.07235e-07\n",
      "Epoch: 1936700, elapsed: 1.08e+01, train loss: 5.29001e-07, val loss: 1.32736e-06, min loss: 5.07235e-07\n",
      "Epoch: 1936800, elapsed: 1.07e+01, train loss: 6.35213e-07, val loss: 1.33844e-06, min loss: 5.07235e-07\n",
      "Epoch: 1936900, elapsed: 1.09e+01, train loss: 1.95504e-06, val loss: 3.38604e-06, min loss: 5.07235e-07\n",
      "Epoch: 1937000, elapsed: 1.09e+01, train loss: 5.27201e-07, val loss: 1.30186e-06, min loss: 5.07235e-07\n",
      "Epoch: 1937100, elapsed: 1.10e+01, train loss: 6.55632e-07, val loss: 1.53413e-06, min loss: 5.07235e-07\n",
      "Epoch: 1937200, elapsed: 1.06e+01, train loss: 8.10596e-07, val loss: 1.50106e-06, min loss: 5.07235e-07\n",
      "Epoch: 1937300, elapsed: 1.09e+01, train loss: 5.26158e-07, val loss: 1.28850e-06, min loss: 5.07235e-07\n",
      "Epoch: 1937400, elapsed: 1.09e+01, train loss: 5.12339e-07, val loss: 1.32772e-06, min loss: 5.07235e-07\n",
      "Epoch: 1937500, elapsed: 1.10e+01, train loss: 6.51141e-07, val loss: 1.59729e-06, min loss: 5.07235e-07\n",
      "Epoch: 1937600, elapsed: 1.09e+01, train loss: 6.76498e-07, val loss: 1.64926e-06, min loss: 5.07235e-07\n",
      "Epoch: 1937700, elapsed: 1.08e+01, train loss: 5.80750e-07, val loss: 1.45630e-06, min loss: 5.07235e-07\n",
      "Epoch: 1937800, elapsed: 1.08e+01, train loss: 5.19343e-07, val loss: 1.29080e-06, min loss: 5.07235e-07\n",
      "Epoch: 1937900, elapsed: 1.09e+01, train loss: 5.22989e-07, val loss: 1.33566e-06, min loss: 5.07235e-07\n",
      "Epoch: 1938000, elapsed: 1.09e+01, train loss: 5.30796e-07, val loss: 1.39091e-06, min loss: 5.07235e-07\n",
      "Epoch: 1938100, elapsed: 1.08e+01, train loss: 5.73305e-07, val loss: 1.42866e-06, min loss: 5.07235e-07\n",
      "Epoch: 1938200, elapsed: 1.08e+01, train loss: 1.11054e-06, val loss: 2.06835e-06, min loss: 5.07235e-07\n",
      "Epoch: 1938300, elapsed: 1.08e+01, train loss: 5.27701e-07, val loss: 1.28400e-06, min loss: 5.07235e-07\n",
      "Epoch: 1938400, elapsed: 1.10e+01, train loss: 5.27031e-07, val loss: 1.33210e-06, min loss: 5.07235e-07\n",
      "Epoch: 1938500, elapsed: 1.08e+01, train loss: 5.43528e-07, val loss: 1.28909e-06, min loss: 5.07235e-07\n",
      "Epoch: 1938600, elapsed: 1.11e+01, train loss: 1.32249e-06, val loss: 1.73289e-06, min loss: 5.07235e-07\n",
      "Epoch: 1938700, elapsed: 1.07e+01, train loss: 5.55291e-07, val loss: 1.37976e-06, min loss: 5.07235e-07\n",
      "Epoch: 1938800, elapsed: 1.09e+01, train loss: 5.06196e-07, val loss: 1.32088e-06, min loss: 5.06196e-07\n",
      "Epoch: 1938900, elapsed: 1.58e+01, train loss: 5.63243e-07, val loss: 1.31416e-06, min loss: 5.06196e-07\n",
      "Epoch: 1939000, elapsed: 1.12e+01, train loss: 5.08504e-07, val loss: 1.29711e-06, min loss: 5.06196e-07\n",
      "Epoch: 1939100, elapsed: 1.10e+01, train loss: 5.11227e-07, val loss: 1.31003e-06, min loss: 5.06196e-07\n",
      "Epoch: 1939200, elapsed: 1.13e+01, train loss: 6.17773e-07, val loss: 1.35178e-06, min loss: 5.06196e-07\n",
      "Epoch: 1939300, elapsed: 1.11e+01, train loss: 5.31660e-07, val loss: 1.32252e-06, min loss: 5.06196e-07\n",
      "Epoch: 1939400, elapsed: 1.10e+01, train loss: 5.32587e-07, val loss: 1.34319e-06, min loss: 5.06196e-07\n",
      "Epoch: 1939500, elapsed: 1.11e+01, train loss: 5.06064e-07, val loss: 1.32294e-06, min loss: 5.06064e-07\n",
      "Epoch: 1939600, elapsed: 1.10e+01, train loss: 5.19946e-07, val loss: 1.30334e-06, min loss: 5.06064e-07\n",
      "Epoch: 1939700, elapsed: 1.09e+01, train loss: 6.28182e-07, val loss: 1.56965e-06, min loss: 5.06064e-07\n",
      "Epoch: 1939800, elapsed: 1.10e+01, train loss: 5.06077e-07, val loss: 1.31654e-06, min loss: 5.06064e-07\n",
      "Epoch: 1939900, elapsed: 1.08e+01, train loss: 5.58155e-07, val loss: 1.34697e-06, min loss: 5.06064e-07\n",
      "Epoch: 1940000, elapsed: 1.09e+01, train loss: 5.12135e-07, val loss: 1.33337e-06, min loss: 5.06064e-07\n",
      "Epoch: 1940100, elapsed: 1.28e+01, train loss: 5.17094e-07, val loss: 1.35542e-06, min loss: 5.06064e-07\n",
      "Epoch: 1940200, elapsed: 1.08e+01, train loss: 1.10523e-06, val loss: 2.15808e-06, min loss: 5.06064e-07\n",
      "Epoch: 1940300, elapsed: 1.08e+01, train loss: 2.26271e-06, val loss: 3.46771e-06, min loss: 5.06064e-07\n",
      "Epoch: 1940400, elapsed: 1.09e+01, train loss: 9.27848e-07, val loss: 1.97026e-06, min loss: 5.06064e-07\n",
      "Epoch: 1940500, elapsed: 1.09e+01, train loss: 8.80365e-07, val loss: 1.47405e-06, min loss: 5.06064e-07\n",
      "Epoch: 1940600, elapsed: 1.11e+01, train loss: 6.55926e-07, val loss: 1.41701e-06, min loss: 5.06064e-07\n",
      "Epoch: 1940700, elapsed: 1.07e+01, train loss: 5.13327e-07, val loss: 1.37039e-06, min loss: 5.06064e-07\n",
      "Epoch: 1940800, elapsed: 1.10e+01, train loss: 7.09899e-07, val loss: 1.44778e-06, min loss: 5.06064e-07\n",
      "Epoch: 1940900, elapsed: 1.11e+01, train loss: 5.27207e-07, val loss: 1.36813e-06, min loss: 5.06064e-07\n",
      "Epoch: 1941000, elapsed: 1.10e+01, train loss: 5.06278e-07, val loss: 1.32450e-06, min loss: 5.06064e-07\n",
      "Epoch: 1941100, elapsed: 1.09e+01, train loss: 5.05801e-07, val loss: 1.31438e-06, min loss: 5.05801e-07\n",
      "Epoch: 1941200, elapsed: 1.09e+01, train loss: 5.31485e-07, val loss: 1.39173e-06, min loss: 5.05801e-07\n",
      "Epoch: 1941300, elapsed: 1.09e+01, train loss: 8.88893e-07, val loss: 1.63666e-06, min loss: 5.05801e-07\n",
      "Epoch: 1941400, elapsed: 1.10e+01, train loss: 5.30149e-07, val loss: 1.37948e-06, min loss: 5.05801e-07\n",
      "Epoch: 1941500, elapsed: 1.09e+01, train loss: 5.12553e-07, val loss: 1.29374e-06, min loss: 5.05801e-07\n",
      "Epoch: 1941600, elapsed: 1.08e+01, train loss: 7.52826e-07, val loss: 1.50486e-06, min loss: 5.05801e-07\n",
      "Epoch: 1941700, elapsed: 1.08e+01, train loss: 6.16765e-07, val loss: 1.35664e-06, min loss: 5.05801e-07\n",
      "Epoch: 1941800, elapsed: 1.08e+01, train loss: 9.69915e-07, val loss: 1.73994e-06, min loss: 5.05801e-07\n",
      "Epoch: 1941900, elapsed: 1.10e+01, train loss: 7.38500e-07, val loss: 1.31393e-06, min loss: 5.05801e-07\n",
      "Epoch: 1942000, elapsed: 1.09e+01, train loss: 7.60008e-07, val loss: 1.42220e-06, min loss: 5.05801e-07\n",
      "Epoch: 1942100, elapsed: 1.08e+01, train loss: 9.25742e-07, val loss: 1.70373e-06, min loss: 5.05801e-07\n",
      "Epoch: 1942200, elapsed: 1.09e+01, train loss: 6.78930e-07, val loss: 1.41417e-06, min loss: 5.05801e-07\n",
      "Epoch: 1942300, elapsed: 1.09e+01, train loss: 7.60497e-07, val loss: 1.53070e-06, min loss: 5.05801e-07\n",
      "Epoch: 1942400, elapsed: 1.65e+01, train loss: 1.02598e-06, val loss: 1.85274e-06, min loss: 5.05801e-07\n",
      "Epoch: 1942500, elapsed: 1.12e+01, train loss: 5.21458e-07, val loss: 1.35638e-06, min loss: 5.05801e-07\n",
      "Epoch: 1942600, elapsed: 1.11e+01, train loss: 5.12222e-07, val loss: 1.34712e-06, min loss: 5.05801e-07\n",
      "Epoch: 1942700, elapsed: 1.11e+01, train loss: 5.06164e-07, val loss: 1.33107e-06, min loss: 5.05801e-07\n",
      "Epoch: 1942800, elapsed: 1.10e+01, train loss: 5.13495e-07, val loss: 1.34786e-06, min loss: 5.05801e-07\n",
      "Epoch: 1942900, elapsed: 1.11e+01, train loss: 5.30942e-07, val loss: 1.31378e-06, min loss: 5.05801e-07\n",
      "Epoch: 1943000, elapsed: 1.11e+01, train loss: 7.70235e-07, val loss: 1.75440e-06, min loss: 5.05801e-07\n",
      "Epoch: 1943100, elapsed: 1.10e+01, train loss: 5.04592e-07, val loss: 1.32828e-06, min loss: 5.04592e-07\n",
      "Epoch: 1943200, elapsed: 1.11e+01, train loss: 5.12047e-07, val loss: 1.29253e-06, min loss: 5.04592e-07\n",
      "Epoch: 1943300, elapsed: 1.09e+01, train loss: 5.04882e-07, val loss: 1.32366e-06, min loss: 5.04592e-07\n",
      "Epoch: 1943400, elapsed: 1.09e+01, train loss: 5.05209e-07, val loss: 1.33319e-06, min loss: 5.04592e-07\n",
      "Epoch: 1943500, elapsed: 1.08e+01, train loss: 6.82055e-07, val loss: 1.42617e-06, min loss: 5.04592e-07\n",
      "Epoch: 1943600, elapsed: 1.09e+01, train loss: 5.04553e-07, val loss: 1.33337e-06, min loss: 5.04553e-07\n",
      "Epoch: 1943700, elapsed: 1.10e+01, train loss: 5.64945e-07, val loss: 1.45261e-06, min loss: 5.04553e-07\n",
      "Epoch: 1943800, elapsed: 1.09e+01, train loss: 5.04327e-07, val loss: 1.32755e-06, min loss: 5.04327e-07\n",
      "Epoch: 1943900, elapsed: 1.08e+01, train loss: 5.06696e-07, val loss: 1.33338e-06, min loss: 5.04327e-07\n",
      "Epoch: 1944000, elapsed: 1.08e+01, train loss: 3.81421e-06, val loss: 3.46046e-06, min loss: 5.04327e-07\n",
      "Epoch: 1944100, elapsed: 1.07e+01, train loss: 5.04403e-07, val loss: 1.32983e-06, min loss: 5.04327e-07\n",
      "Epoch: 1944200, elapsed: 1.11e+01, train loss: 5.60120e-07, val loss: 1.33020e-06, min loss: 5.04327e-07\n",
      "Epoch: 1944300, elapsed: 1.10e+01, train loss: 5.05192e-07, val loss: 1.32542e-06, min loss: 5.04327e-07\n",
      "Epoch: 1944400, elapsed: 1.11e+01, train loss: 5.22835e-07, val loss: 1.31972e-06, min loss: 5.04327e-07\n",
      "Epoch: 1944500, elapsed: 1.07e+01, train loss: 5.93247e-07, val loss: 1.48001e-06, min loss: 5.04327e-07\n",
      "Epoch: 1944600, elapsed: 1.11e+01, train loss: 5.21051e-07, val loss: 1.45559e-06, min loss: 5.04327e-07\n",
      "Epoch: 1944700, elapsed: 1.08e+01, train loss: 8.07121e-07, val loss: 1.62102e-06, min loss: 5.04327e-07\n",
      "Epoch: 1944800, elapsed: 1.09e+01, train loss: 8.85981e-07, val loss: 1.45291e-06, min loss: 5.04327e-07\n",
      "Epoch: 1944900, elapsed: 1.09e+01, train loss: 8.72941e-07, val loss: 1.73448e-06, min loss: 5.04327e-07\n",
      "Epoch: 1945000, elapsed: 1.09e+01, train loss: 5.04151e-07, val loss: 1.32644e-06, min loss: 5.04151e-07\n",
      "Epoch: 1945100, elapsed: 1.29e+01, train loss: 5.79562e-07, val loss: 1.51228e-06, min loss: 5.04151e-07\n",
      "Epoch: 1945200, elapsed: 1.07e+01, train loss: 5.11216e-07, val loss: 1.33591e-06, min loss: 5.04151e-07\n",
      "Epoch: 1945300, elapsed: 1.09e+01, train loss: 5.23846e-07, val loss: 1.31109e-06, min loss: 5.04151e-07\n",
      "Epoch: 1945400, elapsed: 1.08e+01, train loss: 6.08168e-07, val loss: 1.32875e-06, min loss: 5.04151e-07\n",
      "Epoch: 1945500, elapsed: 1.08e+01, train loss: 5.14765e-07, val loss: 1.32113e-06, min loss: 5.04151e-07\n",
      "Epoch: 1945600, elapsed: 1.09e+01, train loss: 5.22394e-07, val loss: 1.36604e-06, min loss: 5.04151e-07\n",
      "Epoch: 1945700, elapsed: 1.08e+01, train loss: 5.06775e-07, val loss: 1.34560e-06, min loss: 5.04151e-07\n",
      "Epoch: 1945800, elapsed: 1.62e+01, train loss: 5.07714e-07, val loss: 1.32712e-06, min loss: 5.04151e-07\n",
      "Epoch: 1945900, elapsed: 1.11e+01, train loss: 5.28546e-07, val loss: 1.32826e-06, min loss: 5.04151e-07\n",
      "Epoch: 1946000, elapsed: 1.10e+01, train loss: 5.04054e-07, val loss: 1.33240e-06, min loss: 5.04054e-07\n",
      "Epoch: 1946100, elapsed: 1.09e+01, train loss: 5.06399e-07, val loss: 1.32387e-06, min loss: 5.04054e-07\n",
      "Epoch: 1946200, elapsed: 1.09e+01, train loss: 5.30935e-07, val loss: 1.27831e-06, min loss: 5.04054e-07\n",
      "Epoch: 1946300, elapsed: 1.10e+01, train loss: 7.95452e-07, val loss: 1.56757e-06, min loss: 5.04054e-07\n",
      "Epoch: 1946400, elapsed: 1.10e+01, train loss: 5.08994e-07, val loss: 1.33218e-06, min loss: 5.04054e-07\n",
      "Epoch: 1946500, elapsed: 1.11e+01, train loss: 7.06939e-07, val loss: 1.62323e-06, min loss: 5.04054e-07\n",
      "Epoch: 1946600, elapsed: 1.08e+01, train loss: 5.22541e-07, val loss: 1.35199e-06, min loss: 5.04054e-07\n",
      "Epoch: 1946700, elapsed: 1.11e+01, train loss: 5.90756e-07, val loss: 1.35741e-06, min loss: 5.04054e-07\n",
      "Epoch: 1946800, elapsed: 1.11e+01, train loss: 5.96171e-07, val loss: 1.41970e-06, min loss: 5.04054e-07\n",
      "Epoch: 1946900, elapsed: 1.10e+01, train loss: 1.28642e-06, val loss: 2.47615e-06, min loss: 5.04054e-07\n",
      "Epoch: 1947000, elapsed: 1.09e+01, train loss: 5.03383e-07, val loss: 1.32989e-06, min loss: 5.03383e-07\n",
      "Epoch: 1947100, elapsed: 1.12e+01, train loss: 5.05195e-07, val loss: 1.31544e-06, min loss: 5.03383e-07\n",
      "Epoch: 1947200, elapsed: 1.09e+01, train loss: 5.83310e-07, val loss: 1.32605e-06, min loss: 5.03383e-07\n",
      "Epoch: 1947300, elapsed: 1.10e+01, train loss: 5.20609e-07, val loss: 1.36489e-06, min loss: 5.03383e-07\n",
      "Epoch: 1947400, elapsed: 1.11e+01, train loss: 5.08054e-07, val loss: 1.36299e-06, min loss: 5.03383e-07\n",
      "Epoch: 1947500, elapsed: 1.09e+01, train loss: 5.06186e-07, val loss: 1.33388e-06, min loss: 5.03383e-07\n",
      "Epoch: 1947600, elapsed: 1.10e+01, train loss: 5.43749e-07, val loss: 1.41400e-06, min loss: 5.03383e-07\n",
      "Epoch: 1947700, elapsed: 1.10e+01, train loss: 5.03649e-07, val loss: 1.33340e-06, min loss: 5.03383e-07\n",
      "Epoch: 1947800, elapsed: 1.09e+01, train loss: 5.06751e-07, val loss: 1.34666e-06, min loss: 5.03383e-07\n",
      "Epoch: 1947900, elapsed: 1.10e+01, train loss: 6.90233e-07, val loss: 1.71284e-06, min loss: 5.03383e-07\n",
      "Epoch: 1948000, elapsed: 1.08e+01, train loss: 5.03767e-07, val loss: 1.33559e-06, min loss: 5.03383e-07\n",
      "Epoch: 1948100, elapsed: 1.12e+01, train loss: 5.13703e-07, val loss: 1.37148e-06, min loss: 5.03383e-07\n",
      "Epoch: 1948200, elapsed: 1.10e+01, train loss: 1.65548e-06, val loss: 2.79182e-06, min loss: 5.03383e-07\n",
      "Epoch: 1948300, elapsed: 1.09e+01, train loss: 1.22656e-06, val loss: 2.05855e-06, min loss: 5.03383e-07\n",
      "Epoch: 1948400, elapsed: 1.08e+01, train loss: 7.24703e-07, val loss: 1.58096e-06, min loss: 5.03383e-07\n",
      "Epoch: 1948500, elapsed: 1.08e+01, train loss: 3.32700e-06, val loss: 3.01442e-06, min loss: 5.03383e-07\n",
      "Epoch: 1948600, elapsed: 1.10e+01, train loss: 5.02992e-07, val loss: 1.33129e-06, min loss: 5.02992e-07\n",
      "Epoch: 1948700, elapsed: 1.08e+01, train loss: 5.52976e-07, val loss: 1.45703e-06, min loss: 5.02992e-07\n",
      "Epoch: 1948800, elapsed: 1.08e+01, train loss: 5.02733e-07, val loss: 1.33500e-06, min loss: 5.02733e-07\n",
      "Epoch: 1948900, elapsed: 1.08e+01, train loss: 6.91126e-07, val loss: 1.74508e-06, min loss: 5.02733e-07\n",
      "Epoch: 1949000, elapsed: 1.09e+01, train loss: 5.02709e-07, val loss: 1.33458e-06, min loss: 5.02709e-07\n",
      "Epoch: 1949100, elapsed: 1.09e+01, train loss: 5.91816e-07, val loss: 1.48564e-06, min loss: 5.02709e-07\n",
      "Epoch: 1949200, elapsed: 1.09e+01, train loss: 5.06967e-07, val loss: 1.33495e-06, min loss: 5.02709e-07\n",
      "Epoch: 1949300, elapsed: 1.62e+01, train loss: 5.34958e-07, val loss: 1.40929e-06, min loss: 5.02709e-07\n",
      "Epoch: 1949400, elapsed: 1.13e+01, train loss: 5.02863e-07, val loss: 1.33246e-06, min loss: 5.02709e-07\n",
      "Epoch: 1949500, elapsed: 1.10e+01, train loss: 5.80889e-07, val loss: 1.33968e-06, min loss: 5.02709e-07\n",
      "Epoch: 1949600, elapsed: 1.11e+01, train loss: 5.07194e-07, val loss: 1.32706e-06, min loss: 5.02709e-07\n",
      "Epoch: 1949700, elapsed: 1.11e+01, train loss: 5.02629e-07, val loss: 1.33278e-06, min loss: 5.02629e-07\n",
      "Epoch: 1949800, elapsed: 1.10e+01, train loss: 5.98387e-07, val loss: 1.49002e-06, min loss: 5.02629e-07\n",
      "Epoch: 1949900, elapsed: 1.11e+01, train loss: 5.40189e-07, val loss: 1.28887e-06, min loss: 5.02629e-07\n",
      "Epoch: 1950000, elapsed: 1.10e+01, train loss: 6.66195e-07, val loss: 1.47852e-06, min loss: 5.02629e-07\n",
      "Epoch: 1950100, elapsed: 1.31e+01, train loss: 5.02410e-07, val loss: 1.33497e-06, min loss: 5.02410e-07\n",
      "Epoch: 1950200, elapsed: 1.08e+01, train loss: 5.23974e-07, val loss: 1.34623e-06, min loss: 5.02410e-07\n",
      "Epoch: 1950300, elapsed: 1.10e+01, train loss: 5.10418e-07, val loss: 1.32980e-06, min loss: 5.02410e-07\n",
      "Epoch: 1950400, elapsed: 1.11e+01, train loss: 5.11674e-07, val loss: 1.32935e-06, min loss: 5.02410e-07\n",
      "Epoch: 1950500, elapsed: 1.09e+01, train loss: 5.02743e-07, val loss: 1.33046e-06, min loss: 5.02410e-07\n",
      "Epoch: 1950600, elapsed: 1.09e+01, train loss: 5.69802e-07, val loss: 1.33242e-06, min loss: 5.02410e-07\n",
      "Epoch: 1950700, elapsed: 1.09e+01, train loss: 5.02133e-07, val loss: 1.33612e-06, min loss: 5.02133e-07\n",
      "Epoch: 1950800, elapsed: 1.09e+01, train loss: 5.09898e-07, val loss: 1.33699e-06, min loss: 5.02133e-07\n",
      "Epoch: 1950900, elapsed: 1.09e+01, train loss: 1.01169e-06, val loss: 1.65033e-06, min loss: 5.02133e-07\n",
      "Epoch: 1951000, elapsed: 1.09e+01, train loss: 5.42511e-07, val loss: 1.39475e-06, min loss: 5.02133e-07\n",
      "Epoch: 1951100, elapsed: 1.08e+01, train loss: 5.24473e-07, val loss: 1.30973e-06, min loss: 5.02133e-07\n",
      "Epoch: 1951200, elapsed: 1.08e+01, train loss: 2.89691e-06, val loss: 3.61892e-06, min loss: 5.02133e-07\n",
      "Epoch: 1951300, elapsed: 1.08e+01, train loss: 5.06216e-07, val loss: 1.33022e-06, min loss: 5.02133e-07\n",
      "Epoch: 1951400, elapsed: 1.09e+01, train loss: 5.87443e-07, val loss: 1.45068e-06, min loss: 5.02133e-07\n",
      "Epoch: 1951500, elapsed: 1.07e+01, train loss: 5.14426e-07, val loss: 1.35336e-06, min loss: 5.02133e-07\n",
      "Epoch: 1951600, elapsed: 1.08e+01, train loss: 5.21865e-07, val loss: 1.34857e-06, min loss: 5.02133e-07\n",
      "Epoch: 1951700, elapsed: 1.08e+01, train loss: 1.02949e-06, val loss: 1.57973e-06, min loss: 5.02133e-07\n",
      "Epoch: 1951800, elapsed: 1.11e+01, train loss: 5.04879e-07, val loss: 1.34859e-06, min loss: 5.02133e-07\n",
      "Epoch: 1951900, elapsed: 1.09e+01, train loss: 5.85757e-07, val loss: 1.50848e-06, min loss: 5.02133e-07\n",
      "Epoch: 1952000, elapsed: 1.09e+01, train loss: 5.78121e-07, val loss: 1.39801e-06, min loss: 5.02133e-07\n",
      "Epoch: 1952100, elapsed: 1.08e+01, train loss: 5.45037e-07, val loss: 1.31975e-06, min loss: 5.02133e-07\n",
      "Epoch: 1952200, elapsed: 1.09e+01, train loss: 5.35036e-07, val loss: 1.34696e-06, min loss: 5.02133e-07\n",
      "Epoch: 1952300, elapsed: 1.08e+01, train loss: 9.00647e-07, val loss: 1.66748e-06, min loss: 5.02133e-07\n",
      "Epoch: 1952400, elapsed: 1.09e+01, train loss: 2.04126e-06, val loss: 2.90309e-06, min loss: 5.02133e-07\n",
      "Epoch: 1952500, elapsed: 1.08e+01, train loss: 8.73379e-07, val loss: 1.77608e-06, min loss: 5.02133e-07\n",
      "Epoch: 1952600, elapsed: 1.07e+01, train loss: 6.53601e-07, val loss: 1.49792e-06, min loss: 5.02133e-07\n",
      "Epoch: 1952700, elapsed: 1.08e+01, train loss: 1.88934e-06, val loss: 1.81404e-06, min loss: 5.02133e-07\n",
      "Epoch: 1952800, elapsed: 1.64e+01, train loss: 5.01463e-07, val loss: 1.34013e-06, min loss: 5.01463e-07\n",
      "Epoch: 1952900, elapsed: 1.11e+01, train loss: 5.02652e-07, val loss: 1.34009e-06, min loss: 5.01463e-07\n",
      "Epoch: 1953000, elapsed: 1.10e+01, train loss: 5.10353e-07, val loss: 1.33487e-06, min loss: 5.01463e-07\n",
      "Epoch: 1953100, elapsed: 1.08e+01, train loss: 6.96216e-07, val loss: 1.50001e-06, min loss: 5.01463e-07\n",
      "Epoch: 1953200, elapsed: 1.09e+01, train loss: 1.38684e-06, val loss: 2.06015e-06, min loss: 5.01463e-07\n",
      "Epoch: 1953300, elapsed: 1.09e+01, train loss: 6.77560e-07, val loss: 1.75028e-06, min loss: 5.01463e-07\n",
      "Epoch: 1953400, elapsed: 1.10e+01, train loss: 5.03303e-07, val loss: 1.34050e-06, min loss: 5.01463e-07\n",
      "Epoch: 1953500, elapsed: 1.11e+01, train loss: 5.38340e-07, val loss: 1.35772e-06, min loss: 5.01463e-07\n",
      "Epoch: 1953600, elapsed: 1.10e+01, train loss: 5.01228e-07, val loss: 1.33983e-06, min loss: 5.01228e-07\n",
      "Epoch: 1953700, elapsed: 1.12e+01, train loss: 5.02920e-07, val loss: 1.33672e-06, min loss: 5.01228e-07\n",
      "Epoch: 1953800, elapsed: 1.10e+01, train loss: 6.56015e-07, val loss: 1.37759e-06, min loss: 5.01228e-07\n",
      "Epoch: 1953900, elapsed: 1.08e+01, train loss: 6.13896e-07, val loss: 1.47500e-06, min loss: 5.01228e-07\n",
      "Epoch: 1954000, elapsed: 1.08e+01, train loss: 5.34935e-07, val loss: 1.36871e-06, min loss: 5.01228e-07\n",
      "Epoch: 1954100, elapsed: 1.10e+01, train loss: 5.19830e-07, val loss: 1.39046e-06, min loss: 5.01228e-07\n",
      "Epoch: 1954200, elapsed: 1.08e+01, train loss: 5.19965e-07, val loss: 1.40132e-06, min loss: 5.01228e-07\n",
      "Epoch: 1954300, elapsed: 1.09e+01, train loss: 6.66304e-07, val loss: 1.43195e-06, min loss: 5.01228e-07\n",
      "Epoch: 1954400, elapsed: 1.09e+01, train loss: 7.67660e-07, val loss: 1.71254e-06, min loss: 5.01228e-07\n",
      "Epoch: 1954500, elapsed: 1.10e+01, train loss: 5.06289e-07, val loss: 1.34080e-06, min loss: 5.01228e-07\n",
      "Epoch: 1954600, elapsed: 1.07e+01, train loss: 5.05969e-07, val loss: 1.35456e-06, min loss: 5.01228e-07\n",
      "Epoch: 1954700, elapsed: 1.08e+01, train loss: 5.04663e-07, val loss: 1.35655e-06, min loss: 5.01228e-07\n",
      "Epoch: 1954800, elapsed: 1.07e+01, train loss: 5.18064e-07, val loss: 1.39203e-06, min loss: 5.01228e-07\n",
      "Epoch: 1954900, elapsed: 1.08e+01, train loss: 9.00350e-07, val loss: 1.61382e-06, min loss: 5.01228e-07\n",
      "Epoch: 1955000, elapsed: 1.09e+01, train loss: 5.20882e-07, val loss: 1.33428e-06, min loss: 5.01228e-07\n",
      "Epoch: 1955100, elapsed: 1.29e+01, train loss: 5.00926e-07, val loss: 1.34314e-06, min loss: 5.00926e-07\n",
      "Epoch: 1955200, elapsed: 1.08e+01, train loss: 5.07984e-07, val loss: 1.31117e-06, min loss: 5.00926e-07\n",
      "Epoch: 1955300, elapsed: 1.09e+01, train loss: 6.48401e-07, val loss: 1.40889e-06, min loss: 5.00926e-07\n",
      "Epoch: 1955400, elapsed: 1.08e+01, train loss: 8.27232e-07, val loss: 2.17084e-06, min loss: 5.00926e-07\n",
      "Epoch: 1955500, elapsed: 1.04e+01, train loss: 5.00716e-07, val loss: 1.34133e-06, min loss: 5.00716e-07\n",
      "Epoch: 1955600, elapsed: 1.07e+01, train loss: 5.05519e-07, val loss: 1.32940e-06, min loss: 5.00716e-07\n",
      "Epoch: 1955700, elapsed: 1.10e+01, train loss: 5.19678e-07, val loss: 1.41410e-06, min loss: 5.00716e-07\n",
      "Epoch: 1955800, elapsed: 1.07e+01, train loss: 6.77732e-07, val loss: 1.51328e-06, min loss: 5.00716e-07\n",
      "Epoch: 1955900, elapsed: 1.07e+01, train loss: 5.63460e-07, val loss: 1.38001e-06, min loss: 5.00716e-07\n",
      "Epoch: 1956000, elapsed: 1.08e+01, train loss: 6.42734e-07, val loss: 1.51114e-06, min loss: 5.00716e-07\n",
      "Epoch: 1956100, elapsed: 1.08e+01, train loss: 5.01301e-07, val loss: 1.33546e-06, min loss: 5.00716e-07\n",
      "Epoch: 1956200, elapsed: 1.08e+01, train loss: 5.01591e-07, val loss: 1.34974e-06, min loss: 5.00716e-07\n",
      "Epoch: 1956300, elapsed: 1.67e+01, train loss: 6.05626e-07, val loss: 1.49170e-06, min loss: 5.00716e-07\n",
      "Epoch: 1956400, elapsed: 1.11e+01, train loss: 5.00787e-07, val loss: 1.34093e-06, min loss: 5.00716e-07\n",
      "Epoch: 1956500, elapsed: 1.11e+01, train loss: 5.02328e-07, val loss: 1.33929e-06, min loss: 5.00716e-07\n",
      "Epoch: 1956600, elapsed: 1.11e+01, train loss: 9.59459e-07, val loss: 1.33186e-06, min loss: 5.00716e-07\n",
      "Epoch: 1956700, elapsed: 1.11e+01, train loss: 5.00653e-07, val loss: 1.34755e-06, min loss: 5.00653e-07\n",
      "Epoch: 1956800, elapsed: 1.08e+01, train loss: 5.01629e-07, val loss: 1.33430e-06, min loss: 5.00653e-07\n",
      "Epoch: 1956900, elapsed: 1.10e+01, train loss: 6.64383e-07, val loss: 1.49414e-06, min loss: 5.00653e-07\n",
      "Epoch: 1957000, elapsed: 1.10e+01, train loss: 7.61234e-07, val loss: 1.64744e-06, min loss: 5.00653e-07\n",
      "Epoch: 1957100, elapsed: 1.10e+01, train loss: 5.00951e-07, val loss: 1.34872e-06, min loss: 5.00653e-07\n",
      "Epoch: 1957200, elapsed: 1.09e+01, train loss: 5.00268e-07, val loss: 1.34706e-06, min loss: 5.00268e-07\n",
      "Epoch: 1957300, elapsed: 1.08e+01, train loss: 5.03403e-07, val loss: 1.34300e-06, min loss: 5.00268e-07\n",
      "Epoch: 1957400, elapsed: 1.06e+01, train loss: 5.58346e-07, val loss: 1.46239e-06, min loss: 5.00268e-07\n",
      "Epoch: 1957500, elapsed: 1.08e+01, train loss: 5.00293e-07, val loss: 1.34457e-06, min loss: 5.00268e-07\n",
      "Epoch: 1957600, elapsed: 1.08e+01, train loss: 5.04352e-07, val loss: 1.33981e-06, min loss: 5.00268e-07\n",
      "Epoch: 1957700, elapsed: 1.10e+01, train loss: 1.06981e-06, val loss: 2.09514e-06, min loss: 5.00268e-07\n",
      "Epoch: 1957800, elapsed: 1.08e+01, train loss: 6.21040e-07, val loss: 1.38858e-06, min loss: 5.00268e-07\n",
      "Epoch: 1957900, elapsed: 1.08e+01, train loss: 5.12870e-07, val loss: 1.34120e-06, min loss: 5.00268e-07\n",
      "Epoch: 1958000, elapsed: 1.07e+01, train loss: 5.72302e-07, val loss: 1.39421e-06, min loss: 5.00268e-07\n",
      "Epoch: 1958100, elapsed: 1.06e+01, train loss: 6.32891e-07, val loss: 1.52165e-06, min loss: 5.00268e-07\n",
      "Epoch: 1958200, elapsed: 1.08e+01, train loss: 5.00149e-07, val loss: 1.34543e-06, min loss: 5.00149e-07\n",
      "Epoch: 1958300, elapsed: 1.09e+01, train loss: 5.17016e-07, val loss: 1.37687e-06, min loss: 5.00149e-07\n",
      "Epoch: 1958400, elapsed: 1.08e+01, train loss: 6.57014e-07, val loss: 1.43650e-06, min loss: 5.00149e-07\n",
      "Epoch: 1958500, elapsed: 1.09e+01, train loss: 5.01476e-07, val loss: 1.34613e-06, min loss: 5.00149e-07\n",
      "Epoch: 1958600, elapsed: 1.07e+01, train loss: 5.01606e-07, val loss: 1.35584e-06, min loss: 5.00149e-07\n",
      "Epoch: 1958700, elapsed: 1.07e+01, train loss: 5.39719e-07, val loss: 1.36880e-06, min loss: 5.00149e-07\n",
      "Epoch: 1958800, elapsed: 1.04e+01, train loss: 5.34675e-07, val loss: 1.35909e-06, min loss: 5.00149e-07\n",
      "Epoch: 1958900, elapsed: 1.07e+01, train loss: 1.22808e-06, val loss: 2.35800e-06, min loss: 5.00149e-07\n",
      "Epoch: 1959000, elapsed: 1.08e+01, train loss: 2.37561e-06, val loss: 3.65799e-06, min loss: 5.00149e-07\n",
      "Epoch: 1959100, elapsed: 1.06e+01, train loss: 5.17870e-07, val loss: 1.40668e-06, min loss: 5.00149e-07\n",
      "Epoch: 1959200, elapsed: 1.07e+01, train loss: 5.01283e-07, val loss: 1.35773e-06, min loss: 5.00149e-07\n",
      "Epoch: 1959300, elapsed: 1.07e+01, train loss: 5.13107e-07, val loss: 1.34175e-06, min loss: 5.00149e-07\n",
      "Epoch: 1959400, elapsed: 1.08e+01, train loss: 7.55944e-07, val loss: 1.52930e-06, min loss: 5.00149e-07\n",
      "Epoch: 1959500, elapsed: 1.07e+01, train loss: 5.03910e-07, val loss: 1.34225e-06, min loss: 5.00149e-07\n",
      "Epoch: 1959600, elapsed: 1.07e+01, train loss: 5.01230e-07, val loss: 1.33852e-06, min loss: 5.00149e-07\n",
      "Epoch: 1959700, elapsed: 1.08e+01, train loss: 5.05794e-07, val loss: 1.34523e-06, min loss: 5.00149e-07\n",
      "Epoch: 1959800, elapsed: 1.65e+01, train loss: 5.01645e-07, val loss: 1.33878e-06, min loss: 5.00149e-07\n",
      "Epoch: 1959900, elapsed: 1.12e+01, train loss: 5.02662e-07, val loss: 1.34963e-06, min loss: 5.00149e-07\n",
      "Epoch: 1960000, elapsed: 1.10e+01, train loss: 5.39331e-07, val loss: 1.41835e-06, min loss: 5.00149e-07\n",
      "Epoch: 1960100, elapsed: 1.29e+01, train loss: 5.04863e-07, val loss: 1.37920e-06, min loss: 5.00149e-07\n",
      "Epoch: 1960200, elapsed: 1.11e+01, train loss: 5.33338e-07, val loss: 1.33647e-06, min loss: 5.00149e-07\n",
      "Epoch: 1960300, elapsed: 1.11e+01, train loss: 8.61932e-07, val loss: 1.73325e-06, min loss: 5.00149e-07\n",
      "Epoch: 1960400, elapsed: 1.10e+01, train loss: 8.89810e-07, val loss: 1.57068e-06, min loss: 5.00149e-07\n",
      "Epoch: 1960500, elapsed: 1.11e+01, train loss: 5.19086e-07, val loss: 1.40012e-06, min loss: 5.00149e-07\n",
      "Epoch: 1960600, elapsed: 1.10e+01, train loss: 5.96863e-07, val loss: 1.46370e-06, min loss: 5.00149e-07\n",
      "Epoch: 1960700, elapsed: 1.13e+01, train loss: 6.09274e-07, val loss: 1.64403e-06, min loss: 5.00149e-07\n",
      "Epoch: 1960800, elapsed: 1.13e+01, train loss: 4.99397e-07, val loss: 1.35437e-06, min loss: 4.99397e-07\n",
      "Epoch: 1960900, elapsed: 1.10e+01, train loss: 5.26833e-07, val loss: 1.43626e-06, min loss: 4.99397e-07\n",
      "Epoch: 1961000, elapsed: 1.08e+01, train loss: 5.02278e-07, val loss: 1.34358e-06, min loss: 4.99397e-07\n",
      "Epoch: 1961100, elapsed: 1.10e+01, train loss: 4.99890e-07, val loss: 1.35199e-06, min loss: 4.99397e-07\n",
      "Epoch: 1961200, elapsed: 1.10e+01, train loss: 2.13378e-06, val loss: 3.25941e-06, min loss: 4.99397e-07\n",
      "Epoch: 1961300, elapsed: 1.11e+01, train loss: 5.00009e-07, val loss: 1.34667e-06, min loss: 4.99397e-07\n",
      "Epoch: 1961400, elapsed: 1.09e+01, train loss: 4.99715e-07, val loss: 1.35182e-06, min loss: 4.99397e-07\n",
      "Epoch: 1961500, elapsed: 1.09e+01, train loss: 1.18950e-06, val loss: 2.32192e-06, min loss: 4.99397e-07\n",
      "Epoch: 1961600, elapsed: 1.10e+01, train loss: 5.26057e-07, val loss: 1.38033e-06, min loss: 4.99397e-07\n",
      "Epoch: 1961700, elapsed: 1.10e+01, train loss: 5.03417e-07, val loss: 1.36739e-06, min loss: 4.99397e-07\n",
      "Epoch: 1961800, elapsed: 1.10e+01, train loss: 6.51158e-07, val loss: 1.53700e-06, min loss: 4.99397e-07\n",
      "Epoch: 1961900, elapsed: 1.09e+01, train loss: 5.04715e-07, val loss: 1.37272e-06, min loss: 4.99397e-07\n",
      "Epoch: 1962000, elapsed: 1.09e+01, train loss: 5.82186e-07, val loss: 1.49438e-06, min loss: 4.99397e-07\n",
      "Epoch: 1962100, elapsed: 1.08e+01, train loss: 5.89980e-07, val loss: 1.33778e-06, min loss: 4.99397e-07\n",
      "Epoch: 1962200, elapsed: 1.09e+01, train loss: 6.76922e-07, val loss: 1.54663e-06, min loss: 4.99397e-07\n",
      "Epoch: 1962300, elapsed: 1.09e+01, train loss: 5.68498e-07, val loss: 1.51132e-06, min loss: 4.99397e-07\n",
      "Epoch: 1962400, elapsed: 1.08e+01, train loss: 6.61325e-07, val loss: 1.40796e-06, min loss: 4.99397e-07\n",
      "Epoch: 1962500, elapsed: 1.08e+01, train loss: 5.07591e-07, val loss: 1.33993e-06, min loss: 4.99397e-07\n",
      "Epoch: 1962600, elapsed: 1.09e+01, train loss: 5.36320e-07, val loss: 1.33622e-06, min loss: 4.99397e-07\n",
      "Epoch: 1962700, elapsed: 1.09e+01, train loss: 1.00305e-06, val loss: 1.91294e-06, min loss: 4.99397e-07\n",
      "Epoch: 1962800, elapsed: 1.09e+01, train loss: 5.09357e-07, val loss: 1.39236e-06, min loss: 4.99397e-07\n",
      "Epoch: 1962900, elapsed: 1.07e+01, train loss: 5.00143e-07, val loss: 1.35753e-06, min loss: 4.99397e-07\n",
      "Epoch: 1963000, elapsed: 1.09e+01, train loss: 5.01171e-07, val loss: 1.33236e-06, min loss: 4.99397e-07\n",
      "Epoch: 1963100, elapsed: 1.09e+01, train loss: 5.09381e-07, val loss: 1.39922e-06, min loss: 4.99397e-07\n",
      "Epoch: 1963200, elapsed: 1.60e+01, train loss: 5.92241e-07, val loss: 1.41531e-06, min loss: 4.99397e-07\n",
      "Epoch: 1963300, elapsed: 1.10e+01, train loss: 4.98151e-07, val loss: 1.35214e-06, min loss: 4.98151e-07\n",
      "Epoch: 1963400, elapsed: 1.10e+01, train loss: 5.03324e-07, val loss: 1.36297e-06, min loss: 4.98151e-07\n",
      "Epoch: 1963500, elapsed: 1.09e+01, train loss: 5.15474e-07, val loss: 1.40128e-06, min loss: 4.98151e-07\n",
      "Epoch: 1963600, elapsed: 1.10e+01, train loss: 5.05108e-07, val loss: 1.35154e-06, min loss: 4.98151e-07\n",
      "Epoch: 1963700, elapsed: 1.10e+01, train loss: 6.16752e-07, val loss: 1.46738e-06, min loss: 4.98151e-07\n",
      "Epoch: 1963800, elapsed: 1.10e+01, train loss: 4.98125e-07, val loss: 1.35999e-06, min loss: 4.98125e-07\n",
      "Epoch: 1963900, elapsed: 1.08e+01, train loss: 5.00111e-07, val loss: 1.34231e-06, min loss: 4.98125e-07\n",
      "Epoch: 1964000, elapsed: 1.11e+01, train loss: 5.58066e-07, val loss: 1.50297e-06, min loss: 4.98125e-07\n",
      "Epoch: 1964100, elapsed: 1.10e+01, train loss: 4.98354e-07, val loss: 1.34952e-06, min loss: 4.98125e-07\n",
      "Epoch: 1964200, elapsed: 1.10e+01, train loss: 5.92640e-07, val loss: 1.51410e-06, min loss: 4.98125e-07\n",
      "Epoch: 1964300, elapsed: 1.11e+01, train loss: 5.00119e-07, val loss: 1.34371e-06, min loss: 4.98125e-07\n",
      "Epoch: 1964400, elapsed: 1.08e+01, train loss: 5.17615e-07, val loss: 1.40872e-06, min loss: 4.98125e-07\n",
      "Epoch: 1964500, elapsed: 1.09e+01, train loss: 5.08519e-07, val loss: 1.37528e-06, min loss: 4.98125e-07\n",
      "Epoch: 1964600, elapsed: 1.09e+01, train loss: 5.00513e-07, val loss: 1.35618e-06, min loss: 4.98125e-07\n",
      "Epoch: 1964700, elapsed: 1.09e+01, train loss: 4.98646e-07, val loss: 1.34476e-06, min loss: 4.98125e-07\n",
      "Epoch: 1964800, elapsed: 1.10e+01, train loss: 5.01806e-07, val loss: 1.34337e-06, min loss: 4.98125e-07\n",
      "Epoch: 1964900, elapsed: 1.09e+01, train loss: 5.16279e-07, val loss: 1.36166e-06, min loss: 4.98125e-07\n",
      "Epoch: 1965000, elapsed: 1.09e+01, train loss: 5.62865e-07, val loss: 1.51872e-06, min loss: 4.98125e-07\n",
      "Epoch: 1965100, elapsed: 1.28e+01, train loss: 4.97997e-07, val loss: 1.35791e-06, min loss: 4.97997e-07\n",
      "Epoch: 1965200, elapsed: 1.07e+01, train loss: 4.99634e-07, val loss: 1.35086e-06, min loss: 4.97997e-07\n",
      "Epoch: 1965300, elapsed: 1.08e+01, train loss: 1.24495e-06, val loss: 1.86447e-06, min loss: 4.97997e-07\n",
      "Epoch: 1965400, elapsed: 1.07e+01, train loss: 7.52533e-07, val loss: 1.48847e-06, min loss: 4.97997e-07\n",
      "Epoch: 1965500, elapsed: 1.09e+01, train loss: 4.98744e-07, val loss: 1.35311e-06, min loss: 4.97997e-07\n",
      "Epoch: 1965600, elapsed: 1.07e+01, train loss: 5.54271e-07, val loss: 1.63110e-06, min loss: 4.97997e-07\n",
      "Epoch: 1965700, elapsed: 1.09e+01, train loss: 4.97963e-07, val loss: 1.35276e-06, min loss: 4.97963e-07\n",
      "Epoch: 1965800, elapsed: 1.07e+01, train loss: 4.97888e-07, val loss: 1.35944e-06, min loss: 4.97888e-07\n",
      "Epoch: 1965900, elapsed: 1.06e+01, train loss: 7.56763e-07, val loss: 1.58054e-06, min loss: 4.97888e-07\n",
      "Epoch: 1966000, elapsed: 1.08e+01, train loss: 6.58927e-07, val loss: 1.65386e-06, min loss: 4.97888e-07\n",
      "Epoch: 1966100, elapsed: 1.07e+01, train loss: 4.97945e-07, val loss: 1.35352e-06, min loss: 4.97888e-07\n",
      "Epoch: 1966200, elapsed: 1.09e+01, train loss: 5.26485e-07, val loss: 1.36926e-06, min loss: 4.97888e-07\n",
      "Epoch: 1966300, elapsed: 1.09e+01, train loss: 7.01157e-07, val loss: 1.71526e-06, min loss: 4.97888e-07\n",
      "Epoch: 1966400, elapsed: 1.06e+01, train loss: 7.88357e-07, val loss: 1.88355e-06, min loss: 4.97888e-07\n",
      "Epoch: 1966500, elapsed: 1.10e+01, train loss: 1.12414e-06, val loss: 1.91393e-06, min loss: 4.97888e-07\n",
      "Epoch: 1966600, elapsed: 1.07e+01, train loss: 5.90420e-07, val loss: 1.54420e-06, min loss: 4.97888e-07\n",
      "Epoch: 1966700, elapsed: 1.60e+01, train loss: 5.61313e-07, val loss: 1.45393e-06, min loss: 4.97888e-07\n",
      "Epoch: 1966800, elapsed: 1.11e+01, train loss: 8.19127e-07, val loss: 1.55947e-06, min loss: 4.97888e-07\n",
      "Epoch: 1966900, elapsed: 1.10e+01, train loss: 9.47922e-07, val loss: 1.89957e-06, min loss: 4.97888e-07\n",
      "Epoch: 1967000, elapsed: 1.12e+01, train loss: 5.00321e-07, val loss: 1.35634e-06, min loss: 4.97888e-07\n",
      "Epoch: 1967100, elapsed: 1.11e+01, train loss: 4.97374e-07, val loss: 1.35242e-06, min loss: 4.97374e-07\n",
      "Epoch: 1967200, elapsed: 1.11e+01, train loss: 5.21008e-07, val loss: 1.39848e-06, min loss: 4.97374e-07\n",
      "Epoch: 1967300, elapsed: 1.10e+01, train loss: 1.21864e-06, val loss: 1.44864e-06, min loss: 4.97374e-07\n",
      "Epoch: 1967400, elapsed: 1.10e+01, train loss: 4.96885e-07, val loss: 1.35935e-06, min loss: 4.96885e-07\n",
      "Epoch: 1967500, elapsed: 1.10e+01, train loss: 5.12796e-07, val loss: 1.35824e-06, min loss: 4.96885e-07\n",
      "Epoch: 1967600, elapsed: 1.09e+01, train loss: 9.23256e-07, val loss: 1.73937e-06, min loss: 4.96885e-07\n",
      "Epoch: 1967700, elapsed: 1.11e+01, train loss: 4.97158e-07, val loss: 1.36350e-06, min loss: 4.96885e-07\n",
      "Epoch: 1967800, elapsed: 1.10e+01, train loss: 5.00072e-07, val loss: 1.36094e-06, min loss: 4.96885e-07\n",
      "Epoch: 1967900, elapsed: 1.08e+01, train loss: 1.64120e-06, val loss: 2.10944e-06, min loss: 4.96885e-07\n",
      "Epoch: 1968000, elapsed: 1.09e+01, train loss: 4.97120e-07, val loss: 1.35991e-06, min loss: 4.96885e-07\n",
      "Epoch: 1968100, elapsed: 1.11e+01, train loss: 4.97485e-07, val loss: 1.36594e-06, min loss: 4.96885e-07\n",
      "Epoch: 1968200, elapsed: 1.09e+01, train loss: 6.18606e-07, val loss: 1.38513e-06, min loss: 4.96885e-07\n",
      "Epoch: 1968300, elapsed: 1.11e+01, train loss: 4.96924e-07, val loss: 1.35966e-06, min loss: 4.96885e-07\n",
      "Epoch: 1968400, elapsed: 1.10e+01, train loss: 4.98955e-07, val loss: 1.37081e-06, min loss: 4.96885e-07\n",
      "Epoch: 1968500, elapsed: 1.08e+01, train loss: 6.93681e-07, val loss: 1.67472e-06, min loss: 4.96885e-07\n",
      "Epoch: 1968600, elapsed: 1.11e+01, train loss: 5.78185e-07, val loss: 1.36248e-06, min loss: 4.96885e-07\n",
      "Epoch: 1968700, elapsed: 1.09e+01, train loss: 5.38603e-07, val loss: 1.38353e-06, min loss: 4.96885e-07\n",
      "Epoch: 1968800, elapsed: 1.10e+01, train loss: 5.32010e-07, val loss: 1.38534e-06, min loss: 4.96885e-07\n",
      "Epoch: 1968900, elapsed: 1.09e+01, train loss: 8.03479e-07, val loss: 1.44172e-06, min loss: 4.96885e-07\n",
      "Epoch: 1969000, elapsed: 1.08e+01, train loss: 4.99213e-07, val loss: 1.34944e-06, min loss: 4.96885e-07\n",
      "Epoch: 1969100, elapsed: 1.09e+01, train loss: 4.97446e-07, val loss: 1.35161e-06, min loss: 4.96885e-07\n",
      "Epoch: 1969200, elapsed: 1.10e+01, train loss: 5.81605e-07, val loss: 1.36095e-06, min loss: 4.96885e-07\n",
      "Epoch: 1969300, elapsed: 1.09e+01, train loss: 7.39817e-07, val loss: 2.01661e-06, min loss: 4.96885e-07\n",
      "Epoch: 1969400, elapsed: 1.10e+01, train loss: 4.96421e-07, val loss: 1.35852e-06, min loss: 4.96421e-07\n",
      "Epoch: 1969500, elapsed: 1.10e+01, train loss: 5.04581e-07, val loss: 1.37125e-06, min loss: 4.96421e-07\n",
      "Epoch: 1969600, elapsed: 1.10e+01, train loss: 5.02369e-07, val loss: 1.38259e-06, min loss: 4.96421e-07\n",
      "Epoch: 1969700, elapsed: 1.10e+01, train loss: 7.44758e-07, val loss: 1.40270e-06, min loss: 4.96421e-07\n",
      "Epoch: 1969800, elapsed: 1.09e+01, train loss: 1.75649e-06, val loss: 2.09495e-06, min loss: 4.96421e-07\n",
      "Epoch: 1969900, elapsed: 1.09e+01, train loss: 5.16558e-07, val loss: 1.40423e-06, min loss: 4.96421e-07\n",
      "Epoch: 1970000, elapsed: 1.10e+01, train loss: 4.96196e-07, val loss: 1.36047e-06, min loss: 4.96196e-07\n",
      "Epoch: 1970100, elapsed: 1.28e+01, train loss: 5.11140e-07, val loss: 1.35164e-06, min loss: 4.96196e-07\n",
      "Epoch: 1970200, elapsed: 1.64e+01, train loss: 5.28258e-07, val loss: 1.41867e-06, min loss: 4.96196e-07\n",
      "Epoch: 1970300, elapsed: 1.13e+01, train loss: 6.17013e-07, val loss: 1.44140e-06, min loss: 4.96196e-07\n",
      "Epoch: 1970400, elapsed: 1.09e+01, train loss: 1.74503e-06, val loss: 2.42536e-06, min loss: 4.96196e-07\n",
      "Epoch: 1970500, elapsed: 1.09e+01, train loss: 1.05233e-06, val loss: 2.24621e-06, min loss: 4.96196e-07\n",
      "Epoch: 1970600, elapsed: 1.10e+01, train loss: 4.98108e-07, val loss: 1.35556e-06, min loss: 4.96196e-07\n",
      "Epoch: 1970700, elapsed: 1.11e+01, train loss: 4.97168e-07, val loss: 1.37429e-06, min loss: 4.96196e-07\n",
      "Epoch: 1970800, elapsed: 1.10e+01, train loss: 5.30803e-07, val loss: 1.41881e-06, min loss: 4.96196e-07\n",
      "Epoch: 1970900, elapsed: 1.09e+01, train loss: 8.72704e-07, val loss: 1.45327e-06, min loss: 4.96196e-07\n",
      "Epoch: 1971000, elapsed: 1.10e+01, train loss: 5.75757e-07, val loss: 1.43296e-06, min loss: 4.96196e-07\n",
      "Epoch: 1971100, elapsed: 1.10e+01, train loss: 4.98736e-07, val loss: 1.36181e-06, min loss: 4.96196e-07\n",
      "Epoch: 1971200, elapsed: 1.10e+01, train loss: 5.04013e-07, val loss: 1.35953e-06, min loss: 4.96196e-07\n",
      "Epoch: 1971300, elapsed: 1.09e+01, train loss: 4.97388e-07, val loss: 1.37608e-06, min loss: 4.96196e-07\n",
      "Epoch: 1971400, elapsed: 1.09e+01, train loss: 4.96321e-07, val loss: 1.35170e-06, min loss: 4.96196e-07\n",
      "Epoch: 1971500, elapsed: 1.10e+01, train loss: 5.05017e-07, val loss: 1.34756e-06, min loss: 4.96196e-07\n",
      "Epoch: 1971600, elapsed: 1.10e+01, train loss: 5.33748e-07, val loss: 1.37861e-06, min loss: 4.96196e-07\n",
      "Epoch: 1971700, elapsed: 1.11e+01, train loss: 6.51006e-07, val loss: 1.60342e-06, min loss: 4.96196e-07\n",
      "Epoch: 1971800, elapsed: 1.09e+01, train loss: 1.55770e-06, val loss: 1.87902e-06, min loss: 4.96196e-07\n",
      "Epoch: 1971900, elapsed: 1.10e+01, train loss: 7.01314e-07, val loss: 1.50163e-06, min loss: 4.96196e-07\n",
      "Epoch: 1972000, elapsed: 1.09e+01, train loss: 1.49403e-06, val loss: 2.74550e-06, min loss: 4.96196e-07\n",
      "Epoch: 1972100, elapsed: 1.11e+01, train loss: 4.95495e-07, val loss: 1.36387e-06, min loss: 4.95495e-07\n",
      "Epoch: 1972200, elapsed: 1.09e+01, train loss: 4.99027e-07, val loss: 1.36207e-06, min loss: 4.95495e-07\n",
      "Epoch: 1972300, elapsed: 1.09e+01, train loss: 6.23097e-07, val loss: 1.50814e-06, min loss: 4.95495e-07\n",
      "Epoch: 1972400, elapsed: 1.10e+01, train loss: 4.95667e-07, val loss: 1.35929e-06, min loss: 4.95495e-07\n",
      "Epoch: 1972500, elapsed: 1.11e+01, train loss: 5.00075e-07, val loss: 1.33249e-06, min loss: 4.95495e-07\n",
      "Epoch: 1972600, elapsed: 1.10e+01, train loss: 4.95662e-07, val loss: 1.36991e-06, min loss: 4.95495e-07\n",
      "Epoch: 1972700, elapsed: 1.08e+01, train loss: 6.48946e-07, val loss: 1.55754e-06, min loss: 4.95495e-07\n",
      "Epoch: 1972800, elapsed: 1.08e+01, train loss: 5.35187e-07, val loss: 1.45368e-06, min loss: 4.95495e-07\n",
      "Epoch: 1972900, elapsed: 1.09e+01, train loss: 4.95861e-07, val loss: 1.36686e-06, min loss: 4.95495e-07\n",
      "Epoch: 1973000, elapsed: 1.09e+01, train loss: 5.32953e-07, val loss: 1.41552e-06, min loss: 4.95495e-07\n",
      "Epoch: 1973100, elapsed: 1.09e+01, train loss: 6.02263e-07, val loss: 1.43138e-06, min loss: 4.95495e-07\n",
      "Epoch: 1973200, elapsed: 1.09e+01, train loss: 5.64148e-07, val loss: 1.52443e-06, min loss: 4.95495e-07\n",
      "Epoch: 1973300, elapsed: 1.10e+01, train loss: 5.27606e-07, val loss: 1.41465e-06, min loss: 4.95495e-07\n",
      "Epoch: 1973400, elapsed: 1.08e+01, train loss: 5.54310e-07, val loss: 1.47110e-06, min loss: 4.95495e-07\n",
      "Epoch: 1973500, elapsed: 1.10e+01, train loss: 5.00919e-07, val loss: 1.40958e-06, min loss: 4.95495e-07\n",
      "Epoch: 1973600, elapsed: 1.10e+01, train loss: 5.96953e-07, val loss: 1.49272e-06, min loss: 4.95495e-07\n",
      "Epoch: 1973700, elapsed: 1.08e+01, train loss: 5.25559e-07, val loss: 1.34042e-06, min loss: 4.95495e-07\n",
      "Epoch: 1973800, elapsed: 1.69e+01, train loss: 4.95692e-07, val loss: 1.36600e-06, min loss: 4.95495e-07\n",
      "Epoch: 1973900, elapsed: 1.10e+01, train loss: 5.50166e-07, val loss: 1.51521e-06, min loss: 4.95495e-07\n",
      "Epoch: 1974000, elapsed: 1.11e+01, train loss: 8.47330e-07, val loss: 1.62053e-06, min loss: 4.95495e-07\n",
      "Epoch: 1974100, elapsed: 1.10e+01, train loss: 5.07816e-07, val loss: 1.42257e-06, min loss: 4.95495e-07\n",
      "Epoch: 1974200, elapsed: 1.10e+01, train loss: 4.95452e-07, val loss: 1.37217e-06, min loss: 4.95452e-07\n",
      "Epoch: 1974300, elapsed: 1.11e+01, train loss: 6.62507e-07, val loss: 1.46739e-06, min loss: 4.95452e-07\n",
      "Epoch: 1974400, elapsed: 1.11e+01, train loss: 5.23308e-07, val loss: 1.36875e-06, min loss: 4.95452e-07\n",
      "Epoch: 1974500, elapsed: 1.11e+01, train loss: 5.15560e-07, val loss: 1.41340e-06, min loss: 4.95452e-07\n",
      "Epoch: 1974600, elapsed: 1.10e+01, train loss: 9.70258e-07, val loss: 2.02344e-06, min loss: 4.95452e-07\n",
      "Epoch: 1974700, elapsed: 1.10e+01, train loss: 5.02535e-07, val loss: 1.38720e-06, min loss: 4.95452e-07\n",
      "Epoch: 1974800, elapsed: 1.11e+01, train loss: 5.87874e-07, val loss: 1.42552e-06, min loss: 4.95452e-07\n",
      "Epoch: 1974900, elapsed: 1.09e+01, train loss: 6.50454e-07, val loss: 1.59950e-06, min loss: 4.95452e-07\n",
      "Epoch: 1975000, elapsed: 1.10e+01, train loss: 1.04507e-06, val loss: 1.74599e-06, min loss: 4.95452e-07\n",
      "Epoch: 1975100, elapsed: 1.28e+01, train loss: 5.60470e-07, val loss: 1.52183e-06, min loss: 4.95452e-07\n",
      "Epoch: 1975200, elapsed: 1.08e+01, train loss: 5.04219e-07, val loss: 1.36231e-06, min loss: 4.95452e-07\n",
      "Epoch: 1975300, elapsed: 1.11e+01, train loss: 5.03059e-07, val loss: 1.35397e-06, min loss: 4.95452e-07\n",
      "Epoch: 1975400, elapsed: 1.11e+01, train loss: 7.34060e-07, val loss: 1.53305e-06, min loss: 4.95452e-07\n",
      "Epoch: 1975500, elapsed: 1.10e+01, train loss: 4.98062e-07, val loss: 1.41212e-06, min loss: 4.95452e-07\n",
      "Epoch: 1975600, elapsed: 1.09e+01, train loss: 7.38577e-07, val loss: 1.39386e-06, min loss: 4.95452e-07\n",
      "Epoch: 1975700, elapsed: 1.09e+01, train loss: 6.69948e-07, val loss: 2.39658e-06, min loss: 4.95452e-07\n",
      "Epoch: 1975800, elapsed: 1.08e+01, train loss: 4.94310e-07, val loss: 1.36907e-06, min loss: 4.94310e-07\n",
      "Epoch: 1975900, elapsed: 1.08e+01, train loss: 1.72573e-06, val loss: 2.86887e-06, min loss: 4.94310e-07\n",
      "Epoch: 1976000, elapsed: 1.08e+01, train loss: 4.94513e-07, val loss: 1.36897e-06, min loss: 4.94310e-07\n",
      "Epoch: 1976100, elapsed: 1.08e+01, train loss: 5.43066e-07, val loss: 1.49812e-06, min loss: 4.94310e-07\n",
      "Epoch: 1976200, elapsed: 1.07e+01, train loss: 4.94842e-07, val loss: 1.37470e-06, min loss: 4.94310e-07\n",
      "Epoch: 1976300, elapsed: 1.10e+01, train loss: 5.33448e-07, val loss: 1.43967e-06, min loss: 4.94310e-07\n",
      "Epoch: 1976400, elapsed: 1.09e+01, train loss: 5.01780e-07, val loss: 1.35994e-06, min loss: 4.94310e-07\n",
      "Epoch: 1976500, elapsed: 1.08e+01, train loss: 6.92690e-07, val loss: 1.43751e-06, min loss: 4.94310e-07\n",
      "Epoch: 1976600, elapsed: 1.07e+01, train loss: 5.24820e-07, val loss: 1.47212e-06, min loss: 4.94310e-07\n",
      "Epoch: 1976700, elapsed: 1.08e+01, train loss: 5.66724e-07, val loss: 1.54693e-06, min loss: 4.94310e-07\n",
      "Epoch: 1976800, elapsed: 1.12e+01, train loss: 5.91719e-07, val loss: 1.48732e-06, min loss: 4.94310e-07\n",
      "Epoch: 1976900, elapsed: 1.06e+01, train loss: 4.95502e-07, val loss: 1.36167e-06, min loss: 4.94310e-07\n",
      "Epoch: 1977000, elapsed: 1.10e+01, train loss: 9.84856e-07, val loss: 1.76024e-06, min loss: 4.94310e-07\n",
      "Epoch: 1977100, elapsed: 1.07e+01, train loss: 8.67444e-07, val loss: 1.53375e-06, min loss: 4.94310e-07\n",
      "Epoch: 1977200, elapsed: 1.10e+01, train loss: 6.41869e-07, val loss: 1.60021e-06, min loss: 4.94310e-07\n",
      "Epoch: 1977300, elapsed: 1.63e+01, train loss: 7.09347e-07, val loss: 1.55477e-06, min loss: 4.94310e-07\n",
      "Epoch: 1977400, elapsed: 1.10e+01, train loss: 9.88557e-07, val loss: 1.79735e-06, min loss: 4.94310e-07\n",
      "Epoch: 1977500, elapsed: 1.10e+01, train loss: 5.47266e-07, val loss: 1.39668e-06, min loss: 4.94310e-07\n",
      "Epoch: 1977600, elapsed: 1.11e+01, train loss: 1.43330e-06, val loss: 1.94543e-06, min loss: 4.94310e-07\n",
      "Epoch: 1977700, elapsed: 1.10e+01, train loss: 7.10125e-07, val loss: 1.80894e-06, min loss: 4.94310e-07\n",
      "Epoch: 1977800, elapsed: 1.08e+01, train loss: 4.94677e-07, val loss: 1.37865e-06, min loss: 4.94310e-07\n",
      "Epoch: 1977900, elapsed: 1.09e+01, train loss: 5.30248e-07, val loss: 1.42024e-06, min loss: 4.94310e-07\n",
      "Epoch: 1978000, elapsed: 1.09e+01, train loss: 5.83004e-07, val loss: 1.60954e-06, min loss: 4.94310e-07\n",
      "Epoch: 1978100, elapsed: 1.09e+01, train loss: 5.97846e-07, val loss: 1.44081e-06, min loss: 4.94310e-07\n",
      "Epoch: 1978200, elapsed: 1.09e+01, train loss: 8.99928e-07, val loss: 1.59127e-06, min loss: 4.94310e-07\n",
      "Epoch: 1978300, elapsed: 1.07e+01, train loss: 4.93630e-07, val loss: 1.37601e-06, min loss: 4.93630e-07\n",
      "Epoch: 1978400, elapsed: 1.08e+01, train loss: 5.22031e-07, val loss: 1.38666e-06, min loss: 4.93630e-07\n",
      "Epoch: 1978500, elapsed: 1.09e+01, train loss: 4.93355e-07, val loss: 1.37364e-06, min loss: 4.93355e-07\n",
      "Epoch: 1978600, elapsed: 1.08e+01, train loss: 4.97920e-07, val loss: 1.40240e-06, min loss: 4.93355e-07\n",
      "Epoch: 1978700, elapsed: 1.08e+01, train loss: 5.29065e-07, val loss: 1.38246e-06, min loss: 4.93355e-07\n",
      "Epoch: 1978800, elapsed: 1.10e+01, train loss: 5.73663e-07, val loss: 1.50378e-06, min loss: 4.93355e-07\n",
      "Epoch: 1978900, elapsed: 1.08e+01, train loss: 5.04222e-07, val loss: 1.40498e-06, min loss: 4.93355e-07\n",
      "Epoch: 1979000, elapsed: 1.09e+01, train loss: 4.93890e-07, val loss: 1.36815e-06, min loss: 4.93355e-07\n",
      "Epoch: 1979100, elapsed: 1.08e+01, train loss: 5.06882e-07, val loss: 1.40687e-06, min loss: 4.93355e-07\n",
      "Epoch: 1979200, elapsed: 1.08e+01, train loss: 1.01995e-06, val loss: 1.88194e-06, min loss: 4.93355e-07\n",
      "Epoch: 1979300, elapsed: 1.09e+01, train loss: 4.93202e-07, val loss: 1.37595e-06, min loss: 4.93202e-07\n",
      "Epoch: 1979400, elapsed: 1.08e+01, train loss: 4.94857e-07, val loss: 1.37903e-06, min loss: 4.93202e-07\n",
      "Epoch: 1979500, elapsed: 1.08e+01, train loss: 1.34901e-06, val loss: 2.27614e-06, min loss: 4.93202e-07\n",
      "Epoch: 1979600, elapsed: 1.08e+01, train loss: 4.93067e-07, val loss: 1.37588e-06, min loss: 4.93067e-07\n",
      "Epoch: 1979700, elapsed: 1.08e+01, train loss: 4.99800e-07, val loss: 1.36604e-06, min loss: 4.93067e-07\n",
      "Epoch: 1979800, elapsed: 1.08e+01, train loss: 5.27016e-07, val loss: 1.44840e-06, min loss: 4.93067e-07\n",
      "Epoch: 1979900, elapsed: 1.08e+01, train loss: 4.93321e-07, val loss: 1.36662e-06, min loss: 4.93067e-07\n",
      "Epoch: 1980000, elapsed: 1.07e+01, train loss: 9.46423e-07, val loss: 1.81461e-06, min loss: 4.93067e-07\n",
      "Epoch: 1980100, elapsed: 1.28e+01, train loss: 4.92960e-07, val loss: 1.37570e-06, min loss: 4.92960e-07\n",
      "Epoch: 1980200, elapsed: 1.07e+01, train loss: 5.07789e-07, val loss: 1.36281e-06, min loss: 4.92960e-07\n",
      "Epoch: 1980300, elapsed: 1.08e+01, train loss: 5.07722e-07, val loss: 1.41792e-06, min loss: 4.92960e-07\n",
      "Epoch: 1980400, elapsed: 1.08e+01, train loss: 4.93839e-07, val loss: 1.37525e-06, min loss: 4.92960e-07\n",
      "Epoch: 1980500, elapsed: 1.06e+01, train loss: 5.98478e-07, val loss: 1.65327e-06, min loss: 4.92960e-07\n",
      "Epoch: 1980600, elapsed: 1.07e+01, train loss: 8.18658e-07, val loss: 1.68386e-06, min loss: 4.92960e-07\n",
      "Epoch: 1980700, elapsed: 1.06e+01, train loss: 4.93072e-07, val loss: 1.37286e-06, min loss: 4.92960e-07\n",
      "Epoch: 1980800, elapsed: 1.67e+01, train loss: 5.70221e-07, val loss: 1.37289e-06, min loss: 4.92960e-07\n",
      "Epoch: 1980900, elapsed: 1.12e+01, train loss: 7.06816e-07, val loss: 1.75369e-06, min loss: 4.92960e-07\n",
      "Epoch: 1981000, elapsed: 1.11e+01, train loss: 4.93143e-07, val loss: 1.37532e-06, min loss: 4.92960e-07\n",
      "Epoch: 1981100, elapsed: 1.11e+01, train loss: 5.63685e-07, val loss: 1.44807e-06, min loss: 4.92960e-07\n",
      "Epoch: 1981200, elapsed: 1.10e+01, train loss: 5.45894e-07, val loss: 1.40974e-06, min loss: 4.92960e-07\n",
      "Epoch: 1981300, elapsed: 1.11e+01, train loss: 1.92379e-06, val loss: 2.14482e-06, min loss: 4.92960e-07\n",
      "Epoch: 1981400, elapsed: 1.09e+01, train loss: 5.00459e-07, val loss: 1.35440e-06, min loss: 4.92960e-07\n",
      "Epoch: 1981500, elapsed: 1.10e+01, train loss: 5.11830e-07, val loss: 1.37814e-06, min loss: 4.92960e-07\n",
      "Epoch: 1981600, elapsed: 1.11e+01, train loss: 1.72994e-06, val loss: 2.87602e-06, min loss: 4.92960e-07\n",
      "Epoch: 1981700, elapsed: 1.10e+01, train loss: 2.47987e-06, val loss: 3.18729e-06, min loss: 4.92960e-07\n",
      "Epoch: 1981800, elapsed: 1.10e+01, train loss: 8.71117e-07, val loss: 1.82797e-06, min loss: 4.92960e-07\n",
      "Epoch: 1981900, elapsed: 1.08e+01, train loss: 6.03998e-07, val loss: 1.45636e-06, min loss: 4.92960e-07\n",
      "Epoch: 1982000, elapsed: 1.11e+01, train loss: 7.65203e-07, val loss: 1.79443e-06, min loss: 4.92960e-07\n",
      "Epoch: 1982100, elapsed: 1.10e+01, train loss: 4.96104e-07, val loss: 1.38780e-06, min loss: 4.92960e-07\n",
      "Epoch: 1982200, elapsed: 1.08e+01, train loss: 1.34965e-06, val loss: 1.99421e-06, min loss: 4.92960e-07\n",
      "Epoch: 1982300, elapsed: 1.09e+01, train loss: 5.11042e-07, val loss: 1.44669e-06, min loss: 4.92960e-07\n",
      "Epoch: 1982400, elapsed: 1.11e+01, train loss: 5.73349e-07, val loss: 1.39757e-06, min loss: 4.92960e-07\n",
      "Epoch: 1982500, elapsed: 1.10e+01, train loss: 1.88728e-06, val loss: 2.88945e-06, min loss: 4.92960e-07\n",
      "Epoch: 1982600, elapsed: 1.07e+01, train loss: 8.95957e-07, val loss: 1.91499e-06, min loss: 4.92960e-07\n",
      "Epoch: 1982700, elapsed: 1.11e+01, train loss: 5.06450e-07, val loss: 1.39173e-06, min loss: 4.92960e-07\n",
      "Epoch: 1982800, elapsed: 1.08e+01, train loss: 4.93228e-07, val loss: 1.37019e-06, min loss: 4.92960e-07\n",
      "Epoch: 1982900, elapsed: 1.09e+01, train loss: 5.01549e-07, val loss: 1.42171e-06, min loss: 4.92960e-07\n",
      "Epoch: 1983000, elapsed: 1.11e+01, train loss: 4.36453e-06, val loss: 4.38502e-06, min loss: 4.92960e-07\n",
      "Epoch: 1983100, elapsed: 1.11e+01, train loss: 4.92117e-07, val loss: 1.38115e-06, min loss: 4.92117e-07\n",
      "Epoch: 1983200, elapsed: 1.07e+01, train loss: 5.22668e-07, val loss: 1.35373e-06, min loss: 4.92117e-07\n",
      "Epoch: 1983300, elapsed: 1.04e+01, train loss: 1.11977e-06, val loss: 1.89425e-06, min loss: 4.92117e-07\n",
      "Epoch: 1983400, elapsed: 1.07e+01, train loss: 4.92258e-07, val loss: 1.37517e-06, min loss: 4.92117e-07\n",
      "Epoch: 1983500, elapsed: 1.06e+01, train loss: 4.94415e-07, val loss: 1.37410e-06, min loss: 4.92117e-07\n",
      "Epoch: 1983600, elapsed: 1.10e+01, train loss: 1.18714e-06, val loss: 1.87427e-06, min loss: 4.92117e-07\n",
      "Epoch: 1983700, elapsed: 1.08e+01, train loss: 6.05434e-07, val loss: 1.56041e-06, min loss: 4.92117e-07\n",
      "Epoch: 1983800, elapsed: 1.09e+01, train loss: 4.94160e-07, val loss: 1.35898e-06, min loss: 4.92117e-07\n",
      "Epoch: 1983900, elapsed: 1.08e+01, train loss: 1.08506e-06, val loss: 2.13926e-06, min loss: 4.92117e-07\n",
      "Epoch: 1984000, elapsed: 1.06e+01, train loss: 2.83634e-06, val loss: 3.54741e-06, min loss: 4.92117e-07\n",
      "Epoch: 1984100, elapsed: 1.08e+01, train loss: 4.99676e-07, val loss: 1.37192e-06, min loss: 4.92117e-07\n",
      "Epoch: 1984200, elapsed: 1.08e+01, train loss: 4.92631e-07, val loss: 1.36791e-06, min loss: 4.92117e-07\n",
      "Epoch: 1984300, elapsed: 1.62e+01, train loss: 5.01093e-07, val loss: 1.41360e-06, min loss: 4.92117e-07\n",
      "Epoch: 1984400, elapsed: 1.10e+01, train loss: 5.17959e-07, val loss: 1.48853e-06, min loss: 4.92117e-07\n",
      "Epoch: 1984500, elapsed: 1.11e+01, train loss: 7.51581e-07, val loss: 1.55079e-06, min loss: 4.92117e-07\n",
      "Epoch: 1984600, elapsed: 1.08e+01, train loss: 4.96862e-07, val loss: 1.40382e-06, min loss: 4.92117e-07\n",
      "Epoch: 1984700, elapsed: 1.08e+01, train loss: 4.91487e-07, val loss: 1.37840e-06, min loss: 4.91487e-07\n",
      "Epoch: 1984800, elapsed: 1.09e+01, train loss: 5.50833e-07, val loss: 1.40799e-06, min loss: 4.91487e-07\n",
      "Epoch: 1984900, elapsed: 1.10e+01, train loss: 5.06806e-07, val loss: 1.37716e-06, min loss: 4.91487e-07\n",
      "Epoch: 1985000, elapsed: 1.08e+01, train loss: 5.44821e-07, val loss: 1.32630e-06, min loss: 4.91487e-07\n",
      "Epoch: 1985100, elapsed: 1.29e+01, train loss: 8.08432e-07, val loss: 1.82866e-06, min loss: 4.91487e-07\n",
      "Epoch: 1985200, elapsed: 1.08e+01, train loss: 1.17311e-06, val loss: 1.72962e-06, min loss: 4.91487e-07\n",
      "Epoch: 1985300, elapsed: 1.08e+01, train loss: 7.27824e-07, val loss: 1.51020e-06, min loss: 4.91487e-07\n",
      "Epoch: 1985400, elapsed: 1.08e+01, train loss: 6.86616e-07, val loss: 1.63476e-06, min loss: 4.91487e-07\n",
      "Epoch: 1985500, elapsed: 1.08e+01, train loss: 1.08943e-06, val loss: 2.33539e-06, min loss: 4.91487e-07\n",
      "Epoch: 1985600, elapsed: 1.07e+01, train loss: 5.79909e-07, val loss: 1.55013e-06, min loss: 4.91487e-07\n",
      "Epoch: 1985700, elapsed: 1.09e+01, train loss: 5.93877e-07, val loss: 1.55402e-06, min loss: 4.91487e-07\n",
      "Epoch: 1985800, elapsed: 1.07e+01, train loss: 4.91921e-07, val loss: 1.37809e-06, min loss: 4.91487e-07\n",
      "Epoch: 1985900, elapsed: 1.09e+01, train loss: 4.92247e-07, val loss: 1.37916e-06, min loss: 4.91487e-07\n",
      "Epoch: 1986000, elapsed: 1.08e+01, train loss: 7.91994e-07, val loss: 1.81871e-06, min loss: 4.91487e-07\n",
      "Epoch: 1986100, elapsed: 1.08e+01, train loss: 4.91288e-07, val loss: 1.38264e-06, min loss: 4.91288e-07\n",
      "Epoch: 1986200, elapsed: 1.07e+01, train loss: 4.91559e-07, val loss: 1.38295e-06, min loss: 4.91288e-07\n",
      "Epoch: 1986300, elapsed: 1.08e+01, train loss: 4.94769e-07, val loss: 1.38947e-06, min loss: 4.91288e-07\n",
      "Epoch: 1986400, elapsed: 1.09e+01, train loss: 4.95004e-07, val loss: 1.39710e-06, min loss: 4.91288e-07\n",
      "Epoch: 1986500, elapsed: 1.07e+01, train loss: 7.55335e-07, val loss: 1.53240e-06, min loss: 4.91288e-07\n",
      "Epoch: 1986600, elapsed: 1.07e+01, train loss: 8.85246e-07, val loss: 1.67469e-06, min loss: 4.91288e-07\n",
      "Epoch: 1986700, elapsed: 1.07e+01, train loss: 5.00801e-07, val loss: 1.40500e-06, min loss: 4.91288e-07\n",
      "Epoch: 1986800, elapsed: 1.06e+01, train loss: 4.96973e-07, val loss: 1.35058e-06, min loss: 4.91288e-07\n",
      "Epoch: 1986900, elapsed: 1.07e+01, train loss: 5.99895e-07, val loss: 1.54408e-06, min loss: 4.91288e-07\n",
      "Epoch: 1987000, elapsed: 1.07e+01, train loss: 5.09227e-07, val loss: 1.36976e-06, min loss: 4.91288e-07\n",
      "Epoch: 1987100, elapsed: 1.08e+01, train loss: 4.92381e-07, val loss: 1.38933e-06, min loss: 4.91288e-07\n",
      "Epoch: 1987200, elapsed: 1.07e+01, train loss: 4.94239e-07, val loss: 1.38066e-06, min loss: 4.91288e-07\n",
      "Epoch: 1987300, elapsed: 1.08e+01, train loss: 2.13298e-06, val loss: 2.00957e-06, min loss: 4.91288e-07\n",
      "Epoch: 1987400, elapsed: 1.06e+01, train loss: 4.90639e-07, val loss: 1.38846e-06, min loss: 4.90639e-07\n",
      "Epoch: 1987500, elapsed: 1.06e+01, train loss: 5.67272e-07, val loss: 1.50269e-06, min loss: 4.90639e-07\n",
      "Epoch: 1987600, elapsed: 1.07e+01, train loss: 5.08962e-07, val loss: 1.37564e-06, min loss: 4.90639e-07\n",
      "Epoch: 1987700, elapsed: 1.09e+01, train loss: 5.19211e-07, val loss: 1.42320e-06, min loss: 4.90639e-07\n",
      "Epoch: 1987800, elapsed: 1.62e+01, train loss: 5.46650e-07, val loss: 1.40879e-06, min loss: 4.90639e-07\n",
      "Epoch: 1987900, elapsed: 1.12e+01, train loss: 5.95755e-07, val loss: 1.39229e-06, min loss: 4.90639e-07\n",
      "Epoch: 1988000, elapsed: 1.10e+01, train loss: 4.11705e-06, val loss: 5.88769e-06, min loss: 4.90639e-07\n",
      "Epoch: 1988100, elapsed: 1.11e+01, train loss: 4.90510e-07, val loss: 1.38675e-06, min loss: 4.90510e-07\n",
      "Epoch: 1988200, elapsed: 1.12e+01, train loss: 9.28077e-07, val loss: 2.15413e-06, min loss: 4.90510e-07\n",
      "Epoch: 1988300, elapsed: 1.10e+01, train loss: 5.06124e-07, val loss: 1.40050e-06, min loss: 4.90510e-07\n",
      "Epoch: 1988400, elapsed: 1.11e+01, train loss: 4.90763e-07, val loss: 1.38038e-06, min loss: 4.90510e-07\n",
      "Epoch: 1988500, elapsed: 1.09e+01, train loss: 4.93259e-07, val loss: 1.34502e-06, min loss: 4.90510e-07\n",
      "Epoch: 1988600, elapsed: 1.09e+01, train loss: 1.07717e-06, val loss: 2.18850e-06, min loss: 4.90510e-07\n",
      "Epoch: 1988700, elapsed: 1.09e+01, train loss: 4.90173e-07, val loss: 1.38589e-06, min loss: 4.90173e-07\n",
      "Epoch: 1988800, elapsed: 1.10e+01, train loss: 4.92818e-07, val loss: 1.38517e-06, min loss: 4.90173e-07\n",
      "Epoch: 1988900, elapsed: 1.11e+01, train loss: 4.90534e-07, val loss: 1.39635e-06, min loss: 4.90173e-07\n",
      "Epoch: 1989000, elapsed: 1.09e+01, train loss: 5.01099e-07, val loss: 1.37380e-06, min loss: 4.90173e-07\n",
      "Epoch: 1989100, elapsed: 1.10e+01, train loss: 1.08370e-06, val loss: 2.42182e-06, min loss: 4.90173e-07\n",
      "Epoch: 1989200, elapsed: 1.10e+01, train loss: 4.90279e-07, val loss: 1.38151e-06, min loss: 4.90173e-07\n",
      "Epoch: 1989300, elapsed: 1.10e+01, train loss: 5.17810e-07, val loss: 1.38792e-06, min loss: 4.90173e-07\n",
      "Epoch: 1989400, elapsed: 1.08e+01, train loss: 2.79245e-06, val loss: 1.92806e-06, min loss: 4.90173e-07\n",
      "Epoch: 1989500, elapsed: 1.08e+01, train loss: 4.90079e-07, val loss: 1.39076e-06, min loss: 4.90079e-07\n",
      "Epoch: 1989600, elapsed: 1.08e+01, train loss: 6.02150e-07, val loss: 1.42934e-06, min loss: 4.90079e-07\n",
      "Epoch: 1989700, elapsed: 1.08e+01, train loss: 2.55507e-06, val loss: 3.42096e-06, min loss: 4.90079e-07\n",
      "Epoch: 1989800, elapsed: 1.08e+01, train loss: 6.48567e-07, val loss: 1.51965e-06, min loss: 4.90079e-07\n",
      "Epoch: 1989900, elapsed: 1.08e+01, train loss: 4.90011e-07, val loss: 1.38879e-06, min loss: 4.90011e-07\n",
      "Epoch: 1990000, elapsed: 1.07e+01, train loss: 4.94837e-07, val loss: 1.39363e-06, min loss: 4.90011e-07\n",
      "Epoch: 1990100, elapsed: 1.28e+01, train loss: 5.97475e-07, val loss: 1.50291e-06, min loss: 4.90011e-07\n",
      "Epoch: 1990200, elapsed: 1.09e+01, train loss: 5.05328e-07, val loss: 1.38641e-06, min loss: 4.90011e-07\n",
      "Epoch: 1990300, elapsed: 1.08e+01, train loss: 5.78503e-07, val loss: 1.50757e-06, min loss: 4.90011e-07\n",
      "Epoch: 1990400, elapsed: 1.07e+01, train loss: 5.92923e-07, val loss: 1.38428e-06, min loss: 4.90011e-07\n",
      "Epoch: 1990500, elapsed: 1.08e+01, train loss: 6.44509e-07, val loss: 1.52934e-06, min loss: 4.90011e-07\n",
      "Epoch: 1990600, elapsed: 1.09e+01, train loss: 5.07602e-07, val loss: 1.38499e-06, min loss: 4.90011e-07\n",
      "Epoch: 1990700, elapsed: 1.07e+01, train loss: 5.62025e-07, val loss: 1.51341e-06, min loss: 4.90011e-07\n",
      "Epoch: 1990800, elapsed: 1.06e+01, train loss: 5.74602e-07, val loss: 1.43331e-06, min loss: 4.90011e-07\n",
      "Epoch: 1990900, elapsed: 1.07e+01, train loss: 6.49899e-07, val loss: 1.59902e-06, min loss: 4.90011e-07\n",
      "Epoch: 1991000, elapsed: 1.06e+01, train loss: 4.89651e-07, val loss: 1.38831e-06, min loss: 4.89651e-07\n",
      "Epoch: 1991100, elapsed: 1.07e+01, train loss: 5.96476e-07, val loss: 1.49417e-06, min loss: 4.89651e-07\n",
      "Epoch: 1991200, elapsed: 1.08e+01, train loss: 5.64072e-07, val loss: 1.60475e-06, min loss: 4.89651e-07\n",
      "Epoch: 1991300, elapsed: 1.08e+01, train loss: 4.89465e-07, val loss: 1.38902e-06, min loss: 4.89465e-07\n",
      "Epoch: 1991400, elapsed: 1.64e+01, train loss: 4.93001e-07, val loss: 1.37429e-06, min loss: 4.89465e-07\n",
      "Epoch: 1991500, elapsed: 1.10e+01, train loss: 1.29558e-06, val loss: 2.36816e-06, min loss: 4.89465e-07\n",
      "Epoch: 1991600, elapsed: 1.09e+01, train loss: 5.36035e-07, val loss: 1.41717e-06, min loss: 4.89465e-07\n",
      "Epoch: 1991700, elapsed: 1.11e+01, train loss: 4.96432e-07, val loss: 1.39338e-06, min loss: 4.89465e-07\n",
      "Epoch: 1991800, elapsed: 1.09e+01, train loss: 1.04422e-06, val loss: 2.34133e-06, min loss: 4.89465e-07\n",
      "Epoch: 1991900, elapsed: 1.11e+01, train loss: 6.01037e-07, val loss: 1.58486e-06, min loss: 4.89465e-07\n",
      "Epoch: 1992000, elapsed: 1.09e+01, train loss: 8.81276e-07, val loss: 1.47499e-06, min loss: 4.89465e-07\n",
      "Epoch: 1992100, elapsed: 1.10e+01, train loss: 5.09878e-07, val loss: 1.38504e-06, min loss: 4.89465e-07\n",
      "Epoch: 1992200, elapsed: 1.10e+01, train loss: 5.32336e-07, val loss: 1.42321e-06, min loss: 4.89465e-07\n",
      "Epoch: 1992300, elapsed: 1.09e+01, train loss: 8.41162e-07, val loss: 1.75075e-06, min loss: 4.89465e-07\n",
      "Epoch: 1992400, elapsed: 1.09e+01, train loss: 5.11543e-07, val loss: 1.39327e-06, min loss: 4.89465e-07\n",
      "Epoch: 1992500, elapsed: 1.08e+01, train loss: 4.89682e-07, val loss: 1.39059e-06, min loss: 4.89465e-07\n",
      "Epoch: 1992600, elapsed: 1.09e+01, train loss: 5.62209e-07, val loss: 1.51019e-06, min loss: 4.89465e-07\n",
      "Epoch: 1992700, elapsed: 1.10e+01, train loss: 1.19751e-06, val loss: 2.45586e-06, min loss: 4.89465e-07\n",
      "Epoch: 1992800, elapsed: 1.08e+01, train loss: 5.00215e-07, val loss: 1.41337e-06, min loss: 4.89465e-07\n",
      "Epoch: 1992900, elapsed: 1.09e+01, train loss: 4.89351e-07, val loss: 1.39091e-06, min loss: 4.89351e-07\n",
      "Epoch: 1993000, elapsed: 1.07e+01, train loss: 4.92902e-07, val loss: 1.39686e-06, min loss: 4.89351e-07\n",
      "Epoch: 1993100, elapsed: 1.10e+01, train loss: 1.89604e-06, val loss: 2.37309e-06, min loss: 4.89351e-07\n",
      "Epoch: 1993200, elapsed: 1.11e+01, train loss: 4.88900e-07, val loss: 1.38992e-06, min loss: 4.88900e-07\n",
      "Epoch: 1993300, elapsed: 1.09e+01, train loss: 5.83561e-07, val loss: 1.44226e-06, min loss: 4.88900e-07\n",
      "Epoch: 1993400, elapsed: 1.08e+01, train loss: 4.88699e-07, val loss: 1.39268e-06, min loss: 4.88699e-07\n",
      "Epoch: 1993500, elapsed: 1.10e+01, train loss: 5.19949e-07, val loss: 1.49441e-06, min loss: 4.88699e-07\n",
      "Epoch: 1993600, elapsed: 1.10e+01, train loss: 4.88654e-07, val loss: 1.39231e-06, min loss: 4.88654e-07\n",
      "Epoch: 1993700, elapsed: 1.08e+01, train loss: 5.13575e-07, val loss: 1.37744e-06, min loss: 4.88654e-07\n",
      "Epoch: 1993800, elapsed: 1.08e+01, train loss: 4.89090e-07, val loss: 1.39741e-06, min loss: 4.88654e-07\n",
      "Epoch: 1993900, elapsed: 1.08e+01, train loss: 1.05662e-06, val loss: 2.23170e-06, min loss: 4.88654e-07\n",
      "Epoch: 1994000, elapsed: 1.08e+01, train loss: 4.88550e-07, val loss: 1.39200e-06, min loss: 4.88550e-07\n",
      "Epoch: 1994100, elapsed: 1.08e+01, train loss: 5.19258e-07, val loss: 1.34558e-06, min loss: 4.88550e-07\n",
      "Epoch: 1994200, elapsed: 1.09e+01, train loss: 4.88725e-07, val loss: 1.39744e-06, min loss: 4.88550e-07\n",
      "Epoch: 1994300, elapsed: 1.07e+01, train loss: 4.89038e-07, val loss: 1.40286e-06, min loss: 4.88550e-07\n",
      "Epoch: 1994400, elapsed: 1.09e+01, train loss: 7.51317e-07, val loss: 1.79620e-06, min loss: 4.88550e-07\n",
      "Epoch: 1994500, elapsed: 1.08e+01, train loss: 4.89117e-07, val loss: 1.39668e-06, min loss: 4.88550e-07\n",
      "Epoch: 1994600, elapsed: 1.07e+01, train loss: 4.99561e-07, val loss: 1.39249e-06, min loss: 4.88550e-07\n",
      "Epoch: 1994700, elapsed: 1.08e+01, train loss: 4.97542e-07, val loss: 1.41961e-06, min loss: 4.88550e-07\n",
      "Epoch: 1994800, elapsed: 1.09e+01, train loss: 4.92745e-07, val loss: 1.40835e-06, min loss: 4.88550e-07\n",
      "Epoch: 1994900, elapsed: 1.64e+01, train loss: 4.89939e-07, val loss: 1.40450e-06, min loss: 4.88550e-07\n",
      "Epoch: 1995000, elapsed: 1.13e+01, train loss: 4.98592e-07, val loss: 1.41408e-06, min loss: 4.88550e-07\n",
      "Epoch: 1995100, elapsed: 1.30e+01, train loss: 5.02295e-07, val loss: 1.46000e-06, min loss: 4.88550e-07\n",
      "Epoch: 1995200, elapsed: 1.11e+01, train loss: 4.91902e-07, val loss: 1.39377e-06, min loss: 4.88550e-07\n",
      "Epoch: 1995300, elapsed: 1.09e+01, train loss: 5.47113e-07, val loss: 1.40000e-06, min loss: 4.88550e-07\n",
      "Epoch: 1995400, elapsed: 1.11e+01, train loss: 5.63422e-07, val loss: 1.48315e-06, min loss: 4.88550e-07\n",
      "Epoch: 1995500, elapsed: 1.09e+01, train loss: 6.05156e-07, val loss: 1.49164e-06, min loss: 4.88550e-07\n",
      "Epoch: 1995600, elapsed: 1.10e+01, train loss: 5.41466e-07, val loss: 1.50038e-06, min loss: 4.88550e-07\n",
      "Epoch: 1995700, elapsed: 1.10e+01, train loss: 5.48170e-07, val loss: 1.37772e-06, min loss: 4.88550e-07\n",
      "Epoch: 1995800, elapsed: 1.11e+01, train loss: 8.44045e-07, val loss: 1.45934e-06, min loss: 4.88550e-07\n",
      "Epoch: 1995900, elapsed: 1.09e+01, train loss: 4.88351e-07, val loss: 1.39388e-06, min loss: 4.88351e-07\n",
      "Epoch: 1996000, elapsed: 1.08e+01, train loss: 5.06212e-07, val loss: 1.42040e-06, min loss: 4.88351e-07\n",
      "Epoch: 1996100, elapsed: 1.08e+01, train loss: 5.03688e-07, val loss: 1.37180e-06, min loss: 4.88351e-07\n",
      "Epoch: 1996200, elapsed: 1.08e+01, train loss: 7.52912e-07, val loss: 1.80337e-06, min loss: 4.88351e-07\n",
      "Epoch: 1996300, elapsed: 1.10e+01, train loss: 5.39405e-07, val loss: 1.41872e-06, min loss: 4.88351e-07\n",
      "Epoch: 1996400, elapsed: 1.08e+01, train loss: 4.88422e-07, val loss: 1.40773e-06, min loss: 4.88351e-07\n",
      "Epoch: 1996500, elapsed: 1.10e+01, train loss: 4.99997e-07, val loss: 1.38220e-06, min loss: 4.88351e-07\n",
      "Epoch: 1996600, elapsed: 1.10e+01, train loss: 4.91434e-07, val loss: 1.41126e-06, min loss: 4.88351e-07\n",
      "Epoch: 1996700, elapsed: 1.09e+01, train loss: 5.02136e-07, val loss: 1.41823e-06, min loss: 4.88351e-07\n",
      "Epoch: 1996800, elapsed: 1.10e+01, train loss: 4.75839e-06, val loss: 5.41893e-06, min loss: 4.88351e-07\n",
      "Epoch: 1996900, elapsed: 1.09e+01, train loss: 4.88495e-07, val loss: 1.39390e-06, min loss: 4.88351e-07\n",
      "Epoch: 1997000, elapsed: 1.10e+01, train loss: 5.63761e-07, val loss: 1.48616e-06, min loss: 4.88351e-07\n",
      "Epoch: 1997100, elapsed: 1.08e+01, train loss: 4.99391e-07, val loss: 1.40848e-06, min loss: 4.88351e-07\n",
      "Epoch: 1997200, elapsed: 1.10e+01, train loss: 6.02121e-07, val loss: 1.38592e-06, min loss: 4.88351e-07\n",
      "Epoch: 1997300, elapsed: 1.08e+01, train loss: 7.49476e-07, val loss: 1.60310e-06, min loss: 4.88351e-07\n",
      "Epoch: 1997400, elapsed: 1.09e+01, train loss: 5.83562e-07, val loss: 1.57523e-06, min loss: 4.88351e-07\n",
      "Epoch: 1997500, elapsed: 1.06e+01, train loss: 5.24460e-07, val loss: 1.42355e-06, min loss: 4.88351e-07\n",
      "Epoch: 1997600, elapsed: 1.09e+01, train loss: 4.94287e-07, val loss: 1.35950e-06, min loss: 4.88351e-07\n",
      "Epoch: 1997700, elapsed: 1.09e+01, train loss: 2.43295e-06, val loss: 2.99836e-06, min loss: 4.88351e-07\n",
      "Epoch: 1997800, elapsed: 1.07e+01, train loss: 4.87365e-07, val loss: 1.39715e-06, min loss: 4.87365e-07\n",
      "Epoch: 1997900, elapsed: 1.07e+01, train loss: 5.10811e-07, val loss: 1.69471e-06, min loss: 4.87365e-07\n",
      "Epoch: 1998000, elapsed: 1.08e+01, train loss: 4.87385e-07, val loss: 1.39650e-06, min loss: 4.87365e-07\n",
      "Epoch: 1998100, elapsed: 1.08e+01, train loss: 5.02448e-07, val loss: 1.41378e-06, min loss: 4.87365e-07\n",
      "Epoch: 1998200, elapsed: 1.08e+01, train loss: 1.55948e-06, val loss: 2.77546e-06, min loss: 4.87365e-07\n",
      "Epoch: 1998300, elapsed: 1.07e+01, train loss: 4.89733e-07, val loss: 1.39214e-06, min loss: 4.87365e-07\n",
      "Epoch: 1998400, elapsed: 1.08e+01, train loss: 4.87404e-07, val loss: 1.40377e-06, min loss: 4.87365e-07\n",
      "Epoch: 1998500, elapsed: 1.63e+01, train loss: 5.39797e-07, val loss: 1.41107e-06, min loss: 4.87365e-07\n",
      "Epoch: 1998600, elapsed: 1.10e+01, train loss: 1.06488e-06, val loss: 2.10829e-06, min loss: 4.87365e-07\n",
      "Epoch: 1998700, elapsed: 1.08e+01, train loss: 5.27388e-07, val loss: 1.45400e-06, min loss: 4.87365e-07\n",
      "Epoch: 1998800, elapsed: 1.11e+01, train loss: 1.08414e-06, val loss: 1.92553e-06, min loss: 4.87365e-07\n",
      "Epoch: 1998900, elapsed: 1.09e+01, train loss: 4.94868e-07, val loss: 1.39099e-06, min loss: 4.87365e-07\n",
      "Epoch: 1999000, elapsed: 1.10e+01, train loss: 4.88892e-07, val loss: 1.40361e-06, min loss: 4.87365e-07\n",
      "Epoch: 1999100, elapsed: 1.11e+01, train loss: 5.05957e-07, val loss: 1.37500e-06, min loss: 4.87365e-07\n",
      "Epoch: 1999200, elapsed: 1.09e+01, train loss: 4.92762e-07, val loss: 1.39611e-06, min loss: 4.87365e-07\n",
      "Epoch: 1999300, elapsed: 1.10e+01, train loss: 4.97384e-07, val loss: 1.42984e-06, min loss: 4.87365e-07\n",
      "Epoch: 1999400, elapsed: 1.08e+01, train loss: 8.74783e-07, val loss: 1.62914e-06, min loss: 4.87365e-07\n",
      "Epoch: 1999500, elapsed: 1.09e+01, train loss: 8.23064e-07, val loss: 2.13922e-06, min loss: 4.87365e-07\n",
      "Epoch: 1999600, elapsed: 1.09e+01, train loss: 5.18156e-07, val loss: 1.42598e-06, min loss: 4.87365e-07\n",
      "Epoch: 1999700, elapsed: 1.09e+01, train loss: 2.47005e-06, val loss: 3.95852e-06, min loss: 4.87365e-07\n",
      "Epoch: 1999800, elapsed: 1.08e+01, train loss: 4.87394e-07, val loss: 1.40377e-06, min loss: 4.87365e-07\n",
      "Epoch: 1999900, elapsed: 1.10e+01, train loss: 4.91360e-07, val loss: 1.40547e-06, min loss: 4.87365e-07\n",
      "Epoch: 2000000, elapsed: 1.10e+01, train loss: 6.46037e-07, val loss: 1.61150e-06, min loss: 4.87365e-07\n",
      "Epoch: 2000100, elapsed: 1.29e+01, train loss: 8.31890e-07, val loss: 1.82651e-06, min loss: 4.87365e-07\n",
      "Epoch: 2000200, elapsed: 1.10e+01, train loss: 5.77439e-07, val loss: 1.56622e-06, min loss: 4.87365e-07\n",
      "Epoch: 2000300, elapsed: 1.10e+01, train loss: 7.48620e-07, val loss: 1.48299e-06, min loss: 4.87365e-07\n",
      "Epoch: 2000400, elapsed: 1.11e+01, train loss: 5.68071e-07, val loss: 1.47130e-06, min loss: 4.87365e-07\n",
      "Epoch: 2000500, elapsed: 1.08e+01, train loss: 4.86619e-07, val loss: 1.40291e-06, min loss: 4.86619e-07\n",
      "Epoch: 2000600, elapsed: 1.08e+01, train loss: 5.34504e-07, val loss: 1.40712e-06, min loss: 4.86619e-07\n",
      "Epoch: 2000700, elapsed: 1.09e+01, train loss: 8.23638e-07, val loss: 1.91500e-06, min loss: 4.86619e-07\n",
      "Epoch: 2000800, elapsed: 1.09e+01, train loss: 5.17606e-07, val loss: 1.40683e-06, min loss: 4.86619e-07\n",
      "Epoch: 2000900, elapsed: 1.08e+01, train loss: 4.86864e-07, val loss: 1.40343e-06, min loss: 4.86619e-07\n",
      "Epoch: 2001000, elapsed: 1.10e+01, train loss: 6.53629e-07, val loss: 1.52965e-06, min loss: 4.86619e-07\n",
      "Epoch: 2001100, elapsed: 1.08e+01, train loss: 4.93533e-07, val loss: 1.42867e-06, min loss: 4.86619e-07\n",
      "Epoch: 2001200, elapsed: 1.09e+01, train loss: 4.86883e-07, val loss: 1.40475e-06, min loss: 4.86619e-07\n",
      "Epoch: 2001300, elapsed: 1.09e+01, train loss: 1.45121e-06, val loss: 1.78712e-06, min loss: 4.86619e-07\n",
      "Epoch: 2001400, elapsed: 1.09e+01, train loss: 1.44242e-06, val loss: 2.63395e-06, min loss: 4.86619e-07\n",
      "Epoch: 2001500, elapsed: 1.08e+01, train loss: 4.86735e-07, val loss: 1.40534e-06, min loss: 4.86619e-07\n",
      "Epoch: 2001600, elapsed: 1.08e+01, train loss: 4.92341e-07, val loss: 1.39323e-06, min loss: 4.86619e-07\n",
      "Epoch: 2001700, elapsed: 1.08e+01, train loss: 5.11775e-07, val loss: 1.47712e-06, min loss: 4.86619e-07\n",
      "Epoch: 2001800, elapsed: 1.08e+01, train loss: 5.34527e-07, val loss: 1.39469e-06, min loss: 4.86619e-07\n",
      "Epoch: 2001900, elapsed: 1.08e+01, train loss: 5.05477e-07, val loss: 1.38104e-06, min loss: 4.86619e-07\n",
      "Epoch: 2002000, elapsed: 1.64e+01, train loss: 6.97404e-07, val loss: 1.59941e-06, min loss: 4.86619e-07\n",
      "Epoch: 2002100, elapsed: 1.13e+01, train loss: 4.86020e-07, val loss: 1.40278e-06, min loss: 4.86020e-07\n",
      "Epoch: 2002200, elapsed: 1.11e+01, train loss: 4.94785e-07, val loss: 1.41651e-06, min loss: 4.86020e-07\n",
      "Epoch: 2002300, elapsed: 1.09e+01, train loss: 6.04733e-07, val loss: 1.45105e-06, min loss: 4.86020e-07\n",
      "Epoch: 2002400, elapsed: 1.11e+01, train loss: 5.04398e-07, val loss: 1.42417e-06, min loss: 4.86020e-07\n",
      "Epoch: 2002500, elapsed: 1.10e+01, train loss: 4.86465e-07, val loss: 1.40622e-06, min loss: 4.86020e-07\n",
      "Epoch: 2002600, elapsed: 1.11e+01, train loss: 4.88903e-07, val loss: 1.41935e-06, min loss: 4.86020e-07\n",
      "Epoch: 2002700, elapsed: 1.09e+01, train loss: 4.85987e-07, val loss: 1.41043e-06, min loss: 4.85987e-07\n",
      "Epoch: 2002800, elapsed: 1.10e+01, train loss: 4.86150e-07, val loss: 1.40764e-06, min loss: 4.85987e-07\n",
      "Epoch: 2002900, elapsed: 1.09e+01, train loss: 5.00832e-07, val loss: 1.42817e-06, min loss: 4.85987e-07\n",
      "Epoch: 2003000, elapsed: 1.09e+01, train loss: 4.86102e-07, val loss: 1.40485e-06, min loss: 4.85987e-07\n",
      "Epoch: 2003100, elapsed: 1.10e+01, train loss: 5.06526e-07, val loss: 1.39447e-06, min loss: 4.85987e-07\n",
      "Epoch: 2003200, elapsed: 1.08e+01, train loss: 4.50282e-06, val loss: 5.13980e-06, min loss: 4.85987e-07\n",
      "Epoch: 2003300, elapsed: 1.08e+01, train loss: 4.85745e-07, val loss: 1.40339e-06, min loss: 4.85745e-07\n",
      "Epoch: 2003400, elapsed: 1.09e+01, train loss: 5.54236e-07, val loss: 1.42854e-06, min loss: 4.85745e-07\n",
      "Epoch: 2003500, elapsed: 1.09e+01, train loss: 4.85603e-07, val loss: 1.40438e-06, min loss: 4.85603e-07\n",
      "Epoch: 2003600, elapsed: 1.09e+01, train loss: 5.02787e-07, val loss: 1.41165e-06, min loss: 4.85603e-07\n",
      "Epoch: 2003700, elapsed: 1.07e+01, train loss: 6.83327e-07, val loss: 1.48938e-06, min loss: 4.85603e-07\n",
      "Epoch: 2003800, elapsed: 1.09e+01, train loss: 1.01545e-06, val loss: 2.55332e-06, min loss: 4.85603e-07\n",
      "Epoch: 2003900, elapsed: 1.08e+01, train loss: 6.47572e-07, val loss: 1.59285e-06, min loss: 4.85603e-07\n",
      "Epoch: 2004000, elapsed: 1.08e+01, train loss: 5.16252e-07, val loss: 1.45484e-06, min loss: 4.85603e-07\n",
      "Epoch: 2004100, elapsed: 1.09e+01, train loss: 3.06078e-06, val loss: 2.89963e-06, min loss: 4.85603e-07\n",
      "Epoch: 2004200, elapsed: 1.08e+01, train loss: 4.85700e-07, val loss: 1.40666e-06, min loss: 4.85603e-07\n",
      "Epoch: 2004300, elapsed: 1.10e+01, train loss: 4.93714e-07, val loss: 1.36967e-06, min loss: 4.85603e-07\n",
      "Epoch: 2004400, elapsed: 1.08e+01, train loss: 9.67426e-07, val loss: 2.06564e-06, min loss: 4.85603e-07\n",
      "Epoch: 2004500, elapsed: 1.08e+01, train loss: 4.87316e-07, val loss: 1.44047e-06, min loss: 4.85603e-07\n",
      "Epoch: 2004600, elapsed: 1.08e+01, train loss: 4.87057e-07, val loss: 1.39359e-06, min loss: 4.85603e-07\n",
      "Epoch: 2004700, elapsed: 1.09e+01, train loss: 6.93585e-07, val loss: 1.45837e-06, min loss: 4.85603e-07\n",
      "Epoch: 2004800, elapsed: 1.06e+01, train loss: 4.85239e-07, val loss: 1.40649e-06, min loss: 4.85239e-07\n",
      "Epoch: 2004900, elapsed: 1.10e+01, train loss: 4.95222e-07, val loss: 1.40269e-06, min loss: 4.85239e-07\n",
      "Epoch: 2005000, elapsed: 1.09e+01, train loss: 6.44867e-07, val loss: 1.73535e-06, min loss: 4.85239e-07\n",
      "Epoch: 2005100, elapsed: 1.26e+01, train loss: 4.86916e-07, val loss: 1.41394e-06, min loss: 4.85239e-07\n",
      "Epoch: 2005200, elapsed: 1.07e+01, train loss: 4.85616e-07, val loss: 1.41210e-06, min loss: 4.85239e-07\n",
      "Epoch: 2005300, elapsed: 1.08e+01, train loss: 5.17429e-07, val loss: 1.41787e-06, min loss: 4.85239e-07\n",
      "Epoch: 2005400, elapsed: 1.07e+01, train loss: 2.98646e-06, val loss: 3.02646e-06, min loss: 4.85239e-07\n",
      "Epoch: 2005500, elapsed: 1.08e+01, train loss: 4.88305e-07, val loss: 1.43716e-06, min loss: 4.85239e-07\n",
      "Epoch: 2005600, elapsed: 1.65e+01, train loss: 5.52047e-07, val loss: 1.41727e-06, min loss: 4.85239e-07\n",
      "Epoch: 2005700, elapsed: 1.11e+01, train loss: 6.25854e-07, val loss: 1.63806e-06, min loss: 4.85239e-07\n",
      "Epoch: 2005800, elapsed: 1.11e+01, train loss: 1.95952e-06, val loss: 3.31950e-06, min loss: 4.85239e-07\n",
      "Epoch: 2005900, elapsed: 1.09e+01, train loss: 9.83771e-07, val loss: 2.09688e-06, min loss: 4.85239e-07\n",
      "Epoch: 2006000, elapsed: 1.10e+01, train loss: 5.63196e-07, val loss: 1.36107e-06, min loss: 4.85239e-07\n",
      "Epoch: 2006100, elapsed: 1.11e+01, train loss: 5.11485e-07, val loss: 1.37707e-06, min loss: 4.85239e-07\n",
      "Epoch: 2006200, elapsed: 1.09e+01, train loss: 6.62033e-07, val loss: 1.58589e-06, min loss: 4.85239e-07\n",
      "Epoch: 2006300, elapsed: 1.08e+01, train loss: 5.05185e-07, val loss: 1.57644e-06, min loss: 4.85239e-07\n",
      "Epoch: 2006400, elapsed: 1.10e+01, train loss: 1.80478e-06, val loss: 3.08921e-06, min loss: 4.85239e-07\n",
      "Epoch: 2006500, elapsed: 1.09e+01, train loss: 6.15616e-07, val loss: 1.65496e-06, min loss: 4.85239e-07\n",
      "Epoch: 2006600, elapsed: 1.10e+01, train loss: 5.25555e-07, val loss: 1.36402e-06, min loss: 4.85239e-07\n",
      "Epoch: 2006700, elapsed: 1.09e+01, train loss: 5.30918e-07, val loss: 1.41944e-06, min loss: 4.85239e-07\n",
      "Epoch: 2006800, elapsed: 1.08e+01, train loss: 6.05779e-07, val loss: 1.52353e-06, min loss: 4.85239e-07\n",
      "Epoch: 2006900, elapsed: 1.09e+01, train loss: 5.26900e-07, val loss: 1.45264e-06, min loss: 4.85239e-07\n",
      "Epoch: 2007000, elapsed: 1.08e+01, train loss: 4.86033e-07, val loss: 1.40750e-06, min loss: 4.85239e-07\n",
      "Epoch: 2007100, elapsed: 1.09e+01, train loss: 5.50205e-07, val loss: 1.53291e-06, min loss: 4.85239e-07\n",
      "Epoch: 2007200, elapsed: 1.07e+01, train loss: 4.86034e-07, val loss: 1.42571e-06, min loss: 4.85239e-07\n",
      "Epoch: 2007300, elapsed: 1.08e+01, train loss: 4.85684e-07, val loss: 1.39709e-06, min loss: 4.85239e-07\n",
      "Epoch: 2007400, elapsed: 1.09e+01, train loss: 5.91242e-07, val loss: 1.72552e-06, min loss: 4.85239e-07\n",
      "Epoch: 2007500, elapsed: 1.08e+01, train loss: 4.84364e-07, val loss: 1.41123e-06, min loss: 4.84364e-07\n",
      "Epoch: 2007600, elapsed: 1.08e+01, train loss: 4.98646e-07, val loss: 1.44479e-06, min loss: 4.84364e-07\n",
      "Epoch: 2007700, elapsed: 1.08e+01, train loss: 4.99040e-07, val loss: 1.47489e-06, min loss: 4.84364e-07\n",
      "Epoch: 2007800, elapsed: 1.07e+01, train loss: 4.87730e-07, val loss: 1.41308e-06, min loss: 4.84364e-07\n",
      "Epoch: 2007900, elapsed: 1.07e+01, train loss: 6.28329e-07, val loss: 1.48146e-06, min loss: 4.84364e-07\n",
      "Epoch: 2008000, elapsed: 1.06e+01, train loss: 5.52045e-07, val loss: 1.41198e-06, min loss: 4.84364e-07\n",
      "Epoch: 2008100, elapsed: 1.09e+01, train loss: 1.72707e-06, val loss: 2.57869e-06, min loss: 4.84364e-07\n",
      "Epoch: 2008200, elapsed: 1.08e+01, train loss: 1.10372e-06, val loss: 2.03121e-06, min loss: 4.84364e-07\n",
      "Epoch: 2008300, elapsed: 1.07e+01, train loss: 1.26843e-06, val loss: 2.39313e-06, min loss: 4.84364e-07\n",
      "Epoch: 2008400, elapsed: 1.09e+01, train loss: 4.89870e-07, val loss: 1.40774e-06, min loss: 4.84364e-07\n",
      "Epoch: 2008500, elapsed: 1.07e+01, train loss: 5.97298e-07, val loss: 1.64654e-06, min loss: 4.84364e-07\n",
      "Epoch: 2008600, elapsed: 1.08e+01, train loss: 4.84697e-07, val loss: 1.40698e-06, min loss: 4.84364e-07\n",
      "Epoch: 2008700, elapsed: 1.08e+01, train loss: 4.84246e-07, val loss: 1.41208e-06, min loss: 4.84246e-07\n",
      "Epoch: 2008800, elapsed: 1.06e+01, train loss: 9.49105e-07, val loss: 2.17422e-06, min loss: 4.84246e-07\n",
      "Epoch: 2008900, elapsed: 1.08e+01, train loss: 4.84707e-07, val loss: 1.41640e-06, min loss: 4.84246e-07\n",
      "Epoch: 2009000, elapsed: 1.07e+01, train loss: 4.86898e-07, val loss: 1.42431e-06, min loss: 4.84246e-07\n",
      "Epoch: 2009100, elapsed: 1.06e+01, train loss: 5.78647e-07, val loss: 1.43515e-06, min loss: 4.84246e-07\n",
      "Epoch: 2009200, elapsed: 1.64e+01, train loss: 6.95027e-07, val loss: 1.70768e-06, min loss: 4.84246e-07\n",
      "Epoch: 2009300, elapsed: 1.12e+01, train loss: 5.02907e-07, val loss: 1.52320e-06, min loss: 4.84246e-07\n",
      "Epoch: 2009400, elapsed: 1.08e+01, train loss: 1.64157e-06, val loss: 2.19949e-06, min loss: 4.84246e-07\n",
      "Epoch: 2009500, elapsed: 1.09e+01, train loss: 4.92988e-07, val loss: 1.45056e-06, min loss: 4.84246e-07\n",
      "Epoch: 2009600, elapsed: 1.10e+01, train loss: 4.84205e-07, val loss: 1.42258e-06, min loss: 4.84205e-07\n",
      "Epoch: 2009700, elapsed: 1.10e+01, train loss: 5.27020e-07, val loss: 1.48874e-06, min loss: 4.84205e-07\n",
      "Epoch: 2009800, elapsed: 1.08e+01, train loss: 4.89923e-07, val loss: 1.41991e-06, min loss: 4.84205e-07\n",
      "Epoch: 2009900, elapsed: 1.09e+01, train loss: 5.69366e-07, val loss: 1.63528e-06, min loss: 4.84205e-07\n",
      "Epoch: 2010000, elapsed: 1.09e+01, train loss: 2.73568e-06, val loss: 4.83117e-06, min loss: 4.84205e-07\n",
      "Epoch: 2010100, elapsed: 1.29e+01, train loss: 4.83720e-07, val loss: 1.41408e-06, min loss: 4.83720e-07\n",
      "Epoch: 2010200, elapsed: 1.07e+01, train loss: 5.81367e-07, val loss: 1.50010e-06, min loss: 4.83720e-07\n",
      "Epoch: 2010300, elapsed: 1.08e+01, train loss: 6.07521e-07, val loss: 1.44973e-06, min loss: 4.83720e-07\n",
      "Epoch: 2010400, elapsed: 1.09e+01, train loss: 8.00446e-07, val loss: 1.68571e-06, min loss: 4.83720e-07\n",
      "Epoch: 2010500, elapsed: 1.07e+01, train loss: 1.95199e-06, val loss: 1.89940e-06, min loss: 4.83720e-07\n",
      "Epoch: 2010600, elapsed: 1.09e+01, train loss: 4.84962e-07, val loss: 1.39664e-06, min loss: 4.83720e-07\n",
      "Epoch: 2010700, elapsed: 1.09e+01, train loss: 4.88973e-07, val loss: 1.43352e-06, min loss: 4.83720e-07\n",
      "Epoch: 2010800, elapsed: 1.08e+01, train loss: 5.47248e-07, val loss: 1.53332e-06, min loss: 4.83720e-07\n",
      "Epoch: 2010900, elapsed: 1.07e+01, train loss: 7.09772e-07, val loss: 1.62305e-06, min loss: 4.83720e-07\n",
      "Epoch: 2011000, elapsed: 1.08e+01, train loss: 5.38830e-07, val loss: 1.55670e-06, min loss: 4.83720e-07\n",
      "Epoch: 2011100, elapsed: 1.09e+01, train loss: 8.66763e-07, val loss: 1.60910e-06, min loss: 4.83720e-07\n",
      "Epoch: 2011200, elapsed: 1.08e+01, train loss: 1.06887e-06, val loss: 1.68354e-06, min loss: 4.83720e-07\n",
      "Epoch: 2011300, elapsed: 1.07e+01, train loss: 2.97036e-06, val loss: 3.89486e-06, min loss: 4.83720e-07\n",
      "Epoch: 2011400, elapsed: 1.07e+01, train loss: 5.76168e-07, val loss: 1.47545e-06, min loss: 4.83720e-07\n",
      "Epoch: 2011500, elapsed: 1.07e+01, train loss: 1.27796e-06, val loss: 2.16456e-06, min loss: 4.83720e-07\n",
      "Epoch: 2011600, elapsed: 1.09e+01, train loss: 1.15255e-06, val loss: 2.20875e-06, min loss: 4.83720e-07\n",
      "Epoch: 2011700, elapsed: 1.09e+01, train loss: 5.51416e-07, val loss: 1.52962e-06, min loss: 4.83720e-07\n",
      "Epoch: 2011800, elapsed: 1.07e+01, train loss: 6.86105e-07, val loss: 1.72102e-06, min loss: 4.83720e-07\n",
      "Epoch: 2011900, elapsed: 1.08e+01, train loss: 6.26304e-07, val loss: 1.55952e-06, min loss: 4.83720e-07\n",
      "Epoch: 2012000, elapsed: 1.07e+01, train loss: 5.02541e-07, val loss: 1.48275e-06, min loss: 4.83720e-07\n",
      "Epoch: 2012100, elapsed: 1.08e+01, train loss: 5.13447e-07, val loss: 1.47468e-06, min loss: 4.83720e-07\n",
      "Epoch: 2012200, elapsed: 1.08e+01, train loss: 5.27241e-07, val loss: 1.54529e-06, min loss: 4.83720e-07\n",
      "Epoch: 2012300, elapsed: 1.08e+01, train loss: 4.94901e-07, val loss: 1.42432e-06, min loss: 4.83720e-07\n",
      "Epoch: 2012400, elapsed: 1.07e+01, train loss: 4.86879e-07, val loss: 1.42647e-06, min loss: 4.83720e-07\n",
      "Epoch: 2012500, elapsed: 1.07e+01, train loss: 4.87744e-07, val loss: 1.42898e-06, min loss: 4.83720e-07\n",
      "Epoch: 2012600, elapsed: 1.07e+01, train loss: 5.44236e-07, val loss: 1.47288e-06, min loss: 4.83720e-07\n",
      "Epoch: 2012700, elapsed: 1.64e+01, train loss: 2.14331e-06, val loss: 2.76604e-06, min loss: 4.83720e-07\n",
      "Epoch: 2012800, elapsed: 1.12e+01, train loss: 1.04362e-06, val loss: 2.07942e-06, min loss: 4.83720e-07\n",
      "Epoch: 2012900, elapsed: 1.10e+01, train loss: 6.83679e-07, val loss: 1.69895e-06, min loss: 4.83720e-07\n",
      "Epoch: 2013000, elapsed: 1.11e+01, train loss: 5.32064e-07, val loss: 1.46035e-06, min loss: 4.83720e-07\n",
      "Epoch: 2013100, elapsed: 1.08e+01, train loss: 4.83239e-07, val loss: 1.42352e-06, min loss: 4.83239e-07\n",
      "Epoch: 2013200, elapsed: 1.10e+01, train loss: 4.84326e-07, val loss: 1.42972e-06, min loss: 4.83239e-07\n",
      "Epoch: 2013300, elapsed: 1.09e+01, train loss: 1.24804e-06, val loss: 1.72549e-06, min loss: 4.83239e-07\n",
      "Epoch: 2013400, elapsed: 1.11e+01, train loss: 4.86517e-07, val loss: 1.43156e-06, min loss: 4.83239e-07\n",
      "Epoch: 2013500, elapsed: 1.10e+01, train loss: 4.83516e-07, val loss: 1.40881e-06, min loss: 4.83239e-07\n",
      "Epoch: 2013600, elapsed: 1.10e+01, train loss: 8.02913e-07, val loss: 1.45815e-06, min loss: 4.83239e-07\n",
      "Epoch: 2013700, elapsed: 1.09e+01, train loss: 4.82429e-07, val loss: 1.41854e-06, min loss: 4.82429e-07\n",
      "Epoch: 2013800, elapsed: 1.09e+01, train loss: 4.90981e-07, val loss: 1.39565e-06, min loss: 4.82429e-07\n",
      "Epoch: 2013900, elapsed: 1.10e+01, train loss: 7.85954e-07, val loss: 2.05298e-06, min loss: 4.82429e-07\n",
      "Epoch: 2014000, elapsed: 1.09e+01, train loss: 4.82449e-07, val loss: 1.41999e-06, min loss: 4.82429e-07\n",
      "Epoch: 2014100, elapsed: 1.08e+01, train loss: 8.38345e-07, val loss: 1.64652e-06, min loss: 4.82429e-07\n",
      "Epoch: 2014200, elapsed: 1.10e+01, train loss: 4.82294e-07, val loss: 1.41812e-06, min loss: 4.82294e-07\n",
      "Epoch: 2014300, elapsed: 1.09e+01, train loss: 5.12542e-07, val loss: 1.46340e-06, min loss: 4.82294e-07\n",
      "Epoch: 2014400, elapsed: 1.09e+01, train loss: 5.25811e-07, val loss: 1.38575e-06, min loss: 4.82294e-07\n",
      "Epoch: 2014500, elapsed: 1.10e+01, train loss: 8.36585e-07, val loss: 1.99044e-06, min loss: 4.82294e-07\n",
      "Epoch: 2014600, elapsed: 1.10e+01, train loss: 4.82465e-07, val loss: 1.41931e-06, min loss: 4.82294e-07\n",
      "Epoch: 2014700, elapsed: 1.09e+01, train loss: 8.38646e-07, val loss: 1.67592e-06, min loss: 4.82294e-07\n",
      "Epoch: 2014800, elapsed: 1.09e+01, train loss: 4.86572e-07, val loss: 1.41409e-06, min loss: 4.82294e-07\n",
      "Epoch: 2014900, elapsed: 1.07e+01, train loss: 4.82725e-07, val loss: 1.42004e-06, min loss: 4.82294e-07\n",
      "Epoch: 2015000, elapsed: 1.09e+01, train loss: 4.86812e-07, val loss: 1.42942e-06, min loss: 4.82294e-07\n",
      "Epoch: 2015100, elapsed: 1.26e+01, train loss: 7.30150e-07, val loss: 1.53957e-06, min loss: 4.82294e-07\n",
      "Epoch: 2015200, elapsed: 1.08e+01, train loss: 5.59072e-07, val loss: 1.46021e-06, min loss: 4.82294e-07\n",
      "Epoch: 2015300, elapsed: 1.08e+01, train loss: 4.82357e-07, val loss: 1.42289e-06, min loss: 4.82294e-07\n",
      "Epoch: 2015400, elapsed: 1.08e+01, train loss: 6.65300e-07, val loss: 1.38308e-06, min loss: 4.82294e-07\n",
      "Epoch: 2015500, elapsed: 1.08e+01, train loss: 1.64414e-06, val loss: 2.88517e-06, min loss: 4.82294e-07\n",
      "Epoch: 2015600, elapsed: 1.06e+01, train loss: 4.82464e-07, val loss: 1.41495e-06, min loss: 4.82294e-07\n",
      "Epoch: 2015700, elapsed: 1.07e+01, train loss: 4.84953e-07, val loss: 1.41037e-06, min loss: 4.82294e-07\n",
      "Epoch: 2015800, elapsed: 1.08e+01, train loss: 5.01895e-07, val loss: 1.40676e-06, min loss: 4.82294e-07\n",
      "Epoch: 2015900, elapsed: 1.09e+01, train loss: 6.86765e-07, val loss: 1.65295e-06, min loss: 4.82294e-07\n",
      "Epoch: 2016000, elapsed: 1.08e+01, train loss: 5.03564e-07, val loss: 1.45854e-06, min loss: 4.82294e-07\n",
      "Epoch: 2016100, elapsed: 1.09e+01, train loss: 4.89283e-07, val loss: 1.44266e-06, min loss: 4.82294e-07\n",
      "Epoch: 2016200, elapsed: 1.08e+01, train loss: 4.94146e-07, val loss: 1.43336e-06, min loss: 4.82294e-07\n",
      "Epoch: 2016300, elapsed: 1.64e+01, train loss: 5.36364e-07, val loss: 1.51151e-06, min loss: 4.82294e-07\n",
      "Epoch: 2016400, elapsed: 1.11e+01, train loss: 5.42945e-07, val loss: 1.47006e-06, min loss: 4.82294e-07\n",
      "Epoch: 2016500, elapsed: 1.09e+01, train loss: 6.35404e-07, val loss: 1.51882e-06, min loss: 4.82294e-07\n",
      "Epoch: 2016600, elapsed: 1.12e+01, train loss: 5.10783e-07, val loss: 1.45724e-06, min loss: 4.82294e-07\n",
      "Epoch: 2016700, elapsed: 1.09e+01, train loss: 5.61096e-07, val loss: 1.51511e-06, min loss: 4.82294e-07\n",
      "Epoch: 2016800, elapsed: 1.09e+01, train loss: 4.82525e-07, val loss: 1.41638e-06, min loss: 4.82294e-07\n",
      "Epoch: 2016900, elapsed: 1.11e+01, train loss: 4.95398e-07, val loss: 1.43981e-06, min loss: 4.82294e-07\n",
      "Epoch: 2017000, elapsed: 1.10e+01, train loss: 4.82687e-07, val loss: 1.41144e-06, min loss: 4.82294e-07\n",
      "Epoch: 2017100, elapsed: 1.10e+01, train loss: 7.07812e-07, val loss: 1.67612e-06, min loss: 4.82294e-07\n",
      "Epoch: 2017200, elapsed: 1.09e+01, train loss: 4.83236e-07, val loss: 1.41069e-06, min loss: 4.82294e-07\n",
      "Epoch: 2017300, elapsed: 1.09e+01, train loss: 4.99055e-07, val loss: 1.47116e-06, min loss: 4.82294e-07\n",
      "Epoch: 2017400, elapsed: 1.10e+01, train loss: 8.76530e-07, val loss: 1.92376e-06, min loss: 4.82294e-07\n",
      "Epoch: 2017500, elapsed: 1.08e+01, train loss: 5.18397e-07, val loss: 1.42296e-06, min loss: 4.82294e-07\n",
      "Epoch: 2017600, elapsed: 1.10e+01, train loss: 8.41781e-07, val loss: 1.81819e-06, min loss: 4.82294e-07\n",
      "Epoch: 2017700, elapsed: 1.09e+01, train loss: 6.64310e-07, val loss: 1.63986e-06, min loss: 4.82294e-07\n",
      "Epoch: 2017800, elapsed: 1.10e+01, train loss: 5.18471e-07, val loss: 1.42252e-06, min loss: 4.82294e-07\n",
      "Epoch: 2017900, elapsed: 1.08e+01, train loss: 6.33818e-07, val loss: 1.75574e-06, min loss: 4.82294e-07\n",
      "Epoch: 2018000, elapsed: 1.10e+01, train loss: 4.90524e-07, val loss: 1.47526e-06, min loss: 4.82294e-07\n",
      "Epoch: 2018100, elapsed: 1.10e+01, train loss: 4.82966e-07, val loss: 1.40808e-06, min loss: 4.82294e-07\n",
      "Epoch: 2018200, elapsed: 1.09e+01, train loss: 1.41270e-06, val loss: 1.92993e-06, min loss: 4.82294e-07\n",
      "Epoch: 2018300, elapsed: 1.08e+01, train loss: 6.85169e-07, val loss: 1.63984e-06, min loss: 4.82294e-07\n",
      "Epoch: 2018400, elapsed: 1.07e+01, train loss: 7.16199e-07, val loss: 1.61043e-06, min loss: 4.82294e-07\n",
      "Epoch: 2018500, elapsed: 1.10e+01, train loss: 5.07472e-07, val loss: 1.53972e-06, min loss: 4.82294e-07\n",
      "Epoch: 2018600, elapsed: 1.08e+01, train loss: 1.01978e-06, val loss: 1.90979e-06, min loss: 4.82294e-07\n",
      "Epoch: 2018700, elapsed: 1.09e+01, train loss: 2.32909e-06, val loss: 3.22976e-06, min loss: 4.82294e-07\n",
      "Epoch: 2018800, elapsed: 1.09e+01, train loss: 5.23758e-07, val loss: 1.38234e-06, min loss: 4.82294e-07\n",
      "Epoch: 2018900, elapsed: 1.09e+01, train loss: 4.86134e-07, val loss: 1.40918e-06, min loss: 4.82294e-07\n",
      "Epoch: 2019000, elapsed: 1.07e+01, train loss: 4.80820e-07, val loss: 1.42323e-06, min loss: 4.80820e-07\n",
      "Epoch: 2019100, elapsed: 1.09e+01, train loss: 5.53220e-07, val loss: 1.57019e-06, min loss: 4.80820e-07\n",
      "Epoch: 2019200, elapsed: 1.08e+01, train loss: 4.80797e-07, val loss: 1.42388e-06, min loss: 4.80797e-07\n",
      "Epoch: 2019300, elapsed: 1.08e+01, train loss: 4.85373e-07, val loss: 1.44562e-06, min loss: 4.80797e-07\n",
      "Epoch: 2019400, elapsed: 1.07e+01, train loss: 4.97785e-07, val loss: 1.47186e-06, min loss: 4.80797e-07\n",
      "Epoch: 2019500, elapsed: 1.07e+01, train loss: 8.17706e-07, val loss: 1.54942e-06, min loss: 4.80797e-07\n",
      "Epoch: 2019600, elapsed: 1.09e+01, train loss: 6.45285e-07, val loss: 1.56718e-06, min loss: 4.80797e-07\n",
      "Epoch: 2019700, elapsed: 1.05e+01, train loss: 5.04644e-07, val loss: 1.49277e-06, min loss: 4.80797e-07\n",
      "Epoch: 2019800, elapsed: 1.08e+01, train loss: 5.02037e-07, val loss: 1.41499e-06, min loss: 4.80797e-07\n",
      "Epoch: 2019900, elapsed: 1.64e+01, train loss: 5.36800e-07, val loss: 1.50921e-06, min loss: 4.80797e-07\n",
      "Epoch: 2020000, elapsed: 1.11e+01, train loss: 4.82057e-07, val loss: 1.43000e-06, min loss: 4.80797e-07\n",
      "Epoch: 2020100, elapsed: 1.29e+01, train loss: 5.50863e-07, val loss: 1.49977e-06, min loss: 4.80797e-07\n",
      "Epoch: 2020200, elapsed: 1.08e+01, train loss: 7.24731e-07, val loss: 2.02910e-06, min loss: 4.80797e-07\n",
      "Epoch: 2020300, elapsed: 1.10e+01, train loss: 4.81764e-07, val loss: 1.43073e-06, min loss: 4.80797e-07\n",
      "Epoch: 2020400, elapsed: 1.10e+01, train loss: 5.19538e-07, val loss: 1.37981e-06, min loss: 4.80797e-07\n",
      "Epoch: 2020500, elapsed: 1.11e+01, train loss: 1.33014e-06, val loss: 1.75357e-06, min loss: 4.80797e-07\n",
      "Epoch: 2020600, elapsed: 1.10e+01, train loss: 4.80425e-07, val loss: 1.42448e-06, min loss: 4.80425e-07\n",
      "Epoch: 2020700, elapsed: 1.10e+01, train loss: 4.86749e-07, val loss: 1.41888e-06, min loss: 4.80425e-07\n",
      "Epoch: 2020800, elapsed: 1.09e+01, train loss: 4.88047e-07, val loss: 1.45152e-06, min loss: 4.80425e-07\n",
      "Epoch: 2020900, elapsed: 1.10e+01, train loss: 4.80933e-07, val loss: 1.41537e-06, min loss: 4.80425e-07\n",
      "Epoch: 2021000, elapsed: 1.09e+01, train loss: 6.02984e-07, val loss: 1.55792e-06, min loss: 4.80425e-07\n",
      "Epoch: 2021100, elapsed: 1.10e+01, train loss: 5.09549e-07, val loss: 1.46297e-06, min loss: 4.80425e-07\n",
      "Epoch: 2021200, elapsed: 1.11e+01, train loss: 5.59641e-07, val loss: 1.48289e-06, min loss: 4.80425e-07\n",
      "Epoch: 2021300, elapsed: 1.10e+01, train loss: 4.05976e-06, val loss: 4.95052e-06, min loss: 4.80425e-07\n",
      "Epoch: 2021400, elapsed: 1.10e+01, train loss: 4.80221e-07, val loss: 1.42770e-06, min loss: 4.80221e-07\n",
      "Epoch: 2021500, elapsed: 1.08e+01, train loss: 4.91586e-07, val loss: 1.44657e-06, min loss: 4.80221e-07\n",
      "Epoch: 2021600, elapsed: 1.08e+01, train loss: 3.86266e-06, val loss: 2.69707e-06, min loss: 4.80221e-07\n",
      "Epoch: 2021700, elapsed: 1.11e+01, train loss: 6.36565e-07, val loss: 1.63584e-06, min loss: 4.80221e-07\n",
      "Epoch: 2021800, elapsed: 1.09e+01, train loss: 5.46895e-07, val loss: 1.44704e-06, min loss: 4.80221e-07\n",
      "Epoch: 2021900, elapsed: 1.10e+01, train loss: 6.35300e-07, val loss: 1.53814e-06, min loss: 4.80221e-07\n",
      "Epoch: 2022000, elapsed: 1.09e+01, train loss: 5.66838e-07, val loss: 1.59445e-06, min loss: 4.80221e-07\n",
      "Epoch: 2022100, elapsed: 1.09e+01, train loss: 5.71170e-07, val loss: 1.49960e-06, min loss: 4.80221e-07\n",
      "Epoch: 2022200, elapsed: 1.10e+01, train loss: 5.40389e-07, val loss: 1.57808e-06, min loss: 4.80221e-07\n",
      "Epoch: 2022300, elapsed: 1.10e+01, train loss: 4.83350e-07, val loss: 1.42592e-06, min loss: 4.80221e-07\n",
      "Epoch: 2022400, elapsed: 1.09e+01, train loss: 6.78897e-07, val loss: 1.60724e-06, min loss: 4.80221e-07\n",
      "Epoch: 2022500, elapsed: 1.10e+01, train loss: 5.09965e-07, val loss: 1.47316e-06, min loss: 4.80221e-07\n",
      "Epoch: 2022600, elapsed: 1.09e+01, train loss: 4.82215e-07, val loss: 1.41470e-06, min loss: 4.80221e-07\n",
      "Epoch: 2022700, elapsed: 1.08e+01, train loss: 4.82425e-07, val loss: 1.42878e-06, min loss: 4.80221e-07\n",
      "Epoch: 2022800, elapsed: 1.08e+01, train loss: 5.82629e-07, val loss: 1.51813e-06, min loss: 4.80221e-07\n",
      "Epoch: 2022900, elapsed: 1.07e+01, train loss: 5.99162e-07, val loss: 1.41971e-06, min loss: 4.80221e-07\n",
      "Epoch: 2023000, elapsed: 1.09e+01, train loss: 4.91972e-07, val loss: 1.43416e-06, min loss: 4.80221e-07\n",
      "Epoch: 2023100, elapsed: 1.08e+01, train loss: 4.80000e-07, val loss: 1.42714e-06, min loss: 4.80000e-07\n",
      "Epoch: 2023200, elapsed: 1.09e+01, train loss: 6.01594e-07, val loss: 1.38050e-06, min loss: 4.80000e-07\n",
      "Epoch: 2023300, elapsed: 1.09e+01, train loss: 8.13947e-07, val loss: 1.59373e-06, min loss: 4.80000e-07\n",
      "Epoch: 2023400, elapsed: 1.07e+01, train loss: 5.10392e-07, val loss: 1.42108e-06, min loss: 4.80000e-07\n",
      "Epoch: 2023500, elapsed: 1.65e+01, train loss: 8.51238e-07, val loss: 1.85526e-06, min loss: 4.80000e-07\n",
      "Epoch: 2023600, elapsed: 1.10e+01, train loss: 4.86682e-07, val loss: 1.42505e-06, min loss: 4.80000e-07\n",
      "Epoch: 2023700, elapsed: 1.10e+01, train loss: 4.80998e-07, val loss: 1.43619e-06, min loss: 4.80000e-07\n",
      "Epoch: 2023800, elapsed: 1.11e+01, train loss: 4.94114e-07, val loss: 1.41583e-06, min loss: 4.80000e-07\n",
      "Epoch: 2023900, elapsed: 1.10e+01, train loss: 2.05641e-06, val loss: 3.34079e-06, min loss: 4.80000e-07\n",
      "Epoch: 2024000, elapsed: 1.10e+01, train loss: 4.79633e-07, val loss: 1.42340e-06, min loss: 4.79633e-07\n",
      "Epoch: 2024100, elapsed: 1.11e+01, train loss: 5.30064e-07, val loss: 1.41486e-06, min loss: 4.79633e-07\n",
      "Epoch: 2024200, elapsed: 1.12e+01, train loss: 5.47918e-07, val loss: 1.48790e-06, min loss: 4.79633e-07\n",
      "Epoch: 2024300, elapsed: 1.10e+01, train loss: 1.51676e-06, val loss: 1.93797e-06, min loss: 4.79633e-07\n",
      "Epoch: 2024400, elapsed: 1.11e+01, train loss: 2.56336e-06, val loss: 3.18555e-06, min loss: 4.79633e-07\n",
      "Epoch: 2024500, elapsed: 1.09e+01, train loss: 4.97675e-07, val loss: 1.47446e-06, min loss: 4.79633e-07\n",
      "Epoch: 2024600, elapsed: 1.09e+01, train loss: 4.79989e-07, val loss: 1.43178e-06, min loss: 4.79633e-07\n",
      "Epoch: 2024700, elapsed: 1.11e+01, train loss: 5.72706e-07, val loss: 1.57773e-06, min loss: 4.79633e-07\n",
      "Epoch: 2024800, elapsed: 1.09e+01, train loss: 5.52895e-07, val loss: 1.53436e-06, min loss: 4.79633e-07\n",
      "Epoch: 2024900, elapsed: 1.11e+01, train loss: 4.87428e-07, val loss: 1.46274e-06, min loss: 4.79633e-07\n",
      "Epoch: 2025000, elapsed: 1.10e+01, train loss: 4.79743e-07, val loss: 1.43421e-06, min loss: 4.79633e-07\n",
      "Epoch: 2025100, elapsed: 1.30e+01, train loss: 4.80682e-07, val loss: 1.40941e-06, min loss: 4.79633e-07\n",
      "Epoch: 2025200, elapsed: 1.09e+01, train loss: 4.96729e-07, val loss: 1.40979e-06, min loss: 4.79633e-07\n",
      "Epoch: 2025300, elapsed: 1.11e+01, train loss: 5.04341e-07, val loss: 1.46867e-06, min loss: 4.79633e-07\n",
      "Epoch: 2025400, elapsed: 1.08e+01, train loss: 9.53920e-07, val loss: 1.97554e-06, min loss: 4.79633e-07\n",
      "Epoch: 2025500, elapsed: 1.09e+01, train loss: 4.81838e-07, val loss: 1.41470e-06, min loss: 4.79633e-07\n",
      "Epoch: 2025600, elapsed: 1.10e+01, train loss: 4.79419e-07, val loss: 1.42480e-06, min loss: 4.79419e-07\n",
      "Epoch: 2025700, elapsed: 1.09e+01, train loss: 4.85551e-07, val loss: 1.51569e-06, min loss: 4.79419e-07\n",
      "Epoch: 2025800, elapsed: 1.10e+01, train loss: 4.95660e-07, val loss: 1.49017e-06, min loss: 4.79419e-07\n",
      "Epoch: 2025900, elapsed: 1.09e+01, train loss: 5.21165e-07, val loss: 1.43959e-06, min loss: 4.79419e-07\n",
      "Epoch: 2026000, elapsed: 1.10e+01, train loss: 6.09958e-07, val loss: 1.78134e-06, min loss: 4.79419e-07\n",
      "Epoch: 2026100, elapsed: 1.10e+01, train loss: 7.07108e-07, val loss: 1.52895e-06, min loss: 4.79419e-07\n",
      "Epoch: 2026200, elapsed: 1.08e+01, train loss: 5.25606e-07, val loss: 1.42047e-06, min loss: 4.79419e-07\n",
      "Epoch: 2026300, elapsed: 1.10e+01, train loss: 5.61937e-07, val loss: 1.59211e-06, min loss: 4.79419e-07\n",
      "Epoch: 2026400, elapsed: 1.09e+01, train loss: 5.04442e-07, val loss: 1.44027e-06, min loss: 4.79419e-07\n",
      "Epoch: 2026500, elapsed: 1.09e+01, train loss: 4.85322e-07, val loss: 1.43562e-06, min loss: 4.79419e-07\n",
      "Epoch: 2026600, elapsed: 1.08e+01, train loss: 4.85784e-07, val loss: 1.46419e-06, min loss: 4.79419e-07\n",
      "Epoch: 2026700, elapsed: 1.09e+01, train loss: 4.81084e-07, val loss: 1.44214e-06, min loss: 4.79419e-07\n",
      "Epoch: 2026800, elapsed: 1.10e+01, train loss: 4.99107e-07, val loss: 1.46488e-06, min loss: 4.79419e-07\n",
      "Epoch: 2026900, elapsed: 1.08e+01, train loss: 5.05627e-07, val loss: 1.41758e-06, min loss: 4.79419e-07\n",
      "Epoch: 2027000, elapsed: 1.08e+01, train loss: 6.84892e-07, val loss: 1.68617e-06, min loss: 4.79419e-07\n",
      "Epoch: 2027100, elapsed: 1.69e+01, train loss: 4.95319e-07, val loss: 1.42129e-06, min loss: 4.79419e-07\n",
      "Epoch: 2027200, elapsed: 1.09e+01, train loss: 6.53271e-07, val loss: 1.63978e-06, min loss: 4.79419e-07\n",
      "Epoch: 2027300, elapsed: 1.10e+01, train loss: 7.01686e-07, val loss: 1.86118e-06, min loss: 4.79419e-07\n",
      "Epoch: 2027400, elapsed: 1.11e+01, train loss: 4.82221e-07, val loss: 1.44322e-06, min loss: 4.79419e-07\n",
      "Epoch: 2027500, elapsed: 1.11e+01, train loss: 5.14181e-07, val loss: 1.49617e-06, min loss: 4.79419e-07\n",
      "Epoch: 2027600, elapsed: 1.12e+01, train loss: 1.39635e-06, val loss: 2.51957e-06, min loss: 4.79419e-07\n",
      "Epoch: 2027700, elapsed: 1.10e+01, train loss: 5.35669e-07, val loss: 1.47808e-06, min loss: 4.79419e-07\n",
      "Epoch: 2027800, elapsed: 1.12e+01, train loss: 4.87670e-07, val loss: 1.48248e-06, min loss: 4.79419e-07\n",
      "Epoch: 2027900, elapsed: 1.08e+01, train loss: 4.84151e-07, val loss: 1.42912e-06, min loss: 4.79419e-07\n",
      "Epoch: 2028000, elapsed: 1.10e+01, train loss: 6.00848e-07, val loss: 1.64481e-06, min loss: 4.79419e-07\n",
      "Epoch: 2028100, elapsed: 1.08e+01, train loss: 1.72278e-06, val loss: 2.59971e-06, min loss: 4.79419e-07\n",
      "Epoch: 2028200, elapsed: 1.08e+01, train loss: 1.01545e-06, val loss: 2.12906e-06, min loss: 4.79419e-07\n",
      "Epoch: 2028300, elapsed: 1.11e+01, train loss: 4.78086e-07, val loss: 1.43151e-06, min loss: 4.78086e-07\n",
      "Epoch: 2028400, elapsed: 1.08e+01, train loss: 4.81418e-07, val loss: 1.44356e-06, min loss: 4.78086e-07\n",
      "Epoch: 2028500, elapsed: 1.09e+01, train loss: 3.52250e-06, val loss: 4.32327e-06, min loss: 4.78086e-07\n",
      "Epoch: 2028600, elapsed: 1.08e+01, train loss: 6.33525e-07, val loss: 1.80236e-06, min loss: 4.78086e-07\n",
      "Epoch: 2028700, elapsed: 1.09e+01, train loss: 4.88358e-07, val loss: 1.46363e-06, min loss: 4.78086e-07\n",
      "Epoch: 2028800, elapsed: 1.11e+01, train loss: 1.05370e-06, val loss: 1.89127e-06, min loss: 4.78086e-07\n",
      "Epoch: 2028900, elapsed: 1.09e+01, train loss: 9.87761e-07, val loss: 1.86077e-06, min loss: 4.78086e-07\n",
      "Epoch: 2029000, elapsed: 1.10e+01, train loss: 1.20435e-06, val loss: 2.50691e-06, min loss: 4.78086e-07\n",
      "Epoch: 2029100, elapsed: 1.08e+01, train loss: 8.72813e-07, val loss: 1.93935e-06, min loss: 4.78086e-07\n",
      "Epoch: 2029200, elapsed: 1.09e+01, train loss: 4.81900e-07, val loss: 1.42635e-06, min loss: 4.78086e-07\n",
      "Epoch: 2029300, elapsed: 1.08e+01, train loss: 4.78737e-07, val loss: 1.44256e-06, min loss: 4.78086e-07\n",
      "Epoch: 2029400, elapsed: 1.07e+01, train loss: 4.88196e-07, val loss: 1.45864e-06, min loss: 4.78086e-07\n",
      "Epoch: 2029500, elapsed: 1.08e+01, train loss: 1.88828e-06, val loss: 1.77887e-06, min loss: 4.78086e-07\n",
      "Epoch: 2029600, elapsed: 1.08e+01, train loss: 4.78718e-07, val loss: 1.43589e-06, min loss: 4.78086e-07\n",
      "Epoch: 2029700, elapsed: 1.07e+01, train loss: 4.85054e-07, val loss: 1.45175e-06, min loss: 4.78086e-07\n",
      "Epoch: 2029800, elapsed: 1.07e+01, train loss: 5.19886e-07, val loss: 1.43238e-06, min loss: 4.78086e-07\n",
      "Epoch: 2029900, elapsed: 1.10e+01, train loss: 8.13404e-07, val loss: 1.72473e-06, min loss: 4.78086e-07\n",
      "Epoch: 2030000, elapsed: 1.08e+01, train loss: 1.64284e-06, val loss: 2.23896e-06, min loss: 4.78086e-07\n",
      "Epoch: 2030100, elapsed: 1.26e+01, train loss: 8.51313e-07, val loss: 1.81154e-06, min loss: 4.78086e-07\n",
      "Epoch: 2030200, elapsed: 1.09e+01, train loss: 4.77712e-07, val loss: 1.43785e-06, min loss: 4.77712e-07\n",
      "Epoch: 2030300, elapsed: 1.08e+01, train loss: 4.79410e-07, val loss: 1.44055e-06, min loss: 4.77712e-07\n",
      "Epoch: 2030400, elapsed: 1.08e+01, train loss: 5.50696e-07, val loss: 1.56163e-06, min loss: 4.77712e-07\n",
      "Epoch: 2030500, elapsed: 1.09e+01, train loss: 1.03566e-06, val loss: 1.83246e-06, min loss: 4.77712e-07\n",
      "Epoch: 2030600, elapsed: 1.10e+01, train loss: 4.86803e-07, val loss: 1.43110e-06, min loss: 4.77712e-07\n",
      "Epoch: 2030700, elapsed: 1.64e+01, train loss: 4.92050e-07, val loss: 1.44232e-06, min loss: 4.77712e-07\n",
      "Epoch: 2030800, elapsed: 1.11e+01, train loss: 1.47311e-06, val loss: 2.33209e-06, min loss: 4.77712e-07\n",
      "Epoch: 2030900, elapsed: 1.09e+01, train loss: 5.84567e-07, val loss: 1.59725e-06, min loss: 4.77712e-07\n",
      "Epoch: 2031000, elapsed: 1.10e+01, train loss: 4.77399e-07, val loss: 1.43101e-06, min loss: 4.77399e-07\n",
      "Epoch: 2031100, elapsed: 1.11e+01, train loss: 4.90494e-07, val loss: 1.46525e-06, min loss: 4.77399e-07\n",
      "Epoch: 2031200, elapsed: 1.11e+01, train loss: 6.24398e-07, val loss: 1.58184e-06, min loss: 4.77399e-07\n",
      "Epoch: 2031300, elapsed: 1.10e+01, train loss: 4.77188e-07, val loss: 1.43957e-06, min loss: 4.77188e-07\n",
      "Epoch: 2031400, elapsed: 1.11e+01, train loss: 4.78664e-07, val loss: 1.43586e-06, min loss: 4.77188e-07\n",
      "Epoch: 2031500, elapsed: 1.08e+01, train loss: 4.77531e-07, val loss: 1.43737e-06, min loss: 4.77188e-07\n",
      "Epoch: 2031600, elapsed: 1.09e+01, train loss: 8.62715e-07, val loss: 1.99912e-06, min loss: 4.77188e-07\n",
      "Epoch: 2031700, elapsed: 1.09e+01, train loss: 2.01611e-06, val loss: 2.99777e-06, min loss: 4.77188e-07\n",
      "Epoch: 2031800, elapsed: 1.10e+01, train loss: 4.82204e-07, val loss: 1.44534e-06, min loss: 4.77188e-07\n",
      "Epoch: 2031900, elapsed: 1.10e+01, train loss: 4.78403e-07, val loss: 1.44403e-06, min loss: 4.77188e-07\n",
      "Epoch: 2032000, elapsed: 1.09e+01, train loss: 4.79781e-07, val loss: 1.43088e-06, min loss: 4.77188e-07\n",
      "Epoch: 2032100, elapsed: 1.10e+01, train loss: 5.17379e-07, val loss: 1.41015e-06, min loss: 4.77188e-07\n",
      "Epoch: 2032200, elapsed: 1.08e+01, train loss: 6.59785e-07, val loss: 1.63103e-06, min loss: 4.77188e-07\n",
      "Epoch: 2032300, elapsed: 1.10e+01, train loss: 4.84564e-07, val loss: 1.46061e-06, min loss: 4.77188e-07\n",
      "Epoch: 2032400, elapsed: 1.08e+01, train loss: 4.90256e-07, val loss: 1.42515e-06, min loss: 4.77188e-07\n",
      "Epoch: 2032500, elapsed: 1.09e+01, train loss: 4.89483e-07, val loss: 1.44227e-06, min loss: 4.77188e-07\n",
      "Epoch: 2032600, elapsed: 1.10e+01, train loss: 4.84205e-07, val loss: 1.45958e-06, min loss: 4.77188e-07\n",
      "Epoch: 2032700, elapsed: 1.10e+01, train loss: 4.83238e-07, val loss: 1.44400e-06, min loss: 4.77188e-07\n",
      "Epoch: 2032800, elapsed: 1.09e+01, train loss: 7.26085e-07, val loss: 1.56903e-06, min loss: 4.77188e-07\n",
      "Epoch: 2032900, elapsed: 1.08e+01, train loss: 5.38538e-07, val loss: 1.54447e-06, min loss: 4.77188e-07\n",
      "Epoch: 2033000, elapsed: 1.10e+01, train loss: 4.93460e-07, val loss: 1.50277e-06, min loss: 4.77188e-07\n",
      "Epoch: 2033100, elapsed: 1.10e+01, train loss: 4.86746e-07, val loss: 1.47414e-06, min loss: 4.77188e-07\n",
      "Epoch: 2033200, elapsed: 1.07e+01, train loss: 5.02285e-07, val loss: 1.61419e-06, min loss: 4.77188e-07\n",
      "Epoch: 2033300, elapsed: 1.08e+01, train loss: 5.35056e-07, val loss: 1.57456e-06, min loss: 4.77188e-07\n",
      "Epoch: 2033400, elapsed: 1.09e+01, train loss: 7.07860e-07, val loss: 1.55051e-06, min loss: 4.77188e-07\n",
      "Epoch: 2033500, elapsed: 1.08e+01, train loss: 4.78052e-07, val loss: 1.44145e-06, min loss: 4.77188e-07\n",
      "Epoch: 2033600, elapsed: 1.09e+01, train loss: 4.83715e-07, val loss: 1.45441e-06, min loss: 4.77188e-07\n",
      "Epoch: 2033700, elapsed: 1.08e+01, train loss: 1.19696e-06, val loss: 2.21737e-06, min loss: 4.77188e-07\n",
      "Epoch: 2033800, elapsed: 1.09e+01, train loss: 5.77546e-07, val loss: 1.68834e-06, min loss: 4.77188e-07\n",
      "Epoch: 2033900, elapsed: 1.10e+01, train loss: 4.76832e-07, val loss: 1.44047e-06, min loss: 4.76832e-07\n",
      "Epoch: 2034000, elapsed: 1.09e+01, train loss: 4.80004e-07, val loss: 1.45226e-06, min loss: 4.76832e-07\n",
      "Epoch: 2034100, elapsed: 1.08e+01, train loss: 7.43976e-07, val loss: 1.48615e-06, min loss: 4.76832e-07\n",
      "Epoch: 2034200, elapsed: 1.06e+01, train loss: 3.91065e-06, val loss: 4.52185e-06, min loss: 4.76832e-07\n",
      "Epoch: 2034300, elapsed: 1.63e+01, train loss: 4.76611e-07, val loss: 1.44149e-06, min loss: 4.76611e-07\n",
      "Epoch: 2034400, elapsed: 1.11e+01, train loss: 4.92008e-07, val loss: 1.40171e-06, min loss: 4.76611e-07\n",
      "Epoch: 2034500, elapsed: 1.11e+01, train loss: 4.78112e-07, val loss: 1.43103e-06, min loss: 4.76611e-07\n",
      "Epoch: 2034600, elapsed: 1.11e+01, train loss: 4.79401e-07, val loss: 1.45910e-06, min loss: 4.76611e-07\n",
      "Epoch: 2034700, elapsed: 1.11e+01, train loss: 4.93317e-07, val loss: 1.46465e-06, min loss: 4.76611e-07\n",
      "Epoch: 2034800, elapsed: 1.10e+01, train loss: 9.79759e-07, val loss: 1.95109e-06, min loss: 4.76611e-07\n",
      "Epoch: 2034900, elapsed: 1.12e+01, train loss: 4.76403e-07, val loss: 1.43747e-06, min loss: 4.76403e-07\n",
      "Epoch: 2035000, elapsed: 1.11e+01, train loss: 9.94260e-07, val loss: 2.33229e-06, min loss: 4.76403e-07\n",
      "Epoch: 2035100, elapsed: 1.30e+01, train loss: 4.76040e-07, val loss: 1.44211e-06, min loss: 4.76040e-07\n",
      "Epoch: 2035200, elapsed: 1.10e+01, train loss: 2.11651e-06, val loss: 2.08067e-06, min loss: 4.76040e-07\n",
      "Epoch: 2035300, elapsed: 1.08e+01, train loss: 4.76003e-07, val loss: 1.44421e-06, min loss: 4.76003e-07\n",
      "Epoch: 2035400, elapsed: 1.10e+01, train loss: 3.38814e-06, val loss: 3.48754e-06, min loss: 4.76003e-07\n",
      "Epoch: 2035500, elapsed: 1.11e+01, train loss: 4.76029e-07, val loss: 1.44429e-06, min loss: 4.76003e-07\n",
      "Epoch: 2035600, elapsed: 1.12e+01, train loss: 4.79258e-07, val loss: 1.44166e-06, min loss: 4.76003e-07\n",
      "Epoch: 2035700, elapsed: 1.09e+01, train loss: 2.29184e-06, val loss: 3.56489e-06, min loss: 4.76003e-07\n",
      "Epoch: 2035800, elapsed: 1.09e+01, train loss: 4.76413e-07, val loss: 1.44766e-06, min loss: 4.76003e-07\n",
      "Epoch: 2035900, elapsed: 1.07e+01, train loss: 1.03772e-06, val loss: 1.72681e-06, min loss: 4.76003e-07\n",
      "Epoch: 2036000, elapsed: 1.09e+01, train loss: 4.81807e-07, val loss: 1.42460e-06, min loss: 4.76003e-07\n",
      "Epoch: 2036100, elapsed: 1.10e+01, train loss: 6.68824e-07, val loss: 1.46389e-06, min loss: 4.76003e-07\n",
      "Epoch: 2036200, elapsed: 1.09e+01, train loss: 6.37734e-07, val loss: 1.61470e-06, min loss: 4.76003e-07\n",
      "Epoch: 2036300, elapsed: 1.10e+01, train loss: 8.98522e-07, val loss: 1.50629e-06, min loss: 4.76003e-07\n",
      "Epoch: 2036400, elapsed: 1.08e+01, train loss: 1.02292e-06, val loss: 2.13766e-06, min loss: 4.76003e-07\n",
      "Epoch: 2036500, elapsed: 1.10e+01, train loss: 4.88505e-07, val loss: 1.42378e-06, min loss: 4.76003e-07\n",
      "Epoch: 2036600, elapsed: 1.08e+01, train loss: 4.81585e-07, val loss: 1.40737e-06, min loss: 4.76003e-07\n",
      "Epoch: 2036700, elapsed: 1.08e+01, train loss: 4.89703e-07, val loss: 1.44373e-06, min loss: 4.76003e-07\n",
      "Epoch: 2036800, elapsed: 1.08e+01, train loss: 4.82470e-07, val loss: 1.42095e-06, min loss: 4.76003e-07\n",
      "Epoch: 2036900, elapsed: 1.10e+01, train loss: 4.82597e-07, val loss: 1.45589e-06, min loss: 4.76003e-07\n",
      "Epoch: 2037000, elapsed: 1.08e+01, train loss: 5.22793e-07, val loss: 1.45841e-06, min loss: 4.76003e-07\n",
      "Epoch: 2037100, elapsed: 1.07e+01, train loss: 9.85525e-07, val loss: 1.96471e-06, min loss: 4.76003e-07\n",
      "Epoch: 2037200, elapsed: 1.09e+01, train loss: 6.00851e-07, val loss: 1.70057e-06, min loss: 4.76003e-07\n",
      "Epoch: 2037300, elapsed: 1.08e+01, train loss: 4.75860e-07, val loss: 1.43940e-06, min loss: 4.75860e-07\n",
      "Epoch: 2037400, elapsed: 1.09e+01, train loss: 4.79637e-07, val loss: 1.43414e-06, min loss: 4.75860e-07\n",
      "Epoch: 2037500, elapsed: 1.09e+01, train loss: 4.81251e-07, val loss: 1.42909e-06, min loss: 4.75860e-07\n",
      "Epoch: 2037600, elapsed: 1.09e+01, train loss: 5.97591e-07, val loss: 1.66339e-06, min loss: 4.75860e-07\n",
      "Epoch: 2037700, elapsed: 1.09e+01, train loss: 1.63260e-06, val loss: 2.60446e-06, min loss: 4.75860e-07\n",
      "Epoch: 2037800, elapsed: 1.08e+01, train loss: 7.81615e-07, val loss: 1.85079e-06, min loss: 4.75860e-07\n",
      "Epoch: 2037900, elapsed: 1.63e+01, train loss: 5.87556e-07, val loss: 1.45026e-06, min loss: 4.75860e-07\n",
      "Epoch: 2038000, elapsed: 1.11e+01, train loss: 7.42637e-07, val loss: 1.61790e-06, min loss: 4.75860e-07\n",
      "Epoch: 2038100, elapsed: 1.11e+01, train loss: 4.82905e-07, val loss: 1.43538e-06, min loss: 4.75860e-07\n",
      "Epoch: 2038200, elapsed: 1.09e+01, train loss: 4.92308e-07, val loss: 1.44021e-06, min loss: 4.75860e-07\n",
      "Epoch: 2038300, elapsed: 1.10e+01, train loss: 1.42019e-06, val loss: 2.72913e-06, min loss: 4.75860e-07\n",
      "Epoch: 2038400, elapsed: 1.10e+01, train loss: 6.04043e-07, val loss: 1.52220e-06, min loss: 4.75860e-07\n",
      "Epoch: 2038500, elapsed: 1.09e+01, train loss: 4.77433e-07, val loss: 1.44452e-06, min loss: 4.75860e-07\n",
      "Epoch: 2038600, elapsed: 1.11e+01, train loss: 4.96493e-07, val loss: 1.49013e-06, min loss: 4.75860e-07\n",
      "Epoch: 2038700, elapsed: 1.11e+01, train loss: 5.53296e-07, val loss: 1.50999e-06, min loss: 4.75860e-07\n",
      "Epoch: 2038800, elapsed: 1.11e+01, train loss: 6.13231e-07, val loss: 1.53025e-06, min loss: 4.75860e-07\n",
      "Epoch: 2038900, elapsed: 1.10e+01, train loss: 4.90689e-07, val loss: 1.48805e-06, min loss: 4.75860e-07\n",
      "Epoch: 2039000, elapsed: 1.08e+01, train loss: 4.75528e-07, val loss: 1.45422e-06, min loss: 4.75528e-07\n",
      "Epoch: 2039100, elapsed: 1.09e+01, train loss: 4.78614e-07, val loss: 1.44262e-06, min loss: 4.75528e-07\n",
      "Epoch: 2039200, elapsed: 1.08e+01, train loss: 1.43308e-06, val loss: 2.17556e-06, min loss: 4.75528e-07\n",
      "Epoch: 2039300, elapsed: 1.10e+01, train loss: 4.74778e-07, val loss: 1.44346e-06, min loss: 4.74778e-07\n",
      "Epoch: 2039400, elapsed: 1.09e+01, train loss: 4.80679e-07, val loss: 1.44327e-06, min loss: 4.74778e-07\n",
      "Epoch: 2039500, elapsed: 1.04e+01, train loss: 4.78472e-07, val loss: 1.47368e-06, min loss: 4.74778e-07\n",
      "Epoch: 2039600, elapsed: 1.06e+01, train loss: 4.87493e-07, val loss: 1.45047e-06, min loss: 4.74778e-07\n",
      "Epoch: 2039700, elapsed: 1.04e+01, train loss: 5.05777e-07, val loss: 1.54002e-06, min loss: 4.74778e-07\n",
      "Epoch: 2039800, elapsed: 1.09e+01, train loss: 6.05577e-07, val loss: 1.51839e-06, min loss: 4.74778e-07\n",
      "Epoch: 2039900, elapsed: 1.05e+01, train loss: 4.78698e-07, val loss: 1.47350e-06, min loss: 4.74778e-07\n",
      "Epoch: 2040000, elapsed: 1.07e+01, train loss: 7.51619e-07, val loss: 1.98701e-06, min loss: 4.74778e-07\n",
      "Epoch: 2040100, elapsed: 1.28e+01, train loss: 4.74540e-07, val loss: 1.44372e-06, min loss: 4.74540e-07\n",
      "Epoch: 2040200, elapsed: 1.08e+01, train loss: 5.15835e-07, val loss: 1.59517e-06, min loss: 4.74540e-07\n",
      "Epoch: 2040300, elapsed: 1.06e+01, train loss: 4.74497e-07, val loss: 1.44515e-06, min loss: 4.74497e-07\n",
      "Epoch: 2040400, elapsed: 1.03e+01, train loss: 1.00107e-06, val loss: 2.31432e-06, min loss: 4.74497e-07\n",
      "Epoch: 2040500, elapsed: 1.04e+01, train loss: 4.74451e-07, val loss: 1.44381e-06, min loss: 4.74451e-07\n",
      "Epoch: 2040600, elapsed: 1.06e+01, train loss: 2.45010e-06, val loss: 3.54235e-06, min loss: 4.74451e-07\n",
      "Epoch: 2040700, elapsed: 1.06e+01, train loss: 4.74514e-07, val loss: 1.44862e-06, min loss: 4.74451e-07\n",
      "Epoch: 2040800, elapsed: 1.08e+01, train loss: 4.74820e-07, val loss: 1.44743e-06, min loss: 4.74451e-07\n",
      "Epoch: 2040900, elapsed: 1.07e+01, train loss: 4.75645e-07, val loss: 1.43636e-06, min loss: 4.74451e-07\n",
      "Epoch: 2041000, elapsed: 1.06e+01, train loss: 5.01441e-07, val loss: 1.47978e-06, min loss: 4.74451e-07\n",
      "Epoch: 2041100, elapsed: 1.09e+01, train loss: 7.28188e-07, val loss: 1.76320e-06, min loss: 4.74451e-07\n",
      "Epoch: 2041200, elapsed: 1.07e+01, train loss: 4.75153e-07, val loss: 1.44590e-06, min loss: 4.74451e-07\n",
      "Epoch: 2041300, elapsed: 1.08e+01, train loss: 5.51773e-07, val loss: 1.63811e-06, min loss: 4.74451e-07\n",
      "Epoch: 2041400, elapsed: 1.08e+01, train loss: 5.21405e-07, val loss: 1.43398e-06, min loss: 4.74451e-07\n",
      "Epoch: 2041500, elapsed: 1.62e+01, train loss: 9.85165e-07, val loss: 2.18529e-06, min loss: 4.74451e-07\n",
      "Epoch: 2041600, elapsed: 1.11e+01, train loss: 8.36111e-07, val loss: 1.55137e-06, min loss: 4.74451e-07\n",
      "Epoch: 2041700, elapsed: 1.12e+01, train loss: 8.41042e-07, val loss: 1.73023e-06, min loss: 4.74451e-07\n",
      "Epoch: 2041800, elapsed: 1.11e+01, train loss: 4.78234e-07, val loss: 1.43931e-06, min loss: 4.74451e-07\n",
      "Epoch: 2041900, elapsed: 1.12e+01, train loss: 4.74902e-07, val loss: 1.45172e-06, min loss: 4.74451e-07\n",
      "Epoch: 2042000, elapsed: 1.09e+01, train loss: 4.88015e-07, val loss: 1.45656e-06, min loss: 4.74451e-07\n",
      "Epoch: 2042100, elapsed: 1.12e+01, train loss: 9.11093e-07, val loss: 2.19421e-06, min loss: 4.74451e-07\n",
      "Epoch: 2042200, elapsed: 1.08e+01, train loss: 4.82852e-07, val loss: 1.47301e-06, min loss: 4.74451e-07\n",
      "Epoch: 2042300, elapsed: 1.10e+01, train loss: 4.85267e-07, val loss: 1.48666e-06, min loss: 4.74451e-07\n",
      "Epoch: 2042400, elapsed: 1.07e+01, train loss: 4.79126e-07, val loss: 1.44008e-06, min loss: 4.74451e-07\n",
      "Epoch: 2042500, elapsed: 1.10e+01, train loss: 1.61279e-06, val loss: 2.12205e-06, min loss: 4.74451e-07\n",
      "Epoch: 2042600, elapsed: 1.09e+01, train loss: 4.77881e-07, val loss: 1.50026e-06, min loss: 4.74451e-07\n",
      "Epoch: 2042700, elapsed: 1.09e+01, train loss: 5.57857e-07, val loss: 1.58474e-06, min loss: 4.74451e-07\n",
      "Epoch: 2042800, elapsed: 1.10e+01, train loss: 5.62007e-07, val loss: 1.47291e-06, min loss: 4.74451e-07\n",
      "Epoch: 2042900, elapsed: 1.09e+01, train loss: 4.76111e-07, val loss: 1.47023e-06, min loss: 4.74451e-07\n",
      "Epoch: 2043000, elapsed: 1.09e+01, train loss: 7.83069e-07, val loss: 2.06115e-06, min loss: 4.74451e-07\n",
      "Epoch: 2043100, elapsed: 1.07e+01, train loss: 4.73785e-07, val loss: 1.45162e-06, min loss: 4.73785e-07\n",
      "Epoch: 2043200, elapsed: 1.09e+01, train loss: 4.74035e-07, val loss: 1.45832e-06, min loss: 4.73785e-07\n",
      "Epoch: 2043300, elapsed: 1.08e+01, train loss: 7.07192e-07, val loss: 1.87775e-06, min loss: 4.73785e-07\n",
      "Epoch: 2043400, elapsed: 1.10e+01, train loss: 4.79956e-07, val loss: 1.46470e-06, min loss: 4.73785e-07\n",
      "Epoch: 2043500, elapsed: 1.09e+01, train loss: 6.68007e-07, val loss: 1.71547e-06, min loss: 4.73785e-07\n",
      "Epoch: 2043600, elapsed: 1.09e+01, train loss: 2.51831e-06, val loss: 4.22088e-06, min loss: 4.73785e-07\n",
      "Epoch: 2043700, elapsed: 1.10e+01, train loss: 4.73557e-07, val loss: 1.44554e-06, min loss: 4.73557e-07\n",
      "Epoch: 2043800, elapsed: 1.08e+01, train loss: 4.84598e-07, val loss: 1.44479e-06, min loss: 4.73557e-07\n",
      "Epoch: 2043900, elapsed: 1.08e+01, train loss: 8.18810e-07, val loss: 2.22665e-06, min loss: 4.73557e-07\n",
      "Epoch: 2044000, elapsed: 1.07e+01, train loss: 5.05344e-07, val loss: 1.47896e-06, min loss: 4.73557e-07\n",
      "Epoch: 2044100, elapsed: 1.09e+01, train loss: 5.76075e-07, val loss: 1.49719e-06, min loss: 4.73557e-07\n",
      "Epoch: 2044200, elapsed: 1.09e+01, train loss: 6.31531e-07, val loss: 1.53887e-06, min loss: 4.73557e-07\n",
      "Epoch: 2044300, elapsed: 1.08e+01, train loss: 6.50621e-07, val loss: 1.69105e-06, min loss: 4.73557e-07\n",
      "Epoch: 2044400, elapsed: 1.08e+01, train loss: 2.23638e-06, val loss: 3.58358e-06, min loss: 4.73557e-07\n",
      "Epoch: 2044500, elapsed: 1.08e+01, train loss: 5.07728e-07, val loss: 1.55319e-06, min loss: 4.73557e-07\n",
      "Epoch: 2044600, elapsed: 1.10e+01, train loss: 6.05493e-07, val loss: 1.64965e-06, min loss: 4.73557e-07\n",
      "Epoch: 2044700, elapsed: 1.09e+01, train loss: 4.88148e-07, val loss: 1.42447e-06, min loss: 4.73557e-07\n",
      "Epoch: 2044800, elapsed: 1.08e+01, train loss: 7.39581e-07, val loss: 1.84092e-06, min loss: 4.73557e-07\n",
      "Epoch: 2044900, elapsed: 1.08e+01, train loss: 2.37433e-06, val loss: 3.44813e-06, min loss: 4.73557e-07\n",
      "Epoch: 2045000, elapsed: 1.09e+01, train loss: 5.45305e-07, val loss: 1.52609e-06, min loss: 4.73557e-07\n",
      "Epoch: 2045100, elapsed: 1.24e+01, train loss: 4.75098e-07, val loss: 1.44867e-06, min loss: 4.73557e-07\n",
      "Epoch: 2045200, elapsed: 1.66e+01, train loss: 5.44917e-07, val loss: 1.56119e-06, min loss: 4.73557e-07\n",
      "Epoch: 2045300, elapsed: 1.09e+01, train loss: 4.85185e-07, val loss: 1.49090e-06, min loss: 4.73557e-07\n",
      "Epoch: 2045400, elapsed: 1.10e+01, train loss: 6.10058e-07, val loss: 1.61121e-06, min loss: 4.73557e-07\n",
      "Epoch: 2045500, elapsed: 1.08e+01, train loss: 4.76440e-07, val loss: 1.44212e-06, min loss: 4.73557e-07\n",
      "Epoch: 2045600, elapsed: 1.09e+01, train loss: 4.92642e-07, val loss: 1.70228e-06, min loss: 4.73557e-07\n",
      "Epoch: 2045700, elapsed: 1.09e+01, train loss: 7.87356e-07, val loss: 1.62702e-06, min loss: 4.73557e-07\n",
      "Epoch: 2045800, elapsed: 1.09e+01, train loss: 5.25116e-07, val loss: 1.57493e-06, min loss: 4.73557e-07\n",
      "Epoch: 2045900, elapsed: 1.11e+01, train loss: 9.05872e-07, val loss: 2.37182e-06, min loss: 4.73557e-07\n",
      "Epoch: 2046000, elapsed: 1.10e+01, train loss: 4.73137e-07, val loss: 1.44858e-06, min loss: 4.73137e-07\n",
      "Epoch: 2046100, elapsed: 1.10e+01, train loss: 4.73030e-07, val loss: 1.45119e-06, min loss: 4.73030e-07\n",
      "Epoch: 2046200, elapsed: 1.06e+01, train loss: 5.19571e-07, val loss: 1.50422e-06, min loss: 4.73030e-07\n",
      "Epoch: 2046300, elapsed: 1.09e+01, train loss: 1.11715e-06, val loss: 1.85606e-06, min loss: 4.73030e-07\n",
      "Epoch: 2046400, elapsed: 1.10e+01, train loss: 4.88023e-07, val loss: 1.44157e-06, min loss: 4.73030e-07\n",
      "Epoch: 2046500, elapsed: 1.09e+01, train loss: 7.76996e-07, val loss: 1.65487e-06, min loss: 4.73030e-07\n",
      "Epoch: 2046600, elapsed: 1.10e+01, train loss: 5.95747e-07, val loss: 1.51706e-06, min loss: 4.73030e-07\n",
      "Epoch: 2046700, elapsed: 1.09e+01, train loss: 5.36498e-07, val loss: 1.52806e-06, min loss: 4.73030e-07\n",
      "Epoch: 2046800, elapsed: 1.08e+01, train loss: 4.73008e-07, val loss: 1.45022e-06, min loss: 4.73008e-07\n",
      "Epoch: 2046900, elapsed: 1.07e+01, train loss: 4.74396e-07, val loss: 1.44766e-06, min loss: 4.73008e-07\n",
      "Epoch: 2047000, elapsed: 1.10e+01, train loss: 4.79171e-07, val loss: 1.46490e-06, min loss: 4.73008e-07\n",
      "Epoch: 2047100, elapsed: 1.10e+01, train loss: 4.74727e-07, val loss: 1.47377e-06, min loss: 4.73008e-07\n",
      "Epoch: 2047200, elapsed: 1.09e+01, train loss: 4.87203e-07, val loss: 1.44702e-06, min loss: 4.73008e-07\n",
      "Epoch: 2047300, elapsed: 1.08e+01, train loss: 1.21022e-06, val loss: 2.70163e-06, min loss: 4.73008e-07\n",
      "Epoch: 2047400, elapsed: 1.07e+01, train loss: 4.72592e-07, val loss: 1.44633e-06, min loss: 4.72592e-07\n",
      "Epoch: 2047500, elapsed: 1.08e+01, train loss: 4.98287e-07, val loss: 1.48855e-06, min loss: 4.72592e-07\n",
      "Epoch: 2047600, elapsed: 1.08e+01, train loss: 1.07123e-06, val loss: 2.42779e-06, min loss: 4.72592e-07\n",
      "Epoch: 2047700, elapsed: 1.07e+01, train loss: 1.44678e-06, val loss: 2.84342e-06, min loss: 4.72592e-07\n",
      "Epoch: 2047800, elapsed: 1.09e+01, train loss: 4.73181e-07, val loss: 1.46736e-06, min loss: 4.72592e-07\n",
      "Epoch: 2047900, elapsed: 1.07e+01, train loss: 4.72861e-07, val loss: 1.44811e-06, min loss: 4.72592e-07\n",
      "Epoch: 2048000, elapsed: 1.08e+01, train loss: 4.72406e-07, val loss: 1.46183e-06, min loss: 4.72406e-07\n",
      "Epoch: 2048100, elapsed: 1.07e+01, train loss: 5.11917e-07, val loss: 1.51656e-06, min loss: 4.72406e-07\n",
      "Epoch: 2048200, elapsed: 1.07e+01, train loss: 5.12477e-07, val loss: 1.42090e-06, min loss: 4.72406e-07\n",
      "Epoch: 2048300, elapsed: 1.08e+01, train loss: 4.72284e-07, val loss: 1.45515e-06, min loss: 4.72284e-07\n",
      "Epoch: 2048400, elapsed: 1.08e+01, train loss: 4.72526e-07, val loss: 1.46242e-06, min loss: 4.72284e-07\n",
      "Epoch: 2048500, elapsed: 1.08e+01, train loss: 4.78016e-07, val loss: 1.48148e-06, min loss: 4.72284e-07\n",
      "Epoch: 2048600, elapsed: 1.09e+01, train loss: 5.28988e-07, val loss: 1.53402e-06, min loss: 4.72284e-07\n",
      "Epoch: 2048700, elapsed: 1.07e+01, train loss: 8.19787e-07, val loss: 1.64031e-06, min loss: 4.72284e-07\n",
      "Epoch: 2048800, elapsed: 1.65e+01, train loss: 2.11744e-06, val loss: 2.72136e-06, min loss: 4.72284e-07\n",
      "Epoch: 2048900, elapsed: 1.12e+01, train loss: 2.24695e-06, val loss: 3.38740e-06, min loss: 4.72284e-07\n",
      "Epoch: 2049000, elapsed: 1.11e+01, train loss: 9.37908e-07, val loss: 1.96924e-06, min loss: 4.72284e-07\n",
      "Epoch: 2049100, elapsed: 1.11e+01, train loss: 4.82068e-07, val loss: 1.44852e-06, min loss: 4.72284e-07\n",
      "Epoch: 2049200, elapsed: 1.10e+01, train loss: 4.73071e-07, val loss: 1.46452e-06, min loss: 4.72284e-07\n",
      "Epoch: 2049300, elapsed: 1.11e+01, train loss: 5.03846e-07, val loss: 1.52481e-06, min loss: 4.72284e-07\n",
      "Epoch: 2049400, elapsed: 1.09e+01, train loss: 3.46054e-06, val loss: 3.68073e-06, min loss: 4.72284e-07\n",
      "Epoch: 2049500, elapsed: 1.12e+01, train loss: 4.71872e-07, val loss: 1.45195e-06, min loss: 4.71872e-07\n",
      "Epoch: 2049600, elapsed: 1.10e+01, train loss: 4.91995e-07, val loss: 1.51212e-06, min loss: 4.71872e-07\n",
      "Epoch: 2049700, elapsed: 1.10e+01, train loss: 1.05448e-06, val loss: 2.38541e-06, min loss: 4.71872e-07\n",
      "Epoch: 2049800, elapsed: 1.11e+01, train loss: 4.71961e-07, val loss: 1.45355e-06, min loss: 4.71872e-07\n",
      "Epoch: 2049900, elapsed: 1.09e+01, train loss: 2.28069e-06, val loss: 4.15727e-06, min loss: 4.71872e-07\n",
      "Epoch: 2050000, elapsed: 1.10e+01, train loss: 4.71677e-07, val loss: 1.45719e-06, min loss: 4.71677e-07\n",
      "Epoch: 2050100, elapsed: 1.29e+01, train loss: 5.07834e-07, val loss: 1.40159e-06, min loss: 4.71677e-07\n",
      "Epoch: 2050200, elapsed: 1.10e+01, train loss: 6.54768e-07, val loss: 1.56182e-06, min loss: 4.71677e-07\n",
      "Epoch: 2050300, elapsed: 1.10e+01, train loss: 4.93654e-07, val loss: 1.47568e-06, min loss: 4.71677e-07\n",
      "Epoch: 2050400, elapsed: 1.09e+01, train loss: 5.93105e-07, val loss: 1.70690e-06, min loss: 4.71677e-07\n",
      "Epoch: 2050500, elapsed: 1.09e+01, train loss: 4.71671e-07, val loss: 1.45245e-06, min loss: 4.71671e-07\n",
      "Epoch: 2050600, elapsed: 1.09e+01, train loss: 4.83057e-07, val loss: 1.49017e-06, min loss: 4.71671e-07\n",
      "Epoch: 2050700, elapsed: 1.08e+01, train loss: 9.87123e-07, val loss: 1.86534e-06, min loss: 4.71671e-07\n",
      "Epoch: 2050800, elapsed: 1.08e+01, train loss: 7.75042e-07, val loss: 1.78155e-06, min loss: 4.71671e-07\n",
      "Epoch: 2050900, elapsed: 1.09e+01, train loss: 2.06605e-06, val loss: 2.82848e-06, min loss: 4.71671e-07\n",
      "Epoch: 2051000, elapsed: 1.08e+01, train loss: 4.71395e-07, val loss: 1.45463e-06, min loss: 4.71395e-07\n",
      "Epoch: 2051100, elapsed: 1.08e+01, train loss: 4.72818e-07, val loss: 1.45642e-06, min loss: 4.71395e-07\n",
      "Epoch: 2051200, elapsed: 1.08e+01, train loss: 4.71525e-07, val loss: 1.46031e-06, min loss: 4.71395e-07\n",
      "Epoch: 2051300, elapsed: 1.08e+01, train loss: 5.11052e-07, val loss: 1.47539e-06, min loss: 4.71395e-07\n",
      "Epoch: 2051400, elapsed: 1.09e+01, train loss: 5.43931e-07, val loss: 1.45524e-06, min loss: 4.71395e-07\n",
      "Epoch: 2051500, elapsed: 1.09e+01, train loss: 9.79801e-07, val loss: 1.70699e-06, min loss: 4.71395e-07\n",
      "Epoch: 2051600, elapsed: 1.07e+01, train loss: 1.57649e-06, val loss: 3.38607e-06, min loss: 4.71395e-07\n",
      "Epoch: 2051700, elapsed: 1.06e+01, train loss: 4.71477e-07, val loss: 1.45615e-06, min loss: 4.71395e-07\n",
      "Epoch: 2051800, elapsed: 1.10e+01, train loss: 4.90484e-07, val loss: 1.44491e-06, min loss: 4.71395e-07\n",
      "Epoch: 2051900, elapsed: 1.09e+01, train loss: 4.86289e-07, val loss: 1.45911e-06, min loss: 4.71395e-07\n",
      "Epoch: 2052000, elapsed: 1.08e+01, train loss: 3.28335e-06, val loss: 2.58757e-06, min loss: 4.71395e-07\n",
      "Epoch: 2052100, elapsed: 1.10e+01, train loss: 4.72799e-07, val loss: 1.45912e-06, min loss: 4.71395e-07\n",
      "Epoch: 2052200, elapsed: 1.08e+01, train loss: 4.72132e-07, val loss: 1.44284e-06, min loss: 4.71395e-07\n",
      "Epoch: 2052300, elapsed: 1.09e+01, train loss: 4.75863e-07, val loss: 1.46364e-06, min loss: 4.71395e-07\n",
      "Epoch: 2052400, elapsed: 1.64e+01, train loss: 4.71737e-07, val loss: 1.45896e-06, min loss: 4.71395e-07\n",
      "Epoch: 2052500, elapsed: 1.12e+01, train loss: 5.84986e-07, val loss: 1.45269e-06, min loss: 4.71395e-07\n",
      "Epoch: 2052600, elapsed: 1.10e+01, train loss: 4.76917e-07, val loss: 1.46019e-06, min loss: 4.71395e-07\n",
      "Epoch: 2052700, elapsed: 1.09e+01, train loss: 4.71594e-07, val loss: 1.45043e-06, min loss: 4.71395e-07\n",
      "Epoch: 2052800, elapsed: 1.08e+01, train loss: 4.73247e-07, val loss: 1.43809e-06, min loss: 4.71395e-07\n",
      "Epoch: 2052900, elapsed: 1.09e+01, train loss: 1.64458e-06, val loss: 1.81418e-06, min loss: 4.71395e-07\n",
      "Epoch: 2053000, elapsed: 1.08e+01, train loss: 4.70808e-07, val loss: 1.46101e-06, min loss: 4.70808e-07\n",
      "Epoch: 2053100, elapsed: 1.09e+01, train loss: 5.80831e-07, val loss: 1.48145e-06, min loss: 4.70808e-07\n",
      "Epoch: 2053200, elapsed: 1.10e+01, train loss: 4.70710e-07, val loss: 1.45794e-06, min loss: 4.70710e-07\n",
      "Epoch: 2053300, elapsed: 1.10e+01, train loss: 4.72618e-07, val loss: 1.46835e-06, min loss: 4.70710e-07\n",
      "Epoch: 2053400, elapsed: 1.10e+01, train loss: 4.82280e-07, val loss: 1.50620e-06, min loss: 4.70710e-07\n",
      "Epoch: 2053500, elapsed: 1.09e+01, train loss: 4.70906e-07, val loss: 1.46006e-06, min loss: 4.70710e-07\n",
      "Epoch: 2053600, elapsed: 1.08e+01, train loss: 5.75159e-07, val loss: 1.43547e-06, min loss: 4.70710e-07\n",
      "Epoch: 2053700, elapsed: 1.09e+01, train loss: 4.70554e-07, val loss: 1.46063e-06, min loss: 4.70554e-07\n",
      "Epoch: 2053800, elapsed: 1.07e+01, train loss: 4.73203e-07, val loss: 1.46906e-06, min loss: 4.70554e-07\n",
      "Epoch: 2053900, elapsed: 1.09e+01, train loss: 4.70849e-07, val loss: 1.45433e-06, min loss: 4.70554e-07\n",
      "Epoch: 2054000, elapsed: 1.09e+01, train loss: 4.77680e-07, val loss: 1.47725e-06, min loss: 4.70554e-07\n",
      "Epoch: 2054100, elapsed: 1.08e+01, train loss: 1.98452e-06, val loss: 2.56200e-06, min loss: 4.70554e-07\n",
      "Epoch: 2054200, elapsed: 1.09e+01, train loss: 4.81063e-07, val loss: 1.51976e-06, min loss: 4.70554e-07\n",
      "Epoch: 2054300, elapsed: 1.07e+01, train loss: 4.74640e-07, val loss: 1.46338e-06, min loss: 4.70554e-07\n",
      "Epoch: 2054400, elapsed: 1.07e+01, train loss: 4.74413e-07, val loss: 1.42575e-06, min loss: 4.70554e-07\n",
      "Epoch: 2054500, elapsed: 1.08e+01, train loss: 4.74048e-07, val loss: 1.46209e-06, min loss: 4.70554e-07\n",
      "Epoch: 2054600, elapsed: 1.08e+01, train loss: 8.15125e-07, val loss: 1.66264e-06, min loss: 4.70554e-07\n",
      "Epoch: 2054700, elapsed: 1.06e+01, train loss: 4.80021e-07, val loss: 1.45553e-06, min loss: 4.70554e-07\n",
      "Epoch: 2054800, elapsed: 1.09e+01, train loss: 4.70403e-07, val loss: 1.45964e-06, min loss: 4.70403e-07\n",
      "Epoch: 2054900, elapsed: 1.08e+01, train loss: 5.32140e-07, val loss: 1.56315e-06, min loss: 4.70403e-07\n",
      "Epoch: 2055000, elapsed: 1.09e+01, train loss: 1.20926e-06, val loss: 1.80379e-06, min loss: 4.70403e-07\n",
      "Epoch: 2055100, elapsed: 1.28e+01, train loss: 5.23080e-07, val loss: 1.44603e-06, min loss: 4.70403e-07\n",
      "Epoch: 2055200, elapsed: 1.08e+01, train loss: 4.77466e-07, val loss: 1.47692e-06, min loss: 4.70403e-07\n",
      "Epoch: 2055300, elapsed: 1.08e+01, train loss: 4.80156e-07, val loss: 1.49541e-06, min loss: 4.70403e-07\n",
      "Epoch: 2055400, elapsed: 1.06e+01, train loss: 4.76124e-07, val loss: 1.45511e-06, min loss: 4.70403e-07\n",
      "Epoch: 2055500, elapsed: 1.08e+01, train loss: 4.72089e-07, val loss: 1.46115e-06, min loss: 4.70403e-07\n",
      "Epoch: 2055600, elapsed: 1.07e+01, train loss: 7.58604e-07, val loss: 1.46677e-06, min loss: 4.70403e-07\n",
      "Epoch: 2055700, elapsed: 1.07e+01, train loss: 4.92237e-07, val loss: 1.50853e-06, min loss: 4.70403e-07\n",
      "Epoch: 2055800, elapsed: 1.08e+01, train loss: 4.70721e-07, val loss: 1.46299e-06, min loss: 4.70403e-07\n",
      "Epoch: 2055900, elapsed: 1.08e+01, train loss: 4.90996e-07, val loss: 1.43830e-06, min loss: 4.70403e-07\n",
      "Epoch: 2056000, elapsed: 1.08e+01, train loss: 6.73746e-07, val loss: 1.57877e-06, min loss: 4.70403e-07\n",
      "Epoch: 2056100, elapsed: 1.68e+01, train loss: 4.70857e-07, val loss: 1.47321e-06, min loss: 4.70403e-07\n",
      "Epoch: 2056200, elapsed: 1.10e+01, train loss: 8.63024e-07, val loss: 1.93714e-06, min loss: 4.70403e-07\n",
      "Epoch: 2056300, elapsed: 1.08e+01, train loss: 4.69822e-07, val loss: 1.46165e-06, min loss: 4.69822e-07\n",
      "Epoch: 2056400, elapsed: 1.08e+01, train loss: 6.60523e-07, val loss: 1.55564e-06, min loss: 4.69822e-07\n",
      "Epoch: 2056500, elapsed: 1.09e+01, train loss: 4.69793e-07, val loss: 1.45880e-06, min loss: 4.69793e-07\n",
      "Epoch: 2056600, elapsed: 1.10e+01, train loss: 4.74998e-07, val loss: 1.47581e-06, min loss: 4.69793e-07\n",
      "Epoch: 2056700, elapsed: 1.07e+01, train loss: 4.69716e-07, val loss: 1.46208e-06, min loss: 4.69716e-07\n",
      "Epoch: 2056800, elapsed: 1.10e+01, train loss: 4.71261e-07, val loss: 1.45828e-06, min loss: 4.69716e-07\n",
      "Epoch: 2056900, elapsed: 1.08e+01, train loss: 4.84200e-07, val loss: 1.45807e-06, min loss: 4.69716e-07\n",
      "Epoch: 2057000, elapsed: 1.11e+01, train loss: 4.70273e-07, val loss: 1.46069e-06, min loss: 4.69716e-07\n",
      "Epoch: 2057100, elapsed: 1.09e+01, train loss: 5.65470e-07, val loss: 1.68313e-06, min loss: 4.69716e-07\n",
      "Epoch: 2057200, elapsed: 1.09e+01, train loss: 9.89468e-07, val loss: 1.96619e-06, min loss: 4.69716e-07\n",
      "Epoch: 2057300, elapsed: 1.08e+01, train loss: 4.69781e-07, val loss: 1.46410e-06, min loss: 4.69716e-07\n",
      "Epoch: 2057400, elapsed: 1.08e+01, train loss: 4.77246e-07, val loss: 1.47155e-06, min loss: 4.69716e-07\n",
      "Epoch: 2057500, elapsed: 1.09e+01, train loss: 5.82633e-07, val loss: 1.56529e-06, min loss: 4.69716e-07\n",
      "Epoch: 2057600, elapsed: 1.11e+01, train loss: 4.73824e-07, val loss: 1.44339e-06, min loss: 4.69716e-07\n",
      "Epoch: 2057700, elapsed: 1.07e+01, train loss: 4.70171e-07, val loss: 1.45466e-06, min loss: 4.69716e-07\n",
      "Epoch: 2057800, elapsed: 1.09e+01, train loss: 6.73666e-07, val loss: 1.69592e-06, min loss: 4.69716e-07\n",
      "Epoch: 2057900, elapsed: 1.10e+01, train loss: 4.69409e-07, val loss: 1.46777e-06, min loss: 4.69409e-07\n",
      "Epoch: 2058000, elapsed: 1.07e+01, train loss: 4.72396e-07, val loss: 1.47609e-06, min loss: 4.69409e-07\n",
      "Epoch: 2058100, elapsed: 1.10e+01, train loss: 1.88320e-06, val loss: 2.12744e-06, min loss: 4.69409e-07\n",
      "Epoch: 2058200, elapsed: 1.09e+01, train loss: 4.69409e-07, val loss: 1.46242e-06, min loss: 4.69409e-07\n",
      "Epoch: 2058300, elapsed: 1.08e+01, train loss: 4.90817e-07, val loss: 1.47071e-06, min loss: 4.69409e-07\n",
      "Epoch: 2058400, elapsed: 1.07e+01, train loss: 6.35590e-07, val loss: 1.65300e-06, min loss: 4.69409e-07\n",
      "Epoch: 2058500, elapsed: 1.07e+01, train loss: 5.60304e-07, val loss: 1.64795e-06, min loss: 4.69409e-07\n",
      "Epoch: 2058600, elapsed: 1.09e+01, train loss: 4.70066e-07, val loss: 1.46594e-06, min loss: 4.69409e-07\n",
      "Epoch: 2058700, elapsed: 1.08e+01, train loss: 2.91199e-06, val loss: 3.59085e-06, min loss: 4.69409e-07\n",
      "Epoch: 2058800, elapsed: 1.08e+01, train loss: 4.69692e-07, val loss: 1.45979e-06, min loss: 4.69409e-07\n",
      "Epoch: 2058900, elapsed: 1.08e+01, train loss: 4.69751e-07, val loss: 1.47501e-06, min loss: 4.69409e-07\n",
      "Epoch: 2059000, elapsed: 1.08e+01, train loss: 6.18476e-07, val loss: 1.67475e-06, min loss: 4.69409e-07\n",
      "Epoch: 2059100, elapsed: 1.08e+01, train loss: 4.82642e-07, val loss: 1.48575e-06, min loss: 4.69409e-07\n",
      "Epoch: 2059200, elapsed: 1.06e+01, train loss: 3.43417e-06, val loss: 3.73993e-06, min loss: 4.69409e-07\n",
      "Epoch: 2059300, elapsed: 1.07e+01, train loss: 5.17527e-07, val loss: 1.50752e-06, min loss: 4.69409e-07\n",
      "Epoch: 2059400, elapsed: 1.06e+01, train loss: 4.73374e-07, val loss: 1.49361e-06, min loss: 4.69409e-07\n",
      "Epoch: 2059500, elapsed: 1.07e+01, train loss: 4.76262e-07, val loss: 1.47909e-06, min loss: 4.69409e-07\n",
      "Epoch: 2059600, elapsed: 1.08e+01, train loss: 4.92838e-07, val loss: 1.54363e-06, min loss: 4.69409e-07\n",
      "Epoch: 2059700, elapsed: 1.06e+01, train loss: 4.81635e-07, val loss: 1.45947e-06, min loss: 4.69409e-07\n",
      "Epoch: 2059800, elapsed: 1.68e+01, train loss: 4.68963e-07, val loss: 1.46593e-06, min loss: 4.68963e-07\n",
      "Epoch: 2059900, elapsed: 1.12e+01, train loss: 6.96254e-07, val loss: 1.62305e-06, min loss: 4.68963e-07\n",
      "Epoch: 2060000, elapsed: 1.11e+01, train loss: 8.99745e-07, val loss: 1.44363e-06, min loss: 4.68963e-07\n",
      "Epoch: 2060100, elapsed: 1.29e+01, train loss: 5.29751e-07, val loss: 1.57609e-06, min loss: 4.68963e-07\n",
      "Epoch: 2060200, elapsed: 1.09e+01, train loss: 4.68844e-07, val loss: 1.46607e-06, min loss: 4.68844e-07\n",
      "Epoch: 2060300, elapsed: 1.09e+01, train loss: 5.01029e-07, val loss: 1.52025e-06, min loss: 4.68844e-07\n",
      "Epoch: 2060400, elapsed: 1.10e+01, train loss: 9.53200e-07, val loss: 2.01925e-06, min loss: 4.68844e-07\n",
      "Epoch: 2060500, elapsed: 1.10e+01, train loss: 6.81311e-07, val loss: 1.67365e-06, min loss: 4.68844e-07\n",
      "Epoch: 2060600, elapsed: 1.10e+01, train loss: 4.70796e-07, val loss: 1.46889e-06, min loss: 4.68844e-07\n",
      "Epoch: 2060700, elapsed: 1.10e+01, train loss: 5.22060e-07, val loss: 1.46775e-06, min loss: 4.68844e-07\n",
      "Epoch: 2060800, elapsed: 1.09e+01, train loss: 4.75453e-07, val loss: 1.44150e-06, min loss: 4.68844e-07\n",
      "Epoch: 2060900, elapsed: 1.09e+01, train loss: 4.75887e-07, val loss: 1.47820e-06, min loss: 4.68844e-07\n",
      "Epoch: 2061000, elapsed: 1.08e+01, train loss: 4.72834e-07, val loss: 1.52667e-06, min loss: 4.68844e-07\n",
      "Epoch: 2061100, elapsed: 1.09e+01, train loss: 1.18297e-06, val loss: 2.23394e-06, min loss: 4.68844e-07\n",
      "Epoch: 2061200, elapsed: 1.08e+01, train loss: 6.82193e-07, val loss: 1.53486e-06, min loss: 4.68844e-07\n",
      "Epoch: 2061300, elapsed: 1.12e+01, train loss: 4.69057e-07, val loss: 1.46461e-06, min loss: 4.68844e-07\n",
      "Epoch: 2061400, elapsed: 1.08e+01, train loss: 4.70299e-07, val loss: 1.47079e-06, min loss: 4.68844e-07\n",
      "Epoch: 2061500, elapsed: 1.09e+01, train loss: 5.01937e-07, val loss: 1.45827e-06, min loss: 4.68844e-07\n",
      "Epoch: 2061600, elapsed: 1.09e+01, train loss: 8.03542e-07, val loss: 1.80431e-06, min loss: 4.68844e-07\n",
      "Epoch: 2061700, elapsed: 1.10e+01, train loss: 6.89753e-07, val loss: 1.41499e-06, min loss: 4.68844e-07\n",
      "Epoch: 2061800, elapsed: 1.10e+01, train loss: 1.38592e-06, val loss: 2.77526e-06, min loss: 4.68844e-07\n",
      "Epoch: 2061900, elapsed: 1.09e+01, train loss: 4.92531e-07, val loss: 1.55051e-06, min loss: 4.68844e-07\n",
      "Epoch: 2062000, elapsed: 1.08e+01, train loss: 4.71048e-07, val loss: 1.46556e-06, min loss: 4.68844e-07\n",
      "Epoch: 2062100, elapsed: 1.09e+01, train loss: 4.77770e-07, val loss: 1.49354e-06, min loss: 4.68844e-07\n",
      "Epoch: 2062200, elapsed: 1.07e+01, train loss: 5.05910e-07, val loss: 1.52803e-06, min loss: 4.68844e-07\n",
      "Epoch: 2062300, elapsed: 1.05e+01, train loss: 9.05615e-07, val loss: 1.71920e-06, min loss: 4.68844e-07\n",
      "Epoch: 2062400, elapsed: 1.03e+01, train loss: 1.38882e-06, val loss: 2.28410e-06, min loss: 4.68844e-07\n",
      "Epoch: 2062500, elapsed: 1.06e+01, train loss: 4.68256e-07, val loss: 1.46818e-06, min loss: 4.68256e-07\n",
      "Epoch: 2062600, elapsed: 1.06e+01, train loss: 4.68607e-07, val loss: 1.45521e-06, min loss: 4.68256e-07\n",
      "Epoch: 2062700, elapsed: 1.07e+01, train loss: 7.45551e-07, val loss: 1.64825e-06, min loss: 4.68256e-07\n",
      "Epoch: 2062800, elapsed: 1.08e+01, train loss: 4.68487e-07, val loss: 1.46168e-06, min loss: 4.68256e-07\n",
      "Epoch: 2062900, elapsed: 1.06e+01, train loss: 5.81935e-07, val loss: 1.47143e-06, min loss: 4.68256e-07\n",
      "Epoch: 2063000, elapsed: 1.08e+01, train loss: 5.36307e-07, val loss: 1.63344e-06, min loss: 4.68256e-07\n",
      "Epoch: 2063100, elapsed: 1.08e+01, train loss: 4.76566e-07, val loss: 1.46985e-06, min loss: 4.68256e-07\n",
      "Epoch: 2063200, elapsed: 1.06e+01, train loss: 2.15723e-06, val loss: 3.78690e-06, min loss: 4.68256e-07\n",
      "Epoch: 2063300, elapsed: 1.09e+01, train loss: 4.68005e-07, val loss: 1.46976e-06, min loss: 4.68005e-07\n",
      "Epoch: 2063400, elapsed: 1.65e+01, train loss: 4.85665e-07, val loss: 1.47650e-06, min loss: 4.68005e-07\n",
      "Epoch: 2063500, elapsed: 1.11e+01, train loss: 5.71709e-07, val loss: 1.44439e-06, min loss: 4.68005e-07\n",
      "Epoch: 2063600, elapsed: 1.10e+01, train loss: 7.20858e-07, val loss: 1.94674e-06, min loss: 4.68005e-07\n",
      "Epoch: 2063700, elapsed: 1.09e+01, train loss: 4.83481e-07, val loss: 1.42607e-06, min loss: 4.68005e-07\n",
      "Epoch: 2063800, elapsed: 1.09e+01, train loss: 5.84325e-07, val loss: 1.50134e-06, min loss: 4.68005e-07\n",
      "Epoch: 2063900, elapsed: 1.12e+01, train loss: 5.70186e-07, val loss: 1.50307e-06, min loss: 4.68005e-07\n",
      "Epoch: 2064000, elapsed: 1.10e+01, train loss: 4.68123e-07, val loss: 1.46708e-06, min loss: 4.68005e-07\n",
      "Epoch: 2064100, elapsed: 1.11e+01, train loss: 5.76788e-07, val loss: 1.55542e-06, min loss: 4.68005e-07\n",
      "Epoch: 2064200, elapsed: 1.11e+01, train loss: 1.24461e-06, val loss: 1.99004e-06, min loss: 4.68005e-07\n",
      "Epoch: 2064300, elapsed: 1.10e+01, train loss: 4.67727e-07, val loss: 1.46280e-06, min loss: 4.67727e-07\n",
      "Epoch: 2064400, elapsed: 1.10e+01, train loss: 4.75245e-07, val loss: 1.52551e-06, min loss: 4.67727e-07\n",
      "Epoch: 2064500, elapsed: 1.08e+01, train loss: 4.67465e-07, val loss: 1.47066e-06, min loss: 4.67465e-07\n",
      "Epoch: 2064600, elapsed: 1.08e+01, train loss: 4.74915e-07, val loss: 1.50970e-06, min loss: 4.67465e-07\n",
      "Epoch: 2064700, elapsed: 1.09e+01, train loss: 4.67507e-07, val loss: 1.46816e-06, min loss: 4.67465e-07\n",
      "Epoch: 2064800, elapsed: 1.08e+01, train loss: 4.70477e-07, val loss: 1.47909e-06, min loss: 4.67465e-07\n",
      "Epoch: 2064900, elapsed: 1.10e+01, train loss: 1.37435e-06, val loss: 2.69361e-06, min loss: 4.67465e-07\n",
      "Epoch: 2065000, elapsed: 1.09e+01, train loss: 2.77887e-06, val loss: 2.93146e-06, min loss: 4.67465e-07\n",
      "Epoch: 2065100, elapsed: 1.29e+01, train loss: 4.68499e-07, val loss: 1.46509e-06, min loss: 4.67465e-07\n",
      "Epoch: 2065200, elapsed: 1.08e+01, train loss: 4.68095e-07, val loss: 1.46575e-06, min loss: 4.67465e-07\n",
      "Epoch: 2065300, elapsed: 1.09e+01, train loss: 5.15558e-07, val loss: 1.47084e-06, min loss: 4.67465e-07\n",
      "Epoch: 2065400, elapsed: 1.09e+01, train loss: 4.70310e-07, val loss: 1.50583e-06, min loss: 4.67465e-07\n",
      "Epoch: 2065500, elapsed: 1.08e+01, train loss: 4.69040e-07, val loss: 1.46308e-06, min loss: 4.67465e-07\n",
      "Epoch: 2065600, elapsed: 1.08e+01, train loss: 4.90413e-07, val loss: 1.50703e-06, min loss: 4.67465e-07\n",
      "Epoch: 2065700, elapsed: 1.09e+01, train loss: 6.19328e-07, val loss: 1.46936e-06, min loss: 4.67465e-07\n",
      "Epoch: 2065800, elapsed: 1.07e+01, train loss: 2.35742e-06, val loss: 2.12223e-06, min loss: 4.67465e-07\n",
      "Epoch: 2065900, elapsed: 1.10e+01, train loss: 4.67963e-07, val loss: 1.48207e-06, min loss: 4.67465e-07\n",
      "Epoch: 2066000, elapsed: 1.08e+01, train loss: 4.80461e-07, val loss: 1.47427e-06, min loss: 4.67465e-07\n",
      "Epoch: 2066100, elapsed: 1.07e+01, train loss: 5.08464e-07, val loss: 1.55049e-06, min loss: 4.67465e-07\n",
      "Epoch: 2066200, elapsed: 1.09e+01, train loss: 5.30457e-07, val loss: 1.51877e-06, min loss: 4.67465e-07\n",
      "Epoch: 2066300, elapsed: 1.08e+01, train loss: 7.08628e-07, val loss: 1.73901e-06, min loss: 4.67465e-07\n",
      "Epoch: 2066400, elapsed: 1.08e+01, train loss: 4.67192e-07, val loss: 1.46921e-06, min loss: 4.67192e-07\n",
      "Epoch: 2066500, elapsed: 1.08e+01, train loss: 4.68409e-07, val loss: 1.46844e-06, min loss: 4.67192e-07\n",
      "Epoch: 2066600, elapsed: 1.08e+01, train loss: 1.59067e-06, val loss: 3.41122e-06, min loss: 4.67192e-07\n",
      "Epoch: 2066700, elapsed: 1.07e+01, train loss: 4.67022e-07, val loss: 1.47205e-06, min loss: 4.67022e-07\n",
      "Epoch: 2066800, elapsed: 1.08e+01, train loss: 5.61324e-07, val loss: 1.51625e-06, min loss: 4.67022e-07\n",
      "Epoch: 2066900, elapsed: 1.09e+01, train loss: 4.66802e-07, val loss: 1.47209e-06, min loss: 4.66802e-07\n",
      "Epoch: 2067000, elapsed: 1.08e+01, train loss: 4.70923e-07, val loss: 1.47220e-06, min loss: 4.66802e-07\n",
      "Epoch: 2067100, elapsed: 1.70e+01, train loss: 1.22775e-06, val loss: 1.88254e-06, min loss: 4.66802e-07\n",
      "Epoch: 2067200, elapsed: 1.10e+01, train loss: 4.66750e-07, val loss: 1.47155e-06, min loss: 4.66750e-07\n",
      "Epoch: 2067300, elapsed: 1.10e+01, train loss: 4.84850e-07, val loss: 1.44601e-06, min loss: 4.66750e-07\n",
      "Epoch: 2067400, elapsed: 1.10e+01, train loss: 4.69234e-07, val loss: 1.46296e-06, min loss: 4.66750e-07\n",
      "Epoch: 2067500, elapsed: 1.10e+01, train loss: 4.66977e-07, val loss: 1.47249e-06, min loss: 4.66750e-07\n",
      "Epoch: 2067600, elapsed: 1.08e+01, train loss: 1.50391e-06, val loss: 2.89733e-06, min loss: 4.66750e-07\n",
      "Epoch: 2067700, elapsed: 1.10e+01, train loss: 4.67231e-07, val loss: 1.47922e-06, min loss: 4.66750e-07\n",
      "Epoch: 2067800, elapsed: 1.10e+01, train loss: 4.71407e-07, val loss: 1.47873e-06, min loss: 4.66750e-07\n",
      "Epoch: 2067900, elapsed: 1.09e+01, train loss: 4.87306e-07, val loss: 1.52897e-06, min loss: 4.66750e-07\n",
      "Epoch: 2068000, elapsed: 1.09e+01, train loss: 4.66561e-07, val loss: 1.47054e-06, min loss: 4.66561e-07\n",
      "Epoch: 2068100, elapsed: 1.09e+01, train loss: 4.68248e-07, val loss: 1.45927e-06, min loss: 4.66561e-07\n",
      "Epoch: 2068200, elapsed: 1.08e+01, train loss: 9.78707e-07, val loss: 1.99294e-06, min loss: 4.66561e-07\n",
      "Epoch: 2068300, elapsed: 1.08e+01, train loss: 5.36697e-07, val loss: 1.56194e-06, min loss: 4.66561e-07\n",
      "Epoch: 2068400, elapsed: 1.10e+01, train loss: 5.30548e-07, val loss: 1.60776e-06, min loss: 4.66561e-07\n",
      "Epoch: 2068500, elapsed: 1.08e+01, train loss: 1.66032e-06, val loss: 2.08898e-06, min loss: 4.66561e-07\n",
      "Epoch: 2068600, elapsed: 1.10e+01, train loss: 1.22799e-06, val loss: 2.28323e-06, min loss: 4.66561e-07\n",
      "Epoch: 2068700, elapsed: 1.08e+01, train loss: 7.72962e-07, val loss: 1.87093e-06, min loss: 4.66561e-07\n",
      "Epoch: 2068800, elapsed: 1.08e+01, train loss: 6.00328e-07, val loss: 1.73842e-06, min loss: 4.66561e-07\n",
      "Epoch: 2068900, elapsed: 1.08e+01, train loss: 6.83784e-07, val loss: 1.59968e-06, min loss: 4.66561e-07\n",
      "Epoch: 2069000, elapsed: 1.09e+01, train loss: 5.12570e-07, val loss: 1.53450e-06, min loss: 4.66561e-07\n",
      "Epoch: 2069100, elapsed: 1.09e+01, train loss: 1.60698e-06, val loss: 3.16303e-06, min loss: 4.66561e-07\n",
      "Epoch: 2069200, elapsed: 1.10e+01, train loss: 4.96304e-07, val loss: 1.54226e-06, min loss: 4.66561e-07\n",
      "Epoch: 2069300, elapsed: 1.06e+01, train loss: 5.22347e-07, val loss: 1.54723e-06, min loss: 4.66561e-07\n",
      "Epoch: 2069400, elapsed: 1.08e+01, train loss: 4.79283e-07, val loss: 1.53608e-06, min loss: 4.66561e-07\n",
      "Epoch: 2069500, elapsed: 1.09e+01, train loss: 4.77918e-07, val loss: 1.46293e-06, min loss: 4.66561e-07\n",
      "Epoch: 2069600, elapsed: 1.08e+01, train loss: 4.83422e-07, val loss: 1.48907e-06, min loss: 4.66561e-07\n",
      "Epoch: 2069700, elapsed: 1.07e+01, train loss: 5.27014e-07, val loss: 1.53519e-06, min loss: 4.66561e-07\n",
      "Epoch: 2069800, elapsed: 1.07e+01, train loss: 4.68720e-07, val loss: 1.47617e-06, min loss: 4.66561e-07\n",
      "Epoch: 2069900, elapsed: 1.07e+01, train loss: 4.75982e-07, val loss: 1.52218e-06, min loss: 4.66561e-07\n",
      "Epoch: 2070000, elapsed: 1.10e+01, train loss: 8.60325e-07, val loss: 1.61935e-06, min loss: 4.66561e-07\n",
      "Epoch: 2070100, elapsed: 1.29e+01, train loss: 4.66632e-07, val loss: 1.48005e-06, min loss: 4.66561e-07\n",
      "Epoch: 2070200, elapsed: 1.09e+01, train loss: 6.17467e-07, val loss: 1.70799e-06, min loss: 4.66561e-07\n",
      "Epoch: 2070300, elapsed: 1.08e+01, train loss: 4.65884e-07, val loss: 1.47359e-06, min loss: 4.65884e-07\n",
      "Epoch: 2070400, elapsed: 1.07e+01, train loss: 6.84563e-07, val loss: 1.53715e-06, min loss: 4.65884e-07\n",
      "Epoch: 2070500, elapsed: 1.07e+01, train loss: 4.65867e-07, val loss: 1.47473e-06, min loss: 4.65867e-07\n",
      "Epoch: 2070600, elapsed: 1.08e+01, train loss: 1.98943e-06, val loss: 3.32023e-06, min loss: 4.65867e-07\n",
      "Epoch: 2070700, elapsed: 1.63e+01, train loss: 4.93229e-07, val loss: 1.54871e-06, min loss: 4.65867e-07\n",
      "Epoch: 2070800, elapsed: 1.12e+01, train loss: 4.68277e-07, val loss: 1.46635e-06, min loss: 4.65867e-07\n",
      "Epoch: 2070900, elapsed: 1.11e+01, train loss: 4.70580e-07, val loss: 1.48151e-06, min loss: 4.65867e-07\n",
      "Epoch: 2071000, elapsed: 1.10e+01, train loss: 5.00688e-07, val loss: 1.51975e-06, min loss: 4.65867e-07\n",
      "Epoch: 2071100, elapsed: 1.10e+01, train loss: 4.96610e-07, val loss: 1.52606e-06, min loss: 4.65867e-07\n",
      "Epoch: 2071200, elapsed: 1.08e+01, train loss: 1.16357e-06, val loss: 1.87418e-06, min loss: 4.65867e-07\n",
      "Epoch: 2071300, elapsed: 1.09e+01, train loss: 5.43540e-07, val loss: 1.59336e-06, min loss: 4.65867e-07\n",
      "Epoch: 2071400, elapsed: 1.09e+01, train loss: 4.71515e-07, val loss: 1.46143e-06, min loss: 4.65867e-07\n",
      "Epoch: 2071500, elapsed: 1.12e+01, train loss: 8.72186e-07, val loss: 1.86457e-06, min loss: 4.65867e-07\n",
      "Epoch: 2071600, elapsed: 1.10e+01, train loss: 1.43808e-06, val loss: 2.49542e-06, min loss: 4.65867e-07\n",
      "Epoch: 2071700, elapsed: 1.11e+01, train loss: 4.88090e-07, val loss: 1.48775e-06, min loss: 4.65867e-07\n",
      "Epoch: 2071800, elapsed: 1.07e+01, train loss: 6.18215e-07, val loss: 1.50972e-06, min loss: 4.65867e-07\n",
      "Epoch: 2071900, elapsed: 1.08e+01, train loss: 6.56894e-07, val loss: 1.54958e-06, min loss: 4.65867e-07\n",
      "Epoch: 2072000, elapsed: 1.09e+01, train loss: 4.68724e-07, val loss: 1.47160e-06, min loss: 4.65867e-07\n",
      "Epoch: 2072100, elapsed: 1.09e+01, train loss: 5.24569e-07, val loss: 1.57917e-06, min loss: 4.65867e-07\n",
      "Epoch: 2072200, elapsed: 1.09e+01, train loss: 4.78946e-07, val loss: 1.51415e-06, min loss: 4.65867e-07\n",
      "Epoch: 2072300, elapsed: 1.09e+01, train loss: 5.63848e-07, val loss: 1.51102e-06, min loss: 4.65867e-07\n",
      "Epoch: 2072400, elapsed: 1.10e+01, train loss: 4.69017e-07, val loss: 1.49073e-06, min loss: 4.65867e-07\n",
      "Epoch: 2072500, elapsed: 1.07e+01, train loss: 4.67105e-07, val loss: 1.47000e-06, min loss: 4.65867e-07\n",
      "Epoch: 2072600, elapsed: 1.08e+01, train loss: 4.73668e-07, val loss: 1.47626e-06, min loss: 4.65867e-07\n",
      "Epoch: 2072700, elapsed: 1.09e+01, train loss: 4.65745e-07, val loss: 1.47624e-06, min loss: 4.65745e-07\n",
      "Epoch: 2072800, elapsed: 1.07e+01, train loss: 4.72482e-07, val loss: 1.48061e-06, min loss: 4.65745e-07\n",
      "Epoch: 2072900, elapsed: 1.10e+01, train loss: 5.10865e-07, val loss: 1.48461e-06, min loss: 4.65745e-07\n",
      "Epoch: 2073000, elapsed: 1.08e+01, train loss: 5.13390e-07, val loss: 1.54102e-06, min loss: 4.65745e-07\n",
      "Epoch: 2073100, elapsed: 1.09e+01, train loss: 4.69452e-07, val loss: 1.46374e-06, min loss: 4.65745e-07\n",
      "Epoch: 2073200, elapsed: 1.09e+01, train loss: 5.45577e-07, val loss: 1.47506e-06, min loss: 4.65745e-07\n",
      "Epoch: 2073300, elapsed: 1.07e+01, train loss: 4.97792e-07, val loss: 1.47197e-06, min loss: 4.65745e-07\n",
      "Epoch: 2073400, elapsed: 1.07e+01, train loss: 4.65550e-07, val loss: 1.48019e-06, min loss: 4.65550e-07\n",
      "Epoch: 2073500, elapsed: 1.09e+01, train loss: 4.77759e-07, val loss: 1.52260e-06, min loss: 4.65550e-07\n",
      "Epoch: 2073600, elapsed: 1.10e+01, train loss: 4.69356e-07, val loss: 1.49178e-06, min loss: 4.65550e-07\n",
      "Epoch: 2073700, elapsed: 1.09e+01, train loss: 4.67201e-07, val loss: 1.47926e-06, min loss: 4.65550e-07\n",
      "Epoch: 2073800, elapsed: 1.08e+01, train loss: 2.06740e-06, val loss: 2.64773e-06, min loss: 4.65550e-07\n",
      "Epoch: 2073900, elapsed: 1.08e+01, train loss: 1.45442e-06, val loss: 1.91128e-06, min loss: 4.65550e-07\n",
      "Epoch: 2074000, elapsed: 1.08e+01, train loss: 1.23765e-06, val loss: 1.57911e-06, min loss: 4.65550e-07\n",
      "Epoch: 2074100, elapsed: 1.09e+01, train loss: 6.64744e-07, val loss: 1.65535e-06, min loss: 4.65550e-07\n",
      "Epoch: 2074200, elapsed: 1.09e+01, train loss: 4.67425e-07, val loss: 1.47153e-06, min loss: 4.65550e-07\n",
      "Epoch: 2074300, elapsed: 1.08e+01, train loss: 4.66203e-07, val loss: 1.47583e-06, min loss: 4.65550e-07\n",
      "Epoch: 2074400, elapsed: 1.64e+01, train loss: 6.61360e-07, val loss: 1.76393e-06, min loss: 4.65550e-07\n",
      "Epoch: 2074500, elapsed: 1.10e+01, train loss: 4.97004e-07, val loss: 1.55725e-06, min loss: 4.65550e-07\n",
      "Epoch: 2074600, elapsed: 1.10e+01, train loss: 4.86922e-07, val loss: 1.53180e-06, min loss: 4.65550e-07\n",
      "Epoch: 2074700, elapsed: 1.10e+01, train loss: 4.73588e-07, val loss: 1.50439e-06, min loss: 4.65550e-07\n",
      "Epoch: 2074800, elapsed: 1.10e+01, train loss: 4.65731e-07, val loss: 1.48965e-06, min loss: 4.65550e-07\n",
      "Epoch: 2074900, elapsed: 1.10e+01, train loss: 4.67683e-07, val loss: 1.49434e-06, min loss: 4.65550e-07\n",
      "Epoch: 2075000, elapsed: 1.10e+01, train loss: 8.64360e-07, val loss: 1.50501e-06, min loss: 4.65550e-07\n",
      "Epoch: 2075100, elapsed: 1.31e+01, train loss: 5.31445e-07, val loss: 1.60875e-06, min loss: 4.65550e-07\n",
      "Epoch: 2075200, elapsed: 1.10e+01, train loss: 4.65428e-07, val loss: 1.49033e-06, min loss: 4.65428e-07\n",
      "Epoch: 2075300, elapsed: 1.09e+01, train loss: 4.77284e-07, val loss: 1.48744e-06, min loss: 4.65428e-07\n",
      "Epoch: 2075400, elapsed: 1.09e+01, train loss: 4.64401e-07, val loss: 1.47881e-06, min loss: 4.64401e-07\n",
      "Epoch: 2075500, elapsed: 1.08e+01, train loss: 4.68842e-07, val loss: 1.47314e-06, min loss: 4.64401e-07\n",
      "Epoch: 2075600, elapsed: 1.09e+01, train loss: 5.68489e-07, val loss: 1.47518e-06, min loss: 4.64401e-07\n",
      "Epoch: 2075700, elapsed: 1.07e+01, train loss: 4.68236e-07, val loss: 1.47138e-06, min loss: 4.64401e-07\n",
      "Epoch: 2075800, elapsed: 1.08e+01, train loss: 4.78356e-07, val loss: 1.49905e-06, min loss: 4.64401e-07\n",
      "Epoch: 2075900, elapsed: 1.08e+01, train loss: 4.94416e-07, val loss: 1.62052e-06, min loss: 4.64401e-07\n",
      "Epoch: 2076000, elapsed: 1.09e+01, train loss: 4.67937e-07, val loss: 1.47396e-06, min loss: 4.64401e-07\n",
      "Epoch: 2076100, elapsed: 1.09e+01, train loss: 4.65502e-07, val loss: 1.47232e-06, min loss: 4.64401e-07\n",
      "Epoch: 2076200, elapsed: 1.09e+01, train loss: 4.75596e-07, val loss: 1.46107e-06, min loss: 4.64401e-07\n",
      "Epoch: 2076300, elapsed: 1.07e+01, train loss: 1.03721e-06, val loss: 1.93268e-06, min loss: 4.64401e-07\n",
      "Epoch: 2076400, elapsed: 1.09e+01, train loss: 1.97611e-06, val loss: 3.65897e-06, min loss: 4.64401e-07\n",
      "Epoch: 2076500, elapsed: 1.09e+01, train loss: 4.70581e-07, val loss: 1.50181e-06, min loss: 4.64401e-07\n",
      "Epoch: 2076600, elapsed: 1.08e+01, train loss: 4.84061e-07, val loss: 1.53828e-06, min loss: 4.64401e-07\n",
      "Epoch: 2076700, elapsed: 1.08e+01, train loss: 6.69587e-07, val loss: 1.49596e-06, min loss: 4.64401e-07\n",
      "Epoch: 2076800, elapsed: 1.09e+01, train loss: 1.25021e-06, val loss: 2.22623e-06, min loss: 4.64401e-07\n",
      "Epoch: 2076900, elapsed: 1.08e+01, train loss: 6.96716e-07, val loss: 1.91480e-06, min loss: 4.64401e-07\n",
      "Epoch: 2077000, elapsed: 1.08e+01, train loss: 4.64010e-07, val loss: 1.48127e-06, min loss: 4.64010e-07\n",
      "Epoch: 2077100, elapsed: 1.09e+01, train loss: 4.64604e-07, val loss: 1.47823e-06, min loss: 4.64010e-07\n",
      "Epoch: 2077200, elapsed: 1.08e+01, train loss: 2.65709e-06, val loss: 4.12838e-06, min loss: 4.64010e-07\n",
      "Epoch: 2077300, elapsed: 1.07e+01, train loss: 5.31917e-07, val loss: 1.65062e-06, min loss: 4.64010e-07\n",
      "Epoch: 2077400, elapsed: 1.11e+01, train loss: 4.65476e-07, val loss: 1.48633e-06, min loss: 4.64010e-07\n",
      "Epoch: 2077500, elapsed: 1.08e+01, train loss: 4.67639e-07, val loss: 1.46386e-06, min loss: 4.64010e-07\n",
      "Epoch: 2077600, elapsed: 1.09e+01, train loss: 5.27664e-07, val loss: 1.55238e-06, min loss: 4.64010e-07\n",
      "Epoch: 2077700, elapsed: 1.07e+01, train loss: 5.45869e-07, val loss: 1.47806e-06, min loss: 4.64010e-07\n",
      "Epoch: 2077800, elapsed: 1.07e+01, train loss: 1.25070e-06, val loss: 2.38856e-06, min loss: 4.64010e-07\n",
      "Epoch: 2077900, elapsed: 1.10e+01, train loss: 4.64007e-07, val loss: 1.48301e-06, min loss: 4.64007e-07\n",
      "Epoch: 2078000, elapsed: 1.09e+01, train loss: 4.64857e-07, val loss: 1.47877e-06, min loss: 4.64007e-07\n",
      "Epoch: 2078100, elapsed: 1.68e+01, train loss: 5.22510e-07, val loss: 1.45415e-06, min loss: 4.64007e-07\n",
      "Epoch: 2078200, elapsed: 1.12e+01, train loss: 4.63916e-07, val loss: 1.47626e-06, min loss: 4.63916e-07\n",
      "Epoch: 2078300, elapsed: 1.11e+01, train loss: 5.68810e-07, val loss: 1.49280e-06, min loss: 4.63916e-07\n",
      "Epoch: 2078400, elapsed: 1.10e+01, train loss: 1.89999e-06, val loss: 2.71983e-06, min loss: 4.63916e-07\n",
      "Epoch: 2078500, elapsed: 1.11e+01, train loss: 6.36292e-07, val loss: 1.69325e-06, min loss: 4.63916e-07\n",
      "Epoch: 2078600, elapsed: 1.11e+01, train loss: 3.32256e-06, val loss: 3.21064e-06, min loss: 4.63916e-07\n",
      "Epoch: 2078700, elapsed: 1.11e+01, train loss: 4.64753e-07, val loss: 1.49544e-06, min loss: 4.63916e-07\n",
      "Epoch: 2078800, elapsed: 1.10e+01, train loss: 1.26906e-06, val loss: 2.14020e-06, min loss: 4.63916e-07\n",
      "Epoch: 2078900, elapsed: 1.11e+01, train loss: 4.73820e-07, val loss: 1.48839e-06, min loss: 4.63916e-07\n",
      "Epoch: 2079000, elapsed: 1.10e+01, train loss: 4.85770e-07, val loss: 1.54871e-06, min loss: 4.63916e-07\n",
      "Epoch: 2079100, elapsed: 1.10e+01, train loss: 5.06711e-07, val loss: 1.60783e-06, min loss: 4.63916e-07\n",
      "Epoch: 2079200, elapsed: 1.08e+01, train loss: 4.71135e-07, val loss: 1.47838e-06, min loss: 4.63916e-07\n",
      "Epoch: 2079300, elapsed: 1.10e+01, train loss: 6.48097e-07, val loss: 1.61570e-06, min loss: 4.63916e-07\n",
      "Epoch: 2079400, elapsed: 1.11e+01, train loss: 4.81734e-07, val loss: 1.48982e-06, min loss: 4.63916e-07\n",
      "Epoch: 2079500, elapsed: 1.10e+01, train loss: 4.72137e-07, val loss: 1.48593e-06, min loss: 4.63916e-07\n",
      "Epoch: 2079600, elapsed: 1.10e+01, train loss: 4.68646e-07, val loss: 1.49124e-06, min loss: 4.63916e-07\n",
      "Epoch: 2079700, elapsed: 1.09e+01, train loss: 5.34797e-07, val loss: 2.38450e-06, min loss: 4.63916e-07\n",
      "Epoch: 2079800, elapsed: 1.11e+01, train loss: 4.63802e-07, val loss: 1.48828e-06, min loss: 4.63802e-07\n",
      "Epoch: 2079900, elapsed: 1.10e+01, train loss: 8.00222e-07, val loss: 2.06125e-06, min loss: 4.63802e-07\n",
      "Epoch: 2080000, elapsed: 1.10e+01, train loss: 4.63199e-07, val loss: 1.48193e-06, min loss: 4.63199e-07\n",
      "Epoch: 2080100, elapsed: 1.30e+01, train loss: 4.73476e-07, val loss: 1.49361e-06, min loss: 4.63199e-07\n",
      "Epoch: 2080200, elapsed: 1.11e+01, train loss: 6.65978e-07, val loss: 1.97855e-06, min loss: 4.63199e-07\n",
      "Epoch: 2080300, elapsed: 1.10e+01, train loss: 4.63166e-07, val loss: 1.48702e-06, min loss: 4.63166e-07\n",
      "Epoch: 2080400, elapsed: 1.10e+01, train loss: 5.46169e-07, val loss: 1.57133e-06, min loss: 4.63166e-07\n",
      "Epoch: 2080500, elapsed: 1.08e+01, train loss: 6.17416e-07, val loss: 1.83659e-06, min loss: 4.63166e-07\n",
      "Epoch: 2080600, elapsed: 1.10e+01, train loss: 4.70579e-07, val loss: 1.47365e-06, min loss: 4.63166e-07\n",
      "Epoch: 2080700, elapsed: 1.10e+01, train loss: 4.73678e-07, val loss: 1.47306e-06, min loss: 4.63166e-07\n",
      "Epoch: 2080800, elapsed: 1.12e+01, train loss: 2.60698e-06, val loss: 3.82745e-06, min loss: 4.63166e-07\n",
      "Epoch: 2080900, elapsed: 1.10e+01, train loss: 4.63224e-07, val loss: 1.48788e-06, min loss: 4.63166e-07\n",
      "Epoch: 2081000, elapsed: 1.09e+01, train loss: 4.68096e-07, val loss: 1.48754e-06, min loss: 4.63166e-07\n",
      "Epoch: 2081100, elapsed: 1.09e+01, train loss: 1.56123e-06, val loss: 2.37447e-06, min loss: 4.63166e-07\n",
      "Epoch: 2081200, elapsed: 1.07e+01, train loss: 5.34604e-07, val loss: 1.50247e-06, min loss: 4.63166e-07\n",
      "Epoch: 2081300, elapsed: 1.08e+01, train loss: 7.17003e-07, val loss: 1.46324e-06, min loss: 4.63166e-07\n",
      "Epoch: 2081400, elapsed: 1.08e+01, train loss: 4.62862e-07, val loss: 1.48411e-06, min loss: 4.62862e-07\n",
      "Epoch: 2081500, elapsed: 1.10e+01, train loss: 9.30577e-07, val loss: 1.64712e-06, min loss: 4.62862e-07\n",
      "Epoch: 2081600, elapsed: 1.08e+01, train loss: 4.84948e-07, val loss: 1.59978e-06, min loss: 4.62862e-07\n",
      "Epoch: 2081700, elapsed: 1.07e+01, train loss: 5.10909e-07, val loss: 1.47501e-06, min loss: 4.62862e-07\n",
      "Epoch: 2081800, elapsed: 1.64e+01, train loss: 4.78377e-07, val loss: 1.49969e-06, min loss: 4.62862e-07\n",
      "Epoch: 2081900, elapsed: 1.12e+01, train loss: 4.94989e-07, val loss: 1.51010e-06, min loss: 4.62862e-07\n",
      "Epoch: 2082000, elapsed: 1.10e+01, train loss: 5.67904e-07, val loss: 1.58553e-06, min loss: 4.62862e-07\n",
      "Epoch: 2082100, elapsed: 1.08e+01, train loss: 4.92028e-07, val loss: 1.44996e-06, min loss: 4.62862e-07\n",
      "Epoch: 2082200, elapsed: 1.10e+01, train loss: 4.64668e-07, val loss: 1.47976e-06, min loss: 4.62862e-07\n",
      "Epoch: 2082300, elapsed: 1.09e+01, train loss: 4.69923e-07, val loss: 1.44974e-06, min loss: 4.62862e-07\n",
      "Epoch: 2082400, elapsed: 1.09e+01, train loss: 5.03111e-07, val loss: 1.45048e-06, min loss: 4.62862e-07\n",
      "Epoch: 2082500, elapsed: 1.10e+01, train loss: 5.06280e-07, val loss: 1.47793e-06, min loss: 4.62862e-07\n",
      "Epoch: 2082600, elapsed: 1.08e+01, train loss: 4.62829e-07, val loss: 1.48018e-06, min loss: 4.62829e-07\n",
      "Epoch: 2082700, elapsed: 1.09e+01, train loss: 6.02758e-07, val loss: 1.53116e-06, min loss: 4.62829e-07\n",
      "Epoch: 2082800, elapsed: 1.08e+01, train loss: 7.95939e-07, val loss: 1.70882e-06, min loss: 4.62829e-07\n",
      "Epoch: 2082900, elapsed: 1.09e+01, train loss: 5.10200e-07, val loss: 1.51084e-06, min loss: 4.62829e-07\n",
      "Epoch: 2083000, elapsed: 1.09e+01, train loss: 5.03339e-07, val loss: 1.53609e-06, min loss: 4.62829e-07\n",
      "Epoch: 2083100, elapsed: 1.07e+01, train loss: 5.71197e-07, val loss: 1.47413e-06, min loss: 4.62829e-07\n",
      "Epoch: 2083200, elapsed: 1.07e+01, train loss: 4.65602e-07, val loss: 1.48039e-06, min loss: 4.62829e-07\n",
      "Epoch: 2083300, elapsed: 1.05e+01, train loss: 5.02876e-07, val loss: 1.54747e-06, min loss: 4.62829e-07\n",
      "Epoch: 2083400, elapsed: 1.07e+01, train loss: 4.75856e-07, val loss: 1.47556e-06, min loss: 4.62829e-07\n",
      "Epoch: 2083500, elapsed: 1.09e+01, train loss: 4.78512e-07, val loss: 1.49793e-06, min loss: 4.62829e-07\n",
      "Epoch: 2083600, elapsed: 1.07e+01, train loss: 8.80422e-07, val loss: 1.69897e-06, min loss: 4.62829e-07\n",
      "Epoch: 2083700, elapsed: 1.08e+01, train loss: 4.62157e-07, val loss: 1.48709e-06, min loss: 4.62157e-07\n",
      "Epoch: 2083800, elapsed: 1.06e+01, train loss: 4.66732e-07, val loss: 1.45885e-06, min loss: 4.62157e-07\n",
      "Epoch: 2083900, elapsed: 1.08e+01, train loss: 5.17022e-07, val loss: 1.59242e-06, min loss: 4.62157e-07\n",
      "Epoch: 2084000, elapsed: 1.08e+01, train loss: 4.13002e-06, val loss: 4.45951e-06, min loss: 4.62157e-07\n",
      "Epoch: 2084100, elapsed: 1.06e+01, train loss: 4.62040e-07, val loss: 1.48476e-06, min loss: 4.62040e-07\n",
      "Epoch: 2084200, elapsed: 1.08e+01, train loss: 5.05111e-07, val loss: 1.53246e-06, min loss: 4.62040e-07\n",
      "Epoch: 2084300, elapsed: 1.08e+01, train loss: 4.62960e-07, val loss: 1.48548e-06, min loss: 4.62040e-07\n",
      "Epoch: 2084400, elapsed: 1.08e+01, train loss: 4.62387e-07, val loss: 1.48191e-06, min loss: 4.62040e-07\n",
      "Epoch: 2084500, elapsed: 1.08e+01, train loss: 5.47061e-07, val loss: 1.51805e-06, min loss: 4.62040e-07\n",
      "Epoch: 2084600, elapsed: 1.08e+01, train loss: 4.61878e-07, val loss: 1.48611e-06, min loss: 4.61878e-07\n",
      "Epoch: 2084700, elapsed: 1.07e+01, train loss: 4.77003e-07, val loss: 1.50910e-06, min loss: 4.61878e-07\n",
      "Epoch: 2084800, elapsed: 1.08e+01, train loss: 1.41574e-06, val loss: 2.39428e-06, min loss: 4.61878e-07\n",
      "Epoch: 2084900, elapsed: 1.08e+01, train loss: 1.17641e-06, val loss: 2.32396e-06, min loss: 4.61878e-07\n",
      "Epoch: 2085000, elapsed: 1.09e+01, train loss: 4.62169e-07, val loss: 1.48906e-06, min loss: 4.61878e-07\n",
      "Epoch: 2085100, elapsed: 1.27e+01, train loss: 8.31689e-07, val loss: 1.70204e-06, min loss: 4.61878e-07\n",
      "Epoch: 2085200, elapsed: 1.09e+01, train loss: 2.14404e-06, val loss: 3.77566e-06, min loss: 4.61878e-07\n",
      "Epoch: 2085300, elapsed: 1.07e+01, train loss: 4.65162e-07, val loss: 1.51954e-06, min loss: 4.61878e-07\n",
      "Epoch: 2085400, elapsed: 1.08e+01, train loss: 4.89954e-07, val loss: 1.53979e-06, min loss: 4.61878e-07\n",
      "Epoch: 2085500, elapsed: 1.68e+01, train loss: 4.61661e-07, val loss: 1.48640e-06, min loss: 4.61661e-07\n",
      "Epoch: 2085600, elapsed: 1.09e+01, train loss: 4.66759e-07, val loss: 1.47039e-06, min loss: 4.61661e-07\n",
      "Epoch: 2085700, elapsed: 1.10e+01, train loss: 4.61553e-07, val loss: 1.48851e-06, min loss: 4.61553e-07\n",
      "Epoch: 2085800, elapsed: 1.09e+01, train loss: 4.63816e-07, val loss: 1.50376e-06, min loss: 4.61553e-07\n",
      "Epoch: 2085900, elapsed: 1.10e+01, train loss: 4.61528e-07, val loss: 1.48676e-06, min loss: 4.61528e-07\n",
      "Epoch: 2086000, elapsed: 1.10e+01, train loss: 4.69491e-07, val loss: 1.46798e-06, min loss: 4.61528e-07\n",
      "Epoch: 2086100, elapsed: 1.10e+01, train loss: 4.61453e-07, val loss: 1.48810e-06, min loss: 4.61453e-07\n",
      "Epoch: 2086200, elapsed: 1.10e+01, train loss: 4.77713e-07, val loss: 1.45631e-06, min loss: 4.61453e-07\n",
      "Epoch: 2086300, elapsed: 1.09e+01, train loss: 4.92202e-07, val loss: 1.54932e-06, min loss: 4.61453e-07\n",
      "Epoch: 2086400, elapsed: 1.07e+01, train loss: 4.64843e-07, val loss: 1.50276e-06, min loss: 4.61453e-07\n",
      "Epoch: 2086500, elapsed: 1.08e+01, train loss: 4.75346e-07, val loss: 1.51184e-06, min loss: 4.61453e-07\n",
      "Epoch: 2086600, elapsed: 1.08e+01, train loss: 1.89484e-06, val loss: 2.64884e-06, min loss: 4.61453e-07\n",
      "Epoch: 2086700, elapsed: 1.08e+01, train loss: 1.28673e-06, val loss: 1.70101e-06, min loss: 4.61453e-07\n",
      "Epoch: 2086800, elapsed: 1.08e+01, train loss: 8.54765e-07, val loss: 1.94043e-06, min loss: 4.61453e-07\n",
      "Epoch: 2086900, elapsed: 1.09e+01, train loss: 4.70985e-07, val loss: 1.51471e-06, min loss: 4.61453e-07\n",
      "Epoch: 2087000, elapsed: 1.11e+01, train loss: 4.61415e-07, val loss: 1.48257e-06, min loss: 4.61415e-07\n",
      "Epoch: 2087100, elapsed: 1.08e+01, train loss: 4.83791e-07, val loss: 1.55251e-06, min loss: 4.61415e-07\n",
      "Epoch: 2087200, elapsed: 1.08e+01, train loss: 5.64897e-07, val loss: 1.67349e-06, min loss: 4.61415e-07\n",
      "Epoch: 2087300, elapsed: 1.08e+01, train loss: 5.36313e-07, val loss: 1.58474e-06, min loss: 4.61415e-07\n",
      "Epoch: 2087400, elapsed: 1.07e+01, train loss: 4.61323e-07, val loss: 1.49075e-06, min loss: 4.61323e-07\n",
      "Epoch: 2087500, elapsed: 1.08e+01, train loss: 4.63111e-07, val loss: 1.48062e-06, min loss: 4.61323e-07\n",
      "Epoch: 2087600, elapsed: 1.08e+01, train loss: 8.74832e-07, val loss: 1.79980e-06, min loss: 4.61323e-07\n",
      "Epoch: 2087700, elapsed: 1.06e+01, train loss: 2.02807e-06, val loss: 3.81095e-06, min loss: 4.61323e-07\n",
      "Epoch: 2087800, elapsed: 1.07e+01, train loss: 4.61308e-07, val loss: 1.48787e-06, min loss: 4.61308e-07\n",
      "Epoch: 2087900, elapsed: 1.10e+01, train loss: 5.49988e-07, val loss: 1.60442e-06, min loss: 4.61308e-07\n",
      "Epoch: 2088000, elapsed: 1.07e+01, train loss: 4.77356e-07, val loss: 1.54196e-06, min loss: 4.61308e-07\n",
      "Epoch: 2088100, elapsed: 1.07e+01, train loss: 4.71699e-07, val loss: 1.53496e-06, min loss: 4.61308e-07\n",
      "Epoch: 2088200, elapsed: 1.06e+01, train loss: 4.74385e-07, val loss: 1.46238e-06, min loss: 4.61308e-07\n",
      "Epoch: 2088300, elapsed: 1.06e+01, train loss: 6.43367e-07, val loss: 1.78358e-06, min loss: 4.61308e-07\n",
      "Epoch: 2088400, elapsed: 1.08e+01, train loss: 4.62189e-07, val loss: 1.49203e-06, min loss: 4.61308e-07\n",
      "Epoch: 2088500, elapsed: 1.07e+01, train loss: 4.62425e-07, val loss: 1.49501e-06, min loss: 4.61308e-07\n",
      "Epoch: 2088600, elapsed: 1.07e+01, train loss: 4.69881e-07, val loss: 1.55887e-06, min loss: 4.61308e-07\n",
      "Epoch: 2088700, elapsed: 1.07e+01, train loss: 6.37767e-07, val loss: 1.82360e-06, min loss: 4.61308e-07\n",
      "Epoch: 2088800, elapsed: 1.07e+01, train loss: 4.61166e-07, val loss: 1.48830e-06, min loss: 4.61166e-07\n",
      "Epoch: 2088900, elapsed: 1.08e+01, train loss: 5.24674e-07, val loss: 1.51306e-06, min loss: 4.61166e-07\n",
      "Epoch: 2089000, elapsed: 1.07e+01, train loss: 9.45813e-07, val loss: 1.53642e-06, min loss: 4.61166e-07\n",
      "Epoch: 2089100, elapsed: 1.07e+01, train loss: 5.16264e-07, val loss: 1.54424e-06, min loss: 4.61166e-07\n",
      "Epoch: 2089200, elapsed: 1.66e+01, train loss: 4.74336e-07, val loss: 1.49450e-06, min loss: 4.61166e-07\n",
      "Epoch: 2089300, elapsed: 1.12e+01, train loss: 3.04168e-06, val loss: 2.85228e-06, min loss: 4.61166e-07\n",
      "Epoch: 2089400, elapsed: 1.12e+01, train loss: 4.87125e-07, val loss: 1.43947e-06, min loss: 4.61166e-07\n",
      "Epoch: 2089500, elapsed: 1.11e+01, train loss: 4.66973e-07, val loss: 1.49067e-06, min loss: 4.61166e-07\n",
      "Epoch: 2089600, elapsed: 1.11e+01, train loss: 4.72740e-07, val loss: 1.53795e-06, min loss: 4.61166e-07\n",
      "Epoch: 2089700, elapsed: 1.10e+01, train loss: 1.17719e-06, val loss: 1.84956e-06, min loss: 4.61166e-07\n",
      "Epoch: 2089800, elapsed: 1.10e+01, train loss: 4.60588e-07, val loss: 1.48930e-06, min loss: 4.60588e-07\n",
      "Epoch: 2089900, elapsed: 1.10e+01, train loss: 1.02768e-06, val loss: 2.12184e-06, min loss: 4.60588e-07\n",
      "Epoch: 2090000, elapsed: 1.10e+01, train loss: 4.60425e-07, val loss: 1.48915e-06, min loss: 4.60425e-07\n",
      "Epoch: 2090100, elapsed: 1.29e+01, train loss: 4.72085e-07, val loss: 1.47710e-06, min loss: 4.60425e-07\n",
      "Epoch: 2090200, elapsed: 1.10e+01, train loss: 4.63094e-07, val loss: 1.48375e-06, min loss: 4.60425e-07\n",
      "Epoch: 2090300, elapsed: 1.08e+01, train loss: 4.69293e-07, val loss: 1.48347e-06, min loss: 4.60425e-07\n",
      "Epoch: 2090400, elapsed: 1.12e+01, train loss: 4.78535e-07, val loss: 1.54850e-06, min loss: 4.60425e-07\n",
      "Epoch: 2090500, elapsed: 1.08e+01, train loss: 6.13786e-07, val loss: 1.80857e-06, min loss: 4.60425e-07\n",
      "Epoch: 2090600, elapsed: 1.10e+01, train loss: 4.78915e-07, val loss: 1.46733e-06, min loss: 4.60425e-07\n",
      "Epoch: 2090700, elapsed: 1.10e+01, train loss: 4.62956e-07, val loss: 1.50380e-06, min loss: 4.60425e-07\n",
      "Epoch: 2090800, elapsed: 1.10e+01, train loss: 7.69842e-07, val loss: 1.83528e-06, min loss: 4.60425e-07\n",
      "Epoch: 2090900, elapsed: 1.09e+01, train loss: 4.60535e-07, val loss: 1.48385e-06, min loss: 4.60425e-07\n",
      "Epoch: 2091000, elapsed: 1.09e+01, train loss: 7.22849e-07, val loss: 1.54924e-06, min loss: 4.60425e-07\n",
      "Epoch: 2091100, elapsed: 1.09e+01, train loss: 4.63739e-07, val loss: 1.49116e-06, min loss: 4.60425e-07\n",
      "Epoch: 2091200, elapsed: 1.09e+01, train loss: 5.05881e-07, val loss: 1.51074e-06, min loss: 4.60425e-07\n",
      "Epoch: 2091300, elapsed: 1.09e+01, train loss: 5.01052e-07, val loss: 1.54701e-06, min loss: 4.60425e-07\n",
      "Epoch: 2091400, elapsed: 1.09e+01, train loss: 4.60897e-07, val loss: 1.48909e-06, min loss: 4.60425e-07\n",
      "Epoch: 2091500, elapsed: 1.09e+01, train loss: 4.72179e-07, val loss: 1.48486e-06, min loss: 4.60425e-07\n",
      "Epoch: 2091600, elapsed: 1.10e+01, train loss: 1.42856e-06, val loss: 2.22535e-06, min loss: 4.60425e-07\n",
      "Epoch: 2091700, elapsed: 1.07e+01, train loss: 4.61615e-07, val loss: 1.49640e-06, min loss: 4.60425e-07\n",
      "Epoch: 2091800, elapsed: 1.08e+01, train loss: 4.62223e-07, val loss: 1.49500e-06, min loss: 4.60425e-07\n",
      "Epoch: 2091900, elapsed: 1.08e+01, train loss: 4.61680e-07, val loss: 1.49899e-06, min loss: 4.60425e-07\n",
      "Epoch: 2092000, elapsed: 1.09e+01, train loss: 4.88089e-07, val loss: 1.55487e-06, min loss: 4.60425e-07\n",
      "Epoch: 2092100, elapsed: 1.09e+01, train loss: 4.61517e-07, val loss: 1.50274e-06, min loss: 4.60425e-07\n",
      "Epoch: 2092200, elapsed: 1.09e+01, train loss: 4.84800e-07, val loss: 1.52423e-06, min loss: 4.60425e-07\n",
      "Epoch: 2092300, elapsed: 1.08e+01, train loss: 4.80245e-07, val loss: 1.47543e-06, min loss: 4.60425e-07\n",
      "Epoch: 2092400, elapsed: 1.07e+01, train loss: 6.35128e-07, val loss: 1.61282e-06, min loss: 4.60425e-07\n",
      "Epoch: 2092500, elapsed: 1.08e+01, train loss: 6.63860e-07, val loss: 1.49948e-06, min loss: 4.60425e-07\n",
      "Epoch: 2092600, elapsed: 1.09e+01, train loss: 4.60089e-07, val loss: 1.49105e-06, min loss: 4.60089e-07\n",
      "Epoch: 2092700, elapsed: 1.08e+01, train loss: 4.62055e-07, val loss: 1.50666e-06, min loss: 4.60089e-07\n",
      "Epoch: 2092800, elapsed: 1.09e+01, train loss: 1.74967e-06, val loss: 2.42521e-06, min loss: 4.60089e-07\n",
      "Epoch: 2092900, elapsed: 1.66e+01, train loss: 4.77812e-07, val loss: 1.47014e-06, min loss: 4.60089e-07\n",
      "Epoch: 2093000, elapsed: 1.09e+01, train loss: 4.63001e-07, val loss: 1.50706e-06, min loss: 4.60089e-07\n",
      "Epoch: 2093100, elapsed: 1.09e+01, train loss: 7.05099e-07, val loss: 1.61277e-06, min loss: 4.60089e-07\n",
      "Epoch: 2093200, elapsed: 1.10e+01, train loss: 5.31860e-07, val loss: 1.57702e-06, min loss: 4.60089e-07\n",
      "Epoch: 2093300, elapsed: 1.10e+01, train loss: 4.60158e-07, val loss: 1.48827e-06, min loss: 4.60089e-07\n",
      "Epoch: 2093400, elapsed: 1.10e+01, train loss: 4.64228e-07, val loss: 1.50709e-06, min loss: 4.60089e-07\n",
      "Epoch: 2093500, elapsed: 1.09e+01, train loss: 4.79565e-07, val loss: 1.48183e-06, min loss: 4.60089e-07\n",
      "Epoch: 2093600, elapsed: 1.09e+01, train loss: 9.60318e-07, val loss: 1.92880e-06, min loss: 4.60089e-07\n",
      "Epoch: 2093700, elapsed: 1.10e+01, train loss: 4.92413e-07, val loss: 1.48836e-06, min loss: 4.60089e-07\n",
      "Epoch: 2093800, elapsed: 1.07e+01, train loss: 4.91539e-07, val loss: 1.44964e-06, min loss: 4.60089e-07\n",
      "Epoch: 2093900, elapsed: 1.10e+01, train loss: 4.71300e-07, val loss: 1.52352e-06, min loss: 4.60089e-07\n",
      "Epoch: 2094000, elapsed: 1.09e+01, train loss: 4.60491e-07, val loss: 1.47370e-06, min loss: 4.60089e-07\n",
      "Epoch: 2094100, elapsed: 1.12e+01, train loss: 4.70356e-07, val loss: 1.52027e-06, min loss: 4.60089e-07\n",
      "Epoch: 2094200, elapsed: 1.09e+01, train loss: 6.50359e-07, val loss: 1.67125e-06, min loss: 4.60089e-07\n",
      "Epoch: 2094300, elapsed: 1.11e+01, train loss: 4.59346e-07, val loss: 1.49052e-06, min loss: 4.59346e-07\n",
      "Epoch: 2094400, elapsed: 1.11e+01, train loss: 4.62190e-07, val loss: 1.47282e-06, min loss: 4.59346e-07\n",
      "Epoch: 2094500, elapsed: 1.10e+01, train loss: 6.47309e-07, val loss: 1.45139e-06, min loss: 4.59346e-07\n",
      "Epoch: 2094600, elapsed: 1.09e+01, train loss: 4.59155e-07, val loss: 1.49434e-06, min loss: 4.59155e-07\n",
      "Epoch: 2094700, elapsed: 1.09e+01, train loss: 4.70318e-07, val loss: 1.51509e-06, min loss: 4.59155e-07\n",
      "Epoch: 2094800, elapsed: 1.07e+01, train loss: 4.63499e-07, val loss: 1.49184e-06, min loss: 4.59155e-07\n",
      "Epoch: 2094900, elapsed: 1.08e+01, train loss: 4.80994e-07, val loss: 1.53276e-06, min loss: 4.59155e-07\n",
      "Epoch: 2095000, elapsed: 1.09e+01, train loss: 5.60925e-07, val loss: 1.63582e-06, min loss: 4.59155e-07\n",
      "Epoch: 2095100, elapsed: 1.28e+01, train loss: 6.74767e-07, val loss: 1.66638e-06, min loss: 4.59155e-07\n",
      "Epoch: 2095200, elapsed: 1.08e+01, train loss: 4.62550e-07, val loss: 1.50751e-06, min loss: 4.59155e-07\n",
      "Epoch: 2095300, elapsed: 1.08e+01, train loss: 4.68172e-07, val loss: 1.50537e-06, min loss: 4.59155e-07\n",
      "Epoch: 2095400, elapsed: 1.08e+01, train loss: 6.11525e-07, val loss: 1.55461e-06, min loss: 4.59155e-07\n",
      "Epoch: 2095500, elapsed: 1.08e+01, train loss: 4.59320e-07, val loss: 1.49484e-06, min loss: 4.59155e-07\n",
      "Epoch: 2095600, elapsed: 1.08e+01, train loss: 9.00510e-07, val loss: 2.94210e-06, min loss: 4.59155e-07\n",
      "Epoch: 2095700, elapsed: 1.09e+01, train loss: 4.58994e-07, val loss: 1.49667e-06, min loss: 4.58994e-07\n",
      "Epoch: 2095800, elapsed: 1.07e+01, train loss: 4.58886e-07, val loss: 1.49519e-06, min loss: 4.58886e-07\n",
      "Epoch: 2095900, elapsed: 1.08e+01, train loss: 4.64068e-07, val loss: 1.50426e-06, min loss: 4.58886e-07\n",
      "Epoch: 2096000, elapsed: 1.08e+01, train loss: 5.67902e-07, val loss: 1.58163e-06, min loss: 4.58886e-07\n",
      "Epoch: 2096100, elapsed: 1.08e+01, train loss: 4.95398e-07, val loss: 1.54445e-06, min loss: 4.58886e-07\n",
      "Epoch: 2096200, elapsed: 1.09e+01, train loss: 1.04557e-06, val loss: 2.12748e-06, min loss: 4.58886e-07\n",
      "Epoch: 2096300, elapsed: 1.07e+01, train loss: 4.58916e-07, val loss: 1.49348e-06, min loss: 4.58886e-07\n",
      "Epoch: 2096400, elapsed: 1.08e+01, train loss: 7.88315e-07, val loss: 1.72913e-06, min loss: 4.58886e-07\n",
      "Epoch: 2096500, elapsed: 1.08e+01, train loss: 4.75985e-07, val loss: 1.53450e-06, min loss: 4.58886e-07\n",
      "Epoch: 2096600, elapsed: 1.68e+01, train loss: 4.65878e-07, val loss: 1.50337e-06, min loss: 4.58886e-07\n",
      "Epoch: 2096700, elapsed: 1.11e+01, train loss: 4.58777e-07, val loss: 1.49700e-06, min loss: 4.58777e-07\n",
      "Epoch: 2096800, elapsed: 1.10e+01, train loss: 4.64808e-07, val loss: 1.49461e-06, min loss: 4.58777e-07\n",
      "Epoch: 2096900, elapsed: 1.09e+01, train loss: 1.45331e-06, val loss: 2.87736e-06, min loss: 4.58777e-07\n",
      "Epoch: 2097000, elapsed: 1.08e+01, train loss: 4.59052e-07, val loss: 1.49002e-06, min loss: 4.58777e-07\n",
      "Epoch: 2097100, elapsed: 1.12e+01, train loss: 4.59261e-07, val loss: 1.49684e-06, min loss: 4.58777e-07\n",
      "Epoch: 2097200, elapsed: 1.29e+01, train loss: 9.52664e-07, val loss: 1.88997e-06, min loss: 4.58777e-07\n",
      "Epoch: 2097300, elapsed: 1.36e+01, train loss: 4.58446e-07, val loss: 1.49671e-06, min loss: 4.58446e-07\n",
      "Epoch: 2097400, elapsed: 1.32e+01, train loss: 4.83155e-07, val loss: 1.56894e-06, min loss: 4.58446e-07\n",
      "Epoch: 2097500, elapsed: 1.29e+01, train loss: 4.59258e-07, val loss: 1.48664e-06, min loss: 4.58446e-07\n",
      "Epoch: 2097600, elapsed: 1.27e+01, train loss: 4.69072e-07, val loss: 1.46805e-06, min loss: 4.58446e-07\n",
      "Epoch: 2097700, elapsed: 1.28e+01, train loss: 4.86737e-07, val loss: 1.58446e-06, min loss: 4.58446e-07\n",
      "Epoch: 2097800, elapsed: 1.33e+01, train loss: 4.59046e-07, val loss: 1.50128e-06, min loss: 4.58446e-07\n",
      "Epoch: 2097900, elapsed: 1.34e+01, train loss: 7.04505e-07, val loss: 1.62944e-06, min loss: 4.58446e-07\n",
      "Epoch: 2098000, elapsed: 1.35e+01, train loss: 4.84611e-07, val loss: 1.50759e-06, min loss: 4.58446e-07\n",
      "Epoch: 2098100, elapsed: 1.35e+01, train loss: 5.92716e-07, val loss: 1.64987e-06, min loss: 4.58446e-07\n",
      "Epoch: 2098200, elapsed: 1.34e+01, train loss: 5.65554e-07, val loss: 1.48519e-06, min loss: 4.58446e-07\n",
      "Epoch: 2098300, elapsed: 1.33e+01, train loss: 7.58057e-07, val loss: 1.89670e-06, min loss: 4.58446e-07\n",
      "Epoch: 2098400, elapsed: 1.36e+01, train loss: 5.43713e-07, val loss: 1.46912e-06, min loss: 4.58446e-07\n",
      "Epoch: 2098500, elapsed: 1.31e+01, train loss: 7.88259e-07, val loss: 2.07053e-06, min loss: 4.58446e-07\n",
      "Epoch: 2098600, elapsed: 1.34e+01, train loss: 5.66464e-07, val loss: 1.69773e-06, min loss: 4.58446e-07\n",
      "Epoch: 2098700, elapsed: 1.34e+01, train loss: 4.62490e-07, val loss: 1.48195e-06, min loss: 4.58446e-07\n",
      "Epoch: 2098800, elapsed: 1.35e+01, train loss: 1.30558e-06, val loss: 2.16577e-06, min loss: 4.58446e-07\n",
      "Epoch: 2098900, elapsed: 1.35e+01, train loss: 4.58022e-07, val loss: 1.49762e-06, min loss: 4.58022e-07\n",
      "Epoch: 2099000, elapsed: 1.38e+01, train loss: 8.12413e-07, val loss: 1.66734e-06, min loss: 4.58022e-07\n",
      "Epoch: 2099100, elapsed: 1.37e+01, train loss: 4.57978e-07, val loss: 1.49577e-06, min loss: 4.57978e-07\n",
      "Epoch: 2099200, elapsed: 1.34e+01, train loss: 4.85158e-07, val loss: 1.52453e-06, min loss: 4.57978e-07\n",
      "Epoch: 2099300, elapsed: 1.29e+01, train loss: 6.86802e-07, val loss: 1.85647e-06, min loss: 4.57978e-07\n",
      "Epoch: 2099400, elapsed: 1.36e+01, train loss: 4.59775e-07, val loss: 1.48908e-06, min loss: 4.57978e-07\n",
      "Epoch: 2099500, elapsed: 1.32e+01, train loss: 5.22938e-07, val loss: 1.50114e-06, min loss: 4.57978e-07\n",
      "Epoch: 2099600, elapsed: 1.34e+01, train loss: 4.62951e-07, val loss: 1.53606e-06, min loss: 4.57978e-07\n",
      "Epoch: 2099700, elapsed: 1.36e+01, train loss: 4.62375e-07, val loss: 1.51039e-06, min loss: 4.57978e-07\n",
      "Epoch: 2099800, elapsed: 1.31e+01, train loss: 5.53294e-07, val loss: 1.56756e-06, min loss: 4.57978e-07\n",
      "Epoch: 2099900, elapsed: 1.34e+01, train loss: 4.57843e-07, val loss: 1.50176e-06, min loss: 4.57843e-07\n",
      "Epoch: 2100000, elapsed: 1.31e+01, train loss: 4.70798e-07, val loss: 1.51864e-06, min loss: 4.57843e-07\n",
      "Epoch: 2100100, elapsed: 1.53e+01, train loss: 4.66625e-07, val loss: 1.45146e-06, min loss: 4.57843e-07\n",
      "Epoch: 2100200, elapsed: 1.23e+01, train loss: 4.58606e-07, val loss: 1.49743e-06, min loss: 4.57843e-07\n",
      "Epoch: 2100300, elapsed: 1.78e+01, train loss: 5.25964e-07, val loss: 1.57745e-06, min loss: 4.57843e-07\n",
      "Epoch: 2100400, elapsed: 1.27e+01, train loss: 1.31001e-06, val loss: 2.41678e-06, min loss: 4.57843e-07\n",
      "Epoch: 2100500, elapsed: 1.25e+01, train loss: 4.57628e-07, val loss: 1.49792e-06, min loss: 4.57628e-07\n",
      "Epoch: 2100600, elapsed: 1.23e+01, train loss: 4.80699e-07, val loss: 1.47182e-06, min loss: 4.57628e-07\n",
      "Epoch: 2100700, elapsed: 1.24e+01, train loss: 4.60775e-07, val loss: 1.48599e-06, min loss: 4.57628e-07\n",
      "Epoch: 2100800, elapsed: 1.24e+01, train loss: 4.58043e-07, val loss: 1.50798e-06, min loss: 4.57628e-07\n",
      "Epoch: 2100900, elapsed: 1.23e+01, train loss: 1.12907e-06, val loss: 2.18921e-06, min loss: 4.57628e-07\n",
      "Epoch: 2101000, elapsed: 1.21e+01, train loss: 1.00700e-06, val loss: 2.31885e-06, min loss: 4.57628e-07\n",
      "Epoch: 2101100, elapsed: 1.24e+01, train loss: 8.28520e-07, val loss: 1.65751e-06, min loss: 4.57628e-07\n",
      "Epoch: 2101200, elapsed: 1.24e+01, train loss: 4.76275e-07, val loss: 1.54071e-06, min loss: 4.57628e-07\n",
      "Epoch: 2101300, elapsed: 1.24e+01, train loss: 4.57657e-07, val loss: 1.49703e-06, min loss: 4.57628e-07\n",
      "Epoch: 2101400, elapsed: 1.21e+01, train loss: 4.64037e-07, val loss: 1.46441e-06, min loss: 4.57628e-07\n",
      "Epoch: 2101500, elapsed: 1.20e+01, train loss: 4.58265e-07, val loss: 1.50269e-06, min loss: 4.57628e-07\n",
      "Epoch: 2101600, elapsed: 1.22e+01, train loss: 4.57280e-07, val loss: 1.50046e-06, min loss: 4.57280e-07\n",
      "Epoch: 2101700, elapsed: 1.21e+01, train loss: 4.58820e-07, val loss: 1.50986e-06, min loss: 4.57280e-07\n",
      "Epoch: 2101800, elapsed: 1.20e+01, train loss: 3.10507e-06, val loss: 5.31596e-06, min loss: 4.57280e-07\n",
      "Epoch: 2101900, elapsed: 1.22e+01, train loss: 4.57233e-07, val loss: 1.50040e-06, min loss: 4.57233e-07\n",
      "Epoch: 2102000, elapsed: 1.24e+01, train loss: 1.29069e-06, val loss: 1.55385e-06, min loss: 4.57233e-07\n",
      "Epoch: 2102100, elapsed: 1.24e+01, train loss: 4.57998e-07, val loss: 1.49118e-06, min loss: 4.57233e-07\n",
      "Epoch: 2102200, elapsed: 1.24e+01, train loss: 4.59391e-07, val loss: 1.49170e-06, min loss: 4.57233e-07\n",
      "Epoch: 2102300, elapsed: 1.30e+01, train loss: 8.06144e-07, val loss: 2.38960e-06, min loss: 4.57233e-07\n",
      "Epoch: 2102400, elapsed: 1.36e+01, train loss: 4.57105e-07, val loss: 1.50028e-06, min loss: 4.57105e-07\n",
      "Epoch: 2102500, elapsed: 1.27e+01, train loss: 6.21111e-07, val loss: 1.87264e-06, min loss: 4.57105e-07\n",
      "Epoch: 2102600, elapsed: 1.21e+01, train loss: 4.57031e-07, val loss: 1.49953e-06, min loss: 4.57031e-07\n",
      "Epoch: 2102700, elapsed: 1.19e+01, train loss: 4.99605e-07, val loss: 1.57774e-06, min loss: 4.57031e-07\n",
      "Epoch: 2102800, elapsed: 1.23e+01, train loss: 5.05066e-07, val loss: 1.61383e-06, min loss: 4.57031e-07\n",
      "Epoch: 2102900, elapsed: 1.21e+01, train loss: 4.56986e-07, val loss: 1.49702e-06, min loss: 4.56986e-07\n",
      "Epoch: 2103000, elapsed: 1.21e+01, train loss: 5.06780e-07, val loss: 1.60820e-06, min loss: 4.56986e-07\n",
      "Epoch: 2103100, elapsed: 1.23e+01, train loss: 4.84883e-07, val loss: 1.57052e-06, min loss: 4.56986e-07\n",
      "Epoch: 2103200, elapsed: 1.23e+01, train loss: 4.90590e-07, val loss: 1.68380e-06, min loss: 4.56986e-07\n",
      "Epoch: 2103300, elapsed: 1.18e+01, train loss: 1.22918e-06, val loss: 2.27836e-06, min loss: 4.56986e-07\n",
      "Epoch: 2103400, elapsed: 1.20e+01, train loss: 4.56837e-07, val loss: 1.50023e-06, min loss: 4.56837e-07\n",
      "Epoch: 2103500, elapsed: 1.20e+01, train loss: 4.61178e-07, val loss: 1.50220e-06, min loss: 4.56837e-07\n",
      "Epoch: 2103600, elapsed: 1.23e+01, train loss: 4.64399e-07, val loss: 1.52642e-06, min loss: 4.56837e-07\n",
      "Epoch: 2103700, elapsed: 1.21e+01, train loss: 4.57815e-07, val loss: 1.49111e-06, min loss: 4.56837e-07\n",
      "Epoch: 2103800, elapsed: 1.18e+01, train loss: 5.13219e-07, val loss: 1.61383e-06, min loss: 4.56837e-07\n",
      "Epoch: 2103900, elapsed: 1.19e+01, train loss: 7.46247e-07, val loss: 1.58947e-06, min loss: 4.56837e-07\n",
      "Epoch: 2104000, elapsed: 1.18e+01, train loss: 6.05507e-07, val loss: 1.60489e-06, min loss: 4.56837e-07\n",
      "Epoch: 2104100, elapsed: 1.83e+01, train loss: 4.56999e-07, val loss: 1.50488e-06, min loss: 4.56837e-07\n",
      "Epoch: 2104200, elapsed: 1.23e+01, train loss: 4.82189e-07, val loss: 1.48673e-06, min loss: 4.56837e-07\n",
      "Epoch: 2104300, elapsed: 1.26e+01, train loss: 6.87703e-07, val loss: 1.76567e-06, min loss: 4.56837e-07\n",
      "Epoch: 2104400, elapsed: 1.26e+01, train loss: 5.17047e-07, val loss: 1.48898e-06, min loss: 4.56837e-07\n",
      "Epoch: 2104500, elapsed: 1.22e+01, train loss: 4.62437e-07, val loss: 1.49151e-06, min loss: 4.56837e-07\n",
      "Epoch: 2104600, elapsed: 1.24e+01, train loss: 4.67168e-07, val loss: 1.55895e-06, min loss: 4.56837e-07\n",
      "Epoch: 2104700, elapsed: 1.23e+01, train loss: 1.08069e-06, val loss: 1.87450e-06, min loss: 4.56837e-07\n",
      "Epoch: 2104800, elapsed: 1.23e+01, train loss: 2.22217e-06, val loss: 3.22488e-06, min loss: 4.56837e-07\n",
      "Epoch: 2104900, elapsed: 1.24e+01, train loss: 4.56488e-07, val loss: 1.50423e-06, min loss: 4.56488e-07\n",
      "Epoch: 2105000, elapsed: 1.21e+01, train loss: 4.61313e-07, val loss: 1.50840e-06, min loss: 4.56488e-07\n",
      "Epoch: 2105100, elapsed: 1.42e+01, train loss: 5.43825e-07, val loss: 1.48947e-06, min loss: 4.56488e-07\n",
      "Epoch: 2105200, elapsed: 1.24e+01, train loss: 4.64345e-07, val loss: 1.49938e-06, min loss: 4.56488e-07\n",
      "Epoch: 2105300, elapsed: 1.21e+01, train loss: 4.57473e-07, val loss: 1.51525e-06, min loss: 4.56488e-07\n",
      "Epoch: 2105400, elapsed: 1.21e+01, train loss: 5.12324e-07, val loss: 1.61087e-06, min loss: 4.56488e-07\n",
      "Epoch: 2105500, elapsed: 1.25e+01, train loss: 6.40041e-07, val loss: 1.83968e-06, min loss: 4.56488e-07\n",
      "Epoch: 2105600, elapsed: 1.22e+01, train loss: 9.12595e-07, val loss: 2.26959e-06, min loss: 4.56488e-07\n",
      "Epoch: 2105700, elapsed: 1.23e+01, train loss: 4.56774e-07, val loss: 1.51128e-06, min loss: 4.56488e-07\n",
      "Epoch: 2105800, elapsed: 1.23e+01, train loss: 6.70002e-07, val loss: 1.80137e-06, min loss: 4.56488e-07\n",
      "Epoch: 2105900, elapsed: 1.20e+01, train loss: 4.56501e-07, val loss: 1.50500e-06, min loss: 4.56488e-07\n",
      "Epoch: 2106000, elapsed: 1.21e+01, train loss: 7.70148e-07, val loss: 2.04139e-06, min loss: 4.56488e-07\n",
      "Epoch: 2106100, elapsed: 1.20e+01, train loss: 4.56224e-07, val loss: 1.50177e-06, min loss: 4.56224e-07\n",
      "Epoch: 2106200, elapsed: 1.24e+01, train loss: 5.94853e-07, val loss: 1.65226e-06, min loss: 4.56224e-07\n",
      "Epoch: 2106300, elapsed: 1.20e+01, train loss: 4.65322e-07, val loss: 1.49522e-06, min loss: 4.56224e-07\n",
      "Epoch: 2106400, elapsed: 1.20e+01, train loss: 4.72128e-07, val loss: 1.47069e-06, min loss: 4.56224e-07\n",
      "Epoch: 2106500, elapsed: 1.18e+01, train loss: 7.95746e-07, val loss: 1.86598e-06, min loss: 4.56224e-07\n",
      "Epoch: 2106600, elapsed: 1.20e+01, train loss: 4.56318e-07, val loss: 1.50345e-06, min loss: 4.56224e-07\n",
      "Epoch: 2106700, elapsed: 1.21e+01, train loss: 4.98886e-07, val loss: 1.51527e-06, min loss: 4.56224e-07\n",
      "Epoch: 2106800, elapsed: 1.21e+01, train loss: 9.92194e-07, val loss: 1.64693e-06, min loss: 4.56224e-07\n",
      "Epoch: 2106900, elapsed: 1.21e+01, train loss: 4.69184e-07, val loss: 1.54142e-06, min loss: 4.56224e-07\n",
      "Epoch: 2107000, elapsed: 1.22e+01, train loss: 4.57069e-07, val loss: 1.51025e-06, min loss: 4.56224e-07\n",
      "Epoch: 2107100, elapsed: 1.22e+01, train loss: 5.07445e-07, val loss: 1.47715e-06, min loss: 4.56224e-07\n",
      "Epoch: 2107200, elapsed: 1.24e+01, train loss: 4.56244e-07, val loss: 1.50307e-06, min loss: 4.56224e-07\n",
      "Epoch: 2107300, elapsed: 1.19e+01, train loss: 5.91914e-07, val loss: 1.62306e-06, min loss: 4.56224e-07\n",
      "Epoch: 2107400, elapsed: 1.21e+01, train loss: 6.13490e-07, val loss: 1.78458e-06, min loss: 4.56224e-07\n",
      "Epoch: 2107500, elapsed: 1.20e+01, train loss: 4.56658e-07, val loss: 1.50596e-06, min loss: 4.56224e-07\n",
      "Epoch: 2107600, elapsed: 1.23e+01, train loss: 6.38951e-07, val loss: 1.70947e-06, min loss: 4.56224e-07\n",
      "Epoch: 2107700, elapsed: 1.23e+01, train loss: 6.21783e-07, val loss: 1.82504e-06, min loss: 4.56224e-07\n",
      "Epoch: 2107800, elapsed: 1.77e+01, train loss: 9.62382e-07, val loss: 2.21389e-06, min loss: 4.56224e-07\n",
      "Epoch: 2107900, elapsed: 1.25e+01, train loss: 4.67421e-07, val loss: 1.48079e-06, min loss: 4.56224e-07\n",
      "Epoch: 2108000, elapsed: 1.23e+01, train loss: 4.73317e-07, val loss: 1.55054e-06, min loss: 4.56224e-07\n",
      "Epoch: 2108100, elapsed: 1.23e+01, train loss: 4.77854e-07, val loss: 1.54604e-06, min loss: 4.56224e-07\n",
      "Epoch: 2108200, elapsed: 1.24e+01, train loss: 8.49665e-07, val loss: 1.84689e-06, min loss: 4.56224e-07\n",
      "Epoch: 2108300, elapsed: 1.26e+01, train loss: 4.55840e-07, val loss: 1.49847e-06, min loss: 4.55840e-07\n",
      "Epoch: 2108400, elapsed: 1.24e+01, train loss: 4.58840e-07, val loss: 1.49657e-06, min loss: 4.55840e-07\n",
      "Epoch: 2108500, elapsed: 1.25e+01, train loss: 4.68137e-07, val loss: 1.49441e-06, min loss: 4.55840e-07\n",
      "Epoch: 2108600, elapsed: 1.23e+01, train loss: 4.56635e-07, val loss: 1.49400e-06, min loss: 4.55840e-07\n",
      "Epoch: 2108700, elapsed: 1.25e+01, train loss: 7.97188e-07, val loss: 1.73535e-06, min loss: 4.55840e-07\n",
      "Epoch: 2108800, elapsed: 1.23e+01, train loss: 4.57881e-07, val loss: 1.49864e-06, min loss: 4.55840e-07\n",
      "Epoch: 2108900, elapsed: 1.23e+01, train loss: 4.57465e-07, val loss: 1.49286e-06, min loss: 4.55840e-07\n",
      "Epoch: 2109000, elapsed: 1.23e+01, train loss: 5.36683e-07, val loss: 1.56692e-06, min loss: 4.55840e-07\n",
      "Epoch: 2109100, elapsed: 1.25e+01, train loss: 1.24616e-06, val loss: 2.28614e-06, min loss: 4.55840e-07\n",
      "Epoch: 2109200, elapsed: 1.22e+01, train loss: 4.82463e-07, val loss: 1.44202e-06, min loss: 4.55840e-07\n",
      "Epoch: 2109300, elapsed: 1.23e+01, train loss: 4.56497e-07, val loss: 1.50554e-06, min loss: 4.55840e-07\n",
      "Epoch: 2109400, elapsed: 1.21e+01, train loss: 4.74841e-07, val loss: 1.54706e-06, min loss: 4.55840e-07\n",
      "Epoch: 2109500, elapsed: 1.21e+01, train loss: 4.55172e-07, val loss: 1.50413e-06, min loss: 4.55172e-07\n",
      "Epoch: 2109600, elapsed: 1.21e+01, train loss: 4.75072e-07, val loss: 1.54683e-06, min loss: 4.55172e-07\n",
      "Epoch: 2109700, elapsed: 1.23e+01, train loss: 4.45378e-06, val loss: 5.93250e-06, min loss: 4.55172e-07\n",
      "Epoch: 2109800, elapsed: 1.20e+01, train loss: 4.55211e-07, val loss: 1.50555e-06, min loss: 4.55172e-07\n",
      "Epoch: 2109900, elapsed: 1.20e+01, train loss: 7.45980e-07, val loss: 1.58893e-06, min loss: 4.55172e-07\n",
      "Epoch: 2110000, elapsed: 1.24e+01, train loss: 4.55056e-07, val loss: 1.50583e-06, min loss: 4.55056e-07\n",
      "Epoch: 2110100, elapsed: 1.43e+01, train loss: 4.98993e-07, val loss: 1.53771e-06, min loss: 4.55056e-07\n",
      "Epoch: 2110200, elapsed: 1.24e+01, train loss: 4.55190e-07, val loss: 1.50079e-06, min loss: 4.55056e-07\n",
      "Epoch: 2110300, elapsed: 1.23e+01, train loss: 4.55726e-07, val loss: 1.50040e-06, min loss: 4.55056e-07\n",
      "Epoch: 2110400, elapsed: 1.23e+01, train loss: 1.34891e-06, val loss: 2.24503e-06, min loss: 4.55056e-07\n",
      "Epoch: 2110500, elapsed: 1.22e+01, train loss: 5.93975e-07, val loss: 1.62075e-06, min loss: 4.55056e-07\n",
      "Epoch: 2110600, elapsed: 1.21e+01, train loss: 5.07989e-07, val loss: 1.51229e-06, min loss: 4.55056e-07\n",
      "Epoch: 2110700, elapsed: 1.22e+01, train loss: 4.58310e-07, val loss: 1.50499e-06, min loss: 4.55056e-07\n",
      "Epoch: 2110800, elapsed: 1.21e+01, train loss: 4.92842e-07, val loss: 1.49672e-06, min loss: 4.55056e-07\n",
      "Epoch: 2110900, elapsed: 1.23e+01, train loss: 5.11908e-07, val loss: 1.62458e-06, min loss: 4.55056e-07\n",
      "Epoch: 2111000, elapsed: 1.21e+01, train loss: 4.55676e-07, val loss: 1.50905e-06, min loss: 4.55056e-07\n",
      "Epoch: 2111100, elapsed: 1.23e+01, train loss: 4.58267e-07, val loss: 1.51528e-06, min loss: 4.55056e-07\n",
      "Epoch: 2111200, elapsed: 1.23e+01, train loss: 1.24966e-06, val loss: 2.53401e-06, min loss: 4.55056e-07\n",
      "Epoch: 2111300, elapsed: 1.21e+01, train loss: 4.54753e-07, val loss: 1.50437e-06, min loss: 4.54753e-07\n",
      "Epoch: 2111400, elapsed: 1.22e+01, train loss: 7.34015e-07, val loss: 2.00680e-06, min loss: 4.54753e-07\n",
      "Epoch: 2111500, elapsed: 1.79e+01, train loss: 4.54698e-07, val loss: 1.50963e-06, min loss: 4.54698e-07\n",
      "Epoch: 2111600, elapsed: 1.28e+01, train loss: 4.56541e-07, val loss: 1.52327e-06, min loss: 4.54698e-07\n",
      "Epoch: 2111700, elapsed: 1.27e+01, train loss: 4.54945e-07, val loss: 1.51569e-06, min loss: 4.54698e-07\n",
      "Epoch: 2111800, elapsed: 1.25e+01, train loss: 4.56774e-07, val loss: 1.50849e-06, min loss: 4.54698e-07\n",
      "Epoch: 2111900, elapsed: 1.25e+01, train loss: 4.72342e-07, val loss: 1.52222e-06, min loss: 4.54698e-07\n",
      "Epoch: 2112000, elapsed: 1.24e+01, train loss: 4.55441e-07, val loss: 1.49903e-06, min loss: 4.54698e-07\n",
      "Epoch: 2112100, elapsed: 1.24e+01, train loss: 5.02346e-06, val loss: 4.71986e-06, min loss: 4.54698e-07\n",
      "Epoch: 2112200, elapsed: 1.25e+01, train loss: 4.54515e-07, val loss: 1.50800e-06, min loss: 4.54515e-07\n",
      "Epoch: 2112300, elapsed: 1.25e+01, train loss: 6.18141e-07, val loss: 1.70174e-06, min loss: 4.54515e-07\n",
      "Epoch: 2112400, elapsed: 1.25e+01, train loss: 5.36901e-07, val loss: 1.57482e-06, min loss: 4.54515e-07\n",
      "Epoch: 2112500, elapsed: 1.26e+01, train loss: 4.54944e-07, val loss: 1.51361e-06, min loss: 4.54515e-07\n",
      "Epoch: 2112600, elapsed: 1.23e+01, train loss: 9.77941e-07, val loss: 2.69612e-06, min loss: 4.54515e-07\n",
      "Epoch: 2112700, elapsed: 1.29e+01, train loss: 4.54400e-07, val loss: 1.50786e-06, min loss: 4.54400e-07\n",
      "Epoch: 2112800, elapsed: 1.21e+01, train loss: 4.67100e-07, val loss: 1.54678e-06, min loss: 4.54400e-07\n",
      "Epoch: 2112900, elapsed: 1.23e+01, train loss: 1.57799e-06, val loss: 2.35848e-06, min loss: 4.54400e-07\n",
      "Epoch: 2113000, elapsed: 1.24e+01, train loss: 1.02158e-06, val loss: 1.86866e-06, min loss: 4.54400e-07\n",
      "Epoch: 2113100, elapsed: 1.27e+01, train loss: 7.04720e-07, val loss: 1.75710e-06, min loss: 4.54400e-07\n",
      "Epoch: 2113200, elapsed: 1.25e+01, train loss: 4.68954e-07, val loss: 1.54500e-06, min loss: 4.54400e-07\n",
      "Epoch: 2113300, elapsed: 1.40e+01, train loss: 9.51952e-07, val loss: 1.68211e-06, min loss: 4.54400e-07\n",
      "Epoch: 2113400, elapsed: 1.37e+01, train loss: 1.93358e-06, val loss: 3.60688e-06, min loss: 4.54400e-07\n",
      "Epoch: 2113500, elapsed: 1.35e+01, train loss: 4.54235e-07, val loss: 1.50605e-06, min loss: 4.54235e-07\n",
      "Epoch: 2113600, elapsed: 1.34e+01, train loss: 4.79100e-07, val loss: 1.52854e-06, min loss: 4.54235e-07\n",
      "Epoch: 2113700, elapsed: 1.35e+01, train loss: 4.99063e-07, val loss: 1.48597e-06, min loss: 4.54235e-07\n",
      "Epoch: 2113800, elapsed: 1.35e+01, train loss: 4.66105e-07, val loss: 1.53846e-06, min loss: 4.54235e-07\n",
      "Epoch: 2113900, elapsed: 1.38e+01, train loss: 4.58148e-07, val loss: 1.49273e-06, min loss: 4.54235e-07\n",
      "Epoch: 2114000, elapsed: 1.34e+01, train loss: 4.55201e-07, val loss: 1.50668e-06, min loss: 4.54235e-07\n",
      "Epoch: 2114100, elapsed: 1.39e+01, train loss: 4.71490e-07, val loss: 1.51091e-06, min loss: 4.54235e-07\n",
      "Epoch: 2114200, elapsed: 1.32e+01, train loss: 4.74234e-07, val loss: 1.57547e-06, min loss: 4.54235e-07\n",
      "Epoch: 2114300, elapsed: 1.40e+01, train loss: 1.85397e-06, val loss: 3.10188e-06, min loss: 4.54235e-07\n",
      "Epoch: 2114400, elapsed: 1.40e+01, train loss: 8.39795e-07, val loss: 2.11734e-06, min loss: 4.54235e-07\n",
      "Epoch: 2114500, elapsed: 1.36e+01, train loss: 1.67016e-06, val loss: 3.40793e-06, min loss: 4.54235e-07\n",
      "Epoch: 2114600, elapsed: 1.36e+01, train loss: 4.53886e-07, val loss: 1.50819e-06, min loss: 4.53886e-07\n",
      "Epoch: 2114700, elapsed: 1.34e+01, train loss: 5.14506e-07, val loss: 1.53143e-06, min loss: 4.53886e-07\n",
      "Epoch: 2114800, elapsed: 1.33e+01, train loss: 1.47368e-06, val loss: 2.30129e-06, min loss: 4.53886e-07\n",
      "Epoch: 2114900, elapsed: 1.40e+01, train loss: 4.54186e-07, val loss: 1.52101e-06, min loss: 4.53886e-07\n",
      "Epoch: 2115000, elapsed: 1.42e+01, train loss: 4.55371e-07, val loss: 1.50409e-06, min loss: 4.53886e-07\n",
      "Epoch: 2115100, elapsed: 1.51e+01, train loss: 4.59317e-07, val loss: 1.49061e-06, min loss: 4.53886e-07\n",
      "Epoch: 2115200, elapsed: 1.20e+01, train loss: 5.02200e-07, val loss: 1.60041e-06, min loss: 4.53886e-07\n",
      "Epoch: 2115300, elapsed: 1.78e+01, train loss: 4.53668e-07, val loss: 1.51092e-06, min loss: 4.53668e-07\n",
      "Epoch: 2115400, elapsed: 1.22e+01, train loss: 4.59590e-07, val loss: 1.51153e-06, min loss: 4.53668e-07\n",
      "Epoch: 2115500, elapsed: 1.23e+01, train loss: 4.61614e-07, val loss: 1.56791e-06, min loss: 4.53668e-07\n",
      "Epoch: 2115600, elapsed: 1.23e+01, train loss: 4.53623e-07, val loss: 1.50904e-06, min loss: 4.53623e-07\n",
      "Epoch: 2115700, elapsed: 1.23e+01, train loss: 4.55718e-07, val loss: 1.49418e-06, min loss: 4.53623e-07\n",
      "Epoch: 2115800, elapsed: 1.22e+01, train loss: 8.85816e-07, val loss: 1.44739e-06, min loss: 4.53623e-07\n",
      "Epoch: 2115900, elapsed: 1.23e+01, train loss: 4.53522e-07, val loss: 1.50866e-06, min loss: 4.53522e-07\n",
      "Epoch: 2116000, elapsed: 1.22e+01, train loss: 4.73137e-07, val loss: 1.51228e-06, min loss: 4.53522e-07\n",
      "Epoch: 2116100, elapsed: 1.25e+01, train loss: 4.54087e-07, val loss: 1.51104e-06, min loss: 4.53522e-07\n",
      "Epoch: 2116200, elapsed: 1.21e+01, train loss: 4.59699e-07, val loss: 1.51977e-06, min loss: 4.53522e-07\n",
      "Epoch: 2116300, elapsed: 1.20e+01, train loss: 9.52158e-07, val loss: 1.91773e-06, min loss: 4.53522e-07\n",
      "Epoch: 2116400, elapsed: 1.21e+01, train loss: 5.06958e-07, val loss: 1.57518e-06, min loss: 4.53522e-07\n",
      "Epoch: 2116500, elapsed: 1.20e+01, train loss: 4.57534e-07, val loss: 1.50584e-06, min loss: 4.53522e-07\n",
      "Epoch: 2116600, elapsed: 1.19e+01, train loss: 4.54321e-07, val loss: 1.51613e-06, min loss: 4.53522e-07\n",
      "Epoch: 2116700, elapsed: 1.19e+01, train loss: 4.60829e-07, val loss: 1.54393e-06, min loss: 4.53522e-07\n",
      "Epoch: 2116800, elapsed: 1.21e+01, train loss: 3.09562e-06, val loss: 4.35449e-06, min loss: 4.53522e-07\n",
      "Epoch: 2116900, elapsed: 1.24e+01, train loss: 7.32449e-07, val loss: 1.86314e-06, min loss: 4.53522e-07\n",
      "Epoch: 2117000, elapsed: 1.20e+01, train loss: 4.99019e-07, val loss: 1.58210e-06, min loss: 4.53522e-07\n",
      "Epoch: 2117100, elapsed: 1.21e+01, train loss: 4.60865e-07, val loss: 1.55134e-06, min loss: 4.53522e-07\n",
      "Epoch: 2117200, elapsed: 1.23e+01, train loss: 7.65813e-07, val loss: 2.03221e-06, min loss: 4.53522e-07\n",
      "Epoch: 2117300, elapsed: 1.24e+01, train loss: 3.79693e-06, val loss: 4.02101e-06, min loss: 4.53522e-07\n",
      "Epoch: 2117400, elapsed: 1.23e+01, train loss: 5.15406e-07, val loss: 1.56366e-06, min loss: 4.53522e-07\n",
      "Epoch: 2117500, elapsed: 1.20e+01, train loss: 4.71901e-07, val loss: 1.50154e-06, min loss: 4.53522e-07\n",
      "Epoch: 2117600, elapsed: 1.21e+01, train loss: 1.04800e-06, val loss: 2.25862e-06, min loss: 4.53522e-07\n",
      "Epoch: 2117700, elapsed: 1.22e+01, train loss: 4.53170e-07, val loss: 1.51590e-06, min loss: 4.53170e-07\n",
      "Epoch: 2117800, elapsed: 1.24e+01, train loss: 4.59469e-07, val loss: 1.50896e-06, min loss: 4.53170e-07\n",
      "Epoch: 2117900, elapsed: 1.22e+01, train loss: 9.06024e-07, val loss: 2.02880e-06, min loss: 4.53170e-07\n",
      "Epoch: 2118000, elapsed: 1.22e+01, train loss: 4.53132e-07, val loss: 1.50277e-06, min loss: 4.53132e-07\n",
      "Epoch: 2118100, elapsed: 1.22e+01, train loss: 5.44980e-07, val loss: 1.45181e-06, min loss: 4.53132e-07\n",
      "Epoch: 2118200, elapsed: 1.23e+01, train loss: 4.52901e-07, val loss: 1.51124e-06, min loss: 4.52901e-07\n",
      "Epoch: 2118300, elapsed: 1.22e+01, train loss: 4.91079e-07, val loss: 1.49719e-06, min loss: 4.52901e-07\n",
      "Epoch: 2118400, elapsed: 1.21e+01, train loss: 2.19179e-06, val loss: 4.05190e-06, min loss: 4.52901e-07\n",
      "Epoch: 2118500, elapsed: 1.21e+01, train loss: 4.53473e-07, val loss: 1.51133e-06, min loss: 4.52901e-07\n",
      "Epoch: 2118600, elapsed: 1.21e+01, train loss: 4.83324e-07, val loss: 1.46677e-06, min loss: 4.52901e-07\n",
      "Epoch: 2118700, elapsed: 1.19e+01, train loss: 4.59374e-07, val loss: 1.54542e-06, min loss: 4.52901e-07\n",
      "Epoch: 2118800, elapsed: 1.22e+01, train loss: 4.70297e-07, val loss: 1.49425e-06, min loss: 4.52901e-07\n",
      "Epoch: 2118900, elapsed: 1.22e+01, train loss: 4.55685e-07, val loss: 1.50308e-06, min loss: 4.52901e-07\n",
      "Epoch: 2119000, elapsed: 1.20e+01, train loss: 4.55170e-07, val loss: 1.50112e-06, min loss: 4.52901e-07\n",
      "Epoch: 2119100, elapsed: 1.80e+01, train loss: 5.72635e-07, val loss: 1.51658e-06, min loss: 4.52901e-07\n",
      "Epoch: 2119200, elapsed: 1.26e+01, train loss: 2.93158e-06, val loss: 3.10663e-06, min loss: 4.52901e-07\n",
      "Epoch: 2119300, elapsed: 1.26e+01, train loss: 4.52973e-07, val loss: 1.50636e-06, min loss: 4.52901e-07\n",
      "Epoch: 2119400, elapsed: 1.26e+01, train loss: 4.53976e-07, val loss: 1.52264e-06, min loss: 4.52901e-07\n",
      "Epoch: 2119500, elapsed: 1.25e+01, train loss: 8.77570e-07, val loss: 2.24054e-06, min loss: 4.52901e-07\n",
      "Epoch: 2119600, elapsed: 1.23e+01, train loss: 4.53203e-07, val loss: 1.51001e-06, min loss: 4.52901e-07\n",
      "Epoch: 2119700, elapsed: 1.25e+01, train loss: 5.17355e-07, val loss: 1.53426e-06, min loss: 4.52901e-07\n",
      "Epoch: 2119800, elapsed: 1.24e+01, train loss: 7.82319e-07, val loss: 1.54379e-06, min loss: 4.52901e-07\n",
      "Epoch: 2119900, elapsed: 1.27e+01, train loss: 5.83923e-07, val loss: 1.51802e-06, min loss: 4.52901e-07\n",
      "Epoch: 2120000, elapsed: 1.26e+01, train loss: 4.58645e-07, val loss: 1.54164e-06, min loss: 4.52901e-07\n",
      "Epoch: 2120100, elapsed: 1.46e+01, train loss: 4.55331e-07, val loss: 1.49703e-06, min loss: 4.52901e-07\n",
      "Epoch: 2120200, elapsed: 1.24e+01, train loss: 4.54575e-07, val loss: 1.52134e-06, min loss: 4.52901e-07\n",
      "Epoch: 2120300, elapsed: 1.26e+01, train loss: 4.60018e-07, val loss: 1.53397e-06, min loss: 4.52901e-07\n",
      "Epoch: 2120400, elapsed: 1.23e+01, train loss: 4.67752e-07, val loss: 1.50131e-06, min loss: 4.52901e-07\n",
      "Epoch: 2120500, elapsed: 1.25e+01, train loss: 2.61223e-06, val loss: 3.22164e-06, min loss: 4.52901e-07\n",
      "Epoch: 2120600, elapsed: 1.23e+01, train loss: 4.52357e-07, val loss: 1.51169e-06, min loss: 4.52357e-07\n",
      "Epoch: 2120700, elapsed: 1.21e+01, train loss: 1.93888e-06, val loss: 2.73798e-06, min loss: 4.52357e-07\n",
      "Epoch: 2120800, elapsed: 1.24e+01, train loss: 6.07401e-07, val loss: 1.48018e-06, min loss: 4.52357e-07\n",
      "Epoch: 2120900, elapsed: 1.24e+01, train loss: 9.98291e-07, val loss: 1.92906e-06, min loss: 4.52357e-07\n",
      "Epoch: 2121000, elapsed: 1.22e+01, train loss: 4.52912e-07, val loss: 1.51879e-06, min loss: 4.52357e-07\n",
      "Epoch: 2121100, elapsed: 1.21e+01, train loss: 4.53657e-07, val loss: 1.51185e-06, min loss: 4.52357e-07\n",
      "Epoch: 2121200, elapsed: 1.20e+01, train loss: 4.64514e-07, val loss: 1.51823e-06, min loss: 4.52357e-07\n",
      "Epoch: 2121300, elapsed: 1.22e+01, train loss: 1.06847e-06, val loss: 1.48308e-06, min loss: 4.52357e-07\n",
      "Epoch: 2121400, elapsed: 1.23e+01, train loss: 4.52152e-07, val loss: 1.51247e-06, min loss: 4.52152e-07\n",
      "Epoch: 2121500, elapsed: 1.20e+01, train loss: 4.56981e-07, val loss: 1.53533e-06, min loss: 4.52152e-07\n",
      "Epoch: 2121600, elapsed: 1.21e+01, train loss: 5.06596e-07, val loss: 1.55005e-06, min loss: 4.52152e-07\n",
      "Epoch: 2121700, elapsed: 1.21e+01, train loss: 4.64280e-07, val loss: 1.49581e-06, min loss: 4.52152e-07\n",
      "Epoch: 2121800, elapsed: 1.20e+01, train loss: 4.52560e-07, val loss: 1.50202e-06, min loss: 4.52152e-07\n",
      "Epoch: 2121900, elapsed: 1.20e+01, train loss: 4.55263e-07, val loss: 1.52069e-06, min loss: 4.52152e-07\n",
      "Epoch: 2122000, elapsed: 1.22e+01, train loss: 9.48075e-07, val loss: 1.84221e-06, min loss: 4.52152e-07\n",
      "Epoch: 2122100, elapsed: 1.21e+01, train loss: 4.51929e-07, val loss: 1.51046e-06, min loss: 4.51929e-07\n",
      "Epoch: 2122200, elapsed: 1.22e+01, train loss: 4.58255e-07, val loss: 1.51095e-06, min loss: 4.51929e-07\n",
      "Epoch: 2122300, elapsed: 1.23e+01, train loss: 4.52024e-07, val loss: 1.50741e-06, min loss: 4.51929e-07\n",
      "Epoch: 2122400, elapsed: 1.25e+01, train loss: 4.54512e-07, val loss: 1.50558e-06, min loss: 4.51929e-07\n",
      "Epoch: 2122500, elapsed: 1.21e+01, train loss: 4.54166e-07, val loss: 1.53021e-06, min loss: 4.51929e-07\n",
      "Epoch: 2122600, elapsed: 1.26e+01, train loss: 4.52355e-07, val loss: 1.51490e-06, min loss: 4.51929e-07\n",
      "Epoch: 2122700, elapsed: 1.24e+01, train loss: 5.97393e-07, val loss: 1.61290e-06, min loss: 4.51929e-07\n",
      "Epoch: 2122800, elapsed: 1.83e+01, train loss: 1.96177e-06, val loss: 2.92624e-06, min loss: 4.51929e-07\n",
      "Epoch: 2122900, elapsed: 1.28e+01, train loss: 4.60398e-07, val loss: 1.55587e-06, min loss: 4.51929e-07\n",
      "Epoch: 2123000, elapsed: 1.30e+01, train loss: 4.52642e-07, val loss: 1.50182e-06, min loss: 4.51929e-07\n",
      "Epoch: 2123100, elapsed: 1.27e+01, train loss: 4.65108e-07, val loss: 1.57595e-06, min loss: 4.51929e-07\n",
      "Epoch: 2123200, elapsed: 1.24e+01, train loss: 4.71851e-07, val loss: 1.60317e-06, min loss: 4.51929e-07\n",
      "Epoch: 2123300, elapsed: 1.22e+01, train loss: 8.31139e-07, val loss: 1.66993e-06, min loss: 4.51929e-07\n",
      "Epoch: 2123400, elapsed: 1.26e+01, train loss: 1.00436e-06, val loss: 1.92072e-06, min loss: 4.51929e-07\n",
      "Epoch: 2123500, elapsed: 1.26e+01, train loss: 4.72628e-07, val loss: 1.53349e-06, min loss: 4.51929e-07\n",
      "Epoch: 2123600, elapsed: 1.24e+01, train loss: 4.84280e-07, val loss: 1.54084e-06, min loss: 4.51929e-07\n",
      "Epoch: 2123700, elapsed: 1.22e+01, train loss: 1.08356e-06, val loss: 1.95746e-06, min loss: 4.51929e-07\n",
      "Epoch: 2123800, elapsed: 1.23e+01, train loss: 4.81820e-07, val loss: 1.51364e-06, min loss: 4.51929e-07\n",
      "Epoch: 2123900, elapsed: 1.21e+01, train loss: 4.51666e-07, val loss: 1.51114e-06, min loss: 4.51666e-07\n",
      "Epoch: 2124000, elapsed: 1.22e+01, train loss: 6.04154e-07, val loss: 1.62340e-06, min loss: 4.51666e-07\n",
      "Epoch: 2124100, elapsed: 1.21e+01, train loss: 6.45057e-07, val loss: 1.68218e-06, min loss: 4.51666e-07\n",
      "Epoch: 2124200, elapsed: 1.25e+01, train loss: 7.87793e-07, val loss: 1.92244e-06, min loss: 4.51666e-07\n",
      "Epoch: 2124300, elapsed: 1.19e+01, train loss: 2.54397e-06, val loss: 2.72219e-06, min loss: 4.51666e-07\n",
      "Epoch: 2124400, elapsed: 1.23e+01, train loss: 7.31290e-07, val loss: 1.67761e-06, min loss: 4.51666e-07\n",
      "Epoch: 2124500, elapsed: 1.23e+01, train loss: 1.50906e-06, val loss: 2.29057e-06, min loss: 4.51666e-07\n",
      "Epoch: 2124600, elapsed: 1.23e+01, train loss: 4.51473e-07, val loss: 1.50982e-06, min loss: 4.51473e-07\n",
      "Epoch: 2124700, elapsed: 1.20e+01, train loss: 4.52278e-07, val loss: 1.51577e-06, min loss: 4.51473e-07\n",
      "Epoch: 2124800, elapsed: 1.21e+01, train loss: 8.72048e-07, val loss: 1.54759e-06, min loss: 4.51473e-07\n",
      "Epoch: 2124900, elapsed: 1.24e+01, train loss: 5.90810e-07, val loss: 1.57777e-06, min loss: 4.51473e-07\n",
      "Epoch: 2125000, elapsed: 1.24e+01, train loss: 4.73852e-07, val loss: 1.48054e-06, min loss: 4.51473e-07\n",
      "Epoch: 2125100, elapsed: 1.42e+01, train loss: 7.54739e-07, val loss: 1.83983e-06, min loss: 4.51473e-07\n",
      "Epoch: 2125200, elapsed: 1.22e+01, train loss: 4.52471e-07, val loss: 1.51271e-06, min loss: 4.51473e-07\n",
      "Epoch: 2125300, elapsed: 1.25e+01, train loss: 4.51580e-07, val loss: 1.51526e-06, min loss: 4.51473e-07\n",
      "Epoch: 2125400, elapsed: 1.23e+01, train loss: 4.88398e-07, val loss: 1.54372e-06, min loss: 4.51473e-07\n",
      "Epoch: 2125500, elapsed: 1.22e+01, train loss: 1.25475e-06, val loss: 2.38765e-06, min loss: 4.51473e-07\n",
      "Epoch: 2125600, elapsed: 1.23e+01, train loss: 4.51862e-07, val loss: 1.50907e-06, min loss: 4.51473e-07\n",
      "Epoch: 2125700, elapsed: 1.23e+01, train loss: 5.67493e-07, val loss: 1.55908e-06, min loss: 4.51473e-07\n",
      "Epoch: 2125800, elapsed: 1.23e+01, train loss: 4.51741e-07, val loss: 1.53288e-06, min loss: 4.51473e-07\n",
      "Epoch: 2125900, elapsed: 1.22e+01, train loss: 9.94203e-07, val loss: 1.96364e-06, min loss: 4.51473e-07\n",
      "Epoch: 2126000, elapsed: 1.23e+01, train loss: 4.82976e-07, val loss: 1.52439e-06, min loss: 4.51473e-07\n",
      "Epoch: 2126100, elapsed: 1.22e+01, train loss: 7.57552e-07, val loss: 1.55113e-06, min loss: 4.51473e-07\n",
      "Epoch: 2126200, elapsed: 1.24e+01, train loss: 4.50887e-07, val loss: 1.51496e-06, min loss: 4.50887e-07\n",
      "Epoch: 2126300, elapsed: 1.27e+01, train loss: 4.57168e-07, val loss: 1.51416e-06, min loss: 4.50887e-07\n",
      "Epoch: 2126400, elapsed: 1.35e+01, train loss: 4.68815e-07, val loss: 1.56792e-06, min loss: 4.50887e-07\n",
      "Epoch: 2126500, elapsed: 1.23e+01, train loss: 1.18345e-06, val loss: 2.44198e-06, min loss: 4.50887e-07\n",
      "Epoch: 2126600, elapsed: 1.85e+01, train loss: 7.74733e-07, val loss: 1.89912e-06, min loss: 4.50887e-07\n",
      "Epoch: 2126700, elapsed: 1.39e+01, train loss: 4.66592e-07, val loss: 1.56953e-06, min loss: 4.50887e-07\n",
      "Epoch: 2126800, elapsed: 1.40e+01, train loss: 5.21746e-07, val loss: 1.64548e-06, min loss: 4.50887e-07\n",
      "Epoch: 2126900, elapsed: 1.39e+01, train loss: 5.05661e-07, val loss: 1.48181e-06, min loss: 4.50887e-07\n",
      "Epoch: 2127000, elapsed: 1.37e+01, train loss: 7.54221e-07, val loss: 2.04860e-06, min loss: 4.50887e-07\n",
      "Epoch: 2127100, elapsed: 1.36e+01, train loss: 4.51175e-07, val loss: 1.51653e-06, min loss: 4.50887e-07\n",
      "Epoch: 2127200, elapsed: 1.41e+01, train loss: 4.51521e-07, val loss: 1.53623e-06, min loss: 4.50887e-07\n",
      "Epoch: 2127300, elapsed: 1.36e+01, train loss: 4.70263e-07, val loss: 1.56914e-06, min loss: 4.50887e-07\n",
      "Epoch: 2127400, elapsed: 1.41e+01, train loss: 7.76541e-07, val loss: 2.26209e-06, min loss: 4.50887e-07\n",
      "Epoch: 2127500, elapsed: 1.41e+01, train loss: 4.82474e-07, val loss: 1.51652e-06, min loss: 4.50887e-07\n",
      "Epoch: 2127600, elapsed: 1.39e+01, train loss: 8.31274e-07, val loss: 1.54523e-06, min loss: 4.50887e-07\n",
      "Epoch: 2127700, elapsed: 1.38e+01, train loss: 6.16347e-07, val loss: 1.75141e-06, min loss: 4.50887e-07\n",
      "Epoch: 2127800, elapsed: 1.37e+01, train loss: 2.10518e-06, val loss: 1.71375e-06, min loss: 4.50887e-07\n",
      "Epoch: 2127900, elapsed: 1.37e+01, train loss: 4.50427e-07, val loss: 1.51407e-06, min loss: 4.50427e-07\n",
      "Epoch: 2128000, elapsed: 1.36e+01, train loss: 6.39795e-07, val loss: 1.58308e-06, min loss: 4.50427e-07\n",
      "Epoch: 2128100, elapsed: 1.36e+01, train loss: 5.12596e-07, val loss: 1.57256e-06, min loss: 4.50427e-07\n",
      "Epoch: 2128200, elapsed: 1.37e+01, train loss: 1.90360e-06, val loss: 2.45470e-06, min loss: 4.50427e-07\n",
      "Epoch: 2128300, elapsed: 1.30e+01, train loss: 1.81330e-06, val loss: 3.08148e-06, min loss: 4.50427e-07\n",
      "Epoch: 2128400, elapsed: 1.17e+01, train loss: 4.51715e-07, val loss: 1.52872e-06, min loss: 4.50427e-07\n",
      "Epoch: 2128500, elapsed: 1.29e+01, train loss: 4.63942e-07, val loss: 1.54365e-06, min loss: 4.50427e-07\n",
      "Epoch: 2128600, elapsed: 1.25e+01, train loss: 4.90350e-07, val loss: 1.56682e-06, min loss: 4.50427e-07\n",
      "Epoch: 2128700, elapsed: 1.39e+01, train loss: 1.53132e-06, val loss: 2.28502e-06, min loss: 4.50427e-07\n",
      "Epoch: 2128800, elapsed: 1.35e+01, train loss: 2.33370e-06, val loss: 2.80044e-06, min loss: 4.50427e-07\n",
      "Epoch: 2128900, elapsed: 1.39e+01, train loss: 7.30833e-07, val loss: 1.47056e-06, min loss: 4.50427e-07\n",
      "Epoch: 2129000, elapsed: 1.42e+01, train loss: 4.62958e-07, val loss: 1.55097e-06, min loss: 4.50427e-07\n",
      "Epoch: 2129100, elapsed: 1.37e+01, train loss: 5.49828e-07, val loss: 1.58581e-06, min loss: 4.50427e-07\n",
      "Epoch: 2129200, elapsed: 1.40e+01, train loss: 4.51102e-07, val loss: 1.52378e-06, min loss: 4.50427e-07\n",
      "Epoch: 2129300, elapsed: 1.39e+01, train loss: 4.53146e-07, val loss: 1.53457e-06, min loss: 4.50427e-07\n",
      "Epoch: 2129400, elapsed: 1.41e+01, train loss: 4.56265e-07, val loss: 1.56270e-06, min loss: 4.50427e-07\n",
      "Epoch: 2129500, elapsed: 1.36e+01, train loss: 4.55998e-07, val loss: 1.50911e-06, min loss: 4.50427e-07\n",
      "Epoch: 2129600, elapsed: 1.35e+01, train loss: 4.67531e-07, val loss: 1.46449e-06, min loss: 4.50427e-07\n",
      "Epoch: 2129700, elapsed: 1.36e+01, train loss: 4.68286e-07, val loss: 1.50446e-06, min loss: 4.50427e-07\n",
      "Epoch: 2129800, elapsed: 1.36e+01, train loss: 6.02068e-07, val loss: 1.63577e-06, min loss: 4.50427e-07\n",
      "Epoch: 2129900, elapsed: 1.36e+01, train loss: 4.53729e-07, val loss: 1.53089e-06, min loss: 4.50427e-07\n",
      "Epoch: 2130000, elapsed: 1.36e+01, train loss: 2.52800e-06, val loss: 3.60683e-06, min loss: 4.50427e-07\n",
      "Epoch: 2130100, elapsed: 1.66e+01, train loss: 5.16650e-07, val loss: 1.64167e-06, min loss: 4.50427e-07\n",
      "Epoch: 2130200, elapsed: 1.37e+01, train loss: 4.52731e-07, val loss: 1.50061e-06, min loss: 4.50427e-07\n",
      "Epoch: 2130300, elapsed: 1.34e+01, train loss: 4.52576e-07, val loss: 1.50437e-06, min loss: 4.50427e-07\n",
      "Epoch: 2130400, elapsed: 2.02e+01, train loss: 4.77929e-07, val loss: 1.58210e-06, min loss: 4.50427e-07\n",
      "Epoch: 2130500, elapsed: 1.37e+01, train loss: 4.65769e-07, val loss: 1.57073e-06, min loss: 4.50427e-07\n",
      "Epoch: 2130600, elapsed: 1.43e+01, train loss: 4.55395e-07, val loss: 1.51691e-06, min loss: 4.50427e-07\n",
      "Epoch: 2130700, elapsed: 1.30e+01, train loss: 4.70605e-07, val loss: 1.48713e-06, min loss: 4.50427e-07\n",
      "Epoch: 2130800, elapsed: 1.23e+01, train loss: 1.21144e-06, val loss: 2.21276e-06, min loss: 4.50427e-07\n",
      "Epoch: 2130900, elapsed: 1.23e+01, train loss: 4.52976e-07, val loss: 1.50953e-06, min loss: 4.50427e-07\n",
      "Epoch: 2131000, elapsed: 1.24e+01, train loss: 5.04554e-07, val loss: 1.46719e-06, min loss: 4.50427e-07\n",
      "Epoch: 2131100, elapsed: 1.24e+01, train loss: 1.11145e-06, val loss: 1.58546e-06, min loss: 4.50427e-07\n",
      "Epoch: 2131200, elapsed: 1.25e+01, train loss: 4.54554e-07, val loss: 1.53122e-06, min loss: 4.50427e-07\n",
      "Epoch: 2131300, elapsed: 1.26e+01, train loss: 4.92422e-07, val loss: 1.51167e-06, min loss: 4.50427e-07\n",
      "Epoch: 2131400, elapsed: 1.23e+01, train loss: 7.17114e-07, val loss: 1.78167e-06, min loss: 4.50427e-07\n",
      "Epoch: 2131500, elapsed: 1.23e+01, train loss: 4.50937e-07, val loss: 1.50993e-06, min loss: 4.50427e-07\n",
      "Epoch: 2131600, elapsed: 1.25e+01, train loss: 4.49974e-07, val loss: 1.52313e-06, min loss: 4.49974e-07\n",
      "Epoch: 2131700, elapsed: 1.23e+01, train loss: 6.14339e-07, val loss: 1.78219e-06, min loss: 4.49974e-07\n",
      "Epoch: 2131800, elapsed: 1.23e+01, train loss: 4.65804e-07, val loss: 1.52555e-06, min loss: 4.49974e-07\n",
      "Epoch: 2131900, elapsed: 1.23e+01, train loss: 4.50946e-07, val loss: 1.53552e-06, min loss: 4.49974e-07\n",
      "Epoch: 2132000, elapsed: 1.23e+01, train loss: 4.88233e-07, val loss: 1.47963e-06, min loss: 4.49974e-07\n",
      "Epoch: 2132100, elapsed: 1.25e+01, train loss: 4.50531e-07, val loss: 1.52940e-06, min loss: 4.49974e-07\n",
      "Epoch: 2132200, elapsed: 1.24e+01, train loss: 4.50597e-07, val loss: 1.51664e-06, min loss: 4.49974e-07\n",
      "Epoch: 2132300, elapsed: 1.32e+01, train loss: 7.39950e-07, val loss: 1.79834e-06, min loss: 4.49974e-07\n",
      "Epoch: 2132400, elapsed: 1.39e+01, train loss: 4.49640e-07, val loss: 1.52509e-06, min loss: 4.49640e-07\n",
      "Epoch: 2132500, elapsed: 1.38e+01, train loss: 4.63846e-07, val loss: 1.55201e-06, min loss: 4.49640e-07\n",
      "Epoch: 2132600, elapsed: 1.39e+01, train loss: 5.06671e-07, val loss: 1.56978e-06, min loss: 4.49640e-07\n",
      "Epoch: 2132700, elapsed: 1.40e+01, train loss: 4.53634e-07, val loss: 1.51743e-06, min loss: 4.49640e-07\n",
      "Epoch: 2132800, elapsed: 1.44e+01, train loss: 5.34229e-07, val loss: 1.60730e-06, min loss: 4.49640e-07\n",
      "Epoch: 2132900, elapsed: 1.35e+01, train loss: 5.06133e-07, val loss: 1.74062e-06, min loss: 4.49640e-07\n",
      "Epoch: 2133000, elapsed: 1.42e+01, train loss: 4.49110e-07, val loss: 1.51915e-06, min loss: 4.49110e-07\n",
      "Epoch: 2133100, elapsed: 1.41e+01, train loss: 4.54203e-07, val loss: 1.52405e-06, min loss: 4.49110e-07\n",
      "Epoch: 2133200, elapsed: 1.39e+01, train loss: 7.07687e-07, val loss: 1.76737e-06, min loss: 4.49110e-07\n",
      "Epoch: 2133300, elapsed: 1.30e+01, train loss: 4.49115e-07, val loss: 1.52031e-06, min loss: 4.49110e-07\n",
      "Epoch: 2133400, elapsed: 1.40e+01, train loss: 4.87191e-07, val loss: 1.49294e-06, min loss: 4.49110e-07\n",
      "Epoch: 2133500, elapsed: 1.34e+01, train loss: 1.29246e-06, val loss: 2.14184e-06, min loss: 4.49110e-07\n",
      "Epoch: 2133600, elapsed: 1.35e+01, train loss: 8.57806e-07, val loss: 1.65712e-06, min loss: 4.49110e-07\n",
      "Epoch: 2133700, elapsed: 1.22e+01, train loss: 7.60949e-07, val loss: 2.06012e-06, min loss: 4.49110e-07\n",
      "Epoch: 2133800, elapsed: 1.23e+01, train loss: 1.01184e-06, val loss: 1.99345e-06, min loss: 4.49110e-07\n",
      "Epoch: 2133900, elapsed: 1.23e+01, train loss: 4.49012e-07, val loss: 1.52728e-06, min loss: 4.49012e-07\n",
      "Epoch: 2134000, elapsed: 1.21e+01, train loss: 4.53404e-07, val loss: 1.50454e-06, min loss: 4.49012e-07\n",
      "Epoch: 2134100, elapsed: 1.23e+01, train loss: 4.81620e-07, val loss: 1.50209e-06, min loss: 4.49012e-07\n",
      "Epoch: 2134200, elapsed: 1.81e+01, train loss: 1.68915e-06, val loss: 3.17249e-06, min loss: 4.49012e-07\n",
      "Epoch: 2134300, elapsed: 1.22e+01, train loss: 9.21120e-07, val loss: 2.29934e-06, min loss: 4.49012e-07\n",
      "Epoch: 2134400, elapsed: 1.21e+01, train loss: 6.22663e-07, val loss: 1.66930e-06, min loss: 4.49012e-07\n",
      "Epoch: 2134500, elapsed: 1.19e+01, train loss: 4.50012e-07, val loss: 1.51370e-06, min loss: 4.49012e-07\n",
      "Epoch: 2134600, elapsed: 1.22e+01, train loss: 1.07017e-06, val loss: 2.72111e-06, min loss: 4.49012e-07\n",
      "Epoch: 2134700, elapsed: 1.23e+01, train loss: 4.48966e-07, val loss: 1.51903e-06, min loss: 4.48966e-07\n",
      "Epoch: 2134800, elapsed: 1.25e+01, train loss: 4.66889e-07, val loss: 1.54334e-06, min loss: 4.48966e-07\n",
      "Epoch: 2134900, elapsed: 1.20e+01, train loss: 1.20481e-06, val loss: 2.50882e-06, min loss: 4.48966e-07\n",
      "Epoch: 2135000, elapsed: 1.23e+01, train loss: 4.53720e-07, val loss: 1.51080e-06, min loss: 4.48966e-07\n",
      "Epoch: 2135100, elapsed: 1.44e+01, train loss: 5.11865e-07, val loss: 1.51722e-06, min loss: 4.48966e-07\n",
      "Epoch: 2135200, elapsed: 1.22e+01, train loss: 5.32687e-07, val loss: 1.67432e-06, min loss: 4.48966e-07\n",
      "Epoch: 2135300, elapsed: 1.21e+01, train loss: 4.48475e-07, val loss: 1.52059e-06, min loss: 4.48475e-07\n",
      "Epoch: 2135400, elapsed: 1.20e+01, train loss: 4.61051e-07, val loss: 1.50838e-06, min loss: 4.48475e-07\n",
      "Epoch: 2135500, elapsed: 1.20e+01, train loss: 4.48416e-07, val loss: 1.51946e-06, min loss: 4.48416e-07\n",
      "Epoch: 2135600, elapsed: 1.19e+01, train loss: 4.73634e-07, val loss: 1.52759e-06, min loss: 4.48416e-07\n",
      "Epoch: 2135700, elapsed: 1.21e+01, train loss: 4.48410e-07, val loss: 1.52064e-06, min loss: 4.48410e-07\n",
      "Epoch: 2135800, elapsed: 1.21e+01, train loss: 6.03748e-07, val loss: 1.55978e-06, min loss: 4.48410e-07\n",
      "Epoch: 2135900, elapsed: 1.22e+01, train loss: 4.48952e-07, val loss: 1.51508e-06, min loss: 4.48410e-07\n",
      "Epoch: 2136000, elapsed: 1.18e+01, train loss: 4.48258e-07, val loss: 1.52042e-06, min loss: 4.48258e-07\n",
      "Epoch: 2136100, elapsed: 1.20e+01, train loss: 4.58199e-07, val loss: 1.51212e-06, min loss: 4.48258e-07\n",
      "Epoch: 2136200, elapsed: 1.23e+01, train loss: 4.50119e-07, val loss: 1.51898e-06, min loss: 4.48258e-07\n",
      "Epoch: 2136300, elapsed: 1.20e+01, train loss: 8.63130e-07, val loss: 1.98735e-06, min loss: 4.48258e-07\n",
      "Epoch: 2136400, elapsed: 1.22e+01, train loss: 4.60039e-07, val loss: 1.51431e-06, min loss: 4.48258e-07\n",
      "Epoch: 2136500, elapsed: 1.21e+01, train loss: 4.48744e-07, val loss: 1.51152e-06, min loss: 4.48258e-07\n",
      "Epoch: 2136600, elapsed: 1.23e+01, train loss: 4.75055e-07, val loss: 1.50252e-06, min loss: 4.48258e-07\n",
      "Epoch: 2136700, elapsed: 1.21e+01, train loss: 4.54918e-07, val loss: 1.52218e-06, min loss: 4.48258e-07\n",
      "Epoch: 2136800, elapsed: 1.22e+01, train loss: 5.14777e-07, val loss: 1.50151e-06, min loss: 4.48258e-07\n",
      "Epoch: 2136900, elapsed: 1.19e+01, train loss: 5.20290e-07, val loss: 1.57827e-06, min loss: 4.48258e-07\n",
      "Epoch: 2137000, elapsed: 1.21e+01, train loss: 7.26558e-07, val loss: 1.67505e-06, min loss: 4.48258e-07\n",
      "Epoch: 2137100, elapsed: 1.22e+01, train loss: 4.78315e-07, val loss: 1.49041e-06, min loss: 4.48258e-07\n",
      "Epoch: 2137200, elapsed: 1.24e+01, train loss: 5.56579e-07, val loss: 1.73744e-06, min loss: 4.48258e-07\n",
      "Epoch: 2137300, elapsed: 1.19e+01, train loss: 4.58578e-07, val loss: 1.53975e-06, min loss: 4.48258e-07\n",
      "Epoch: 2137400, elapsed: 1.21e+01, train loss: 1.35923e-06, val loss: 2.48949e-06, min loss: 4.48258e-07\n",
      "Epoch: 2137500, elapsed: 1.21e+01, train loss: 5.51884e-07, val loss: 1.71231e-06, min loss: 4.48258e-07\n",
      "Epoch: 2137600, elapsed: 1.23e+01, train loss: 8.77636e-07, val loss: 1.74606e-06, min loss: 4.48258e-07\n",
      "Epoch: 2137700, elapsed: 1.23e+01, train loss: 7.56772e-07, val loss: 2.00371e-06, min loss: 4.48258e-07\n",
      "Epoch: 2137800, elapsed: 1.21e+01, train loss: 5.40974e-07, val loss: 1.82777e-06, min loss: 4.48258e-07\n",
      "Epoch: 2137900, elapsed: 1.77e+01, train loss: 4.50656e-07, val loss: 1.52869e-06, min loss: 4.48258e-07\n",
      "Epoch: 2138000, elapsed: 1.26e+01, train loss: 4.50167e-07, val loss: 1.51722e-06, min loss: 4.48258e-07\n",
      "Epoch: 2138100, elapsed: 1.24e+01, train loss: 4.75832e-07, val loss: 1.60162e-06, min loss: 4.48258e-07\n",
      "Epoch: 2138200, elapsed: 1.26e+01, train loss: 4.58114e-07, val loss: 1.49054e-06, min loss: 4.48258e-07\n",
      "Epoch: 2138300, elapsed: 1.24e+01, train loss: 5.16380e-07, val loss: 1.49219e-06, min loss: 4.48258e-07\n",
      "Epoch: 2138400, elapsed: 1.25e+01, train loss: 4.50091e-07, val loss: 1.52961e-06, min loss: 4.48258e-07\n",
      "Epoch: 2138500, elapsed: 1.28e+01, train loss: 4.47957e-07, val loss: 1.52735e-06, min loss: 4.47957e-07\n",
      "Epoch: 2138600, elapsed: 1.35e+01, train loss: 6.39091e-07, val loss: 1.54291e-06, min loss: 4.47957e-07\n",
      "Epoch: 2138700, elapsed: 1.29e+01, train loss: 4.48090e-07, val loss: 1.52123e-06, min loss: 4.47957e-07\n",
      "Epoch: 2138800, elapsed: 1.26e+01, train loss: 4.51558e-07, val loss: 1.52105e-06, min loss: 4.47957e-07\n",
      "Epoch: 2138900, elapsed: 1.24e+01, train loss: 4.61436e-07, val loss: 1.50735e-06, min loss: 4.47957e-07\n",
      "Epoch: 2139000, elapsed: 1.23e+01, train loss: 5.35721e-07, val loss: 1.63005e-06, min loss: 4.47957e-07\n",
      "Epoch: 2139100, elapsed: 1.25e+01, train loss: 5.04690e-07, val loss: 1.57483e-06, min loss: 4.47957e-07\n",
      "Epoch: 2139200, elapsed: 1.24e+01, train loss: 5.62785e-07, val loss: 1.92656e-06, min loss: 4.47957e-07\n",
      "Epoch: 2139300, elapsed: 1.21e+01, train loss: 9.50247e-07, val loss: 1.91111e-06, min loss: 4.47957e-07\n",
      "Epoch: 2139400, elapsed: 1.24e+01, train loss: 9.90469e-07, val loss: 2.11211e-06, min loss: 4.47957e-07\n",
      "Epoch: 2139500, elapsed: 1.22e+01, train loss: 1.35861e-06, val loss: 2.58363e-06, min loss: 4.47957e-07\n",
      "Epoch: 2139600, elapsed: 1.21e+01, train loss: 5.46441e-07, val loss: 1.74452e-06, min loss: 4.47957e-07\n",
      "Epoch: 2139700, elapsed: 1.22e+01, train loss: 4.58872e-07, val loss: 1.55305e-06, min loss: 4.47957e-07\n",
      "Epoch: 2139800, elapsed: 1.21e+01, train loss: 8.48044e-07, val loss: 2.02985e-06, min loss: 4.47957e-07\n",
      "Epoch: 2139900, elapsed: 1.32e+01, train loss: 4.62773e-07, val loss: 1.54418e-06, min loss: 4.47957e-07\n",
      "Epoch: 2140000, elapsed: 1.35e+01, train loss: 4.48751e-07, val loss: 1.53305e-06, min loss: 4.47957e-07\n",
      "Epoch: 2140100, elapsed: 1.61e+01, train loss: 4.76513e-07, val loss: 1.64631e-06, min loss: 4.47957e-07\n",
      "Epoch: 2140200, elapsed: 1.35e+01, train loss: 5.45676e-07, val loss: 1.59236e-06, min loss: 4.47957e-07\n",
      "Epoch: 2140300, elapsed: 1.34e+01, train loss: 6.26056e-07, val loss: 1.73513e-06, min loss: 4.47957e-07\n",
      "Epoch: 2140400, elapsed: 1.44e+01, train loss: 4.70791e-07, val loss: 1.49942e-06, min loss: 4.47957e-07\n",
      "Epoch: 2140500, elapsed: 1.39e+01, train loss: 4.50577e-07, val loss: 1.50249e-06, min loss: 4.47957e-07\n",
      "Epoch: 2140600, elapsed: 1.25e+01, train loss: 4.50825e-07, val loss: 1.51383e-06, min loss: 4.47957e-07\n",
      "Epoch: 2140700, elapsed: 1.23e+01, train loss: 4.48261e-07, val loss: 1.51305e-06, min loss: 4.47957e-07\n",
      "Epoch: 2140800, elapsed: 1.23e+01, train loss: 4.52241e-07, val loss: 1.54363e-06, min loss: 4.47957e-07\n",
      "Epoch: 2140900, elapsed: 1.23e+01, train loss: 4.87071e-07, val loss: 1.57613e-06, min loss: 4.47957e-07\n",
      "Epoch: 2141000, elapsed: 1.24e+01, train loss: 4.49603e-07, val loss: 1.51944e-06, min loss: 4.47957e-07\n",
      "Epoch: 2141100, elapsed: 1.23e+01, train loss: 4.48530e-07, val loss: 1.52515e-06, min loss: 4.47957e-07\n",
      "Epoch: 2141200, elapsed: 1.20e+01, train loss: 6.06820e-07, val loss: 1.85584e-06, min loss: 4.47957e-07\n",
      "Epoch: 2141300, elapsed: 1.22e+01, train loss: 1.97724e-06, val loss: 3.57694e-06, min loss: 4.47957e-07\n",
      "Epoch: 2141400, elapsed: 1.21e+01, train loss: 4.65605e-07, val loss: 1.53704e-06, min loss: 4.47957e-07\n",
      "Epoch: 2141500, elapsed: 1.22e+01, train loss: 4.47294e-07, val loss: 1.51151e-06, min loss: 4.47294e-07\n",
      "Epoch: 2141600, elapsed: 1.19e+01, train loss: 4.48486e-07, val loss: 1.52572e-06, min loss: 4.47294e-07\n",
      "Epoch: 2141700, elapsed: 1.83e+01, train loss: 5.31415e-07, val loss: 1.62178e-06, min loss: 4.47294e-07\n",
      "Epoch: 2141800, elapsed: 1.24e+01, train loss: 9.53557e-07, val loss: 1.99137e-06, min loss: 4.47294e-07\n",
      "Epoch: 2141900, elapsed: 1.24e+01, train loss: 5.72377e-07, val loss: 1.61534e-06, min loss: 4.47294e-07\n",
      "Epoch: 2142000, elapsed: 1.25e+01, train loss: 4.65818e-07, val loss: 1.47619e-06, min loss: 4.47294e-07\n",
      "Epoch: 2142100, elapsed: 1.22e+01, train loss: 4.58754e-07, val loss: 1.52199e-06, min loss: 4.47294e-07\n",
      "Epoch: 2142200, elapsed: 1.24e+01, train loss: 4.56148e-07, val loss: 1.50049e-06, min loss: 4.47294e-07\n",
      "Epoch: 2142300, elapsed: 1.24e+01, train loss: 1.49605e-06, val loss: 2.26085e-06, min loss: 4.47294e-07\n",
      "Epoch: 2142400, elapsed: 1.24e+01, train loss: 4.46664e-07, val loss: 1.52328e-06, min loss: 4.46664e-07\n",
      "Epoch: 2142500, elapsed: 1.26e+01, train loss: 4.65120e-07, val loss: 1.58201e-06, min loss: 4.46664e-07\n",
      "Epoch: 2142600, elapsed: 1.26e+01, train loss: 4.47567e-07, val loss: 1.53308e-06, min loss: 4.46664e-07\n",
      "Epoch: 2142700, elapsed: 1.24e+01, train loss: 4.86909e-07, val loss: 1.56822e-06, min loss: 4.46664e-07\n",
      "Epoch: 2142800, elapsed: 1.24e+01, train loss: 5.84498e-07, val loss: 1.60257e-06, min loss: 4.46664e-07\n",
      "Epoch: 2142900, elapsed: 1.22e+01, train loss: 4.61743e-07, val loss: 1.52797e-06, min loss: 4.46664e-07\n",
      "Epoch: 2143000, elapsed: 1.25e+01, train loss: 8.85835e-07, val loss: 1.82779e-06, min loss: 4.46664e-07\n",
      "Epoch: 2143100, elapsed: 1.26e+01, train loss: 6.32953e-07, val loss: 1.74943e-06, min loss: 4.46664e-07\n",
      "Epoch: 2143200, elapsed: 1.24e+01, train loss: 4.47054e-07, val loss: 1.52197e-06, min loss: 4.46664e-07\n",
      "Epoch: 2143300, elapsed: 1.22e+01, train loss: 9.16497e-07, val loss: 2.05492e-06, min loss: 4.46664e-07\n",
      "Epoch: 2143400, elapsed: 1.24e+01, train loss: 4.46435e-07, val loss: 1.52397e-06, min loss: 4.46435e-07\n",
      "Epoch: 2143500, elapsed: 1.24e+01, train loss: 4.51246e-07, val loss: 1.51815e-06, min loss: 4.46435e-07\n",
      "Epoch: 2143600, elapsed: 1.22e+01, train loss: 4.93257e-07, val loss: 1.61821e-06, min loss: 4.46435e-07\n",
      "Epoch: 2143700, elapsed: 1.23e+01, train loss: 4.47888e-07, val loss: 1.53881e-06, min loss: 4.46435e-07\n",
      "Epoch: 2143800, elapsed: 1.23e+01, train loss: 3.83835e-06, val loss: 4.64956e-06, min loss: 4.46435e-07\n",
      "Epoch: 2143900, elapsed: 1.18e+01, train loss: 4.47397e-07, val loss: 1.52983e-06, min loss: 4.46435e-07\n",
      "Epoch: 2144000, elapsed: 1.21e+01, train loss: 6.01074e-07, val loss: 1.56130e-06, min loss: 4.46435e-07\n",
      "Epoch: 2144100, elapsed: 1.19e+01, train loss: 7.33278e-07, val loss: 1.97616e-06, min loss: 4.46435e-07\n",
      "Epoch: 2144200, elapsed: 1.22e+01, train loss: 4.50949e-07, val loss: 1.52083e-06, min loss: 4.46435e-07\n",
      "Epoch: 2144300, elapsed: 1.23e+01, train loss: 4.52060e-07, val loss: 1.52350e-06, min loss: 4.46435e-07\n",
      "Epoch: 2144400, elapsed: 1.21e+01, train loss: 6.27901e-07, val loss: 1.72179e-06, min loss: 4.46435e-07\n",
      "Epoch: 2144500, elapsed: 1.20e+01, train loss: 4.48863e-07, val loss: 1.51902e-06, min loss: 4.46435e-07\n",
      "Epoch: 2144600, elapsed: 1.21e+01, train loss: 4.49347e-07, val loss: 1.51667e-06, min loss: 4.46435e-07\n",
      "Epoch: 2144700, elapsed: 1.22e+01, train loss: 8.31538e-07, val loss: 1.82571e-06, min loss: 4.46435e-07\n",
      "Epoch: 2144800, elapsed: 1.20e+01, train loss: 4.46080e-07, val loss: 1.52694e-06, min loss: 4.46080e-07\n",
      "Epoch: 2144900, elapsed: 1.21e+01, train loss: 5.35359e-07, val loss: 1.49554e-06, min loss: 4.46080e-07\n",
      "Epoch: 2145000, elapsed: 1.21e+01, train loss: 4.46032e-07, val loss: 1.52108e-06, min loss: 4.46032e-07\n",
      "Epoch: 2145100, elapsed: 1.45e+01, train loss: 4.47870e-07, val loss: 1.52846e-06, min loss: 4.46032e-07\n",
      "Epoch: 2145200, elapsed: 1.23e+01, train loss: 1.37194e-06, val loss: 2.81931e-06, min loss: 4.46032e-07\n",
      "Epoch: 2145300, elapsed: 1.24e+01, train loss: 2.01543e-06, val loss: 3.63860e-06, min loss: 4.46032e-07\n",
      "Epoch: 2145400, elapsed: 1.22e+01, train loss: 4.69159e-07, val loss: 1.50213e-06, min loss: 4.46032e-07\n",
      "Epoch: 2145500, elapsed: 1.82e+01, train loss: 4.46275e-07, val loss: 1.53384e-06, min loss: 4.46032e-07\n",
      "Epoch: 2145600, elapsed: 1.27e+01, train loss: 4.58318e-07, val loss: 1.51815e-06, min loss: 4.46032e-07\n",
      "Epoch: 2145700, elapsed: 1.25e+01, train loss: 4.88794e-07, val loss: 1.63205e-06, min loss: 4.46032e-07\n",
      "Epoch: 2145800, elapsed: 1.22e+01, train loss: 7.19111e-07, val loss: 1.81461e-06, min loss: 4.46032e-07\n",
      "Epoch: 2145900, elapsed: 1.23e+01, train loss: 4.51132e-07, val loss: 1.52750e-06, min loss: 4.46032e-07\n",
      "Epoch: 2146000, elapsed: 1.24e+01, train loss: 4.63277e-07, val loss: 1.50453e-06, min loss: 4.46032e-07\n",
      "Epoch: 2146100, elapsed: 1.23e+01, train loss: 1.04950e-06, val loss: 2.33183e-06, min loss: 4.46032e-07\n",
      "Epoch: 2146200, elapsed: 1.22e+01, train loss: 4.54653e-07, val loss: 1.50536e-06, min loss: 4.46032e-07\n",
      "Epoch: 2146300, elapsed: 1.22e+01, train loss: 8.61555e-07, val loss: 1.86515e-06, min loss: 4.46032e-07\n",
      "Epoch: 2146400, elapsed: 1.28e+01, train loss: 2.29338e-06, val loss: 3.33495e-06, min loss: 4.46032e-07\n",
      "Epoch: 2146500, elapsed: 1.24e+01, train loss: 8.93495e-07, val loss: 1.99021e-06, min loss: 4.46032e-07\n",
      "Epoch: 2146600, elapsed: 1.24e+01, train loss: 7.82494e-07, val loss: 1.91279e-06, min loss: 4.46032e-07\n",
      "Epoch: 2146700, elapsed: 1.21e+01, train loss: 6.13489e-07, val loss: 1.65224e-06, min loss: 4.46032e-07\n",
      "Epoch: 2146800, elapsed: 1.23e+01, train loss: 9.17093e-07, val loss: 2.03943e-06, min loss: 4.46032e-07\n",
      "Epoch: 2146900, elapsed: 1.22e+01, train loss: 4.45843e-07, val loss: 1.51839e-06, min loss: 4.45843e-07\n",
      "Epoch: 2147000, elapsed: 1.24e+01, train loss: 4.48997e-07, val loss: 1.52548e-06, min loss: 4.45843e-07\n",
      "Epoch: 2147100, elapsed: 1.26e+01, train loss: 5.63967e-07, val loss: 1.74866e-06, min loss: 4.45843e-07\n",
      "Epoch: 2147200, elapsed: 1.21e+01, train loss: 4.53886e-07, val loss: 1.53692e-06, min loss: 4.45843e-07\n",
      "Epoch: 2147300, elapsed: 1.20e+01, train loss: 4.69634e-07, val loss: 1.55619e-06, min loss: 4.45843e-07\n",
      "Epoch: 2147400, elapsed: 1.22e+01, train loss: 4.46437e-07, val loss: 1.52312e-06, min loss: 4.45843e-07\n",
      "Epoch: 2147500, elapsed: 1.22e+01, train loss: 4.51037e-07, val loss: 1.53306e-06, min loss: 4.45843e-07\n",
      "Epoch: 2147600, elapsed: 1.21e+01, train loss: 4.91800e-07, val loss: 1.60518e-06, min loss: 4.45843e-07\n",
      "Epoch: 2147700, elapsed: 1.22e+01, train loss: 4.85776e-07, val loss: 1.46371e-06, min loss: 4.45843e-07\n",
      "Epoch: 2147800, elapsed: 1.24e+01, train loss: 4.54074e-07, val loss: 1.57846e-06, min loss: 4.45843e-07\n",
      "Epoch: 2147900, elapsed: 1.23e+01, train loss: 1.39884e-06, val loss: 1.98772e-06, min loss: 4.45843e-07\n",
      "Epoch: 2148000, elapsed: 1.23e+01, train loss: 4.45827e-07, val loss: 1.52180e-06, min loss: 4.45827e-07\n",
      "Epoch: 2148100, elapsed: 1.24e+01, train loss: 4.48024e-07, val loss: 1.51189e-06, min loss: 4.45827e-07\n",
      "Epoch: 2148200, elapsed: 1.21e+01, train loss: 5.34901e-06, val loss: 5.12950e-06, min loss: 4.45827e-07\n",
      "Epoch: 2148300, elapsed: 1.22e+01, train loss: 4.45305e-07, val loss: 1.52503e-06, min loss: 4.45305e-07\n",
      "Epoch: 2148400, elapsed: 1.22e+01, train loss: 4.53443e-07, val loss: 1.53218e-06, min loss: 4.45305e-07\n",
      "Epoch: 2148500, elapsed: 1.20e+01, train loss: 4.45852e-07, val loss: 1.51640e-06, min loss: 4.45305e-07\n",
      "Epoch: 2148600, elapsed: 1.22e+01, train loss: 4.45124e-07, val loss: 1.52372e-06, min loss: 4.45124e-07\n",
      "Epoch: 2148700, elapsed: 1.19e+01, train loss: 4.48893e-07, val loss: 1.51815e-06, min loss: 4.45124e-07\n",
      "Epoch: 2148800, elapsed: 1.19e+01, train loss: 4.45065e-07, val loss: 1.52630e-06, min loss: 4.45065e-07\n",
      "Epoch: 2148900, elapsed: 1.21e+01, train loss: 5.20505e-07, val loss: 1.58809e-06, min loss: 4.45065e-07\n",
      "Epoch: 2149000, elapsed: 1.22e+01, train loss: 4.67047e-07, val loss: 1.50799e-06, min loss: 4.45065e-07\n",
      "Epoch: 2149100, elapsed: 1.20e+01, train loss: 4.45220e-07, val loss: 1.51771e-06, min loss: 4.45065e-07\n",
      "Epoch: 2149200, elapsed: 1.24e+01, train loss: 4.45648e-07, val loss: 1.52174e-06, min loss: 4.45065e-07\n",
      "Epoch: 2149300, elapsed: 1.79e+01, train loss: 7.67949e-07, val loss: 1.61302e-06, min loss: 4.45065e-07\n",
      "Epoch: 2149400, elapsed: 1.27e+01, train loss: 4.44981e-07, val loss: 1.52506e-06, min loss: 4.44981e-07\n",
      "Epoch: 2149500, elapsed: 1.23e+01, train loss: 4.50626e-07, val loss: 1.51843e-06, min loss: 4.44981e-07\n",
      "Epoch: 2149600, elapsed: 1.26e+01, train loss: 4.75843e-07, val loss: 1.66254e-06, min loss: 4.44981e-07\n",
      "Epoch: 2149700, elapsed: 1.24e+01, train loss: 4.45013e-07, val loss: 1.52596e-06, min loss: 4.44981e-07\n",
      "Epoch: 2149800, elapsed: 1.23e+01, train loss: 7.92509e-07, val loss: 1.91208e-06, min loss: 4.44981e-07\n",
      "Epoch: 2149900, elapsed: 1.22e+01, train loss: 6.76958e-07, val loss: 1.55862e-06, min loss: 4.44981e-07\n",
      "Epoch: 2150000, elapsed: 1.25e+01, train loss: 5.21399e-07, val loss: 1.58230e-06, min loss: 4.44981e-07\n",
      "Epoch: 2150100, elapsed: 1.48e+01, train loss: 4.50401e-07, val loss: 1.49753e-06, min loss: 4.44981e-07\n",
      "Epoch: 2150200, elapsed: 1.28e+01, train loss: 8.69682e-07, val loss: 2.18499e-06, min loss: 4.44981e-07\n",
      "Epoch: 2150300, elapsed: 1.27e+01, train loss: 1.70531e-06, val loss: 2.95000e-06, min loss: 4.44981e-07\n",
      "Epoch: 2150400, elapsed: 1.23e+01, train loss: 4.44765e-07, val loss: 1.53252e-06, min loss: 4.44765e-07\n",
      "Epoch: 2150500, elapsed: 1.24e+01, train loss: 4.46613e-07, val loss: 1.52847e-06, min loss: 4.44765e-07\n",
      "Epoch: 2150600, elapsed: 1.24e+01, train loss: 4.67200e-07, val loss: 1.57902e-06, min loss: 4.44765e-07\n",
      "Epoch: 2150700, elapsed: 1.23e+01, train loss: 3.55967e-06, val loss: 4.23961e-06, min loss: 4.44765e-07\n",
      "Epoch: 2150800, elapsed: 1.25e+01, train loss: 4.45793e-07, val loss: 1.53528e-06, min loss: 4.44765e-07\n",
      "Epoch: 2150900, elapsed: 1.22e+01, train loss: 6.97251e-07, val loss: 1.91155e-06, min loss: 4.44765e-07\n",
      "Epoch: 2151000, elapsed: 1.22e+01, train loss: 7.17701e-07, val loss: 2.03186e-06, min loss: 4.44765e-07\n",
      "Epoch: 2151100, elapsed: 1.22e+01, train loss: 4.44760e-07, val loss: 1.52984e-06, min loss: 4.44760e-07\n",
      "Epoch: 2151200, elapsed: 1.23e+01, train loss: 4.88137e-07, val loss: 1.75671e-06, min loss: 4.44760e-07\n",
      "Epoch: 2151300, elapsed: 1.21e+01, train loss: 8.71169e-07, val loss: 1.72256e-06, min loss: 4.44760e-07\n",
      "Epoch: 2151400, elapsed: 1.22e+01, train loss: 8.97693e-07, val loss: 2.10742e-06, min loss: 4.44760e-07\n",
      "Epoch: 2151500, elapsed: 1.21e+01, train loss: 4.44748e-07, val loss: 1.51687e-06, min loss: 4.44748e-07\n",
      "Epoch: 2151600, elapsed: 1.21e+01, train loss: 4.46051e-07, val loss: 1.53725e-06, min loss: 4.44748e-07\n",
      "Epoch: 2151700, elapsed: 1.24e+01, train loss: 4.45314e-07, val loss: 1.52640e-06, min loss: 4.44748e-07\n",
      "Epoch: 2151800, elapsed: 1.25e+01, train loss: 4.44886e-07, val loss: 1.52278e-06, min loss: 4.44748e-07\n",
      "Epoch: 2151900, elapsed: 1.22e+01, train loss: 1.10729e-06, val loss: 2.75356e-06, min loss: 4.44748e-07\n",
      "Epoch: 2152000, elapsed: 1.23e+01, train loss: 4.44476e-07, val loss: 1.53156e-06, min loss: 4.44476e-07\n",
      "Epoch: 2152100, elapsed: 1.28e+01, train loss: 4.69866e-07, val loss: 1.55457e-06, min loss: 4.44476e-07\n",
      "Epoch: 2152200, elapsed: 1.34e+01, train loss: 4.44853e-07, val loss: 1.51868e-06, min loss: 4.44476e-07\n",
      "Epoch: 2152300, elapsed: 1.33e+01, train loss: 4.51889e-07, val loss: 1.53010e-06, min loss: 4.44476e-07\n",
      "Epoch: 2152400, elapsed: 1.34e+01, train loss: 6.45522e-07, val loss: 1.74917e-06, min loss: 4.44476e-07\n",
      "Epoch: 2152500, elapsed: 1.42e+01, train loss: 4.91960e-07, val loss: 1.69719e-06, min loss: 4.44476e-07\n",
      "Epoch: 2152600, elapsed: 1.38e+01, train loss: 4.44313e-07, val loss: 1.53160e-06, min loss: 4.44313e-07\n",
      "Epoch: 2152700, elapsed: 1.43e+01, train loss: 4.47472e-07, val loss: 1.53619e-06, min loss: 4.44313e-07\n",
      "Epoch: 2152800, elapsed: 1.37e+01, train loss: 5.82413e-07, val loss: 1.56308e-06, min loss: 4.44313e-07\n",
      "Epoch: 2152900, elapsed: 1.38e+01, train loss: 4.50090e-07, val loss: 1.54149e-06, min loss: 4.44313e-07\n",
      "Epoch: 2153000, elapsed: 1.37e+01, train loss: 4.58002e-07, val loss: 1.51567e-06, min loss: 4.44313e-07\n",
      "Epoch: 2153100, elapsed: 2.09e+01, train loss: 5.67309e-07, val loss: 1.53496e-06, min loss: 4.44313e-07\n",
      "Epoch: 2153200, elapsed: 1.35e+01, train loss: 1.08277e-06, val loss: 2.14018e-06, min loss: 4.44313e-07\n",
      "Epoch: 2153300, elapsed: 1.26e+01, train loss: 4.44324e-07, val loss: 1.52364e-06, min loss: 4.44313e-07\n",
      "Epoch: 2153400, elapsed: 1.21e+01, train loss: 4.46863e-07, val loss: 1.53116e-06, min loss: 4.44313e-07\n",
      "Epoch: 2153500, elapsed: 1.26e+01, train loss: 4.44722e-07, val loss: 1.52787e-06, min loss: 4.44313e-07\n",
      "Epoch: 2153600, elapsed: 1.23e+01, train loss: 4.44845e-07, val loss: 1.53849e-06, min loss: 4.44313e-07\n",
      "Epoch: 2153700, elapsed: 1.25e+01, train loss: 9.58982e-07, val loss: 1.90644e-06, min loss: 4.44313e-07\n",
      "Epoch: 2153800, elapsed: 1.26e+01, train loss: 5.11549e-07, val loss: 1.52399e-06, min loss: 4.44313e-07\n",
      "Epoch: 2153900, elapsed: 1.27e+01, train loss: 4.56411e-07, val loss: 1.56828e-06, min loss: 4.44313e-07\n",
      "Epoch: 2154000, elapsed: 1.23e+01, train loss: 4.64892e-07, val loss: 1.60950e-06, min loss: 4.44313e-07\n",
      "Epoch: 2154100, elapsed: 1.23e+01, train loss: 1.96648e-06, val loss: 3.01695e-06, min loss: 4.44313e-07\n",
      "Epoch: 2154200, elapsed: 1.24e+01, train loss: 5.99382e-07, val loss: 1.74754e-06, min loss: 4.44313e-07\n",
      "Epoch: 2154300, elapsed: 1.22e+01, train loss: 4.43933e-07, val loss: 1.53101e-06, min loss: 4.43933e-07\n",
      "Epoch: 2154400, elapsed: 1.24e+01, train loss: 4.47566e-07, val loss: 1.52665e-06, min loss: 4.43933e-07\n",
      "Epoch: 2154500, elapsed: 1.22e+01, train loss: 5.61906e-07, val loss: 1.64687e-06, min loss: 4.43933e-07\n",
      "Epoch: 2154600, elapsed: 1.22e+01, train loss: 4.44055e-07, val loss: 1.55242e-06, min loss: 4.43933e-07\n",
      "Epoch: 2154700, elapsed: 1.22e+01, train loss: 8.01736e-07, val loss: 1.58130e-06, min loss: 4.43933e-07\n",
      "Epoch: 2154800, elapsed: 1.23e+01, train loss: 2.18832e-06, val loss: 2.12885e-06, min loss: 4.43933e-07\n",
      "Epoch: 2154900, elapsed: 1.25e+01, train loss: 4.69306e-07, val loss: 1.48960e-06, min loss: 4.43933e-07\n",
      "Epoch: 2155000, elapsed: 1.23e+01, train loss: 4.52317e-07, val loss: 1.49236e-06, min loss: 4.43933e-07\n",
      "Epoch: 2155100, elapsed: 1.47e+01, train loss: 6.61996e-07, val loss: 2.11952e-06, min loss: 4.43933e-07\n",
      "Epoch: 2155200, elapsed: 1.23e+01, train loss: 4.43645e-07, val loss: 1.52837e-06, min loss: 4.43645e-07\n",
      "Epoch: 2155300, elapsed: 1.25e+01, train loss: 4.63567e-07, val loss: 1.58248e-06, min loss: 4.43645e-07\n",
      "Epoch: 2155400, elapsed: 1.22e+01, train loss: 6.98190e-07, val loss: 1.84967e-06, min loss: 4.43645e-07\n",
      "Epoch: 2155500, elapsed: 1.21e+01, train loss: 5.71452e-07, val loss: 1.65277e-06, min loss: 4.43645e-07\n",
      "Epoch: 2155600, elapsed: 1.23e+01, train loss: 4.47253e-07, val loss: 1.53630e-06, min loss: 4.43645e-07\n",
      "Epoch: 2155700, elapsed: 1.22e+01, train loss: 4.48870e-07, val loss: 1.55973e-06, min loss: 4.43645e-07\n",
      "Epoch: 2155800, elapsed: 1.24e+01, train loss: 4.73505e-07, val loss: 1.79192e-06, min loss: 4.43645e-07\n",
      "Epoch: 2155900, elapsed: 1.23e+01, train loss: 4.45659e-07, val loss: 1.51368e-06, min loss: 4.43645e-07\n",
      "Epoch: 2156000, elapsed: 1.26e+01, train loss: 4.60140e-07, val loss: 1.57511e-06, min loss: 4.43645e-07\n",
      "Epoch: 2156100, elapsed: 1.24e+01, train loss: 4.43677e-07, val loss: 1.54264e-06, min loss: 4.43645e-07\n",
      "Epoch: 2156200, elapsed: 1.21e+01, train loss: 4.43692e-07, val loss: 1.53517e-06, min loss: 4.43645e-07\n",
      "Epoch: 2156300, elapsed: 1.27e+01, train loss: 7.94948e-07, val loss: 1.93607e-06, min loss: 4.43645e-07\n",
      "Epoch: 2156400, elapsed: 1.37e+01, train loss: 4.43143e-07, val loss: 1.52912e-06, min loss: 4.43143e-07\n",
      "Epoch: 2156500, elapsed: 1.33e+01, train loss: 4.57971e-07, val loss: 1.49245e-06, min loss: 4.43143e-07\n",
      "Epoch: 2156600, elapsed: 1.24e+01, train loss: 5.35490e-07, val loss: 1.86706e-06, min loss: 4.43143e-07\n",
      "Epoch: 2156700, elapsed: 1.21e+01, train loss: 4.61136e-07, val loss: 1.55309e-06, min loss: 4.43143e-07\n",
      "Epoch: 2156800, elapsed: 1.23e+01, train loss: 4.44246e-07, val loss: 1.51095e-06, min loss: 4.43143e-07\n",
      "Epoch: 2156900, elapsed: 1.19e+01, train loss: 4.47796e-07, val loss: 1.53554e-06, min loss: 4.43143e-07\n",
      "Epoch: 2157000, elapsed: 1.82e+01, train loss: 5.27105e-07, val loss: 1.73042e-06, min loss: 4.43143e-07\n",
      "Epoch: 2157100, elapsed: 1.25e+01, train loss: 1.10679e-06, val loss: 2.13355e-06, min loss: 4.43143e-07\n",
      "Epoch: 2157200, elapsed: 1.25e+01, train loss: 5.28470e-07, val loss: 1.63917e-06, min loss: 4.43143e-07\n",
      "Epoch: 2157300, elapsed: 1.25e+01, train loss: 4.43500e-07, val loss: 1.52951e-06, min loss: 4.43143e-07\n",
      "Epoch: 2157400, elapsed: 1.25e+01, train loss: 4.51451e-07, val loss: 1.56615e-06, min loss: 4.43143e-07\n",
      "Epoch: 2157500, elapsed: 1.24e+01, train loss: 4.42886e-07, val loss: 1.53097e-06, min loss: 4.42886e-07\n",
      "Epoch: 2157600, elapsed: 1.24e+01, train loss: 5.91557e-07, val loss: 1.61179e-06, min loss: 4.42886e-07\n",
      "Epoch: 2157700, elapsed: 1.25e+01, train loss: 4.42834e-07, val loss: 1.53063e-06, min loss: 4.42834e-07\n",
      "Epoch: 2157800, elapsed: 1.21e+01, train loss: 4.76801e-07, val loss: 1.60869e-06, min loss: 4.42834e-07\n",
      "Epoch: 2157900, elapsed: 1.24e+01, train loss: 4.57626e-07, val loss: 1.50828e-06, min loss: 4.42834e-07\n",
      "Epoch: 2158000, elapsed: 1.21e+01, train loss: 4.51472e-07, val loss: 1.53921e-06, min loss: 4.42834e-07\n",
      "Epoch: 2158100, elapsed: 1.24e+01, train loss: 4.53804e-07, val loss: 1.55107e-06, min loss: 4.42834e-07\n",
      "Epoch: 2158200, elapsed: 1.24e+01, train loss: 4.55872e-07, val loss: 1.52992e-06, min loss: 4.42834e-07\n",
      "Epoch: 2158300, elapsed: 1.21e+01, train loss: 5.73122e-07, val loss: 1.61993e-06, min loss: 4.42834e-07\n",
      "Epoch: 2158400, elapsed: 1.23e+01, train loss: 4.42803e-07, val loss: 1.52998e-06, min loss: 4.42803e-07\n",
      "Epoch: 2158500, elapsed: 1.25e+01, train loss: 4.77428e-07, val loss: 1.51502e-06, min loss: 4.42803e-07\n",
      "Epoch: 2158600, elapsed: 1.20e+01, train loss: 4.47656e-07, val loss: 1.55561e-06, min loss: 4.42803e-07\n",
      "Epoch: 2158700, elapsed: 1.21e+01, train loss: 4.54942e-07, val loss: 1.54705e-06, min loss: 4.42803e-07\n",
      "Epoch: 2158800, elapsed: 1.25e+01, train loss: 6.02097e-07, val loss: 1.86628e-06, min loss: 4.42803e-07\n",
      "Epoch: 2158900, elapsed: 1.23e+01, train loss: 4.42630e-07, val loss: 1.52915e-06, min loss: 4.42630e-07\n",
      "Epoch: 2159000, elapsed: 1.24e+01, train loss: 5.77003e-07, val loss: 1.80931e-06, min loss: 4.42630e-07\n",
      "Epoch: 2159100, elapsed: 1.22e+01, train loss: 4.42522e-07, val loss: 1.53143e-06, min loss: 4.42522e-07\n",
      "Epoch: 2159200, elapsed: 1.24e+01, train loss: 4.84286e-07, val loss: 1.65547e-06, min loss: 4.42522e-07\n",
      "Epoch: 2159300, elapsed: 1.21e+01, train loss: 4.42437e-07, val loss: 1.53179e-06, min loss: 4.42437e-07\n",
      "Epoch: 2159400, elapsed: 1.23e+01, train loss: 4.74002e-07, val loss: 1.52137e-06, min loss: 4.42437e-07\n",
      "Epoch: 2159500, elapsed: 1.22e+01, train loss: 4.48562e-07, val loss: 1.53771e-06, min loss: 4.42437e-07\n",
      "Epoch: 2159600, elapsed: 1.23e+01, train loss: 4.56362e-07, val loss: 1.49772e-06, min loss: 4.42437e-07\n",
      "Epoch: 2159700, elapsed: 1.21e+01, train loss: 5.21205e-07, val loss: 1.49133e-06, min loss: 4.42437e-07\n",
      "Epoch: 2159800, elapsed: 1.23e+01, train loss: 4.45641e-07, val loss: 1.52468e-06, min loss: 4.42437e-07\n",
      "Epoch: 2159900, elapsed: 1.22e+01, train loss: 4.42969e-07, val loss: 1.52352e-06, min loss: 4.42437e-07\n",
      "Epoch: 2160000, elapsed: 1.26e+01, train loss: 5.23102e-07, val loss: 1.57110e-06, min loss: 4.42437e-07\n",
      "Epoch: 2160100, elapsed: 1.45e+01, train loss: 1.12635e-06, val loss: 2.31439e-06, min loss: 4.42437e-07\n",
      "Epoch: 2160200, elapsed: 1.22e+01, train loss: 5.03274e-07, val loss: 1.58731e-06, min loss: 4.42437e-07\n",
      "Epoch: 2160300, elapsed: 1.29e+01, train loss: 2.95735e-06, val loss: 3.50490e-06, min loss: 4.42437e-07\n",
      "Epoch: 2160400, elapsed: 1.37e+01, train loss: 4.89562e-07, val loss: 1.64848e-06, min loss: 4.42437e-07\n",
      "Epoch: 2160500, elapsed: 1.36e+01, train loss: 4.48100e-07, val loss: 1.51700e-06, min loss: 4.42437e-07\n",
      "Epoch: 2160600, elapsed: 1.35e+01, train loss: 6.26292e-07, val loss: 1.76501e-06, min loss: 4.42437e-07\n",
      "Epoch: 2160700, elapsed: 1.34e+01, train loss: 4.46819e-07, val loss: 1.50866e-06, min loss: 4.42437e-07\n",
      "Epoch: 2160800, elapsed: 2.06e+01, train loss: 5.20598e-07, val loss: 1.46290e-06, min loss: 4.42437e-07\n",
      "Epoch: 2160900, elapsed: 1.38e+01, train loss: 4.45160e-07, val loss: 1.52777e-06, min loss: 4.42437e-07\n",
      "Epoch: 2161000, elapsed: 1.38e+01, train loss: 4.43926e-07, val loss: 1.52236e-06, min loss: 4.42437e-07\n",
      "Epoch: 2161100, elapsed: 1.36e+01, train loss: 5.84788e-07, val loss: 1.60094e-06, min loss: 4.42437e-07\n",
      "Epoch: 2161200, elapsed: 1.41e+01, train loss: 5.50061e-07, val loss: 1.58931e-06, min loss: 4.42437e-07\n",
      "Epoch: 2161300, elapsed: 1.38e+01, train loss: 6.76936e-07, val loss: 1.55482e-06, min loss: 4.42437e-07\n",
      "Epoch: 2161400, elapsed: 1.37e+01, train loss: 1.24982e-06, val loss: 2.32647e-06, min loss: 4.42437e-07\n",
      "Epoch: 2161500, elapsed: 1.39e+01, train loss: 4.45641e-07, val loss: 1.55936e-06, min loss: 4.42437e-07\n",
      "Epoch: 2161600, elapsed: 1.30e+01, train loss: 4.49803e-07, val loss: 1.51785e-06, min loss: 4.42437e-07\n",
      "Epoch: 2161700, elapsed: 1.21e+01, train loss: 6.48637e-07, val loss: 1.69379e-06, min loss: 4.42437e-07\n",
      "Epoch: 2161800, elapsed: 1.23e+01, train loss: 6.48552e-07, val loss: 1.55993e-06, min loss: 4.42437e-07\n",
      "Epoch: 2161900, elapsed: 1.23e+01, train loss: 4.62929e-07, val loss: 1.56683e-06, min loss: 4.42437e-07\n",
      "Epoch: 2162000, elapsed: 1.23e+01, train loss: 4.44862e-07, val loss: 1.54797e-06, min loss: 4.42437e-07\n",
      "Epoch: 2162100, elapsed: 1.22e+01, train loss: 4.47963e-07, val loss: 1.51727e-06, min loss: 4.42437e-07\n",
      "Epoch: 2162200, elapsed: 1.23e+01, train loss: 4.71641e-07, val loss: 1.53980e-06, min loss: 4.42437e-07\n",
      "Epoch: 2162300, elapsed: 1.21e+01, train loss: 4.51098e-07, val loss: 1.51656e-06, min loss: 4.42437e-07\n",
      "Epoch: 2162400, elapsed: 1.23e+01, train loss: 4.54777e-07, val loss: 1.60650e-06, min loss: 4.42437e-07\n",
      "Epoch: 2162500, elapsed: 1.22e+01, train loss: 4.49789e-07, val loss: 1.58613e-06, min loss: 4.42437e-07\n",
      "Epoch: 2162600, elapsed: 1.24e+01, train loss: 6.02181e-07, val loss: 1.66724e-06, min loss: 4.42437e-07\n",
      "Epoch: 2162700, elapsed: 1.24e+01, train loss: 6.93583e-07, val loss: 1.97122e-06, min loss: 4.42437e-07\n",
      "Epoch: 2162800, elapsed: 1.22e+01, train loss: 6.43243e-07, val loss: 1.84611e-06, min loss: 4.42437e-07\n",
      "Epoch: 2162900, elapsed: 1.24e+01, train loss: 4.42143e-07, val loss: 1.52337e-06, min loss: 4.42143e-07\n",
      "Epoch: 2163000, elapsed: 1.27e+01, train loss: 4.50640e-07, val loss: 1.56541e-06, min loss: 4.42143e-07\n",
      "Epoch: 2163100, elapsed: 1.22e+01, train loss: 8.61061e-07, val loss: 1.93253e-06, min loss: 4.42143e-07\n",
      "Epoch: 2163200, elapsed: 1.23e+01, train loss: 4.41687e-07, val loss: 1.53876e-06, min loss: 4.41687e-07\n",
      "Epoch: 2163300, elapsed: 1.22e+01, train loss: 4.44747e-07, val loss: 1.51950e-06, min loss: 4.41687e-07\n",
      "Epoch: 2163400, elapsed: 1.21e+01, train loss: 2.88007e-06, val loss: 4.43200e-06, min loss: 4.41687e-07\n",
      "Epoch: 2163500, elapsed: 1.21e+01, train loss: 4.63716e-07, val loss: 1.58407e-06, min loss: 4.41687e-07\n",
      "Epoch: 2163600, elapsed: 1.21e+01, train loss: 6.89258e-07, val loss: 1.78361e-06, min loss: 4.41687e-07\n",
      "Epoch: 2163700, elapsed: 1.20e+01, train loss: 4.41406e-07, val loss: 1.53450e-06, min loss: 4.41406e-07\n",
      "Epoch: 2163800, elapsed: 1.19e+01, train loss: 4.48533e-07, val loss: 1.51858e-06, min loss: 4.41406e-07\n",
      "Epoch: 2163900, elapsed: 1.22e+01, train loss: 5.00757e-07, val loss: 1.61143e-06, min loss: 4.41406e-07\n",
      "Epoch: 2164000, elapsed: 1.21e+01, train loss: 4.41433e-07, val loss: 1.53087e-06, min loss: 4.41406e-07\n",
      "Epoch: 2164100, elapsed: 1.23e+01, train loss: 4.46085e-07, val loss: 1.52945e-06, min loss: 4.41406e-07\n",
      "Epoch: 2164200, elapsed: 1.21e+01, train loss: 1.64103e-06, val loss: 2.80975e-06, min loss: 4.41406e-07\n",
      "Epoch: 2164300, elapsed: 1.22e+01, train loss: 4.41817e-07, val loss: 1.53094e-06, min loss: 4.41406e-07\n",
      "Epoch: 2164400, elapsed: 1.24e+01, train loss: 4.44261e-07, val loss: 1.54104e-06, min loss: 4.41406e-07\n",
      "Epoch: 2164500, elapsed: 1.22e+01, train loss: 5.40705e-07, val loss: 1.70486e-06, min loss: 4.41406e-07\n",
      "Epoch: 2164600, elapsed: 1.80e+01, train loss: 4.41204e-07, val loss: 1.53115e-06, min loss: 4.41204e-07\n",
      "Epoch: 2164700, elapsed: 1.26e+01, train loss: 4.94987e-07, val loss: 1.63676e-06, min loss: 4.41204e-07\n",
      "Epoch: 2164800, elapsed: 1.26e+01, train loss: 4.92390e-07, val loss: 1.65487e-06, min loss: 4.41204e-07\n",
      "Epoch: 2164900, elapsed: 1.24e+01, train loss: 4.41371e-07, val loss: 1.54011e-06, min loss: 4.41204e-07\n",
      "Epoch: 2165000, elapsed: 1.24e+01, train loss: 6.31380e-07, val loss: 1.66204e-06, min loss: 4.41204e-07\n",
      "Epoch: 2165100, elapsed: 1.46e+01, train loss: 4.41819e-07, val loss: 1.53885e-06, min loss: 4.41204e-07\n",
      "Epoch: 2165200, elapsed: 1.24e+01, train loss: 4.41143e-07, val loss: 1.53197e-06, min loss: 4.41143e-07\n",
      "Epoch: 2165300, elapsed: 1.26e+01, train loss: 5.96162e-07, val loss: 1.84933e-06, min loss: 4.41143e-07\n",
      "Epoch: 2165400, elapsed: 1.25e+01, train loss: 1.52651e-06, val loss: 2.65268e-06, min loss: 4.41143e-07\n",
      "Epoch: 2165500, elapsed: 1.23e+01, train loss: 4.68190e-07, val loss: 1.59648e-06, min loss: 4.41143e-07\n",
      "Epoch: 2165600, elapsed: 1.24e+01, train loss: 4.41259e-07, val loss: 1.53357e-06, min loss: 4.41143e-07\n",
      "Epoch: 2165700, elapsed: 1.19e+01, train loss: 5.19294e-07, val loss: 1.53708e-06, min loss: 4.41143e-07\n",
      "Epoch: 2165800, elapsed: 1.24e+01, train loss: 4.40838e-07, val loss: 1.53473e-06, min loss: 4.40838e-07\n",
      "Epoch: 2165900, elapsed: 1.21e+01, train loss: 4.47453e-07, val loss: 1.50808e-06, min loss: 4.40838e-07\n",
      "Epoch: 2166000, elapsed: 1.25e+01, train loss: 4.65523e-07, val loss: 1.61827e-06, min loss: 4.40838e-07\n",
      "Epoch: 2166100, elapsed: 1.24e+01, train loss: 4.42297e-07, val loss: 1.54072e-06, min loss: 4.40838e-07\n",
      "Epoch: 2166200, elapsed: 1.24e+01, train loss: 4.59452e-07, val loss: 1.57748e-06, min loss: 4.40838e-07\n",
      "Epoch: 2166300, elapsed: 1.22e+01, train loss: 5.36185e-07, val loss: 1.78239e-06, min loss: 4.40838e-07\n",
      "Epoch: 2166400, elapsed: 1.25e+01, train loss: 7.24111e-07, val loss: 1.92163e-06, min loss: 4.40838e-07\n",
      "Epoch: 2166500, elapsed: 1.20e+01, train loss: 4.41870e-07, val loss: 1.53235e-06, min loss: 4.40838e-07\n",
      "Epoch: 2166600, elapsed: 1.20e+01, train loss: 4.42658e-07, val loss: 1.53510e-06, min loss: 4.40838e-07\n",
      "Epoch: 2166700, elapsed: 1.21e+01, train loss: 5.28536e-07, val loss: 1.64647e-06, min loss: 4.40838e-07\n",
      "Epoch: 2166800, elapsed: 1.24e+01, train loss: 3.18491e-06, val loss: 4.93795e-06, min loss: 4.40838e-07\n",
      "Epoch: 2166900, elapsed: 1.20e+01, train loss: 4.40748e-07, val loss: 1.53869e-06, min loss: 4.40748e-07\n",
      "Epoch: 2167000, elapsed: 1.21e+01, train loss: 7.73279e-07, val loss: 1.84291e-06, min loss: 4.40748e-07\n",
      "Epoch: 2167100, elapsed: 1.19e+01, train loss: 4.40679e-07, val loss: 1.53287e-06, min loss: 4.40679e-07\n",
      "Epoch: 2167200, elapsed: 1.22e+01, train loss: 4.43076e-07, val loss: 1.51516e-06, min loss: 4.40679e-07\n",
      "Epoch: 2167300, elapsed: 1.21e+01, train loss: 5.66035e-07, val loss: 1.55428e-06, min loss: 4.40679e-07\n",
      "Epoch: 2167400, elapsed: 1.20e+01, train loss: 5.14039e-07, val loss: 1.64109e-06, min loss: 4.40679e-07\n",
      "Epoch: 2167500, elapsed: 1.20e+01, train loss: 4.43197e-07, val loss: 1.55901e-06, min loss: 4.40679e-07\n",
      "Epoch: 2167600, elapsed: 1.24e+01, train loss: 4.68292e-07, val loss: 1.55064e-06, min loss: 4.40679e-07\n",
      "Epoch: 2167700, elapsed: 1.24e+01, train loss: 4.40744e-07, val loss: 1.53120e-06, min loss: 4.40679e-07\n",
      "Epoch: 2167800, elapsed: 1.25e+01, train loss: 4.47166e-07, val loss: 1.57655e-06, min loss: 4.40679e-07\n",
      "Epoch: 2167900, elapsed: 1.22e+01, train loss: 4.96076e-07, val loss: 1.57100e-06, min loss: 4.40679e-07\n",
      "Epoch: 2168000, elapsed: 1.20e+01, train loss: 4.57194e-07, val loss: 1.55725e-06, min loss: 4.40679e-07\n",
      "Epoch: 2168100, elapsed: 1.21e+01, train loss: 5.34336e-07, val loss: 1.58800e-06, min loss: 4.40679e-07\n",
      "Epoch: 2168200, elapsed: 1.20e+01, train loss: 4.40438e-07, val loss: 1.53564e-06, min loss: 4.40438e-07\n",
      "Epoch: 2168300, elapsed: 1.22e+01, train loss: 1.22438e-06, val loss: 2.99228e-06, min loss: 4.40438e-07\n",
      "Epoch: 2168400, elapsed: 1.20e+01, train loss: 4.40408e-07, val loss: 1.53607e-06, min loss: 4.40408e-07\n",
      "Epoch: 2168500, elapsed: 1.86e+01, train loss: 5.27192e-07, val loss: 1.56391e-06, min loss: 4.40408e-07\n",
      "Epoch: 2168600, elapsed: 1.30e+01, train loss: 4.40131e-07, val loss: 1.53527e-06, min loss: 4.40131e-07\n",
      "Epoch: 2168700, elapsed: 1.23e+01, train loss: 4.48120e-07, val loss: 1.56547e-06, min loss: 4.40131e-07\n",
      "Epoch: 2168800, elapsed: 1.24e+01, train loss: 4.40086e-07, val loss: 1.53511e-06, min loss: 4.40086e-07\n",
      "Epoch: 2168900, elapsed: 1.25e+01, train loss: 4.43471e-07, val loss: 1.54602e-06, min loss: 4.40086e-07\n",
      "Epoch: 2169000, elapsed: 1.25e+01, train loss: 4.40200e-07, val loss: 1.53708e-06, min loss: 4.40086e-07\n",
      "Epoch: 2169100, elapsed: 1.27e+01, train loss: 4.49530e-07, val loss: 1.52002e-06, min loss: 4.40086e-07\n",
      "Epoch: 2169200, elapsed: 1.22e+01, train loss: 4.44524e-07, val loss: 1.58092e-06, min loss: 4.40086e-07\n",
      "Epoch: 2169300, elapsed: 1.25e+01, train loss: 4.40814e-07, val loss: 1.52688e-06, min loss: 4.40086e-07\n",
      "Epoch: 2169400, elapsed: 1.24e+01, train loss: 5.20775e-07, val loss: 2.14351e-06, min loss: 4.40086e-07\n",
      "Epoch: 2169500, elapsed: 1.36e+01, train loss: 4.51677e-07, val loss: 1.56230e-06, min loss: 4.40086e-07\n",
      "Epoch: 2169600, elapsed: 1.37e+01, train loss: 4.41951e-07, val loss: 1.52740e-06, min loss: 4.40086e-07\n",
      "Epoch: 2169700, elapsed: 1.37e+01, train loss: 4.89280e-07, val loss: 1.53366e-06, min loss: 4.40086e-07\n",
      "Epoch: 2169800, elapsed: 1.32e+01, train loss: 1.79719e-06, val loss: 2.57058e-06, min loss: 4.40086e-07\n",
      "Epoch: 2169900, elapsed: 1.35e+01, train loss: 4.47004e-07, val loss: 1.51827e-06, min loss: 4.40086e-07\n",
      "Epoch: 2170000, elapsed: 1.37e+01, train loss: 4.40088e-07, val loss: 1.54064e-06, min loss: 4.40086e-07\n",
      "Epoch: 2170100, elapsed: 1.58e+01, train loss: 4.65958e-07, val loss: 1.57522e-06, min loss: 4.40086e-07\n",
      "Epoch: 2170200, elapsed: 1.32e+01, train loss: 2.55075e-06, val loss: 3.84987e-06, min loss: 4.40086e-07\n",
      "Epoch: 2170300, elapsed: 1.37e+01, train loss: 4.42283e-07, val loss: 1.52670e-06, min loss: 4.40086e-07\n",
      "Epoch: 2170400, elapsed: 1.34e+01, train loss: 4.40795e-07, val loss: 1.52384e-06, min loss: 4.40086e-07\n",
      "Epoch: 2170500, elapsed: 1.40e+01, train loss: 4.65429e-07, val loss: 1.61689e-06, min loss: 4.40086e-07\n",
      "Epoch: 2170600, elapsed: 1.34e+01, train loss: 4.51467e-07, val loss: 1.54031e-06, min loss: 4.40086e-07\n",
      "Epoch: 2170700, elapsed: 1.35e+01, train loss: 6.25163e-07, val loss: 1.68153e-06, min loss: 4.40086e-07\n",
      "Epoch: 2170800, elapsed: 1.32e+01, train loss: 4.43928e-07, val loss: 1.53970e-06, min loss: 4.40086e-07\n",
      "Epoch: 2170900, elapsed: 1.21e+01, train loss: 4.40317e-07, val loss: 1.53192e-06, min loss: 4.40086e-07\n",
      "Epoch: 2171000, elapsed: 1.26e+01, train loss: 5.86868e-07, val loss: 1.52783e-06, min loss: 4.40086e-07\n",
      "Epoch: 2171100, elapsed: 1.22e+01, train loss: 6.03503e-07, val loss: 1.76540e-06, min loss: 4.40086e-07\n",
      "Epoch: 2171200, elapsed: 1.25e+01, train loss: 4.67973e-07, val loss: 1.64732e-06, min loss: 4.40086e-07\n",
      "Epoch: 2171300, elapsed: 1.26e+01, train loss: 4.44905e-07, val loss: 1.52344e-06, min loss: 4.40086e-07\n",
      "Epoch: 2171400, elapsed: 1.34e+01, train loss: 4.40771e-07, val loss: 1.54216e-06, min loss: 4.40086e-07\n",
      "Epoch: 2171500, elapsed: 1.33e+01, train loss: 4.42381e-07, val loss: 1.53178e-06, min loss: 4.40086e-07\n",
      "Epoch: 2171600, elapsed: 1.37e+01, train loss: 4.41659e-07, val loss: 1.52370e-06, min loss: 4.40086e-07\n",
      "Epoch: 2171700, elapsed: 1.37e+01, train loss: 5.34240e-07, val loss: 1.70150e-06, min loss: 4.40086e-07\n",
      "Epoch: 2171800, elapsed: 1.35e+01, train loss: 5.05874e-07, val loss: 1.61701e-06, min loss: 4.40086e-07\n",
      "Epoch: 2171900, elapsed: 1.31e+01, train loss: 1.80255e-06, val loss: 2.70853e-06, min loss: 4.40086e-07\n",
      "Epoch: 2172000, elapsed: 1.19e+01, train loss: 1.25799e-06, val loss: 2.58971e-06, min loss: 4.40086e-07\n",
      "Epoch: 2172100, elapsed: 1.21e+01, train loss: 4.39614e-07, val loss: 1.54126e-06, min loss: 4.39614e-07\n",
      "Epoch: 2172200, elapsed: 1.23e+01, train loss: 4.65742e-07, val loss: 1.54393e-06, min loss: 4.39614e-07\n",
      "Epoch: 2172300, elapsed: 1.81e+01, train loss: 4.59333e-07, val loss: 1.61639e-06, min loss: 4.39614e-07\n",
      "Epoch: 2172400, elapsed: 1.26e+01, train loss: 4.40565e-07, val loss: 1.53280e-06, min loss: 4.39614e-07\n",
      "Epoch: 2172500, elapsed: 1.25e+01, train loss: 5.11119e-07, val loss: 1.63009e-06, min loss: 4.39614e-07\n",
      "Epoch: 2172600, elapsed: 1.23e+01, train loss: 4.43677e-07, val loss: 1.54658e-06, min loss: 4.39614e-07\n",
      "Epoch: 2172700, elapsed: 1.22e+01, train loss: 4.40058e-07, val loss: 1.52879e-06, min loss: 4.39614e-07\n",
      "Epoch: 2172800, elapsed: 1.22e+01, train loss: 4.52123e-07, val loss: 1.52891e-06, min loss: 4.39614e-07\n",
      "Epoch: 2172900, elapsed: 1.24e+01, train loss: 4.43451e-07, val loss: 1.52946e-06, min loss: 4.39614e-07\n",
      "Epoch: 2173000, elapsed: 1.25e+01, train loss: 4.56553e-07, val loss: 1.66933e-06, min loss: 4.39614e-07\n",
      "Epoch: 2173100, elapsed: 1.25e+01, train loss: 4.44995e-07, val loss: 1.53034e-06, min loss: 4.39614e-07\n",
      "Epoch: 2173200, elapsed: 1.26e+01, train loss: 4.40036e-07, val loss: 1.55123e-06, min loss: 4.39614e-07\n",
      "Epoch: 2173300, elapsed: 1.23e+01, train loss: 8.27199e-07, val loss: 2.02890e-06, min loss: 4.39614e-07\n",
      "Epoch: 2173400, elapsed: 1.24e+01, train loss: 5.61256e-07, val loss: 1.75856e-06, min loss: 4.39614e-07\n",
      "Epoch: 2173500, elapsed: 1.25e+01, train loss: 4.93142e-07, val loss: 1.55433e-06, min loss: 4.39614e-07\n",
      "Epoch: 2173600, elapsed: 1.23e+01, train loss: 4.43370e-07, val loss: 1.55234e-06, min loss: 4.39614e-07\n",
      "Epoch: 2173700, elapsed: 1.24e+01, train loss: 4.39744e-07, val loss: 1.52535e-06, min loss: 4.39614e-07\n",
      "Epoch: 2173800, elapsed: 1.24e+01, train loss: 7.02577e-07, val loss: 1.86792e-06, min loss: 4.39614e-07\n",
      "Epoch: 2173900, elapsed: 1.22e+01, train loss: 4.84061e-07, val loss: 1.50419e-06, min loss: 4.39614e-07\n",
      "Epoch: 2174000, elapsed: 1.20e+01, train loss: 4.63823e-07, val loss: 1.61120e-06, min loss: 4.39614e-07\n",
      "Epoch: 2174100, elapsed: 1.22e+01, train loss: 4.58567e-07, val loss: 1.52532e-06, min loss: 4.39614e-07\n",
      "Epoch: 2174200, elapsed: 1.23e+01, train loss: 4.49202e-07, val loss: 1.55636e-06, min loss: 4.39614e-07\n",
      "Epoch: 2174300, elapsed: 1.24e+01, train loss: 4.83596e-07, val loss: 1.63464e-06, min loss: 4.39614e-07\n",
      "Epoch: 2174400, elapsed: 1.21e+01, train loss: 8.77239e-07, val loss: 2.01306e-06, min loss: 4.39614e-07\n",
      "Epoch: 2174500, elapsed: 1.23e+01, train loss: 4.41506e-07, val loss: 1.53314e-06, min loss: 4.39614e-07\n",
      "Epoch: 2174600, elapsed: 1.20e+01, train loss: 4.40130e-07, val loss: 1.53123e-06, min loss: 4.39614e-07\n",
      "Epoch: 2174700, elapsed: 1.22e+01, train loss: 4.74216e-07, val loss: 1.52531e-06, min loss: 4.39614e-07\n",
      "Epoch: 2174800, elapsed: 1.20e+01, train loss: 5.71888e-07, val loss: 1.56373e-06, min loss: 4.39614e-07\n",
      "Epoch: 2174900, elapsed: 1.22e+01, train loss: 2.34797e-06, val loss: 3.07592e-06, min loss: 4.39614e-07\n",
      "Epoch: 2175000, elapsed: 1.20e+01, train loss: 4.71072e-07, val loss: 1.46707e-06, min loss: 4.39614e-07\n",
      "Epoch: 2175100, elapsed: 1.45e+01, train loss: 4.38737e-07, val loss: 1.54303e-06, min loss: 4.38737e-07\n",
      "Epoch: 2175200, elapsed: 1.22e+01, train loss: 6.90964e-07, val loss: 1.61089e-06, min loss: 4.38737e-07\n",
      "Epoch: 2175300, elapsed: 1.22e+01, train loss: 4.38642e-07, val loss: 1.53521e-06, min loss: 4.38642e-07\n",
      "Epoch: 2175400, elapsed: 1.21e+01, train loss: 4.39358e-07, val loss: 1.54136e-06, min loss: 4.38642e-07\n",
      "Epoch: 2175500, elapsed: 1.21e+01, train loss: 7.70835e-07, val loss: 1.67667e-06, min loss: 4.38642e-07\n",
      "Epoch: 2175600, elapsed: 1.22e+01, train loss: 4.66686e-07, val loss: 1.62883e-06, min loss: 4.38642e-07\n",
      "Epoch: 2175700, elapsed: 1.21e+01, train loss: 5.30098e-07, val loss: 1.58573e-06, min loss: 4.38642e-07\n",
      "Epoch: 2175800, elapsed: 1.25e+01, train loss: 4.38634e-07, val loss: 1.53344e-06, min loss: 4.38634e-07\n",
      "Epoch: 2175900, elapsed: 1.22e+01, train loss: 4.58411e-07, val loss: 1.60340e-06, min loss: 4.38634e-07\n",
      "Epoch: 2176000, elapsed: 1.21e+01, train loss: 4.49540e-07, val loss: 1.54266e-06, min loss: 4.38634e-07\n",
      "Epoch: 2176100, elapsed: 1.84e+01, train loss: 4.69055e-07, val loss: 1.60792e-06, min loss: 4.38634e-07\n",
      "Epoch: 2176200, elapsed: 1.27e+01, train loss: 6.83729e-07, val loss: 1.67406e-06, min loss: 4.38634e-07\n",
      "Epoch: 2176300, elapsed: 1.22e+01, train loss: 4.39297e-07, val loss: 1.54307e-06, min loss: 4.38634e-07\n",
      "Epoch: 2176400, elapsed: 1.25e+01, train loss: 4.38881e-07, val loss: 1.55045e-06, min loss: 4.38634e-07\n",
      "Epoch: 2176500, elapsed: 1.25e+01, train loss: 4.46453e-07, val loss: 1.55291e-06, min loss: 4.38634e-07\n",
      "Epoch: 2176600, elapsed: 1.28e+01, train loss: 4.46768e-07, val loss: 1.52857e-06, min loss: 4.38634e-07\n",
      "Epoch: 2176700, elapsed: 1.25e+01, train loss: 4.93324e-07, val loss: 1.66901e-06, min loss: 4.38634e-07\n",
      "Epoch: 2176800, elapsed: 1.23e+01, train loss: 4.90466e-07, val loss: 1.50257e-06, min loss: 4.38634e-07\n",
      "Epoch: 2176900, elapsed: 1.24e+01, train loss: 5.10398e-07, val loss: 1.56405e-06, min loss: 4.38634e-07\n",
      "Epoch: 2177000, elapsed: 1.24e+01, train loss: 2.08912e-06, val loss: 3.60000e-06, min loss: 4.38634e-07\n",
      "Epoch: 2177100, elapsed: 1.22e+01, train loss: 4.38142e-07, val loss: 1.54193e-06, min loss: 4.38142e-07\n",
      "Epoch: 2177200, elapsed: 1.25e+01, train loss: 4.52119e-07, val loss: 1.55581e-06, min loss: 4.38142e-07\n",
      "Epoch: 2177300, elapsed: 1.23e+01, train loss: 4.40510e-07, val loss: 1.52884e-06, min loss: 4.38142e-07\n",
      "Epoch: 2177400, elapsed: 1.22e+01, train loss: 4.52543e-07, val loss: 1.58366e-06, min loss: 4.38142e-07\n",
      "Epoch: 2177500, elapsed: 1.23e+01, train loss: 5.28898e-07, val loss: 1.67803e-06, min loss: 4.38142e-07\n",
      "Epoch: 2177600, elapsed: 1.23e+01, train loss: 4.38238e-07, val loss: 1.54504e-06, min loss: 4.38142e-07\n",
      "Epoch: 2177700, elapsed: 1.20e+01, train loss: 4.69617e-07, val loss: 1.51529e-06, min loss: 4.38142e-07\n",
      "Epoch: 2177800, elapsed: 1.23e+01, train loss: 1.53168e-06, val loss: 2.85538e-06, min loss: 4.38142e-07\n",
      "Epoch: 2177900, elapsed: 1.20e+01, train loss: 5.43400e-07, val loss: 1.53194e-06, min loss: 4.38142e-07\n",
      "Epoch: 2178000, elapsed: 1.21e+01, train loss: 4.43554e-07, val loss: 1.52448e-06, min loss: 4.38142e-07\n",
      "Epoch: 2178100, elapsed: 1.24e+01, train loss: 4.51664e-07, val loss: 1.52325e-06, min loss: 4.38142e-07\n",
      "Epoch: 2178200, elapsed: 1.22e+01, train loss: 4.58627e-07, val loss: 1.52524e-06, min loss: 4.38142e-07\n",
      "Epoch: 2178300, elapsed: 1.23e+01, train loss: 4.88348e-07, val loss: 1.54048e-06, min loss: 4.38142e-07\n",
      "Epoch: 2178400, elapsed: 1.23e+01, train loss: 4.42411e-07, val loss: 1.55057e-06, min loss: 4.38142e-07\n",
      "Epoch: 2178500, elapsed: 1.24e+01, train loss: 6.46509e-07, val loss: 1.94069e-06, min loss: 4.38142e-07\n",
      "Epoch: 2178600, elapsed: 1.21e+01, train loss: 2.50391e-06, val loss: 3.44437e-06, min loss: 4.38142e-07\n",
      "Epoch: 2178700, elapsed: 1.20e+01, train loss: 4.49887e-07, val loss: 1.52182e-06, min loss: 4.38142e-07\n",
      "Epoch: 2178800, elapsed: 1.20e+01, train loss: 4.39202e-07, val loss: 1.53516e-06, min loss: 4.38142e-07\n",
      "Epoch: 2178900, elapsed: 1.21e+01, train loss: 4.48690e-07, val loss: 1.53704e-06, min loss: 4.38142e-07\n",
      "Epoch: 2179000, elapsed: 1.21e+01, train loss: 4.48094e-07, val loss: 1.56964e-06, min loss: 4.38142e-07\n",
      "Epoch: 2179100, elapsed: 1.22e+01, train loss: 4.38211e-07, val loss: 1.53494e-06, min loss: 4.38142e-07\n",
      "Epoch: 2179200, elapsed: 1.21e+01, train loss: 5.04176e-07, val loss: 1.63566e-06, min loss: 4.38142e-07\n",
      "Epoch: 2179300, elapsed: 1.21e+01, train loss: 6.52455e-07, val loss: 1.74638e-06, min loss: 4.38142e-07\n",
      "Epoch: 2179400, elapsed: 1.24e+01, train loss: 4.37736e-07, val loss: 1.53907e-06, min loss: 4.37736e-07\n",
      "Epoch: 2179500, elapsed: 1.21e+01, train loss: 5.82656e-07, val loss: 1.57066e-06, min loss: 4.37736e-07\n",
      "Epoch: 2179600, elapsed: 1.21e+01, train loss: 7.19546e-07, val loss: 1.63762e-06, min loss: 4.37736e-07\n",
      "Epoch: 2179700, elapsed: 1.22e+01, train loss: 4.38610e-07, val loss: 1.54208e-06, min loss: 4.37736e-07\n",
      "Epoch: 2179800, elapsed: 1.21e+01, train loss: 4.39539e-07, val loss: 1.51250e-06, min loss: 4.37736e-07\n",
      "Epoch: 2179900, elapsed: 1.22e+01, train loss: 6.51479e-07, val loss: 1.93914e-06, min loss: 4.37736e-07\n",
      "Epoch: 2180000, elapsed: 1.82e+01, train loss: 4.43514e-07, val loss: 1.53009e-06, min loss: 4.37736e-07\n",
      "Epoch: 2180100, elapsed: 1.49e+01, train loss: 4.39013e-07, val loss: 1.53925e-06, min loss: 4.37736e-07\n",
      "Epoch: 2180200, elapsed: 1.24e+01, train loss: 4.37690e-07, val loss: 1.53742e-06, min loss: 4.37690e-07\n",
      "Epoch: 2180300, elapsed: 1.23e+01, train loss: 6.17226e-07, val loss: 1.92648e-06, min loss: 4.37690e-07\n",
      "Epoch: 2180400, elapsed: 1.24e+01, train loss: 4.48565e-07, val loss: 1.52803e-06, min loss: 4.37690e-07\n",
      "Epoch: 2180500, elapsed: 1.23e+01, train loss: 4.45761e-07, val loss: 1.58646e-06, min loss: 4.37690e-07\n",
      "Epoch: 2180600, elapsed: 1.20e+01, train loss: 5.21679e-07, val loss: 1.64754e-06, min loss: 4.37690e-07\n",
      "Epoch: 2180700, elapsed: 1.22e+01, train loss: 5.91692e-07, val loss: 1.79402e-06, min loss: 4.37690e-07\n",
      "Epoch: 2180800, elapsed: 1.26e+01, train loss: 8.22136e-07, val loss: 1.96305e-06, min loss: 4.37690e-07\n",
      "Epoch: 2180900, elapsed: 1.24e+01, train loss: 5.78581e-07, val loss: 1.53715e-06, min loss: 4.37690e-07\n",
      "Epoch: 2181000, elapsed: 1.21e+01, train loss: 5.11928e-07, val loss: 1.66288e-06, min loss: 4.37690e-07\n",
      "Epoch: 2181100, elapsed: 1.20e+01, train loss: 4.43410e-07, val loss: 1.54616e-06, min loss: 4.37690e-07\n",
      "Epoch: 2181200, elapsed: 1.21e+01, train loss: 4.43403e-07, val loss: 1.52825e-06, min loss: 4.37690e-07\n",
      "Epoch: 2181300, elapsed: 1.24e+01, train loss: 4.37297e-07, val loss: 1.54468e-06, min loss: 4.37297e-07\n",
      "Epoch: 2181400, elapsed: 1.23e+01, train loss: 6.11314e-07, val loss: 1.80545e-06, min loss: 4.37297e-07\n",
      "Epoch: 2181500, elapsed: 1.22e+01, train loss: 4.96963e-07, val loss: 1.53877e-06, min loss: 4.37297e-07\n",
      "Epoch: 2181600, elapsed: 1.21e+01, train loss: 4.43701e-07, val loss: 1.55906e-06, min loss: 4.37297e-07\n",
      "Epoch: 2181700, elapsed: 1.22e+01, train loss: 4.40315e-07, val loss: 1.55175e-06, min loss: 4.37297e-07\n",
      "Epoch: 2181800, elapsed: 1.21e+01, train loss: 4.66385e-07, val loss: 1.62200e-06, min loss: 4.37297e-07\n",
      "Epoch: 2181900, elapsed: 1.21e+01, train loss: 5.68909e-07, val loss: 1.80921e-06, min loss: 4.37297e-07\n",
      "Epoch: 2182000, elapsed: 1.23e+01, train loss: 4.37212e-07, val loss: 1.54187e-06, min loss: 4.37212e-07\n",
      "Epoch: 2182100, elapsed: 1.21e+01, train loss: 4.43545e-07, val loss: 1.52770e-06, min loss: 4.37212e-07\n",
      "Epoch: 2182200, elapsed: 1.22e+01, train loss: 4.40817e-07, val loss: 1.56156e-06, min loss: 4.37212e-07\n",
      "Epoch: 2182300, elapsed: 1.22e+01, train loss: 4.40566e-07, val loss: 1.53031e-06, min loss: 4.37212e-07\n",
      "Epoch: 2182400, elapsed: 1.20e+01, train loss: 5.26646e-07, val loss: 1.66625e-06, min loss: 4.37212e-07\n",
      "Epoch: 2182500, elapsed: 1.20e+01, train loss: 6.30001e-07, val loss: 1.73477e-06, min loss: 4.37212e-07\n",
      "Epoch: 2182600, elapsed: 1.20e+01, train loss: 4.66474e-07, val loss: 1.53807e-06, min loss: 4.37212e-07\n",
      "Epoch: 2182700, elapsed: 1.20e+01, train loss: 4.52060e-07, val loss: 1.60511e-06, min loss: 4.37212e-07\n",
      "Epoch: 2182800, elapsed: 1.21e+01, train loss: 4.53980e-07, val loss: 1.54636e-06, min loss: 4.37212e-07\n",
      "Epoch: 2182900, elapsed: 1.20e+01, train loss: 4.58585e-07, val loss: 1.60130e-06, min loss: 4.37212e-07\n",
      "Epoch: 2183000, elapsed: 1.23e+01, train loss: 6.42604e-07, val loss: 1.93516e-06, min loss: 4.37212e-07\n",
      "Epoch: 2183100, elapsed: 1.21e+01, train loss: 6.01162e-07, val loss: 1.78579e-06, min loss: 4.37212e-07\n",
      "Epoch: 2183200, elapsed: 1.20e+01, train loss: 7.38580e-07, val loss: 1.92878e-06, min loss: 4.37212e-07\n",
      "Epoch: 2183300, elapsed: 1.20e+01, train loss: 5.02129e-07, val loss: 1.59534e-06, min loss: 4.37212e-07\n",
      "Epoch: 2183400, elapsed: 1.22e+01, train loss: 4.37738e-07, val loss: 1.55468e-06, min loss: 4.37212e-07\n",
      "Epoch: 2183500, elapsed: 1.21e+01, train loss: 5.27572e-07, val loss: 1.62874e-06, min loss: 4.37212e-07\n",
      "Epoch: 2183600, elapsed: 1.19e+01, train loss: 1.07952e-06, val loss: 1.82916e-06, min loss: 4.37212e-07\n",
      "Epoch: 2183700, elapsed: 1.23e+01, train loss: 4.41705e-07, val loss: 1.55870e-06, min loss: 4.37212e-07\n",
      "Epoch: 2183800, elapsed: 1.19e+01, train loss: 4.39301e-07, val loss: 1.57885e-06, min loss: 4.37212e-07\n",
      "Epoch: 2183900, elapsed: 1.81e+01, train loss: 7.10631e-07, val loss: 1.75562e-06, min loss: 4.37212e-07\n",
      "Epoch: 2184000, elapsed: 1.25e+01, train loss: 4.37817e-07, val loss: 1.53882e-06, min loss: 4.37212e-07\n",
      "Epoch: 2184100, elapsed: 1.21e+01, train loss: 4.46112e-07, val loss: 1.56162e-06, min loss: 4.37212e-07\n",
      "Epoch: 2184200, elapsed: 1.23e+01, train loss: 7.30838e-07, val loss: 2.64783e-06, min loss: 4.37212e-07\n",
      "Epoch: 2184300, elapsed: 1.23e+01, train loss: 4.36411e-07, val loss: 1.54197e-06, min loss: 4.36411e-07\n",
      "Epoch: 2184400, elapsed: 1.22e+01, train loss: 1.18823e-06, val loss: 1.61603e-06, min loss: 4.36411e-07\n",
      "Epoch: 2184500, elapsed: 1.22e+01, train loss: 4.52884e-07, val loss: 1.58883e-06, min loss: 4.36411e-07\n",
      "Epoch: 2184600, elapsed: 1.23e+01, train loss: 4.36488e-07, val loss: 1.55120e-06, min loss: 4.36411e-07\n",
      "Epoch: 2184700, elapsed: 1.22e+01, train loss: 4.39414e-07, val loss: 1.53227e-06, min loss: 4.36411e-07\n",
      "Epoch: 2184800, elapsed: 1.21e+01, train loss: 8.90651e-07, val loss: 2.05472e-06, min loss: 4.36411e-07\n",
      "Epoch: 2184900, elapsed: 1.23e+01, train loss: 6.52303e-07, val loss: 1.55144e-06, min loss: 4.36411e-07\n",
      "Epoch: 2185000, elapsed: 1.24e+01, train loss: 7.17251e-07, val loss: 1.54722e-06, min loss: 4.36411e-07\n",
      "Epoch: 2185100, elapsed: 1.43e+01, train loss: 1.09229e-06, val loss: 2.00635e-06, min loss: 4.36411e-07\n",
      "Epoch: 2185200, elapsed: 1.22e+01, train loss: 1.14666e-06, val loss: 1.91530e-06, min loss: 4.36411e-07\n",
      "Epoch: 2185300, elapsed: 1.23e+01, train loss: 1.07371e-06, val loss: 2.28154e-06, min loss: 4.36411e-07\n",
      "Epoch: 2185400, elapsed: 1.21e+01, train loss: 1.00692e-06, val loss: 1.92050e-06, min loss: 4.36411e-07\n",
      "Epoch: 2185500, elapsed: 1.22e+01, train loss: 5.17597e-07, val loss: 1.66753e-06, min loss: 4.36411e-07\n",
      "Epoch: 2185600, elapsed: 1.23e+01, train loss: 4.58589e-07, val loss: 1.53961e-06, min loss: 4.36411e-07\n",
      "Epoch: 2185700, elapsed: 1.20e+01, train loss: 5.27042e-07, val loss: 1.70445e-06, min loss: 4.36411e-07\n",
      "Epoch: 2185800, elapsed: 1.24e+01, train loss: 4.45906e-07, val loss: 1.53458e-06, min loss: 4.36411e-07\n",
      "Epoch: 2185900, elapsed: 1.23e+01, train loss: 4.39251e-07, val loss: 1.56437e-06, min loss: 4.36411e-07\n",
      "Epoch: 2186000, elapsed: 1.23e+01, train loss: 4.37095e-07, val loss: 1.53655e-06, min loss: 4.36411e-07\n",
      "Epoch: 2186100, elapsed: 1.19e+01, train loss: 4.40675e-07, val loss: 1.56024e-06, min loss: 4.36411e-07\n",
      "Epoch: 2186200, elapsed: 1.18e+01, train loss: 4.64553e-07, val loss: 1.53154e-06, min loss: 4.36411e-07\n",
      "Epoch: 2186300, elapsed: 1.22e+01, train loss: 4.54083e-07, val loss: 1.54504e-06, min loss: 4.36411e-07\n",
      "Epoch: 2186400, elapsed: 1.21e+01, train loss: 4.40977e-07, val loss: 1.55969e-06, min loss: 4.36411e-07\n",
      "Epoch: 2186500, elapsed: 1.21e+01, train loss: 5.16097e-07, val loss: 1.73638e-06, min loss: 4.36411e-07\n",
      "Epoch: 2186600, elapsed: 1.23e+01, train loss: 4.50156e-07, val loss: 1.55199e-06, min loss: 4.36411e-07\n",
      "Epoch: 2186700, elapsed: 1.21e+01, train loss: 6.18723e-07, val loss: 1.59449e-06, min loss: 4.36411e-07\n",
      "Epoch: 2186800, elapsed: 1.20e+01, train loss: 4.75906e-07, val loss: 1.53625e-06, min loss: 4.36411e-07\n",
      "Epoch: 2186900, elapsed: 1.21e+01, train loss: 4.39787e-07, val loss: 1.53064e-06, min loss: 4.36411e-07\n",
      "Epoch: 2187000, elapsed: 1.22e+01, train loss: 4.36830e-07, val loss: 1.53583e-06, min loss: 4.36411e-07\n",
      "Epoch: 2187100, elapsed: 1.21e+01, train loss: 1.10057e-06, val loss: 1.61384e-06, min loss: 4.36411e-07\n",
      "Epoch: 2187200, elapsed: 1.21e+01, train loss: 4.35641e-07, val loss: 1.54285e-06, min loss: 4.35641e-07\n",
      "Epoch: 2187300, elapsed: 1.20e+01, train loss: 7.08846e-07, val loss: 1.58285e-06, min loss: 4.35641e-07\n",
      "Epoch: 2187400, elapsed: 1.23e+01, train loss: 4.35554e-07, val loss: 1.54099e-06, min loss: 4.35554e-07\n",
      "Epoch: 2187500, elapsed: 1.20e+01, train loss: 4.49452e-07, val loss: 1.56101e-06, min loss: 4.35554e-07\n",
      "Epoch: 2187600, elapsed: 1.21e+01, train loss: 4.42030e-07, val loss: 1.56493e-06, min loss: 4.35554e-07\n",
      "Epoch: 2187700, elapsed: 1.76e+01, train loss: 4.40423e-07, val loss: 1.51903e-06, min loss: 4.35554e-07\n",
      "Epoch: 2187800, elapsed: 1.24e+01, train loss: 1.37380e-06, val loss: 2.49070e-06, min loss: 4.35554e-07\n",
      "Epoch: 2187900, elapsed: 1.21e+01, train loss: 4.35517e-07, val loss: 1.54116e-06, min loss: 4.35517e-07\n",
      "Epoch: 2188000, elapsed: 1.19e+01, train loss: 6.02059e-07, val loss: 1.67643e-06, min loss: 4.35517e-07\n",
      "Epoch: 2188100, elapsed: 1.23e+01, train loss: 6.10816e-07, val loss: 1.80472e-06, min loss: 4.35517e-07\n",
      "Epoch: 2188200, elapsed: 1.26e+01, train loss: 6.05816e-07, val loss: 1.53098e-06, min loss: 4.35517e-07\n",
      "Epoch: 2188300, elapsed: 1.22e+01, train loss: 8.26812e-07, val loss: 1.85815e-06, min loss: 4.35517e-07\n",
      "Epoch: 2188400, elapsed: 1.23e+01, train loss: 1.93980e-06, val loss: 2.86406e-06, min loss: 4.35517e-07\n",
      "Epoch: 2188500, elapsed: 1.24e+01, train loss: 1.43295e-06, val loss: 2.65136e-06, min loss: 4.35517e-07\n",
      "Epoch: 2188600, elapsed: 1.22e+01, train loss: 7.52035e-07, val loss: 1.93935e-06, min loss: 4.35517e-07\n",
      "Epoch: 2188700, elapsed: 1.21e+01, train loss: 5.30784e-07, val loss: 1.64900e-06, min loss: 4.35517e-07\n",
      "Epoch: 2188800, elapsed: 1.22e+01, train loss: 4.35297e-07, val loss: 1.54356e-06, min loss: 4.35297e-07\n",
      "Epoch: 2188900, elapsed: 1.21e+01, train loss: 4.37787e-07, val loss: 1.51818e-06, min loss: 4.35297e-07\n",
      "Epoch: 2189000, elapsed: 1.23e+01, train loss: 4.38907e-07, val loss: 1.51610e-06, min loss: 4.35297e-07\n",
      "Epoch: 2189100, elapsed: 1.29e+01, train loss: 1.42801e-06, val loss: 2.05406e-06, min loss: 4.35297e-07\n",
      "Epoch: 2189200, elapsed: 1.24e+01, train loss: 4.41173e-07, val loss: 1.50160e-06, min loss: 4.35297e-07\n",
      "Epoch: 2189300, elapsed: 1.24e+01, train loss: 4.39869e-07, val loss: 1.56200e-06, min loss: 4.35297e-07\n",
      "Epoch: 2189400, elapsed: 1.23e+01, train loss: 4.82506e-07, val loss: 1.58863e-06, min loss: 4.35297e-07\n",
      "Epoch: 2189500, elapsed: 1.19e+01, train loss: 4.35381e-07, val loss: 1.54905e-06, min loss: 4.35297e-07\n",
      "Epoch: 2189600, elapsed: 1.23e+01, train loss: 4.68867e-07, val loss: 1.63758e-06, min loss: 4.35297e-07\n",
      "Epoch: 2189700, elapsed: 1.19e+01, train loss: 4.59858e-07, val loss: 1.61407e-06, min loss: 4.35297e-07\n",
      "Epoch: 2189800, elapsed: 1.22e+01, train loss: 4.47457e-07, val loss: 1.52074e-06, min loss: 4.35297e-07\n",
      "Epoch: 2189900, elapsed: 1.21e+01, train loss: 4.85030e-07, val loss: 1.54549e-06, min loss: 4.35297e-07\n",
      "Epoch: 2190000, elapsed: 1.21e+01, train loss: 4.38177e-07, val loss: 1.56139e-06, min loss: 4.35297e-07\n",
      "Epoch: 2190100, elapsed: 1.57e+01, train loss: 1.37866e-06, val loss: 2.72016e-06, min loss: 4.35297e-07\n",
      "Epoch: 2190200, elapsed: 1.21e+01, train loss: 1.21673e-06, val loss: 2.71780e-06, min loss: 4.35297e-07\n",
      "Epoch: 2190300, elapsed: 1.23e+01, train loss: 4.46790e-07, val loss: 1.49167e-06, min loss: 4.35297e-07\n",
      "Epoch: 2190400, elapsed: 1.30e+01, train loss: 4.42989e-07, val loss: 1.51742e-06, min loss: 4.35297e-07\n",
      "Epoch: 2190500, elapsed: 1.34e+01, train loss: 1.03034e-06, val loss: 1.64403e-06, min loss: 4.35297e-07\n",
      "Epoch: 2190600, elapsed: 1.35e+01, train loss: 4.50447e-07, val loss: 1.63590e-06, min loss: 4.35297e-07\n",
      "Epoch: 2190700, elapsed: 1.31e+01, train loss: 4.40861e-07, val loss: 1.51197e-06, min loss: 4.35297e-07\n",
      "Epoch: 2190800, elapsed: 1.19e+01, train loss: 4.84926e-07, val loss: 1.67714e-06, min loss: 4.35297e-07\n",
      "Epoch: 2190900, elapsed: 1.23e+01, train loss: 6.10380e-07, val loss: 1.83397e-06, min loss: 4.35297e-07\n",
      "Epoch: 2191000, elapsed: 1.20e+01, train loss: 6.40766e-07, val loss: 1.77886e-06, min loss: 4.35297e-07\n",
      "Epoch: 2191100, elapsed: 1.21e+01, train loss: 4.51989e-07, val loss: 1.53249e-06, min loss: 4.35297e-07\n",
      "Epoch: 2191200, elapsed: 1.20e+01, train loss: 4.39248e-07, val loss: 1.51651e-06, min loss: 4.35297e-07\n",
      "Epoch: 2191300, elapsed: 1.22e+01, train loss: 4.41516e-07, val loss: 1.56592e-06, min loss: 4.35297e-07\n",
      "Epoch: 2191400, elapsed: 1.37e+01, train loss: 4.38407e-07, val loss: 1.54489e-06, min loss: 4.35297e-07\n",
      "Epoch: 2191500, elapsed: 1.34e+01, train loss: 4.89277e-07, val loss: 1.56413e-06, min loss: 4.35297e-07\n",
      "Epoch: 2191600, elapsed: 2.03e+01, train loss: 5.00465e-07, val loss: 1.66677e-06, min loss: 4.35297e-07\n",
      "Epoch: 2191700, elapsed: 1.42e+01, train loss: 4.34552e-07, val loss: 1.54210e-06, min loss: 4.34552e-07\n",
      "Epoch: 2191800, elapsed: 1.45e+01, train loss: 4.37036e-07, val loss: 1.53281e-06, min loss: 4.34552e-07\n",
      "Epoch: 2191900, elapsed: 1.39e+01, train loss: 1.05787e-06, val loss: 2.08710e-06, min loss: 4.34552e-07\n",
      "Epoch: 2192000, elapsed: 1.37e+01, train loss: 5.15223e-07, val loss: 1.56264e-06, min loss: 4.34552e-07\n",
      "Epoch: 2192100, elapsed: 1.44e+01, train loss: 4.39165e-07, val loss: 1.53542e-06, min loss: 4.34552e-07\n",
      "Epoch: 2192200, elapsed: 1.29e+01, train loss: 6.54269e-07, val loss: 1.88275e-06, min loss: 4.34552e-07\n",
      "Epoch: 2192300, elapsed: 1.21e+01, train loss: 1.57468e-06, val loss: 2.29668e-06, min loss: 4.34552e-07\n",
      "Epoch: 2192400, elapsed: 1.25e+01, train loss: 4.95637e-07, val loss: 1.59953e-06, min loss: 4.34552e-07\n",
      "Epoch: 2192500, elapsed: 1.25e+01, train loss: 4.38703e-07, val loss: 1.56546e-06, min loss: 4.34552e-07\n",
      "Epoch: 2192600, elapsed: 1.23e+01, train loss: 4.67248e-07, val loss: 1.53739e-06, min loss: 4.34552e-07\n",
      "Epoch: 2192700, elapsed: 1.23e+01, train loss: 4.34268e-07, val loss: 1.54233e-06, min loss: 4.34268e-07\n",
      "Epoch: 2192800, elapsed: 1.23e+01, train loss: 4.41616e-07, val loss: 1.55129e-06, min loss: 4.34268e-07\n",
      "Epoch: 2192900, elapsed: 1.26e+01, train loss: 4.34564e-07, val loss: 1.54793e-06, min loss: 4.34268e-07\n",
      "Epoch: 2193000, elapsed: 1.24e+01, train loss: 4.64422e-07, val loss: 1.54356e-06, min loss: 4.34268e-07\n",
      "Epoch: 2193100, elapsed: 1.23e+01, train loss: 6.88278e-07, val loss: 1.80804e-06, min loss: 4.34268e-07\n",
      "Epoch: 2193200, elapsed: 1.23e+01, train loss: 1.05457e-06, val loss: 2.08528e-06, min loss: 4.34268e-07\n",
      "Epoch: 2193300, elapsed: 1.24e+01, train loss: 4.34383e-07, val loss: 1.54805e-06, min loss: 4.34268e-07\n",
      "Epoch: 2193400, elapsed: 1.22e+01, train loss: 4.56243e-07, val loss: 1.56115e-06, min loss: 4.34268e-07\n",
      "Epoch: 2193500, elapsed: 1.24e+01, train loss: 4.35030e-07, val loss: 1.53967e-06, min loss: 4.34268e-07\n",
      "Epoch: 2193600, elapsed: 1.24e+01, train loss: 4.34730e-07, val loss: 1.54599e-06, min loss: 4.34268e-07\n",
      "Epoch: 2193700, elapsed: 1.24e+01, train loss: 1.10461e-06, val loss: 2.25429e-06, min loss: 4.34268e-07\n",
      "Epoch: 2193800, elapsed: 1.20e+01, train loss: 4.72453e-07, val loss: 1.61081e-06, min loss: 4.34268e-07\n",
      "Epoch: 2193900, elapsed: 1.22e+01, train loss: 4.85838e-07, val loss: 1.62082e-06, min loss: 4.34268e-07\n",
      "Epoch: 2194000, elapsed: 1.20e+01, train loss: 4.45150e-07, val loss: 1.54907e-06, min loss: 4.34268e-07\n",
      "Epoch: 2194100, elapsed: 1.23e+01, train loss: 4.71453e-07, val loss: 1.51986e-06, min loss: 4.34268e-07\n",
      "Epoch: 2194200, elapsed: 1.24e+01, train loss: 4.84990e-07, val loss: 1.58985e-06, min loss: 4.34268e-07\n",
      "Epoch: 2194300, elapsed: 1.24e+01, train loss: 4.56012e-07, val loss: 1.55601e-06, min loss: 4.34268e-07\n",
      "Epoch: 2194400, elapsed: 1.24e+01, train loss: 4.34750e-07, val loss: 1.54774e-06, min loss: 4.34268e-07\n",
      "Epoch: 2194500, elapsed: 1.22e+01, train loss: 4.38293e-07, val loss: 1.58544e-06, min loss: 4.34268e-07\n",
      "Epoch: 2194600, elapsed: 1.23e+01, train loss: 4.84592e-07, val loss: 1.69046e-06, min loss: 4.34268e-07\n",
      "Epoch: 2194700, elapsed: 1.20e+01, train loss: 4.35725e-07, val loss: 1.54119e-06, min loss: 4.34268e-07\n",
      "Epoch: 2194800, elapsed: 1.23e+01, train loss: 4.34813e-07, val loss: 1.52615e-06, min loss: 4.34268e-07\n",
      "Epoch: 2194900, elapsed: 1.23e+01, train loss: 4.38967e-07, val loss: 1.56636e-06, min loss: 4.34268e-07\n",
      "Epoch: 2195000, elapsed: 1.22e+01, train loss: 7.36276e-07, val loss: 1.81800e-06, min loss: 4.34268e-07\n",
      "Epoch: 2195100, elapsed: 1.44e+01, train loss: 5.60746e-07, val loss: 1.75995e-06, min loss: 4.34268e-07\n",
      "Epoch: 2195200, elapsed: 1.23e+01, train loss: 7.59561e-07, val loss: 1.68054e-06, min loss: 4.34268e-07\n",
      "Epoch: 2195300, elapsed: 1.25e+01, train loss: 4.33678e-07, val loss: 1.54254e-06, min loss: 4.33678e-07\n",
      "Epoch: 2195400, elapsed: 1.20e+01, train loss: 4.63036e-07, val loss: 1.54298e-06, min loss: 4.33678e-07\n",
      "Epoch: 2195500, elapsed: 1.87e+01, train loss: 4.33600e-07, val loss: 1.54387e-06, min loss: 4.33600e-07\n",
      "Epoch: 2195600, elapsed: 1.40e+01, train loss: 4.62401e-07, val loss: 1.62689e-06, min loss: 4.33600e-07\n",
      "Epoch: 2195700, elapsed: 1.39e+01, train loss: 4.33532e-07, val loss: 1.54376e-06, min loss: 4.33532e-07\n",
      "Epoch: 2195800, elapsed: 1.42e+01, train loss: 4.50607e-07, val loss: 1.53931e-06, min loss: 4.33532e-07\n",
      "Epoch: 2195900, elapsed: 1.35e+01, train loss: 4.33506e-07, val loss: 1.54508e-06, min loss: 4.33506e-07\n",
      "Epoch: 2196000, elapsed: 1.35e+01, train loss: 4.38121e-07, val loss: 1.56490e-06, min loss: 4.33506e-07\n",
      "Epoch: 2196100, elapsed: 1.37e+01, train loss: 6.76311e-07, val loss: 1.75771e-06, min loss: 4.33506e-07\n",
      "Epoch: 2196200, elapsed: 1.44e+01, train loss: 1.95037e-06, val loss: 2.13809e-06, min loss: 4.33506e-07\n",
      "Epoch: 2196300, elapsed: 1.38e+01, train loss: 5.12032e-07, val loss: 1.55710e-06, min loss: 4.33506e-07\n",
      "Epoch: 2196400, elapsed: 1.40e+01, train loss: 4.37287e-07, val loss: 1.55326e-06, min loss: 4.33506e-07\n",
      "Epoch: 2196500, elapsed: 1.39e+01, train loss: 1.36555e-06, val loss: 2.13296e-06, min loss: 4.33506e-07\n",
      "Epoch: 2196600, elapsed: 1.38e+01, train loss: 6.38214e-07, val loss: 1.78097e-06, min loss: 4.33506e-07\n",
      "Epoch: 2196700, elapsed: 1.36e+01, train loss: 3.59766e-06, val loss: 4.31260e-06, min loss: 4.33506e-07\n",
      "Epoch: 2196800, elapsed: 1.40e+01, train loss: 4.38474e-07, val loss: 1.59692e-06, min loss: 4.33506e-07\n",
      "Epoch: 2196900, elapsed: 1.39e+01, train loss: 8.64556e-07, val loss: 1.66918e-06, min loss: 4.33506e-07\n",
      "Epoch: 2197000, elapsed: 1.37e+01, train loss: 6.16978e-07, val loss: 1.64109e-06, min loss: 4.33506e-07\n",
      "Epoch: 2197100, elapsed: 1.36e+01, train loss: 1.18025e-06, val loss: 2.43084e-06, min loss: 4.33506e-07\n",
      "Epoch: 2197200, elapsed: 1.37e+01, train loss: 7.41064e-07, val loss: 1.69028e-06, min loss: 4.33506e-07\n",
      "Epoch: 2197300, elapsed: 1.35e+01, train loss: 4.37292e-07, val loss: 1.53696e-06, min loss: 4.33506e-07\n",
      "Epoch: 2197400, elapsed: 1.40e+01, train loss: 4.97619e-07, val loss: 1.59118e-06, min loss: 4.33506e-07\n",
      "Epoch: 2197500, elapsed: 1.36e+01, train loss: 4.45049e-07, val loss: 1.57304e-06, min loss: 4.33506e-07\n",
      "Epoch: 2197600, elapsed: 1.35e+01, train loss: 4.50918e-07, val loss: 1.54251e-06, min loss: 4.33506e-07\n",
      "Epoch: 2197700, elapsed: 1.35e+01, train loss: 4.56722e-07, val loss: 1.56306e-06, min loss: 4.33506e-07\n",
      "Epoch: 2197800, elapsed: 1.36e+01, train loss: 4.45766e-07, val loss: 1.51170e-06, min loss: 4.33506e-07\n",
      "Epoch: 2197900, elapsed: 1.35e+01, train loss: 4.35169e-07, val loss: 1.54232e-06, min loss: 4.33506e-07\n",
      "Epoch: 2198000, elapsed: 1.36e+01, train loss: 4.35284e-07, val loss: 1.52176e-06, min loss: 4.33506e-07\n",
      "Epoch: 2198100, elapsed: 1.34e+01, train loss: 4.43278e-07, val loss: 1.57494e-06, min loss: 4.33506e-07\n",
      "Epoch: 2198200, elapsed: 1.37e+01, train loss: 4.70661e-07, val loss: 1.52632e-06, min loss: 4.33506e-07\n",
      "Epoch: 2198300, elapsed: 1.35e+01, train loss: 2.15124e-06, val loss: 3.74424e-06, min loss: 4.33506e-07\n",
      "Epoch: 2198400, elapsed: 1.35e+01, train loss: 4.36084e-07, val loss: 1.53528e-06, min loss: 4.33506e-07\n",
      "Epoch: 2198500, elapsed: 1.40e+01, train loss: 4.33350e-07, val loss: 1.53755e-06, min loss: 4.33350e-07\n",
      "Epoch: 2198600, elapsed: 1.36e+01, train loss: 4.37991e-07, val loss: 1.51717e-06, min loss: 4.33350e-07\n",
      "Epoch: 2198700, elapsed: 1.34e+01, train loss: 7.56009e-07, val loss: 1.68428e-06, min loss: 4.33350e-07\n",
      "Epoch: 2198800, elapsed: 1.33e+01, train loss: 4.33210e-07, val loss: 1.54128e-06, min loss: 4.33210e-07\n",
      "Epoch: 2198900, elapsed: 1.39e+01, train loss: 5.71335e-07, val loss: 1.69710e-06, min loss: 4.33210e-07\n",
      "Epoch: 2199000, elapsed: 1.36e+01, train loss: 9.94353e-07, val loss: 2.31147e-06, min loss: 4.33210e-07\n",
      "Epoch: 2199100, elapsed: 1.37e+01, train loss: 8.59382e-07, val loss: 1.89583e-06, min loss: 4.33210e-07\n",
      "Epoch: 2199200, elapsed: 1.35e+01, train loss: 2.08088e-06, val loss: 2.76402e-06, min loss: 4.33210e-07\n",
      "Epoch: 2199300, elapsed: 1.38e+01, train loss: 2.26288e-06, val loss: 3.59430e-06, min loss: 4.33210e-07\n",
      "Epoch: 2199400, elapsed: 2.05e+01, train loss: 5.51684e-07, val loss: 1.59844e-06, min loss: 4.33210e-07\n",
      "Epoch: 2199500, elapsed: 1.37e+01, train loss: 4.32863e-07, val loss: 1.54155e-06, min loss: 4.32863e-07\n",
      "Epoch: 2199600, elapsed: 1.40e+01, train loss: 4.34248e-07, val loss: 1.52819e-06, min loss: 4.32863e-07\n",
      "Epoch: 2199700, elapsed: 1.37e+01, train loss: 1.38414e-06, val loss: 2.64836e-06, min loss: 4.32863e-07\n",
      "Epoch: 2199800, elapsed: 1.41e+01, train loss: 4.63590e-07, val loss: 1.60356e-06, min loss: 4.32863e-07\n",
      "Epoch: 2199900, elapsed: 1.39e+01, train loss: 4.93631e-07, val loss: 1.52522e-06, min loss: 4.32863e-07\n",
      "Epoch: 2200000, elapsed: 1.38e+01, train loss: 4.65242e-07, val loss: 1.65587e-06, min loss: 4.32863e-07\n",
      "Epoch: 2200100, elapsed: 1.62e+01, train loss: 4.50621e-07, val loss: 1.60350e-06, min loss: 4.32863e-07\n",
      "Epoch: 2200200, elapsed: 1.32e+01, train loss: 4.32554e-07, val loss: 1.54242e-06, min loss: 4.32554e-07\n",
      "Epoch: 2200300, elapsed: 1.37e+01, train loss: 5.64993e-07, val loss: 1.66823e-06, min loss: 4.32554e-07\n",
      "Epoch: 2200400, elapsed: 1.33e+01, train loss: 4.41284e-07, val loss: 1.49603e-06, min loss: 4.32554e-07\n",
      "Epoch: 2200500, elapsed: 1.33e+01, train loss: 4.94198e-07, val loss: 1.56018e-06, min loss: 4.32554e-07\n",
      "Epoch: 2200600, elapsed: 1.34e+01, train loss: 4.33844e-07, val loss: 1.56110e-06, min loss: 4.32554e-07\n",
      "Epoch: 2200700, elapsed: 1.40e+01, train loss: 5.09878e-07, val loss: 1.69984e-06, min loss: 4.32554e-07\n",
      "Epoch: 2200800, elapsed: 1.39e+01, train loss: 4.32360e-07, val loss: 1.54589e-06, min loss: 4.32360e-07\n",
      "Epoch: 2200900, elapsed: 1.29e+01, train loss: 1.00370e-06, val loss: 2.34945e-06, min loss: 4.32360e-07\n",
      "Epoch: 2201000, elapsed: 1.23e+01, train loss: 4.32320e-07, val loss: 1.54417e-06, min loss: 4.32320e-07\n",
      "Epoch: 2201100, elapsed: 1.23e+01, train loss: 5.86796e-07, val loss: 1.62248e-06, min loss: 4.32320e-07\n",
      "Epoch: 2201200, elapsed: 1.22e+01, train loss: 4.32213e-07, val loss: 1.54609e-06, min loss: 4.32213e-07\n",
      "Epoch: 2201300, elapsed: 1.21e+01, train loss: 4.69762e-07, val loss: 1.52348e-06, min loss: 4.32213e-07\n",
      "Epoch: 2201400, elapsed: 1.21e+01, train loss: 4.76021e-07, val loss: 1.62117e-06, min loss: 4.32213e-07\n",
      "Epoch: 2201500, elapsed: 1.21e+01, train loss: 1.02275e-06, val loss: 2.29816e-06, min loss: 4.32213e-07\n",
      "Epoch: 2201600, elapsed: 1.22e+01, train loss: 6.25436e-07, val loss: 1.76167e-06, min loss: 4.32213e-07\n",
      "Epoch: 2201700, elapsed: 1.20e+01, train loss: 1.16380e-06, val loss: 2.24133e-06, min loss: 4.32213e-07\n",
      "Epoch: 2201800, elapsed: 1.24e+01, train loss: 5.22455e-07, val loss: 1.56951e-06, min loss: 4.32213e-07\n",
      "Epoch: 2201900, elapsed: 1.22e+01, train loss: 4.33011e-07, val loss: 1.53268e-06, min loss: 4.32213e-07\n",
      "Epoch: 2202000, elapsed: 1.23e+01, train loss: 4.34004e-07, val loss: 1.55817e-06, min loss: 4.32213e-07\n",
      "Epoch: 2202100, elapsed: 1.24e+01, train loss: 6.74443e-07, val loss: 1.70316e-06, min loss: 4.32213e-07\n",
      "Epoch: 2202200, elapsed: 1.19e+01, train loss: 4.33512e-07, val loss: 1.54869e-06, min loss: 4.32213e-07\n",
      "Epoch: 2202300, elapsed: 1.20e+01, train loss: 4.36073e-07, val loss: 1.52944e-06, min loss: 4.32213e-07\n",
      "Epoch: 2202400, elapsed: 1.20e+01, train loss: 3.20633e-06, val loss: 3.51968e-06, min loss: 4.32213e-07\n",
      "Epoch: 2202500, elapsed: 1.21e+01, train loss: 4.32136e-07, val loss: 1.54962e-06, min loss: 4.32136e-07\n",
      "Epoch: 2202600, elapsed: 1.22e+01, train loss: 4.41988e-07, val loss: 1.54052e-06, min loss: 4.32136e-07\n",
      "Epoch: 2202700, elapsed: 1.24e+01, train loss: 7.52243e-07, val loss: 1.93637e-06, min loss: 4.32136e-07\n",
      "Epoch: 2202800, elapsed: 1.26e+01, train loss: 4.32388e-07, val loss: 1.53944e-06, min loss: 4.32136e-07\n",
      "Epoch: 2202900, elapsed: 1.20e+01, train loss: 4.32613e-07, val loss: 1.55120e-06, min loss: 4.32136e-07\n",
      "Epoch: 2203000, elapsed: 1.23e+01, train loss: 4.73352e-07, val loss: 1.67350e-06, min loss: 4.32136e-07\n",
      "Epoch: 2203100, elapsed: 1.26e+01, train loss: 4.31763e-07, val loss: 1.54421e-06, min loss: 4.31763e-07\n",
      "Epoch: 2203200, elapsed: 1.24e+01, train loss: 4.88370e-07, val loss: 1.62630e-06, min loss: 4.31763e-07\n",
      "Epoch: 2203300, elapsed: 1.85e+01, train loss: 4.31720e-07, val loss: 1.54623e-06, min loss: 4.31720e-07\n",
      "Epoch: 2203400, elapsed: 1.27e+01, train loss: 4.41637e-07, val loss: 1.56671e-06, min loss: 4.31720e-07\n",
      "Epoch: 2203500, elapsed: 1.27e+01, train loss: 4.31661e-07, val loss: 1.54588e-06, min loss: 4.31661e-07\n",
      "Epoch: 2203600, elapsed: 1.26e+01, train loss: 4.35100e-07, val loss: 1.54030e-06, min loss: 4.31661e-07\n",
      "Epoch: 2203700, elapsed: 1.24e+01, train loss: 5.85575e-07, val loss: 1.56104e-06, min loss: 4.31661e-07\n",
      "Epoch: 2203800, elapsed: 1.26e+01, train loss: 4.33570e-07, val loss: 1.53888e-06, min loss: 4.31661e-07\n",
      "Epoch: 2203900, elapsed: 1.25e+01, train loss: 4.33861e-07, val loss: 1.55040e-06, min loss: 4.31661e-07\n",
      "Epoch: 2204000, elapsed: 1.27e+01, train loss: 4.65020e-07, val loss: 1.61868e-06, min loss: 4.31661e-07\n",
      "Epoch: 2204100, elapsed: 1.24e+01, train loss: 8.97928e-07, val loss: 1.97897e-06, min loss: 4.31661e-07\n",
      "Epoch: 2204200, elapsed: 1.26e+01, train loss: 4.54292e-07, val loss: 1.51129e-06, min loss: 4.31661e-07\n",
      "Epoch: 2204300, elapsed: 1.25e+01, train loss: 1.01441e-06, val loss: 1.68714e-06, min loss: 4.31661e-07\n",
      "Epoch: 2204400, elapsed: 1.24e+01, train loss: 4.43123e-07, val loss: 1.54001e-06, min loss: 4.31661e-07\n",
      "Epoch: 2204500, elapsed: 1.28e+01, train loss: 4.31759e-07, val loss: 1.54681e-06, min loss: 4.31661e-07\n",
      "Epoch: 2204600, elapsed: 1.25e+01, train loss: 4.52772e-07, val loss: 1.49141e-06, min loss: 4.31661e-07\n",
      "Epoch: 2204700, elapsed: 1.23e+01, train loss: 4.38740e-07, val loss: 1.53464e-06, min loss: 4.31661e-07\n",
      "Epoch: 2204800, elapsed: 1.27e+01, train loss: 4.43300e-07, val loss: 1.55732e-06, min loss: 4.31661e-07\n",
      "Epoch: 2204900, elapsed: 1.23e+01, train loss: 4.37594e-07, val loss: 1.53500e-06, min loss: 4.31661e-07\n",
      "Epoch: 2205000, elapsed: 1.25e+01, train loss: 4.39347e-07, val loss: 1.54476e-06, min loss: 4.31661e-07\n",
      "Epoch: 2205100, elapsed: 1.45e+01, train loss: 7.51942e-07, val loss: 2.05974e-06, min loss: 4.31661e-07\n",
      "Epoch: 2205200, elapsed: 1.23e+01, train loss: 4.75728e-07, val loss: 1.62840e-06, min loss: 4.31661e-07\n",
      "Epoch: 2205300, elapsed: 1.23e+01, train loss: 4.31697e-07, val loss: 1.54473e-06, min loss: 4.31661e-07\n",
      "Epoch: 2205400, elapsed: 1.24e+01, train loss: 4.37363e-07, val loss: 1.56424e-06, min loss: 4.31661e-07\n",
      "Epoch: 2205500, elapsed: 1.21e+01, train loss: 1.84836e-06, val loss: 2.32033e-06, min loss: 4.31661e-07\n",
      "Epoch: 2205600, elapsed: 1.21e+01, train loss: 4.31687e-07, val loss: 1.54824e-06, min loss: 4.31661e-07\n",
      "Epoch: 2205700, elapsed: 1.22e+01, train loss: 4.36475e-07, val loss: 1.53901e-06, min loss: 4.31661e-07\n",
      "Epoch: 2205800, elapsed: 1.22e+01, train loss: 4.59137e-07, val loss: 1.63150e-06, min loss: 4.31661e-07\n",
      "Epoch: 2205900, elapsed: 1.22e+01, train loss: 4.45868e-07, val loss: 1.61309e-06, min loss: 4.31661e-07\n",
      "Epoch: 2206000, elapsed: 1.26e+01, train loss: 4.35340e-07, val loss: 1.55164e-06, min loss: 4.31661e-07\n",
      "Epoch: 2206100, elapsed: 1.25e+01, train loss: 4.43355e-07, val loss: 1.58577e-06, min loss: 4.31661e-07\n",
      "Epoch: 2206200, elapsed: 1.23e+01, train loss: 4.37113e-07, val loss: 1.52023e-06, min loss: 4.31661e-07\n",
      "Epoch: 2206300, elapsed: 1.22e+01, train loss: 4.60030e-07, val loss: 1.53265e-06, min loss: 4.31661e-07\n",
      "Epoch: 2206400, elapsed: 1.24e+01, train loss: 6.32083e-07, val loss: 1.84508e-06, min loss: 4.31661e-07\n",
      "Epoch: 2206500, elapsed: 1.23e+01, train loss: 6.92053e-07, val loss: 1.63317e-06, min loss: 4.31661e-07\n",
      "Epoch: 2206600, elapsed: 1.24e+01, train loss: 4.34003e-07, val loss: 1.53428e-06, min loss: 4.31661e-07\n",
      "Epoch: 2206700, elapsed: 1.21e+01, train loss: 4.33313e-07, val loss: 1.55081e-06, min loss: 4.31661e-07\n",
      "Epoch: 2206800, elapsed: 1.22e+01, train loss: 4.35753e-07, val loss: 1.53173e-06, min loss: 4.31661e-07\n",
      "Epoch: 2206900, elapsed: 1.25e+01, train loss: 4.43628e-07, val loss: 1.53198e-06, min loss: 4.31661e-07\n",
      "Epoch: 2207000, elapsed: 1.20e+01, train loss: 4.48638e-07, val loss: 1.56683e-06, min loss: 4.31661e-07\n",
      "Epoch: 2207100, elapsed: 1.26e+01, train loss: 1.34208e-06, val loss: 1.91252e-06, min loss: 4.31661e-07\n",
      "Epoch: 2207200, elapsed: 1.91e+01, train loss: 3.29741e-06, val loss: 3.74520e-06, min loss: 4.31661e-07\n",
      "Epoch: 2207300, elapsed: 1.38e+01, train loss: 4.72418e-07, val loss: 1.57240e-06, min loss: 4.31661e-07\n",
      "Epoch: 2207400, elapsed: 1.43e+01, train loss: 4.45852e-07, val loss: 1.51230e-06, min loss: 4.31661e-07\n",
      "Epoch: 2207500, elapsed: 1.37e+01, train loss: 4.39561e-07, val loss: 1.52102e-06, min loss: 4.31661e-07\n",
      "Epoch: 2207600, elapsed: 1.42e+01, train loss: 4.30834e-07, val loss: 1.55340e-06, min loss: 4.30834e-07\n",
      "Epoch: 2207700, elapsed: 1.37e+01, train loss: 8.59485e-07, val loss: 1.98483e-06, min loss: 4.30834e-07\n",
      "Epoch: 2207800, elapsed: 1.42e+01, train loss: 1.87408e-06, val loss: 3.48289e-06, min loss: 4.30834e-07\n",
      "Epoch: 2207900, elapsed: 1.37e+01, train loss: 4.35994e-07, val loss: 1.53104e-06, min loss: 4.30834e-07\n",
      "Epoch: 2208000, elapsed: 1.33e+01, train loss: 4.32750e-07, val loss: 1.52409e-06, min loss: 4.30834e-07\n",
      "Epoch: 2208100, elapsed: 1.37e+01, train loss: 4.35502e-07, val loss: 1.56603e-06, min loss: 4.30834e-07\n",
      "Epoch: 2208200, elapsed: 1.34e+01, train loss: 4.57485e-07, val loss: 1.61915e-06, min loss: 4.30834e-07\n",
      "Epoch: 2208300, elapsed: 1.32e+01, train loss: 4.31867e-07, val loss: 1.56331e-06, min loss: 4.30834e-07\n",
      "Epoch: 2208400, elapsed: 1.35e+01, train loss: 4.35614e-07, val loss: 1.50889e-06, min loss: 4.30834e-07\n",
      "Epoch: 2208500, elapsed: 1.36e+01, train loss: 5.72063e-07, val loss: 1.68885e-06, min loss: 4.30834e-07\n",
      "Epoch: 2208600, elapsed: 1.41e+01, train loss: 8.29278e-07, val loss: 2.20253e-06, min loss: 4.30834e-07\n",
      "Epoch: 2208700, elapsed: 1.35e+01, train loss: 1.51989e-06, val loss: 2.71184e-06, min loss: 4.30834e-07\n",
      "Epoch: 2208800, elapsed: 1.33e+01, train loss: 2.69674e-06, val loss: 4.15232e-06, min loss: 4.30834e-07\n",
      "Epoch: 2208900, elapsed: 1.25e+01, train loss: 4.30625e-07, val loss: 1.54414e-06, min loss: 4.30625e-07\n",
      "Epoch: 2209000, elapsed: 1.23e+01, train loss: 4.32411e-07, val loss: 1.53947e-06, min loss: 4.30625e-07\n",
      "Epoch: 2209100, elapsed: 1.22e+01, train loss: 5.38472e-07, val loss: 1.72738e-06, min loss: 4.30625e-07\n",
      "Epoch: 2209200, elapsed: 1.21e+01, train loss: 5.51873e-07, val loss: 1.55680e-06, min loss: 4.30625e-07\n",
      "Epoch: 2209300, elapsed: 1.21e+01, train loss: 4.36690e-07, val loss: 1.53178e-06, min loss: 4.30625e-07\n",
      "Epoch: 2209400, elapsed: 1.20e+01, train loss: 5.02857e-07, val loss: 1.56853e-06, min loss: 4.30625e-07\n",
      "Epoch: 2209500, elapsed: 1.22e+01, train loss: 7.40149e-07, val loss: 1.93921e-06, min loss: 4.30625e-07\n",
      "Epoch: 2209600, elapsed: 1.21e+01, train loss: 1.09794e-06, val loss: 2.49571e-06, min loss: 4.30625e-07\n",
      "Epoch: 2209700, elapsed: 1.22e+01, train loss: 8.64877e-07, val loss: 1.93177e-06, min loss: 4.30625e-07\n",
      "Epoch: 2209800, elapsed: 1.20e+01, train loss: 4.39813e-07, val loss: 1.52846e-06, min loss: 4.30625e-07\n",
      "Epoch: 2209900, elapsed: 1.24e+01, train loss: 4.31872e-07, val loss: 1.53971e-06, min loss: 4.30625e-07\n",
      "Epoch: 2210000, elapsed: 1.23e+01, train loss: 6.56933e-07, val loss: 1.82575e-06, min loss: 4.30625e-07\n",
      "Epoch: 2210100, elapsed: 1.43e+01, train loss: 4.30251e-07, val loss: 1.53813e-06, min loss: 4.30251e-07\n",
      "Epoch: 2210200, elapsed: 1.19e+01, train loss: 4.45310e-07, val loss: 1.54852e-06, min loss: 4.30251e-07\n",
      "Epoch: 2210300, elapsed: 1.18e+01, train loss: 4.34357e-07, val loss: 1.56680e-06, min loss: 4.30251e-07\n",
      "Epoch: 2210400, elapsed: 1.20e+01, train loss: 4.32246e-07, val loss: 1.55595e-06, min loss: 4.30251e-07\n",
      "Epoch: 2210500, elapsed: 1.19e+01, train loss: 5.43194e-07, val loss: 1.46399e-06, min loss: 4.30251e-07\n",
      "Epoch: 2210600, elapsed: 1.23e+01, train loss: 4.59092e-07, val loss: 1.54375e-06, min loss: 4.30251e-07\n",
      "Epoch: 2210700, elapsed: 1.22e+01, train loss: 4.30436e-07, val loss: 1.55055e-06, min loss: 4.30251e-07\n",
      "Epoch: 2210800, elapsed: 1.23e+01, train loss: 4.32536e-07, val loss: 1.56642e-06, min loss: 4.30251e-07\n",
      "Epoch: 2210900, elapsed: 1.21e+01, train loss: 4.31627e-07, val loss: 1.56184e-06, min loss: 4.30251e-07\n",
      "Epoch: 2211000, elapsed: 1.20e+01, train loss: 4.30495e-07, val loss: 1.55324e-06, min loss: 4.30251e-07\n",
      "Epoch: 2211100, elapsed: 1.85e+01, train loss: 4.32665e-07, val loss: 1.55086e-06, min loss: 4.30251e-07\n",
      "Epoch: 2211200, elapsed: 1.29e+01, train loss: 4.33832e-07, val loss: 1.54492e-06, min loss: 4.30251e-07\n",
      "Epoch: 2211300, elapsed: 1.27e+01, train loss: 5.20287e-07, val loss: 1.60482e-06, min loss: 4.30251e-07\n",
      "Epoch: 2211400, elapsed: 1.24e+01, train loss: 4.30522e-07, val loss: 1.55416e-06, min loss: 4.30251e-07\n",
      "Epoch: 2211500, elapsed: 1.23e+01, train loss: 4.31182e-07, val loss: 1.54792e-06, min loss: 4.30251e-07\n",
      "Epoch: 2211600, elapsed: 1.22e+01, train loss: 4.42280e-07, val loss: 1.53458e-06, min loss: 4.30251e-07\n",
      "Epoch: 2211700, elapsed: 1.25e+01, train loss: 4.30407e-07, val loss: 1.53620e-06, min loss: 4.30251e-07\n",
      "Epoch: 2211800, elapsed: 1.22e+01, train loss: 4.59196e-07, val loss: 1.53795e-06, min loss: 4.30251e-07\n",
      "Epoch: 2211900, elapsed: 1.26e+01, train loss: 4.45903e-07, val loss: 1.56446e-06, min loss: 4.30251e-07\n",
      "Epoch: 2212000, elapsed: 1.23e+01, train loss: 4.41119e-07, val loss: 1.53050e-06, min loss: 4.30251e-07\n",
      "Epoch: 2212100, elapsed: 1.26e+01, train loss: 4.41040e-07, val loss: 1.60526e-06, min loss: 4.30251e-07\n",
      "Epoch: 2212200, elapsed: 1.24e+01, train loss: 5.05184e-07, val loss: 1.66321e-06, min loss: 4.30251e-07\n",
      "Epoch: 2212300, elapsed: 1.24e+01, train loss: 4.58249e-07, val loss: 1.58613e-06, min loss: 4.30251e-07\n",
      "Epoch: 2212400, elapsed: 1.24e+01, train loss: 4.81982e-07, val loss: 1.55574e-06, min loss: 4.30251e-07\n",
      "Epoch: 2212500, elapsed: 1.22e+01, train loss: 7.40368e-07, val loss: 1.73634e-06, min loss: 4.30251e-07\n",
      "Epoch: 2212600, elapsed: 1.23e+01, train loss: 4.52640e-07, val loss: 1.63076e-06, min loss: 4.30251e-07\n",
      "Epoch: 2212700, elapsed: 1.24e+01, train loss: 4.33798e-07, val loss: 1.53972e-06, min loss: 4.30251e-07\n",
      "Epoch: 2212800, elapsed: 1.24e+01, train loss: 4.30639e-07, val loss: 1.55120e-06, min loss: 4.30251e-07\n",
      "Epoch: 2212900, elapsed: 1.22e+01, train loss: 4.33561e-07, val loss: 1.52665e-06, min loss: 4.30251e-07\n",
      "Epoch: 2213000, elapsed: 1.26e+01, train loss: 7.42668e-07, val loss: 1.57988e-06, min loss: 4.30251e-07\n",
      "Epoch: 2213100, elapsed: 1.24e+01, train loss: 4.29589e-07, val loss: 1.55487e-06, min loss: 4.29589e-07\n",
      "Epoch: 2213200, elapsed: 1.22e+01, train loss: 4.33878e-07, val loss: 1.53384e-06, min loss: 4.29589e-07\n",
      "Epoch: 2213300, elapsed: 1.25e+01, train loss: 5.35400e-07, val loss: 1.70621e-06, min loss: 4.29589e-07\n",
      "Epoch: 2213400, elapsed: 1.25e+01, train loss: 6.08265e-07, val loss: 1.60473e-06, min loss: 4.29589e-07\n",
      "Epoch: 2213500, elapsed: 1.23e+01, train loss: 4.43509e-07, val loss: 1.52951e-06, min loss: 4.29589e-07\n",
      "Epoch: 2213600, elapsed: 1.25e+01, train loss: 4.29439e-07, val loss: 1.54761e-06, min loss: 4.29439e-07\n",
      "Epoch: 2213700, elapsed: 1.24e+01, train loss: 4.34700e-07, val loss: 1.53391e-06, min loss: 4.29439e-07\n",
      "Epoch: 2213800, elapsed: 1.23e+01, train loss: 4.29631e-07, val loss: 1.55786e-06, min loss: 4.29439e-07\n",
      "Epoch: 2213900, elapsed: 1.23e+01, train loss: 4.52300e-07, val loss: 1.64460e-06, min loss: 4.29439e-07\n",
      "Epoch: 2214000, elapsed: 1.27e+01, train loss: 5.65381e-07, val loss: 1.85067e-06, min loss: 4.29439e-07\n",
      "Epoch: 2214100, elapsed: 1.33e+01, train loss: 4.29208e-07, val loss: 1.54515e-06, min loss: 4.29208e-07\n",
      "Epoch: 2214200, elapsed: 1.37e+01, train loss: 4.30480e-07, val loss: 1.54766e-06, min loss: 4.29208e-07\n",
      "Epoch: 2214300, elapsed: 1.36e+01, train loss: 4.29113e-07, val loss: 1.54789e-06, min loss: 4.29113e-07\n",
      "Epoch: 2214400, elapsed: 1.34e+01, train loss: 4.30466e-07, val loss: 1.55794e-06, min loss: 4.29113e-07\n",
      "Epoch: 2214500, elapsed: 1.34e+01, train loss: 4.29768e-07, val loss: 1.54484e-06, min loss: 4.29113e-07\n",
      "Epoch: 2214600, elapsed: 1.37e+01, train loss: 4.29295e-07, val loss: 1.54637e-06, min loss: 4.29113e-07\n",
      "Epoch: 2214700, elapsed: 1.35e+01, train loss: 5.28569e-07, val loss: 1.66973e-06, min loss: 4.29113e-07\n",
      "Epoch: 2214800, elapsed: 1.37e+01, train loss: 6.42971e-07, val loss: 1.72163e-06, min loss: 4.29113e-07\n",
      "Epoch: 2214900, elapsed: 1.37e+01, train loss: 4.46658e-07, val loss: 1.56259e-06, min loss: 4.29113e-07\n",
      "Epoch: 2215000, elapsed: 2.01e+01, train loss: 4.30029e-07, val loss: 1.54177e-06, min loss: 4.29113e-07\n",
      "Epoch: 2215100, elapsed: 1.71e+01, train loss: 4.42408e-07, val loss: 1.54292e-06, min loss: 4.29113e-07\n",
      "Epoch: 2215200, elapsed: 1.35e+01, train loss: 4.57916e-07, val loss: 1.53377e-06, min loss: 4.29113e-07\n",
      "Epoch: 2215300, elapsed: 1.38e+01, train loss: 8.46934e-07, val loss: 2.09813e-06, min loss: 4.29113e-07\n",
      "Epoch: 2215400, elapsed: 1.38e+01, train loss: 5.22531e-07, val loss: 1.58880e-06, min loss: 4.29113e-07\n",
      "Epoch: 2215500, elapsed: 1.38e+01, train loss: 4.32231e-07, val loss: 1.56347e-06, min loss: 4.29113e-07\n",
      "Epoch: 2215600, elapsed: 1.38e+01, train loss: 4.43394e-07, val loss: 1.56791e-06, min loss: 4.29113e-07\n",
      "Epoch: 2215700, elapsed: 1.36e+01, train loss: 4.41475e-07, val loss: 1.57649e-06, min loss: 4.29113e-07\n",
      "Epoch: 2215800, elapsed: 1.39e+01, train loss: 4.42638e-07, val loss: 1.56378e-06, min loss: 4.29113e-07\n",
      "Epoch: 2215900, elapsed: 1.44e+01, train loss: 4.92559e-07, val loss: 1.63573e-06, min loss: 4.29113e-07\n",
      "Epoch: 2216000, elapsed: 1.35e+01, train loss: 5.03682e-07, val loss: 1.69287e-06, min loss: 4.29113e-07\n",
      "Epoch: 2216100, elapsed: 1.40e+01, train loss: 9.18328e-07, val loss: 2.04131e-06, min loss: 4.29113e-07\n",
      "Epoch: 2216200, elapsed: 1.26e+01, train loss: 7.74620e-07, val loss: 1.80856e-06, min loss: 4.29113e-07\n",
      "Epoch: 2216300, elapsed: 1.22e+01, train loss: 4.39328e-07, val loss: 1.56212e-06, min loss: 4.29113e-07\n",
      "Epoch: 2216400, elapsed: 1.22e+01, train loss: 4.44459e-07, val loss: 1.53170e-06, min loss: 4.29113e-07\n",
      "Epoch: 2216500, elapsed: 1.23e+01, train loss: 4.30568e-07, val loss: 1.52185e-06, min loss: 4.29113e-07\n",
      "Epoch: 2216600, elapsed: 1.23e+01, train loss: 4.29485e-07, val loss: 1.54515e-06, min loss: 4.29113e-07\n",
      "Epoch: 2216700, elapsed: 1.25e+01, train loss: 4.33219e-07, val loss: 1.54037e-06, min loss: 4.29113e-07\n",
      "Epoch: 2216800, elapsed: 1.21e+01, train loss: 4.29504e-07, val loss: 1.55161e-06, min loss: 4.29113e-07\n",
      "Epoch: 2216900, elapsed: 1.25e+01, train loss: 4.28476e-07, val loss: 1.54824e-06, min loss: 4.28476e-07\n",
      "Epoch: 2217000, elapsed: 1.20e+01, train loss: 4.33127e-07, val loss: 1.54874e-06, min loss: 4.28476e-07\n",
      "Epoch: 2217100, elapsed: 1.23e+01, train loss: 4.28438e-07, val loss: 1.55001e-06, min loss: 4.28438e-07\n",
      "Epoch: 2217200, elapsed: 1.20e+01, train loss: 4.28989e-07, val loss: 1.53512e-06, min loss: 4.28438e-07\n",
      "Epoch: 2217300, elapsed: 1.20e+01, train loss: 4.28998e-07, val loss: 1.56267e-06, min loss: 4.28438e-07\n",
      "Epoch: 2217400, elapsed: 1.20e+01, train loss: 4.49828e-07, val loss: 1.59707e-06, min loss: 4.28438e-07\n",
      "Epoch: 2217500, elapsed: 1.21e+01, train loss: 4.29345e-07, val loss: 1.55496e-06, min loss: 4.28438e-07\n",
      "Epoch: 2217600, elapsed: 1.22e+01, train loss: 4.52969e-07, val loss: 1.55784e-06, min loss: 4.28438e-07\n",
      "Epoch: 2217700, elapsed: 1.17e+01, train loss: 6.67296e-07, val loss: 1.99316e-06, min loss: 4.28438e-07\n",
      "Epoch: 2217800, elapsed: 1.20e+01, train loss: 4.28674e-07, val loss: 1.54814e-06, min loss: 4.28438e-07\n",
      "Epoch: 2217900, elapsed: 1.19e+01, train loss: 4.53745e-07, val loss: 1.47032e-06, min loss: 4.28438e-07\n",
      "Epoch: 2218000, elapsed: 1.20e+01, train loss: 4.28238e-07, val loss: 1.54674e-06, min loss: 4.28238e-07\n",
      "Epoch: 2218100, elapsed: 1.21e+01, train loss: 4.29590e-07, val loss: 1.54874e-06, min loss: 4.28238e-07\n",
      "Epoch: 2218200, elapsed: 1.21e+01, train loss: 4.28516e-07, val loss: 1.54278e-06, min loss: 4.28238e-07\n",
      "Epoch: 2218300, elapsed: 1.21e+01, train loss: 4.30695e-07, val loss: 1.56888e-06, min loss: 4.28238e-07\n",
      "Epoch: 2218400, elapsed: 1.21e+01, train loss: 7.12091e-07, val loss: 1.81818e-06, min loss: 4.28238e-07\n",
      "Epoch: 2218500, elapsed: 1.22e+01, train loss: 4.54357e-07, val loss: 1.53549e-06, min loss: 4.28238e-07\n",
      "Epoch: 2218600, elapsed: 1.24e+01, train loss: 4.58122e-07, val loss: 1.61951e-06, min loss: 4.28238e-07\n",
      "Epoch: 2218700, elapsed: 1.21e+01, train loss: 1.19332e-06, val loss: 1.87033e-06, min loss: 4.28238e-07\n",
      "Epoch: 2218800, elapsed: 1.21e+01, train loss: 4.54446e-07, val loss: 1.65139e-06, min loss: 4.28238e-07\n",
      "Epoch: 2218900, elapsed: 1.79e+01, train loss: 4.37746e-07, val loss: 1.53736e-06, min loss: 4.28238e-07\n",
      "Epoch: 2219000, elapsed: 1.25e+01, train loss: 4.30688e-07, val loss: 1.52497e-06, min loss: 4.28238e-07\n",
      "Epoch: 2219100, elapsed: 1.23e+01, train loss: 4.31585e-07, val loss: 1.55552e-06, min loss: 4.28238e-07\n",
      "Epoch: 2219200, elapsed: 1.22e+01, train loss: 6.00968e-07, val loss: 1.67223e-06, min loss: 4.28238e-07\n",
      "Epoch: 2219300, elapsed: 1.23e+01, train loss: 4.52042e-07, val loss: 1.55194e-06, min loss: 4.28238e-07\n",
      "Epoch: 2219400, elapsed: 1.24e+01, train loss: 4.33148e-07, val loss: 1.58544e-06, min loss: 4.28238e-07\n",
      "Epoch: 2219500, elapsed: 1.35e+01, train loss: 4.33601e-07, val loss: 1.56381e-06, min loss: 4.28238e-07\n",
      "Epoch: 2219600, elapsed: 1.38e+01, train loss: 4.39496e-07, val loss: 1.56231e-06, min loss: 4.28238e-07\n",
      "Epoch: 2219700, elapsed: 1.35e+01, train loss: 4.57693e-07, val loss: 1.58167e-06, min loss: 4.28238e-07\n",
      "Epoch: 2219800, elapsed: 1.36e+01, train loss: 2.37856e-06, val loss: 2.93226e-06, min loss: 4.28238e-07\n",
      "Epoch: 2219900, elapsed: 1.37e+01, train loss: 4.29405e-07, val loss: 1.57409e-06, min loss: 4.28238e-07\n",
      "Epoch: 2220000, elapsed: 1.36e+01, train loss: 4.47810e-07, val loss: 1.58977e-06, min loss: 4.28238e-07\n",
      "Epoch: 2220100, elapsed: 1.64e+01, train loss: 4.41340e-07, val loss: 1.53800e-06, min loss: 4.28238e-07\n",
      "Epoch: 2220200, elapsed: 1.36e+01, train loss: 4.27745e-07, val loss: 1.54748e-06, min loss: 4.27745e-07\n",
      "Epoch: 2220300, elapsed: 1.35e+01, train loss: 4.30105e-07, val loss: 1.54562e-06, min loss: 4.27745e-07\n",
      "Epoch: 2220400, elapsed: 1.36e+01, train loss: 7.59861e-07, val loss: 2.15172e-06, min loss: 4.27745e-07\n",
      "Epoch: 2220500, elapsed: 1.32e+01, train loss: 5.63949e-07, val loss: 1.70192e-06, min loss: 4.27745e-07\n",
      "Epoch: 2220600, elapsed: 1.36e+01, train loss: 4.73113e-07, val loss: 1.56661e-06, min loss: 4.27745e-07\n",
      "Epoch: 2220700, elapsed: 1.35e+01, train loss: 4.31297e-07, val loss: 1.54551e-06, min loss: 4.27745e-07\n",
      "Epoch: 2220800, elapsed: 1.36e+01, train loss: 4.29700e-07, val loss: 1.54115e-06, min loss: 4.27745e-07\n",
      "Epoch: 2220900, elapsed: 1.37e+01, train loss: 4.44097e-07, val loss: 1.53422e-06, min loss: 4.27745e-07\n",
      "Epoch: 2221000, elapsed: 1.34e+01, train loss: 4.14120e-06, val loss: 5.24717e-06, min loss: 4.27745e-07\n",
      "Epoch: 2221100, elapsed: 1.39e+01, train loss: 4.27541e-07, val loss: 1.54873e-06, min loss: 4.27541e-07\n",
      "Epoch: 2221200, elapsed: 1.36e+01, train loss: 7.26440e-07, val loss: 1.70204e-06, min loss: 4.27541e-07\n",
      "Epoch: 2221300, elapsed: 1.32e+01, train loss: 4.27458e-07, val loss: 1.54805e-06, min loss: 4.27458e-07\n",
      "Epoch: 2221400, elapsed: 1.34e+01, train loss: 4.27992e-07, val loss: 1.54503e-06, min loss: 4.27458e-07\n",
      "Epoch: 2221500, elapsed: 1.36e+01, train loss: 4.36780e-07, val loss: 1.55555e-06, min loss: 4.27458e-07\n",
      "Epoch: 2221600, elapsed: 1.34e+01, train loss: 4.31935e-07, val loss: 1.56249e-06, min loss: 4.27458e-07\n",
      "Epoch: 2221700, elapsed: 1.34e+01, train loss: 4.64022e-07, val loss: 1.63203e-06, min loss: 4.27458e-07\n",
      "Epoch: 2221800, elapsed: 1.33e+01, train loss: 6.27811e-07, val loss: 1.81145e-06, min loss: 4.27458e-07\n",
      "Epoch: 2221900, elapsed: 1.32e+01, train loss: 6.18897e-07, val loss: 1.92222e-06, min loss: 4.27458e-07\n",
      "Epoch: 2222000, elapsed: 1.36e+01, train loss: 5.90137e-07, val loss: 1.57198e-06, min loss: 4.27458e-07\n",
      "Epoch: 2222100, elapsed: 1.35e+01, train loss: 7.06937e-07, val loss: 1.57060e-06, min loss: 4.27458e-07\n",
      "Epoch: 2222200, elapsed: 1.32e+01, train loss: 7.82899e-07, val loss: 1.69521e-06, min loss: 4.27458e-07\n",
      "Epoch: 2222300, elapsed: 1.37e+01, train loss: 4.31835e-07, val loss: 1.54094e-06, min loss: 4.27458e-07\n",
      "Epoch: 2222400, elapsed: 1.33e+01, train loss: 4.52657e-07, val loss: 1.58529e-06, min loss: 4.27458e-07\n",
      "Epoch: 2222500, elapsed: 1.33e+01, train loss: 4.39390e-07, val loss: 1.58010e-06, min loss: 4.27458e-07\n",
      "Epoch: 2222600, elapsed: 1.45e+01, train loss: 5.39540e-07, val loss: 1.68933e-06, min loss: 4.27458e-07\n",
      "Epoch: 2222700, elapsed: 1.34e+01, train loss: 1.26768e-06, val loss: 2.52096e-06, min loss: 4.27458e-07\n",
      "Epoch: 2222800, elapsed: 1.34e+01, train loss: 4.73200e-07, val loss: 1.61482e-06, min loss: 4.27458e-07\n",
      "Epoch: 2222900, elapsed: 2.03e+01, train loss: 4.38337e-07, val loss: 1.52646e-06, min loss: 4.27458e-07\n",
      "Epoch: 2223000, elapsed: 1.42e+01, train loss: 4.27889e-07, val loss: 1.54280e-06, min loss: 4.27458e-07\n",
      "Epoch: 2223100, elapsed: 1.37e+01, train loss: 4.96277e-07, val loss: 1.51962e-06, min loss: 4.27458e-07\n",
      "Epoch: 2223200, elapsed: 1.46e+01, train loss: 4.47148e-07, val loss: 1.57818e-06, min loss: 4.27458e-07\n",
      "Epoch: 2223300, elapsed: 1.38e+01, train loss: 4.75484e-07, val loss: 1.58879e-06, min loss: 4.27458e-07\n",
      "Epoch: 2223400, elapsed: 1.41e+01, train loss: 5.20420e-07, val loss: 1.48756e-06, min loss: 4.27458e-07\n",
      "Epoch: 2223500, elapsed: 1.25e+01, train loss: 4.62640e-07, val loss: 1.53065e-06, min loss: 4.27458e-07\n",
      "Epoch: 2223600, elapsed: 1.23e+01, train loss: 4.35742e-07, val loss: 1.56129e-06, min loss: 4.27458e-07\n",
      "Epoch: 2223700, elapsed: 1.22e+01, train loss: 4.33579e-07, val loss: 1.52217e-06, min loss: 4.27458e-07\n",
      "Epoch: 2223800, elapsed: 1.25e+01, train loss: 4.41147e-07, val loss: 1.51565e-06, min loss: 4.27458e-07\n",
      "Epoch: 2223900, elapsed: 1.21e+01, train loss: 5.15907e-06, val loss: 4.78256e-06, min loss: 4.27458e-07\n",
      "Epoch: 2224000, elapsed: 1.21e+01, train loss: 4.26935e-07, val loss: 1.54981e-06, min loss: 4.26935e-07\n",
      "Epoch: 2224100, elapsed: 1.22e+01, train loss: 7.70814e-07, val loss: 1.59503e-06, min loss: 4.26935e-07\n",
      "Epoch: 2224200, elapsed: 1.23e+01, train loss: 4.26960e-07, val loss: 1.55050e-06, min loss: 4.26935e-07\n",
      "Epoch: 2224300, elapsed: 1.24e+01, train loss: 4.27056e-07, val loss: 1.56032e-06, min loss: 4.26935e-07\n",
      "Epoch: 2224400, elapsed: 1.21e+01, train loss: 4.47684e-07, val loss: 1.61073e-06, min loss: 4.26935e-07\n",
      "Epoch: 2224500, elapsed: 1.25e+01, train loss: 7.72962e-07, val loss: 2.05816e-06, min loss: 4.26935e-07\n",
      "Epoch: 2224600, elapsed: 1.20e+01, train loss: 4.29554e-07, val loss: 1.55092e-06, min loss: 4.26935e-07\n",
      "Epoch: 2224700, elapsed: 1.23e+01, train loss: 4.28484e-07, val loss: 1.53621e-06, min loss: 4.26935e-07\n",
      "Epoch: 2224800, elapsed: 1.21e+01, train loss: 4.59377e-07, val loss: 1.52179e-06, min loss: 4.26935e-07\n",
      "Epoch: 2224900, elapsed: 1.22e+01, train loss: 9.06510e-07, val loss: 1.66643e-06, min loss: 4.26935e-07\n",
      "Epoch: 2225000, elapsed: 1.24e+01, train loss: 5.23051e-07, val loss: 1.70840e-06, min loss: 4.26935e-07\n",
      "Epoch: 2225100, elapsed: 1.46e+01, train loss: 5.23500e-07, val loss: 1.65963e-06, min loss: 4.26935e-07\n",
      "Epoch: 2225200, elapsed: 1.22e+01, train loss: 1.66941e-06, val loss: 2.90792e-06, min loss: 4.26935e-07\n",
      "Epoch: 2225300, elapsed: 1.24e+01, train loss: 4.26711e-07, val loss: 1.54999e-06, min loss: 4.26711e-07\n",
      "Epoch: 2225400, elapsed: 1.23e+01, train loss: 5.44710e-07, val loss: 1.69246e-06, min loss: 4.26711e-07\n",
      "Epoch: 2225500, elapsed: 1.24e+01, train loss: 4.40435e-07, val loss: 1.60868e-06, min loss: 4.26711e-07\n",
      "Epoch: 2225600, elapsed: 1.22e+01, train loss: 6.94070e-07, val loss: 1.84898e-06, min loss: 4.26711e-07\n",
      "Epoch: 2225700, elapsed: 1.23e+01, train loss: 4.71083e-07, val loss: 1.60729e-06, min loss: 4.26711e-07\n",
      "Epoch: 2225800, elapsed: 1.22e+01, train loss: 4.26820e-07, val loss: 1.54422e-06, min loss: 4.26711e-07\n",
      "Epoch: 2225900, elapsed: 1.21e+01, train loss: 4.67753e-07, val loss: 1.60116e-06, min loss: 4.26711e-07\n",
      "Epoch: 2226000, elapsed: 1.22e+01, train loss: 8.12211e-07, val loss: 1.83207e-06, min loss: 4.26711e-07\n",
      "Epoch: 2226100, elapsed: 1.21e+01, train loss: 5.53198e-07, val loss: 1.66524e-06, min loss: 4.26711e-07\n",
      "Epoch: 2226200, elapsed: 1.21e+01, train loss: 7.15742e-07, val loss: 1.86599e-06, min loss: 4.26711e-07\n",
      "Epoch: 2226300, elapsed: 1.20e+01, train loss: 4.31292e-07, val loss: 1.53635e-06, min loss: 4.26711e-07\n",
      "Epoch: 2226400, elapsed: 1.21e+01, train loss: 4.26852e-07, val loss: 1.53930e-06, min loss: 4.26711e-07\n",
      "Epoch: 2226500, elapsed: 1.22e+01, train loss: 6.36326e-07, val loss: 1.81782e-06, min loss: 4.26711e-07\n",
      "Epoch: 2226600, elapsed: 1.21e+01, train loss: 4.26200e-07, val loss: 1.54856e-06, min loss: 4.26200e-07\n",
      "Epoch: 2226700, elapsed: 1.22e+01, train loss: 5.60068e-07, val loss: 1.53744e-06, min loss: 4.26200e-07\n",
      "Epoch: 2226800, elapsed: 1.83e+01, train loss: 4.26172e-07, val loss: 1.54789e-06, min loss: 4.26172e-07\n",
      "Epoch: 2226900, elapsed: 1.25e+01, train loss: 4.53883e-07, val loss: 1.58377e-06, min loss: 4.26172e-07\n",
      "Epoch: 2227000, elapsed: 1.25e+01, train loss: 5.28577e-07, val loss: 1.64330e-06, min loss: 4.26172e-07\n",
      "Epoch: 2227100, elapsed: 1.23e+01, train loss: 4.39020e-07, val loss: 1.58537e-06, min loss: 4.26172e-07\n",
      "Epoch: 2227200, elapsed: 1.26e+01, train loss: 4.32857e-07, val loss: 1.57535e-06, min loss: 4.26172e-07\n",
      "Epoch: 2227300, elapsed: 1.26e+01, train loss: 4.27615e-07, val loss: 1.53095e-06, min loss: 4.26172e-07\n",
      "Epoch: 2227400, elapsed: 1.25e+01, train loss: 4.29482e-07, val loss: 1.54288e-06, min loss: 4.26172e-07\n",
      "Epoch: 2227500, elapsed: 1.27e+01, train loss: 4.42361e-07, val loss: 1.57397e-06, min loss: 4.26172e-07\n",
      "Epoch: 2227600, elapsed: 1.25e+01, train loss: 5.05235e-07, val loss: 1.74494e-06, min loss: 4.26172e-07\n",
      "Epoch: 2227700, elapsed: 1.24e+01, train loss: 4.26121e-07, val loss: 1.54241e-06, min loss: 4.26121e-07\n",
      "Epoch: 2227800, elapsed: 1.22e+01, train loss: 4.33385e-07, val loss: 1.54899e-06, min loss: 4.26121e-07\n",
      "Epoch: 2227900, elapsed: 1.21e+01, train loss: 2.13555e-06, val loss: 2.91333e-06, min loss: 4.26121e-07\n",
      "Epoch: 2228000, elapsed: 1.24e+01, train loss: 1.37020e-06, val loss: 2.99361e-06, min loss: 4.26121e-07\n",
      "Epoch: 2228100, elapsed: 1.21e+01, train loss: 4.33929e-07, val loss: 1.55597e-06, min loss: 4.26121e-07\n",
      "Epoch: 2228200, elapsed: 1.21e+01, train loss: 4.26173e-07, val loss: 1.54092e-06, min loss: 4.26121e-07\n",
      "Epoch: 2228300, elapsed: 1.24e+01, train loss: 4.30435e-07, val loss: 1.56609e-06, min loss: 4.26121e-07\n",
      "Epoch: 2228400, elapsed: 1.23e+01, train loss: 2.92570e-06, val loss: 3.71663e-06, min loss: 4.26121e-07\n",
      "Epoch: 2228500, elapsed: 1.25e+01, train loss: 4.25797e-07, val loss: 1.54837e-06, min loss: 4.25797e-07\n",
      "Epoch: 2228600, elapsed: 1.22e+01, train loss: 4.30903e-07, val loss: 1.52959e-06, min loss: 4.25797e-07\n",
      "Epoch: 2228700, elapsed: 1.24e+01, train loss: 1.07589e-06, val loss: 2.25464e-06, min loss: 4.25797e-07\n",
      "Epoch: 2228800, elapsed: 1.20e+01, train loss: 4.66024e-07, val loss: 1.67158e-06, min loss: 4.25797e-07\n",
      "Epoch: 2228900, elapsed: 1.21e+01, train loss: 4.30376e-07, val loss: 1.55457e-06, min loss: 4.25797e-07\n",
      "Epoch: 2229000, elapsed: 1.20e+01, train loss: 4.26337e-07, val loss: 1.54791e-06, min loss: 4.25797e-07\n",
      "Epoch: 2229100, elapsed: 1.19e+01, train loss: 4.27993e-07, val loss: 1.56427e-06, min loss: 4.25797e-07\n",
      "Epoch: 2229200, elapsed: 1.21e+01, train loss: 4.27870e-07, val loss: 1.55155e-06, min loss: 4.25797e-07\n",
      "Epoch: 2229300, elapsed: 1.22e+01, train loss: 5.50688e-07, val loss: 1.82431e-06, min loss: 4.25797e-07\n",
      "Epoch: 2229400, elapsed: 1.22e+01, train loss: 4.31251e-07, val loss: 1.56057e-06, min loss: 4.25797e-07\n",
      "Epoch: 2229500, elapsed: 1.21e+01, train loss: 4.27605e-07, val loss: 1.55790e-06, min loss: 4.25797e-07\n",
      "Epoch: 2229600, elapsed: 1.21e+01, train loss: 4.45162e-07, val loss: 1.55293e-06, min loss: 4.25797e-07\n",
      "Epoch: 2229700, elapsed: 1.20e+01, train loss: 4.66933e-07, val loss: 1.53856e-06, min loss: 4.25797e-07\n",
      "Epoch: 2229800, elapsed: 1.21e+01, train loss: 4.25954e-07, val loss: 1.55118e-06, min loss: 4.25797e-07\n",
      "Epoch: 2229900, elapsed: 1.20e+01, train loss: 1.20917e-06, val loss: 2.12280e-06, min loss: 4.25797e-07\n",
      "Epoch: 2230000, elapsed: 1.23e+01, train loss: 6.12525e-07, val loss: 1.58500e-06, min loss: 4.25797e-07\n",
      "Epoch: 2230100, elapsed: 1.43e+01, train loss: 1.94258e-06, val loss: 2.93500e-06, min loss: 4.25797e-07\n",
      "Epoch: 2230200, elapsed: 1.20e+01, train loss: 6.93363e-07, val loss: 1.77144e-06, min loss: 4.25797e-07\n",
      "Epoch: 2230300, elapsed: 1.21e+01, train loss: 4.26787e-07, val loss: 1.56253e-06, min loss: 4.25797e-07\n",
      "Epoch: 2230400, elapsed: 1.25e+01, train loss: 4.26331e-07, val loss: 1.55216e-06, min loss: 4.25797e-07\n",
      "Epoch: 2230500, elapsed: 1.21e+01, train loss: 8.56872e-07, val loss: 1.95260e-06, min loss: 4.25797e-07\n",
      "Epoch: 2230600, elapsed: 1.21e+01, train loss: 4.25256e-07, val loss: 1.54906e-06, min loss: 4.25256e-07\n",
      "Epoch: 2230700, elapsed: 1.83e+01, train loss: 4.29067e-07, val loss: 1.54688e-06, min loss: 4.25256e-07\n",
      "Epoch: 2230800, elapsed: 1.26e+01, train loss: 4.27915e-07, val loss: 1.57126e-06, min loss: 4.25256e-07\n",
      "Epoch: 2230900, elapsed: 1.26e+01, train loss: 4.30630e-07, val loss: 1.55365e-06, min loss: 4.25256e-07\n",
      "Epoch: 2231000, elapsed: 1.22e+01, train loss: 7.46616e-07, val loss: 1.92760e-06, min loss: 4.25256e-07\n",
      "Epoch: 2231100, elapsed: 1.23e+01, train loss: 4.43159e-07, val loss: 1.54052e-06, min loss: 4.25256e-07\n",
      "Epoch: 2231200, elapsed: 1.21e+01, train loss: 4.46708e-07, val loss: 1.54500e-06, min loss: 4.25256e-07\n",
      "Epoch: 2231300, elapsed: 1.22e+01, train loss: 5.07691e-07, val loss: 1.60931e-06, min loss: 4.25256e-07\n",
      "Epoch: 2231400, elapsed: 1.24e+01, train loss: 5.00236e-07, val loss: 1.62952e-06, min loss: 4.25256e-07\n",
      "Epoch: 2231500, elapsed: 1.21e+01, train loss: 4.35226e-07, val loss: 1.57686e-06, min loss: 4.25256e-07\n",
      "Epoch: 2231600, elapsed: 1.20e+01, train loss: 4.34576e-07, val loss: 1.57393e-06, min loss: 4.25256e-07\n",
      "Epoch: 2231700, elapsed: 1.21e+01, train loss: 4.27093e-07, val loss: 1.56328e-06, min loss: 4.25256e-07\n",
      "Epoch: 2231800, elapsed: 1.23e+01, train loss: 4.26529e-07, val loss: 1.55310e-06, min loss: 4.25256e-07\n",
      "Epoch: 2231900, elapsed: 1.22e+01, train loss: 4.91811e-07, val loss: 1.60363e-06, min loss: 4.25256e-07\n",
      "Epoch: 2232000, elapsed: 1.20e+01, train loss: 1.63740e-06, val loss: 2.60466e-06, min loss: 4.25256e-07\n",
      "Epoch: 2232100, elapsed: 1.24e+01, train loss: 4.25523e-07, val loss: 1.54282e-06, min loss: 4.25256e-07\n",
      "Epoch: 2232200, elapsed: 1.22e+01, train loss: 4.35316e-07, val loss: 1.56871e-06, min loss: 4.25256e-07\n",
      "Epoch: 2232300, elapsed: 1.23e+01, train loss: 5.58833e-07, val loss: 1.65067e-06, min loss: 4.25256e-07\n",
      "Epoch: 2232400, elapsed: 1.22e+01, train loss: 2.37380e-06, val loss: 3.36962e-06, min loss: 4.25256e-07\n",
      "Epoch: 2232500, elapsed: 1.22e+01, train loss: 4.26962e-07, val loss: 1.52659e-06, min loss: 4.25256e-07\n",
      "Epoch: 2232600, elapsed: 1.22e+01, train loss: 4.25551e-07, val loss: 1.54870e-06, min loss: 4.25256e-07\n",
      "Epoch: 2232700, elapsed: 1.23e+01, train loss: 4.38517e-07, val loss: 1.55544e-06, min loss: 4.25256e-07\n",
      "Epoch: 2232800, elapsed: 1.23e+01, train loss: 5.05092e-07, val loss: 1.49643e-06, min loss: 4.25256e-07\n",
      "Epoch: 2232900, elapsed: 1.23e+01, train loss: 5.70807e-07, val loss: 1.69927e-06, min loss: 4.25256e-07\n",
      "Epoch: 2233000, elapsed: 1.22e+01, train loss: 1.69953e-06, val loss: 3.18175e-06, min loss: 4.25256e-07\n",
      "Epoch: 2233100, elapsed: 1.24e+01, train loss: 4.83928e-07, val loss: 1.63410e-06, min loss: 4.25256e-07\n",
      "Epoch: 2233200, elapsed: 1.27e+01, train loss: 4.26214e-07, val loss: 1.52782e-06, min loss: 4.25256e-07\n",
      "Epoch: 2233300, elapsed: 1.23e+01, train loss: 1.10142e-06, val loss: 2.07860e-06, min loss: 4.25256e-07\n",
      "Epoch: 2233400, elapsed: 1.23e+01, train loss: 4.24700e-07, val loss: 1.54805e-06, min loss: 4.24700e-07\n",
      "Epoch: 2233500, elapsed: 1.22e+01, train loss: 4.54225e-07, val loss: 1.53276e-06, min loss: 4.24700e-07\n",
      "Epoch: 2233600, elapsed: 1.19e+01, train loss: 4.52128e-07, val loss: 1.55308e-06, min loss: 4.24700e-07\n",
      "Epoch: 2233700, elapsed: 1.21e+01, train loss: 1.58491e-06, val loss: 2.59894e-06, min loss: 4.24700e-07\n",
      "Epoch: 2233800, elapsed: 1.17e+01, train loss: 4.49771e-07, val loss: 1.54049e-06, min loss: 4.24700e-07\n",
      "Epoch: 2233900, elapsed: 1.27e+01, train loss: 4.70479e-07, val loss: 1.69714e-06, min loss: 4.24700e-07\n",
      "Epoch: 2234000, elapsed: 1.31e+01, train loss: 4.32720e-07, val loss: 1.51838e-06, min loss: 4.24700e-07\n",
      "Epoch: 2234100, elapsed: 1.31e+01, train loss: 4.27570e-07, val loss: 1.54703e-06, min loss: 4.24700e-07\n",
      "Epoch: 2234200, elapsed: 1.36e+01, train loss: 4.60465e-07, val loss: 1.62716e-06, min loss: 4.24700e-07\n",
      "Epoch: 2234300, elapsed: 1.37e+01, train loss: 1.16467e-06, val loss: 1.64195e-06, min loss: 4.24700e-07\n",
      "Epoch: 2234400, elapsed: 1.40e+01, train loss: 3.08600e-06, val loss: 4.57076e-06, min loss: 4.24700e-07\n",
      "Epoch: 2234500, elapsed: 1.38e+01, train loss: 4.26132e-07, val loss: 1.54711e-06, min loss: 4.24700e-07\n",
      "Epoch: 2234600, elapsed: 1.39e+01, train loss: 5.08692e-07, val loss: 1.63538e-06, min loss: 4.24700e-07\n",
      "Epoch: 2234700, elapsed: 1.90e+01, train loss: 4.24307e-07, val loss: 1.54909e-06, min loss: 4.24307e-07\n",
      "Epoch: 2234800, elapsed: 1.27e+01, train loss: 4.40844e-07, val loss: 1.55054e-06, min loss: 4.24307e-07\n",
      "Epoch: 2234900, elapsed: 1.25e+01, train loss: 9.62523e-07, val loss: 2.02142e-06, min loss: 4.24307e-07\n",
      "Epoch: 2235000, elapsed: 1.25e+01, train loss: 5.31659e-07, val loss: 1.47984e-06, min loss: 4.24307e-07\n",
      "Epoch: 2235100, elapsed: 1.48e+01, train loss: 1.88690e-06, val loss: 2.47743e-06, min loss: 4.24307e-07\n",
      "Epoch: 2235200, elapsed: 1.22e+01, train loss: 7.05855e-07, val loss: 1.87555e-06, min loss: 4.24307e-07\n",
      "Epoch: 2235300, elapsed: 1.24e+01, train loss: 4.32210e-07, val loss: 1.54947e-06, min loss: 4.24307e-07\n",
      "Epoch: 2235400, elapsed: 1.23e+01, train loss: 4.70922e-07, val loss: 1.67039e-06, min loss: 4.24307e-07\n",
      "Epoch: 2235500, elapsed: 1.23e+01, train loss: 5.66256e-07, val loss: 1.84650e-06, min loss: 4.24307e-07\n",
      "Epoch: 2235600, elapsed: 1.25e+01, train loss: 8.00975e-07, val loss: 1.77688e-06, min loss: 4.24307e-07\n",
      "Epoch: 2235700, elapsed: 1.23e+01, train loss: 5.89763e-07, val loss: 1.74840e-06, min loss: 4.24307e-07\n",
      "Epoch: 2235800, elapsed: 1.24e+01, train loss: 4.77048e-07, val loss: 1.60992e-06, min loss: 4.24307e-07\n",
      "Epoch: 2235900, elapsed: 1.20e+01, train loss: 5.87920e-07, val loss: 1.68046e-06, min loss: 4.24307e-07\n",
      "Epoch: 2236000, elapsed: 1.20e+01, train loss: 4.26187e-07, val loss: 1.54962e-06, min loss: 4.24307e-07\n",
      "Epoch: 2236100, elapsed: 1.21e+01, train loss: 5.33918e-07, val loss: 1.77227e-06, min loss: 4.24307e-07\n",
      "Epoch: 2236200, elapsed: 1.22e+01, train loss: 1.17271e-06, val loss: 2.02201e-06, min loss: 4.24307e-07\n",
      "Epoch: 2236300, elapsed: 1.21e+01, train loss: 5.72197e-07, val loss: 1.78526e-06, min loss: 4.24307e-07\n",
      "Epoch: 2236400, elapsed: 1.19e+01, train loss: 4.24134e-07, val loss: 1.54656e-06, min loss: 4.24134e-07\n",
      "Epoch: 2236500, elapsed: 1.23e+01, train loss: 4.56638e-07, val loss: 1.56740e-06, min loss: 4.24134e-07\n",
      "Epoch: 2236600, elapsed: 1.27e+01, train loss: 4.97780e-07, val loss: 1.70857e-06, min loss: 4.24134e-07\n",
      "Epoch: 2236700, elapsed: 1.38e+01, train loss: 4.24119e-07, val loss: 1.54673e-06, min loss: 4.24119e-07\n",
      "Epoch: 2236800, elapsed: 1.39e+01, train loss: 4.52293e-07, val loss: 1.58681e-06, min loss: 4.24119e-07\n",
      "Epoch: 2236900, elapsed: 1.38e+01, train loss: 4.36099e-07, val loss: 1.56673e-06, min loss: 4.24119e-07\n",
      "Epoch: 2237000, elapsed: 1.33e+01, train loss: 4.26775e-07, val loss: 1.56569e-06, min loss: 4.24119e-07\n",
      "Epoch: 2237100, elapsed: 1.29e+01, train loss: 4.76936e-07, val loss: 1.61113e-06, min loss: 4.24119e-07\n",
      "Epoch: 2237200, elapsed: 1.22e+01, train loss: 4.23719e-07, val loss: 1.54878e-06, min loss: 4.23719e-07\n",
      "Epoch: 2237300, elapsed: 1.23e+01, train loss: 4.30152e-07, val loss: 1.54916e-06, min loss: 4.23719e-07\n",
      "Epoch: 2237400, elapsed: 1.21e+01, train loss: 7.47401e-07, val loss: 1.95861e-06, min loss: 4.23719e-07\n",
      "Epoch: 2237500, elapsed: 1.23e+01, train loss: 4.24146e-07, val loss: 1.54558e-06, min loss: 4.23719e-07\n",
      "Epoch: 2237600, elapsed: 1.21e+01, train loss: 4.24299e-07, val loss: 1.53455e-06, min loss: 4.23719e-07\n",
      "Epoch: 2237700, elapsed: 1.23e+01, train loss: 8.74914e-07, val loss: 2.35852e-06, min loss: 4.23719e-07\n",
      "Epoch: 2237800, elapsed: 1.19e+01, train loss: 4.34490e-07, val loss: 1.54297e-06, min loss: 4.23719e-07\n",
      "Epoch: 2237900, elapsed: 1.22e+01, train loss: 4.23978e-07, val loss: 1.54734e-06, min loss: 4.23719e-07\n",
      "Epoch: 2238000, elapsed: 1.19e+01, train loss: 4.25863e-07, val loss: 1.53817e-06, min loss: 4.23719e-07\n",
      "Epoch: 2238100, elapsed: 1.22e+01, train loss: 4.38929e-07, val loss: 1.52384e-06, min loss: 4.23719e-07\n",
      "Epoch: 2238200, elapsed: 1.19e+01, train loss: 4.24419e-07, val loss: 1.55219e-06, min loss: 4.23719e-07\n",
      "Epoch: 2238300, elapsed: 1.20e+01, train loss: 4.84964e-07, val loss: 1.66385e-06, min loss: 4.23719e-07\n",
      "Epoch: 2238400, elapsed: 1.22e+01, train loss: 4.72987e-07, val loss: 1.58098e-06, min loss: 4.23719e-07\n",
      "Epoch: 2238500, elapsed: 1.21e+01, train loss: 4.26812e-07, val loss: 1.56623e-06, min loss: 4.23719e-07\n",
      "Epoch: 2238600, elapsed: 1.82e+01, train loss: 4.25759e-07, val loss: 1.52186e-06, min loss: 4.23719e-07\n",
      "Epoch: 2238700, elapsed: 1.26e+01, train loss: 4.23938e-07, val loss: 1.53711e-06, min loss: 4.23719e-07\n",
      "Epoch: 2238800, elapsed: 1.25e+01, train loss: 4.23341e-07, val loss: 1.54752e-06, min loss: 4.23341e-07\n",
      "Epoch: 2238900, elapsed: 1.24e+01, train loss: 4.26860e-07, val loss: 1.54614e-06, min loss: 4.23341e-07\n",
      "Epoch: 2239000, elapsed: 1.24e+01, train loss: 4.23338e-07, val loss: 1.54841e-06, min loss: 4.23338e-07\n",
      "Epoch: 2239100, elapsed: 1.23e+01, train loss: 5.32824e-07, val loss: 1.63481e-06, min loss: 4.23338e-07\n",
      "Epoch: 2239200, elapsed: 1.25e+01, train loss: 4.24059e-07, val loss: 1.55422e-06, min loss: 4.23338e-07\n",
      "Epoch: 2239300, elapsed: 1.36e+01, train loss: 4.24727e-07, val loss: 1.55259e-06, min loss: 4.23338e-07\n",
      "Epoch: 2239400, elapsed: 1.38e+01, train loss: 4.34559e-07, val loss: 1.55900e-06, min loss: 4.23338e-07\n",
      "Epoch: 2239500, elapsed: 1.36e+01, train loss: 4.37097e-07, val loss: 1.56570e-06, min loss: 4.23338e-07\n",
      "Epoch: 2239600, elapsed: 1.36e+01, train loss: 4.26460e-07, val loss: 1.50885e-06, min loss: 4.23338e-07\n",
      "Epoch: 2239700, elapsed: 1.35e+01, train loss: 4.51132e-07, val loss: 1.62291e-06, min loss: 4.23338e-07\n",
      "Epoch: 2239800, elapsed: 1.34e+01, train loss: 5.23736e-07, val loss: 1.58592e-06, min loss: 4.23338e-07\n",
      "Epoch: 2239900, elapsed: 1.34e+01, train loss: 4.59528e-07, val loss: 1.70041e-06, min loss: 4.23338e-07\n",
      "Epoch: 2240000, elapsed: 1.33e+01, train loss: 4.23232e-07, val loss: 1.54544e-06, min loss: 4.23232e-07\n",
      "Epoch: 2240100, elapsed: 1.61e+01, train loss: 4.29204e-07, val loss: 1.55106e-06, min loss: 4.23232e-07\n",
      "Epoch: 2240200, elapsed: 1.40e+01, train loss: 1.01141e-06, val loss: 2.19026e-06, min loss: 4.23232e-07\n",
      "Epoch: 2240300, elapsed: 1.37e+01, train loss: 5.35267e-07, val loss: 1.67081e-06, min loss: 4.23232e-07\n",
      "Epoch: 2240400, elapsed: 1.36e+01, train loss: 4.23964e-07, val loss: 1.54664e-06, min loss: 4.23232e-07\n",
      "Epoch: 2240500, elapsed: 1.37e+01, train loss: 6.48040e-07, val loss: 1.65442e-06, min loss: 4.23232e-07\n",
      "Epoch: 2240600, elapsed: 1.36e+01, train loss: 5.44384e-07, val loss: 1.72758e-06, min loss: 4.23232e-07\n",
      "Epoch: 2240700, elapsed: 1.31e+01, train loss: 4.96910e-07, val loss: 1.55976e-06, min loss: 4.23232e-07\n",
      "Epoch: 2240800, elapsed: 1.36e+01, train loss: 4.26065e-07, val loss: 1.56399e-06, min loss: 4.23232e-07\n",
      "Epoch: 2240900, elapsed: 1.32e+01, train loss: 4.37904e-07, val loss: 1.52095e-06, min loss: 4.23232e-07\n",
      "Epoch: 2241000, elapsed: 1.39e+01, train loss: 4.23298e-07, val loss: 1.55042e-06, min loss: 4.23232e-07\n",
      "Epoch: 2241100, elapsed: 1.39e+01, train loss: 4.40097e-07, val loss: 1.56865e-06, min loss: 4.23232e-07\n",
      "Epoch: 2241200, elapsed: 1.38e+01, train loss: 6.54361e-07, val loss: 1.63588e-06, min loss: 4.23232e-07\n",
      "Epoch: 2241300, elapsed: 1.36e+01, train loss: 9.60088e-07, val loss: 1.81795e-06, min loss: 4.23232e-07\n",
      "Epoch: 2241400, elapsed: 1.37e+01, train loss: 7.87282e-07, val loss: 2.15435e-06, min loss: 4.23232e-07\n",
      "Epoch: 2241500, elapsed: 1.42e+01, train loss: 8.99243e-07, val loss: 1.85202e-06, min loss: 4.23232e-07\n",
      "Epoch: 2241600, elapsed: 1.34e+01, train loss: 4.25473e-07, val loss: 1.55997e-06, min loss: 4.23232e-07\n",
      "Epoch: 2241700, elapsed: 1.32e+01, train loss: 4.23245e-07, val loss: 1.55114e-06, min loss: 4.23232e-07\n",
      "Epoch: 2241800, elapsed: 1.36e+01, train loss: 4.48115e-07, val loss: 1.55826e-06, min loss: 4.23232e-07\n",
      "Epoch: 2241900, elapsed: 1.35e+01, train loss: 4.42750e-07, val loss: 1.53688e-06, min loss: 4.23232e-07\n",
      "Epoch: 2242000, elapsed: 1.34e+01, train loss: 4.35521e-07, val loss: 1.53484e-06, min loss: 4.23232e-07\n",
      "Epoch: 2242100, elapsed: 1.32e+01, train loss: 4.25229e-07, val loss: 1.53788e-06, min loss: 4.23232e-07\n",
      "Epoch: 2242200, elapsed: 1.35e+01, train loss: 4.49866e-07, val loss: 1.71820e-06, min loss: 4.23232e-07\n",
      "Epoch: 2242300, elapsed: 1.35e+01, train loss: 6.05892e-07, val loss: 1.62026e-06, min loss: 4.23232e-07\n",
      "Epoch: 2242400, elapsed: 1.36e+01, train loss: 4.63322e-07, val loss: 1.54727e-06, min loss: 4.23232e-07\n",
      "Epoch: 2242500, elapsed: 1.36e+01, train loss: 4.26215e-07, val loss: 1.56494e-06, min loss: 4.23232e-07\n",
      "Epoch: 2242600, elapsed: 1.99e+01, train loss: 4.38280e-07, val loss: 1.58559e-06, min loss: 4.23232e-07\n",
      "Epoch: 2242700, elapsed: 1.38e+01, train loss: 4.64748e-07, val loss: 1.64068e-06, min loss: 4.23232e-07\n",
      "Epoch: 2242800, elapsed: 1.25e+01, train loss: 4.22454e-07, val loss: 1.54997e-06, min loss: 4.22454e-07\n",
      "Epoch: 2242900, elapsed: 1.23e+01, train loss: 5.74293e-07, val loss: 1.80716e-06, min loss: 4.22454e-07\n",
      "Epoch: 2243000, elapsed: 1.22e+01, train loss: 4.42813e-07, val loss: 1.60557e-06, min loss: 4.22454e-07\n",
      "Epoch: 2243100, elapsed: 1.24e+01, train loss: 4.27772e-07, val loss: 1.54314e-06, min loss: 4.22454e-07\n",
      "Epoch: 2243200, elapsed: 1.23e+01, train loss: 4.50321e-07, val loss: 1.56746e-06, min loss: 4.22454e-07\n",
      "Epoch: 2243300, elapsed: 1.24e+01, train loss: 5.50521e-07, val loss: 1.68133e-06, min loss: 4.22454e-07\n",
      "Epoch: 2243400, elapsed: 1.22e+01, train loss: 9.01496e-07, val loss: 2.36263e-06, min loss: 4.22454e-07\n",
      "Epoch: 2243500, elapsed: 1.25e+01, train loss: 1.37281e-06, val loss: 2.75476e-06, min loss: 4.22454e-07\n",
      "Epoch: 2243600, elapsed: 1.19e+01, train loss: 4.58534e-07, val loss: 1.64061e-06, min loss: 4.22454e-07\n",
      "Epoch: 2243700, elapsed: 1.25e+01, train loss: 4.28055e-07, val loss: 1.55889e-06, min loss: 4.22454e-07\n",
      "Epoch: 2243800, elapsed: 1.25e+01, train loss: 4.22231e-07, val loss: 1.54775e-06, min loss: 4.22231e-07\n",
      "Epoch: 2243900, elapsed: 1.22e+01, train loss: 4.22866e-07, val loss: 1.54909e-06, min loss: 4.22231e-07\n",
      "Epoch: 2244000, elapsed: 1.23e+01, train loss: 7.76596e-07, val loss: 2.07731e-06, min loss: 4.22231e-07\n",
      "Epoch: 2244100, elapsed: 1.23e+01, train loss: 4.22353e-07, val loss: 1.55016e-06, min loss: 4.22231e-07\n",
      "Epoch: 2244200, elapsed: 1.23e+01, train loss: 4.45784e-07, val loss: 1.58638e-06, min loss: 4.22231e-07\n",
      "Epoch: 2244300, elapsed: 1.23e+01, train loss: 4.24029e-07, val loss: 1.53089e-06, min loss: 4.22231e-07\n",
      "Epoch: 2244400, elapsed: 1.23e+01, train loss: 4.23054e-07, val loss: 1.54968e-06, min loss: 4.22231e-07\n",
      "Epoch: 2244500, elapsed: 1.26e+01, train loss: 4.96936e-07, val loss: 1.60481e-06, min loss: 4.22231e-07\n",
      "Epoch: 2244600, elapsed: 1.22e+01, train loss: 1.16407e-06, val loss: 2.37718e-06, min loss: 4.22231e-07\n",
      "Epoch: 2244700, elapsed: 1.22e+01, train loss: 4.92361e-07, val loss: 1.56602e-06, min loss: 4.22231e-07\n",
      "Epoch: 2244800, elapsed: 1.23e+01, train loss: 4.36532e-07, val loss: 1.61934e-06, min loss: 4.22231e-07\n",
      "Epoch: 2244900, elapsed: 1.22e+01, train loss: 4.55062e-07, val loss: 1.62370e-06, min loss: 4.22231e-07\n",
      "Epoch: 2245000, elapsed: 1.21e+01, train loss: 4.56821e-07, val loss: 1.65546e-06, min loss: 4.22231e-07\n",
      "Epoch: 2245100, elapsed: 1.48e+01, train loss: 4.22331e-07, val loss: 1.54443e-06, min loss: 4.22231e-07\n",
      "Epoch: 2245200, elapsed: 1.22e+01, train loss: 4.40686e-07, val loss: 1.49740e-06, min loss: 4.22231e-07\n",
      "Epoch: 2245300, elapsed: 1.20e+01, train loss: 2.27160e-06, val loss: 3.05513e-06, min loss: 4.22231e-07\n",
      "Epoch: 2245400, elapsed: 1.19e+01, train loss: 5.12402e-07, val loss: 1.58694e-06, min loss: 4.22231e-07\n",
      "Epoch: 2245500, elapsed: 1.22e+01, train loss: 4.31501e-07, val loss: 1.56580e-06, min loss: 4.22231e-07\n",
      "Epoch: 2245600, elapsed: 1.22e+01, train loss: 4.23596e-07, val loss: 1.54220e-06, min loss: 4.22231e-07\n",
      "Epoch: 2245700, elapsed: 1.20e+01, train loss: 4.52835e-07, val loss: 1.61542e-06, min loss: 4.22231e-07\n",
      "Epoch: 2245800, elapsed: 1.19e+01, train loss: 1.13441e-06, val loss: 2.61110e-06, min loss: 4.22231e-07\n",
      "Epoch: 2245900, elapsed: 1.20e+01, train loss: 2.37499e-06, val loss: 3.10772e-06, min loss: 4.22231e-07\n",
      "Epoch: 2246000, elapsed: 1.22e+01, train loss: 4.22199e-07, val loss: 1.55070e-06, min loss: 4.22199e-07\n",
      "Epoch: 2246100, elapsed: 1.21e+01, train loss: 4.23919e-07, val loss: 1.56797e-06, min loss: 4.22199e-07\n",
      "Epoch: 2246200, elapsed: 1.22e+01, train loss: 2.78710e-06, val loss: 4.44276e-06, min loss: 4.22199e-07\n",
      "Epoch: 2246300, elapsed: 1.20e+01, train loss: 5.52321e-07, val loss: 1.71839e-06, min loss: 4.22199e-07\n",
      "Epoch: 2246400, elapsed: 1.21e+01, train loss: 6.82831e-07, val loss: 1.98212e-06, min loss: 4.22199e-07\n",
      "Epoch: 2246500, elapsed: 1.20e+01, train loss: 2.31925e-06, val loss: 2.26124e-06, min loss: 4.22199e-07\n",
      "Epoch: 2246600, elapsed: 1.86e+01, train loss: 5.13948e-07, val loss: 1.49208e-06, min loss: 4.22199e-07\n",
      "Epoch: 2246700, elapsed: 1.23e+01, train loss: 4.28465e-07, val loss: 1.52521e-06, min loss: 4.22199e-07\n",
      "Epoch: 2246800, elapsed: 1.27e+01, train loss: 4.23324e-07, val loss: 1.55387e-06, min loss: 4.22199e-07\n",
      "Epoch: 2246900, elapsed: 1.24e+01, train loss: 4.33999e-07, val loss: 1.56088e-06, min loss: 4.22199e-07\n",
      "Epoch: 2247000, elapsed: 1.26e+01, train loss: 6.79331e-07, val loss: 1.85740e-06, min loss: 4.22199e-07\n",
      "Epoch: 2247100, elapsed: 1.24e+01, train loss: 1.48186e-06, val loss: 2.60628e-06, min loss: 4.22199e-07\n",
      "Epoch: 2247200, elapsed: 1.23e+01, train loss: 4.47461e-07, val loss: 1.55195e-06, min loss: 4.22199e-07\n",
      "Epoch: 2247300, elapsed: 1.23e+01, train loss: 4.22056e-07, val loss: 1.56225e-06, min loss: 4.22056e-07\n",
      "Epoch: 2247400, elapsed: 1.24e+01, train loss: 4.22640e-07, val loss: 1.55659e-06, min loss: 4.22056e-07\n",
      "Epoch: 2247500, elapsed: 1.24e+01, train loss: 4.24675e-07, val loss: 1.54265e-06, min loss: 4.22056e-07\n",
      "Epoch: 2247600, elapsed: 1.22e+01, train loss: 4.62059e-07, val loss: 1.61458e-06, min loss: 4.22056e-07\n",
      "Epoch: 2247700, elapsed: 1.25e+01, train loss: 4.84723e-07, val loss: 1.61482e-06, min loss: 4.22056e-07\n",
      "Epoch: 2247800, elapsed: 1.24e+01, train loss: 4.21422e-07, val loss: 1.55162e-06, min loss: 4.21422e-07\n",
      "Epoch: 2247900, elapsed: 1.22e+01, train loss: 4.28762e-07, val loss: 1.50523e-06, min loss: 4.21422e-07\n",
      "Epoch: 2248000, elapsed: 1.24e+01, train loss: 4.85928e-07, val loss: 1.66108e-06, min loss: 4.21422e-07\n",
      "Epoch: 2248100, elapsed: 1.22e+01, train loss: 4.22270e-07, val loss: 1.55910e-06, min loss: 4.21422e-07\n",
      "Epoch: 2248200, elapsed: 1.25e+01, train loss: 4.95082e-07, val loss: 1.56836e-06, min loss: 4.21422e-07\n",
      "Epoch: 2248300, elapsed: 1.22e+01, train loss: 1.05871e-06, val loss: 2.17825e-06, min loss: 4.21422e-07\n",
      "Epoch: 2248400, elapsed: 1.22e+01, train loss: 4.29450e-07, val loss: 1.54829e-06, min loss: 4.21422e-07\n",
      "Epoch: 2248500, elapsed: 1.21e+01, train loss: 4.30270e-07, val loss: 1.56627e-06, min loss: 4.21422e-07\n",
      "Epoch: 2248600, elapsed: 1.23e+01, train loss: 4.22390e-07, val loss: 1.54622e-06, min loss: 4.21422e-07\n",
      "Epoch: 2248700, elapsed: 1.22e+01, train loss: 4.23086e-07, val loss: 1.56869e-06, min loss: 4.21422e-07\n",
      "Epoch: 2248800, elapsed: 1.24e+01, train loss: 6.10180e-07, val loss: 1.56383e-06, min loss: 4.21422e-07\n",
      "Epoch: 2248900, elapsed: 1.22e+01, train loss: 4.21043e-07, val loss: 1.54775e-06, min loss: 4.21043e-07\n",
      "Epoch: 2249000, elapsed: 1.22e+01, train loss: 4.29431e-07, val loss: 1.52354e-06, min loss: 4.21043e-07\n",
      "Epoch: 2249100, elapsed: 1.21e+01, train loss: 1.90114e-06, val loss: 2.15622e-06, min loss: 4.21043e-07\n",
      "Epoch: 2249200, elapsed: 1.23e+01, train loss: 4.20972e-07, val loss: 1.54717e-06, min loss: 4.20972e-07\n",
      "Epoch: 2249300, elapsed: 1.22e+01, train loss: 4.43069e-07, val loss: 1.61953e-06, min loss: 4.20972e-07\n",
      "Epoch: 2249400, elapsed: 1.24e+01, train loss: 4.21076e-07, val loss: 1.54258e-06, min loss: 4.20972e-07\n",
      "Epoch: 2249500, elapsed: 1.22e+01, train loss: 4.23144e-07, val loss: 1.55673e-06, min loss: 4.20972e-07\n",
      "Epoch: 2249600, elapsed: 1.23e+01, train loss: 4.36530e-07, val loss: 1.61711e-06, min loss: 4.20972e-07\n",
      "Epoch: 2249700, elapsed: 1.21e+01, train loss: 4.21640e-07, val loss: 1.54701e-06, min loss: 4.20972e-07\n",
      "Epoch: 2249800, elapsed: 1.21e+01, train loss: 4.38581e-07, val loss: 1.52598e-06, min loss: 4.20972e-07\n",
      "Epoch: 2249900, elapsed: 1.24e+01, train loss: 5.10463e-07, val loss: 1.62033e-06, min loss: 4.20972e-07\n",
      "Epoch: 2250000, elapsed: 1.20e+01, train loss: 1.16125e-06, val loss: 2.27094e-06, min loss: 4.20972e-07\n",
      "Epoch: 2250100, elapsed: 1.45e+01, train loss: 4.24602e-07, val loss: 1.54859e-06, min loss: 4.20972e-07\n",
      "Epoch: 2250200, elapsed: 1.22e+01, train loss: 4.21866e-07, val loss: 1.55190e-06, min loss: 4.20972e-07\n",
      "Epoch: 2250300, elapsed: 1.22e+01, train loss: 4.39073e-07, val loss: 1.50725e-06, min loss: 4.20972e-07\n",
      "Epoch: 2250400, elapsed: 1.23e+01, train loss: 4.48960e-07, val loss: 1.57216e-06, min loss: 4.20972e-07\n",
      "Epoch: 2250500, elapsed: 1.79e+01, train loss: 4.21316e-07, val loss: 1.55132e-06, min loss: 4.20972e-07\n",
      "Epoch: 2250600, elapsed: 1.25e+01, train loss: 4.22295e-07, val loss: 1.54764e-06, min loss: 4.20972e-07\n",
      "Epoch: 2250700, elapsed: 1.25e+01, train loss: 1.06460e-06, val loss: 1.76181e-06, min loss: 4.20972e-07\n",
      "Epoch: 2250800, elapsed: 1.24e+01, train loss: 4.20592e-07, val loss: 1.54677e-06, min loss: 4.20592e-07\n",
      "Epoch: 2250900, elapsed: 1.25e+01, train loss: 4.92553e-07, val loss: 1.56262e-06, min loss: 4.20592e-07\n",
      "Epoch: 2251000, elapsed: 1.25e+01, train loss: 4.20550e-07, val loss: 1.54714e-06, min loss: 4.20550e-07\n",
      "Epoch: 2251100, elapsed: 1.26e+01, train loss: 4.29762e-07, val loss: 1.56917e-06, min loss: 4.20550e-07\n",
      "Epoch: 2251200, elapsed: 1.22e+01, train loss: 4.20463e-07, val loss: 1.54744e-06, min loss: 4.20463e-07\n",
      "Epoch: 2251300, elapsed: 1.23e+01, train loss: 4.45986e-07, val loss: 1.61936e-06, min loss: 4.20463e-07\n",
      "Epoch: 2251400, elapsed: 1.23e+01, train loss: 4.20516e-07, val loss: 1.54599e-06, min loss: 4.20463e-07\n",
      "Epoch: 2251500, elapsed: 1.23e+01, train loss: 7.34751e-06, val loss: 7.23076e-06, min loss: 4.20463e-07\n",
      "Epoch: 2251600, elapsed: 1.22e+01, train loss: 4.20504e-07, val loss: 1.54310e-06, min loss: 4.20463e-07\n",
      "Epoch: 2251700, elapsed: 1.23e+01, train loss: 4.20620e-07, val loss: 1.55243e-06, min loss: 4.20463e-07\n",
      "Epoch: 2251800, elapsed: 1.23e+01, train loss: 4.29953e-07, val loss: 1.59293e-06, min loss: 4.20463e-07\n",
      "Epoch: 2251900, elapsed: 1.22e+01, train loss: 8.26155e-07, val loss: 1.85561e-06, min loss: 4.20463e-07\n",
      "Epoch: 2252000, elapsed: 1.23e+01, train loss: 9.04272e-07, val loss: 2.35544e-06, min loss: 4.20463e-07\n",
      "Epoch: 2252100, elapsed: 1.24e+01, train loss: 4.20537e-07, val loss: 1.55398e-06, min loss: 4.20463e-07\n",
      "Epoch: 2252200, elapsed: 1.24e+01, train loss: 5.17993e-07, val loss: 1.56344e-06, min loss: 4.20463e-07\n",
      "Epoch: 2252300, elapsed: 1.21e+01, train loss: 2.24602e-06, val loss: 2.59681e-06, min loss: 4.20463e-07\n",
      "Epoch: 2252400, elapsed: 1.22e+01, train loss: 4.20281e-07, val loss: 1.54491e-06, min loss: 4.20281e-07\n",
      "Epoch: 2252500, elapsed: 1.37e+01, train loss: 4.24516e-07, val loss: 1.53748e-06, min loss: 4.20281e-07\n",
      "Epoch: 2252600, elapsed: 1.36e+01, train loss: 6.60115e-07, val loss: 1.79365e-06, min loss: 4.20281e-07\n",
      "Epoch: 2252700, elapsed: 1.35e+01, train loss: 4.20578e-07, val loss: 1.54157e-06, min loss: 4.20281e-07\n",
      "Epoch: 2252800, elapsed: 1.33e+01, train loss: 4.34641e-07, val loss: 1.55207e-06, min loss: 4.20281e-07\n",
      "Epoch: 2252900, elapsed: 1.35e+01, train loss: 4.20282e-07, val loss: 1.54531e-06, min loss: 4.20281e-07\n",
      "Epoch: 2253000, elapsed: 1.32e+01, train loss: 4.22677e-07, val loss: 1.56463e-06, min loss: 4.20281e-07\n",
      "Epoch: 2253100, elapsed: 1.37e+01, train loss: 4.80286e-07, val loss: 1.69452e-06, min loss: 4.20281e-07\n",
      "Epoch: 2253200, elapsed: 1.34e+01, train loss: 4.20230e-07, val loss: 1.54889e-06, min loss: 4.20230e-07\n",
      "Epoch: 2253300, elapsed: 1.33e+01, train loss: 6.07867e-07, val loss: 1.65491e-06, min loss: 4.20230e-07\n",
      "Epoch: 2253400, elapsed: 1.36e+01, train loss: 5.37956e-07, val loss: 1.73308e-06, min loss: 4.20230e-07\n",
      "Epoch: 2253500, elapsed: 1.32e+01, train loss: 4.68887e-07, val loss: 1.53749e-06, min loss: 4.20230e-07\n",
      "Epoch: 2253600, elapsed: 1.37e+01, train loss: 4.41075e-07, val loss: 1.60553e-06, min loss: 4.20230e-07\n",
      "Epoch: 2253700, elapsed: 1.33e+01, train loss: 4.28075e-07, val loss: 1.54749e-06, min loss: 4.20230e-07\n",
      "Epoch: 2253800, elapsed: 1.38e+01, train loss: 4.21298e-07, val loss: 1.54914e-06, min loss: 4.20230e-07\n",
      "Epoch: 2253900, elapsed: 1.33e+01, train loss: 4.70288e-07, val loss: 1.63541e-06, min loss: 4.20230e-07\n",
      "Epoch: 2254000, elapsed: 1.37e+01, train loss: 4.29825e-07, val loss: 1.54646e-06, min loss: 4.20230e-07\n",
      "Epoch: 2254100, elapsed: 1.36e+01, train loss: 4.37997e-07, val loss: 1.55571e-06, min loss: 4.20230e-07\n",
      "Epoch: 2254200, elapsed: 1.37e+01, train loss: 4.78129e-07, val loss: 1.64015e-06, min loss: 4.20230e-07\n",
      "Epoch: 2254300, elapsed: 1.32e+01, train loss: 4.91772e-07, val loss: 1.64614e-06, min loss: 4.20230e-07\n",
      "Epoch: 2254400, elapsed: 1.37e+01, train loss: 9.31501e-07, val loss: 1.78124e-06, min loss: 4.20230e-07\n",
      "Epoch: 2254500, elapsed: 1.84e+01, train loss: 1.14205e-06, val loss: 2.33472e-06, min loss: 4.20230e-07\n",
      "Epoch: 2254600, elapsed: 1.30e+01, train loss: 7.18464e-07, val loss: 2.06328e-06, min loss: 4.20230e-07\n",
      "Epoch: 2254700, elapsed: 1.24e+01, train loss: 5.42606e-07, val loss: 1.64506e-06, min loss: 4.20230e-07\n",
      "Epoch: 2254800, elapsed: 1.22e+01, train loss: 4.21330e-07, val loss: 1.54421e-06, min loss: 4.20230e-07\n",
      "Epoch: 2254900, elapsed: 1.24e+01, train loss: 4.72539e-07, val loss: 1.54052e-06, min loss: 4.20230e-07\n",
      "Epoch: 2255000, elapsed: 1.26e+01, train loss: 6.69103e-07, val loss: 1.62120e-06, min loss: 4.20230e-07\n",
      "Epoch: 2255100, elapsed: 1.48e+01, train loss: 4.48790e-07, val loss: 1.56287e-06, min loss: 4.20230e-07\n",
      "Epoch: 2255200, elapsed: 1.23e+01, train loss: 1.16059e-06, val loss: 1.75227e-06, min loss: 4.20230e-07\n",
      "Epoch: 2255300, elapsed: 1.25e+01, train loss: 4.66237e-07, val loss: 1.58230e-06, min loss: 4.20230e-07\n",
      "Epoch: 2255400, elapsed: 1.25e+01, train loss: 4.20197e-07, val loss: 1.54107e-06, min loss: 4.20197e-07\n",
      "Epoch: 2255500, elapsed: 1.26e+01, train loss: 4.23947e-07, val loss: 1.55566e-06, min loss: 4.20197e-07\n",
      "Epoch: 2255600, elapsed: 1.22e+01, train loss: 4.23040e-07, val loss: 1.55232e-06, min loss: 4.20197e-07\n",
      "Epoch: 2255700, elapsed: 1.24e+01, train loss: 4.20605e-07, val loss: 1.54562e-06, min loss: 4.20197e-07\n",
      "Epoch: 2255800, elapsed: 1.24e+01, train loss: 6.22639e-07, val loss: 1.61506e-06, min loss: 4.20197e-07\n",
      "Epoch: 2255900, elapsed: 1.24e+01, train loss: 4.22148e-07, val loss: 1.52725e-06, min loss: 4.20197e-07\n",
      "Epoch: 2256000, elapsed: 1.22e+01, train loss: 1.16981e-06, val loss: 2.57040e-06, min loss: 4.20197e-07\n",
      "Epoch: 2256100, elapsed: 1.24e+01, train loss: 4.19479e-07, val loss: 1.54250e-06, min loss: 4.19479e-07\n",
      "Epoch: 2256200, elapsed: 1.33e+01, train loss: 4.27823e-07, val loss: 1.55924e-06, min loss: 4.19479e-07\n",
      "Epoch: 2256300, elapsed: 1.36e+01, train loss: 4.29582e-07, val loss: 1.60024e-06, min loss: 4.19479e-07\n",
      "Epoch: 2256400, elapsed: 1.35e+01, train loss: 4.27019e-07, val loss: 1.59083e-06, min loss: 4.19479e-07\n",
      "Epoch: 2256500, elapsed: 1.43e+01, train loss: 4.20117e-07, val loss: 1.54727e-06, min loss: 4.19479e-07\n",
      "Epoch: 2256600, elapsed: 1.37e+01, train loss: 4.30956e-07, val loss: 1.52411e-06, min loss: 4.19479e-07\n",
      "Epoch: 2256700, elapsed: 1.37e+01, train loss: 4.19253e-07, val loss: 1.54502e-06, min loss: 4.19253e-07\n",
      "Epoch: 2256800, elapsed: 1.39e+01, train loss: 4.21423e-07, val loss: 1.53477e-06, min loss: 4.19253e-07\n",
      "Epoch: 2256900, elapsed: 1.38e+01, train loss: 4.40901e-07, val loss: 1.62165e-06, min loss: 4.19253e-07\n",
      "Epoch: 2257000, elapsed: 1.33e+01, train loss: 4.22591e-07, val loss: 1.54326e-06, min loss: 4.19253e-07\n",
      "Epoch: 2257100, elapsed: 1.37e+01, train loss: 1.18795e-06, val loss: 2.76903e-06, min loss: 4.19253e-07\n",
      "Epoch: 2257200, elapsed: 1.36e+01, train loss: 7.27323e-07, val loss: 1.78247e-06, min loss: 4.19253e-07\n",
      "Epoch: 2257300, elapsed: 1.39e+01, train loss: 4.56659e-07, val loss: 1.58518e-06, min loss: 4.19253e-07\n",
      "Epoch: 2257400, elapsed: 1.36e+01, train loss: 9.25507e-07, val loss: 1.82774e-06, min loss: 4.19253e-07\n",
      "Epoch: 2257500, elapsed: 1.40e+01, train loss: 4.20407e-07, val loss: 1.54100e-06, min loss: 4.19253e-07\n",
      "Epoch: 2257600, elapsed: 1.35e+01, train loss: 4.37309e-07, val loss: 1.54107e-06, min loss: 4.19253e-07\n",
      "Epoch: 2257700, elapsed: 1.33e+01, train loss: 1.01107e-06, val loss: 2.84131e-06, min loss: 4.19253e-07\n",
      "Epoch: 2257800, elapsed: 1.35e+01, train loss: 4.19030e-07, val loss: 1.54693e-06, min loss: 4.19030e-07\n",
      "Epoch: 2257900, elapsed: 1.34e+01, train loss: 6.14870e-07, val loss: 1.83109e-06, min loss: 4.19030e-07\n",
      "Epoch: 2258000, elapsed: 1.32e+01, train loss: 4.20335e-07, val loss: 1.54069e-06, min loss: 4.19030e-07\n",
      "Epoch: 2258100, elapsed: 1.24e+01, train loss: 4.19793e-07, val loss: 1.55852e-06, min loss: 4.19030e-07\n",
      "Epoch: 2258200, elapsed: 1.23e+01, train loss: 4.32449e-07, val loss: 1.61822e-06, min loss: 4.19030e-07\n",
      "Epoch: 2258300, elapsed: 1.22e+01, train loss: 4.39667e-07, val loss: 1.60636e-06, min loss: 4.19030e-07\n",
      "Epoch: 2258400, elapsed: 1.23e+01, train loss: 4.42684e-07, val loss: 1.54745e-06, min loss: 4.19030e-07\n",
      "Epoch: 2258500, elapsed: 1.82e+01, train loss: 6.27055e-07, val loss: 1.57240e-06, min loss: 4.19030e-07\n",
      "Epoch: 2258600, elapsed: 1.28e+01, train loss: 1.72972e-06, val loss: 2.50974e-06, min loss: 4.19030e-07\n",
      "Epoch: 2258700, elapsed: 1.28e+01, train loss: 6.18103e-07, val loss: 1.88503e-06, min loss: 4.19030e-07\n",
      "Epoch: 2258800, elapsed: 1.26e+01, train loss: 8.36049e-07, val loss: 1.86456e-06, min loss: 4.19030e-07\n",
      "Epoch: 2258900, elapsed: 1.26e+01, train loss: 4.74094e-07, val loss: 1.65675e-06, min loss: 4.19030e-07\n",
      "Epoch: 2259000, elapsed: 1.25e+01, train loss: 8.33690e-07, val loss: 1.80830e-06, min loss: 4.19030e-07\n",
      "Epoch: 2259100, elapsed: 1.22e+01, train loss: 1.59183e-06, val loss: 2.83876e-06, min loss: 4.19030e-07\n",
      "Epoch: 2259200, elapsed: 1.24e+01, train loss: 9.20163e-07, val loss: 2.05870e-06, min loss: 4.19030e-07\n",
      "Epoch: 2259300, elapsed: 1.22e+01, train loss: 2.43670e-06, val loss: 3.07455e-06, min loss: 4.19030e-07\n",
      "Epoch: 2259400, elapsed: 1.24e+01, train loss: 6.08283e-07, val loss: 1.59336e-06, min loss: 4.19030e-07\n",
      "Epoch: 2259500, elapsed: 1.25e+01, train loss: 4.19394e-07, val loss: 1.54984e-06, min loss: 4.19030e-07\n",
      "Epoch: 2259600, elapsed: 1.25e+01, train loss: 4.98085e-07, val loss: 1.60105e-06, min loss: 4.19030e-07\n",
      "Epoch: 2259700, elapsed: 1.23e+01, train loss: 4.53967e-07, val loss: 1.62585e-06, min loss: 4.19030e-07\n",
      "Epoch: 2259800, elapsed: 1.22e+01, train loss: 4.19099e-07, val loss: 1.54940e-06, min loss: 4.19030e-07\n",
      "Epoch: 2259900, elapsed: 1.23e+01, train loss: 5.16308e-07, val loss: 1.82887e-06, min loss: 4.19030e-07\n",
      "Epoch: 2260000, elapsed: 1.24e+01, train loss: 2.36226e-06, val loss: 3.26065e-06, min loss: 4.19030e-07\n",
      "Epoch: 2260100, elapsed: 1.45e+01, train loss: 4.40784e-07, val loss: 1.48162e-06, min loss: 4.19030e-07\n",
      "Epoch: 2260200, elapsed: 1.22e+01, train loss: 4.18556e-07, val loss: 1.54195e-06, min loss: 4.18556e-07\n",
      "Epoch: 2260300, elapsed: 1.24e+01, train loss: 4.47013e-07, val loss: 1.62770e-06, min loss: 4.18556e-07\n",
      "Epoch: 2260400, elapsed: 1.24e+01, train loss: 5.37578e-07, val loss: 1.66243e-06, min loss: 4.18556e-07\n",
      "Epoch: 2260500, elapsed: 1.22e+01, train loss: 4.18630e-07, val loss: 1.54128e-06, min loss: 4.18556e-07\n",
      "Epoch: 2260600, elapsed: 1.24e+01, train loss: 7.08366e-07, val loss: 1.66109e-06, min loss: 4.18556e-07\n",
      "Epoch: 2260700, elapsed: 1.25e+01, train loss: 4.18316e-07, val loss: 1.54540e-06, min loss: 4.18316e-07\n",
      "Epoch: 2260800, elapsed: 1.23e+01, train loss: 4.22926e-07, val loss: 1.55145e-06, min loss: 4.18316e-07\n",
      "Epoch: 2260900, elapsed: 1.25e+01, train loss: 4.18919e-07, val loss: 1.53360e-06, min loss: 4.18316e-07\n",
      "Epoch: 2261000, elapsed: 1.23e+01, train loss: 4.22216e-07, val loss: 1.52147e-06, min loss: 4.18316e-07\n",
      "Epoch: 2261100, elapsed: 1.23e+01, train loss: 4.24321e-07, val loss: 1.52637e-06, min loss: 4.18316e-07\n",
      "Epoch: 2261200, elapsed: 1.21e+01, train loss: 4.72746e-07, val loss: 1.60022e-06, min loss: 4.18316e-07\n",
      "Epoch: 2261300, elapsed: 1.21e+01, train loss: 4.18888e-07, val loss: 1.54901e-06, min loss: 4.18316e-07\n",
      "Epoch: 2261400, elapsed: 1.26e+01, train loss: 4.19669e-07, val loss: 1.53221e-06, min loss: 4.18316e-07\n",
      "Epoch: 2261500, elapsed: 1.20e+01, train loss: 4.21986e-07, val loss: 1.52791e-06, min loss: 4.18316e-07\n",
      "Epoch: 2261600, elapsed: 1.23e+01, train loss: 5.19006e-07, val loss: 1.55761e-06, min loss: 4.18316e-07\n",
      "Epoch: 2261700, elapsed: 1.21e+01, train loss: 5.79248e-07, val loss: 1.72509e-06, min loss: 4.18316e-07\n",
      "Epoch: 2261800, elapsed: 1.19e+01, train loss: 8.25519e-07, val loss: 1.77001e-06, min loss: 4.18316e-07\n",
      "Epoch: 2261900, elapsed: 1.21e+01, train loss: 4.83207e-07, val loss: 1.73278e-06, min loss: 4.18316e-07\n",
      "Epoch: 2262000, elapsed: 1.23e+01, train loss: 1.00623e-06, val loss: 2.16369e-06, min loss: 4.18316e-07\n",
      "Epoch: 2262100, elapsed: 1.22e+01, train loss: 5.35891e-07, val loss: 1.95447e-06, min loss: 4.18316e-07\n",
      "Epoch: 2262200, elapsed: 1.23e+01, train loss: 4.18479e-07, val loss: 1.54471e-06, min loss: 4.18316e-07\n",
      "Epoch: 2262300, elapsed: 1.22e+01, train loss: 5.29409e-07, val loss: 1.94437e-06, min loss: 4.18316e-07\n",
      "Epoch: 2262400, elapsed: 1.20e+01, train loss: 4.17947e-07, val loss: 1.54967e-06, min loss: 4.17947e-07\n",
      "Epoch: 2262500, elapsed: 1.84e+01, train loss: 4.31672e-07, val loss: 1.51803e-06, min loss: 4.17947e-07\n",
      "Epoch: 2262600, elapsed: 1.30e+01, train loss: 8.72836e-07, val loss: 2.00908e-06, min loss: 4.17947e-07\n",
      "Epoch: 2262700, elapsed: 1.27e+01, train loss: 4.18074e-07, val loss: 1.55152e-06, min loss: 4.17947e-07\n",
      "Epoch: 2262800, elapsed: 1.23e+01, train loss: 7.39676e-07, val loss: 1.65052e-06, min loss: 4.17947e-07\n",
      "Epoch: 2262900, elapsed: 1.24e+01, train loss: 4.18776e-07, val loss: 1.53027e-06, min loss: 4.17947e-07\n",
      "Epoch: 2263000, elapsed: 1.26e+01, train loss: 4.47645e-07, val loss: 1.54150e-06, min loss: 4.17947e-07\n",
      "Epoch: 2263100, elapsed: 1.26e+01, train loss: 4.25641e-07, val loss: 1.53642e-06, min loss: 4.17947e-07\n",
      "Epoch: 2263200, elapsed: 1.22e+01, train loss: 4.19017e-07, val loss: 1.55188e-06, min loss: 4.17947e-07\n",
      "Epoch: 2263300, elapsed: 1.23e+01, train loss: 4.21359e-07, val loss: 1.56788e-06, min loss: 4.17947e-07\n",
      "Epoch: 2263400, elapsed: 1.21e+01, train loss: 4.77011e-07, val loss: 1.54453e-06, min loss: 4.17947e-07\n",
      "Epoch: 2263500, elapsed: 1.25e+01, train loss: 6.55296e-07, val loss: 1.50180e-06, min loss: 4.17947e-07\n",
      "Epoch: 2263600, elapsed: 1.25e+01, train loss: 4.99970e-07, val loss: 1.67864e-06, min loss: 4.17947e-07\n",
      "Epoch: 2263700, elapsed: 1.21e+01, train loss: 4.25783e-07, val loss: 1.54668e-06, min loss: 4.17947e-07\n",
      "Epoch: 2263800, elapsed: 1.24e+01, train loss: 4.26507e-07, val loss: 1.54830e-06, min loss: 4.17947e-07\n",
      "Epoch: 2263900, elapsed: 1.25e+01, train loss: 4.76643e-07, val loss: 1.56043e-06, min loss: 4.17947e-07\n",
      "Epoch: 2264000, elapsed: 1.24e+01, train loss: 6.35938e-07, val loss: 1.96805e-06, min loss: 4.17947e-07\n",
      "Epoch: 2264100, elapsed: 1.23e+01, train loss: 4.51116e-07, val loss: 1.59794e-06, min loss: 4.17947e-07\n",
      "Epoch: 2264200, elapsed: 1.22e+01, train loss: 7.45795e-07, val loss: 1.73042e-06, min loss: 4.17947e-07\n",
      "Epoch: 2264300, elapsed: 1.22e+01, train loss: 6.14646e-07, val loss: 1.76576e-06, min loss: 4.17947e-07\n",
      "Epoch: 2264400, elapsed: 1.22e+01, train loss: 3.14772e-06, val loss: 4.31405e-06, min loss: 4.17947e-07\n",
      "Epoch: 2264500, elapsed: 1.23e+01, train loss: 4.17521e-07, val loss: 1.54593e-06, min loss: 4.17521e-07\n",
      "Epoch: 2264600, elapsed: 1.19e+01, train loss: 4.29168e-07, val loss: 1.56285e-06, min loss: 4.17521e-07\n",
      "Epoch: 2264700, elapsed: 1.23e+01, train loss: 9.43802e-07, val loss: 2.07396e-06, min loss: 4.17521e-07\n",
      "Epoch: 2264800, elapsed: 1.25e+01, train loss: 4.22942e-07, val loss: 1.55717e-06, min loss: 4.17521e-07\n",
      "Epoch: 2264900, elapsed: 1.24e+01, train loss: 4.18569e-07, val loss: 1.56243e-06, min loss: 4.17521e-07\n",
      "Epoch: 2265000, elapsed: 1.20e+01, train loss: 4.25304e-07, val loss: 1.56395e-06, min loss: 4.17521e-07\n",
      "Epoch: 2265100, elapsed: 1.50e+01, train loss: 4.33154e-07, val loss: 1.55990e-06, min loss: 4.17521e-07\n",
      "Epoch: 2265200, elapsed: 1.22e+01, train loss: 4.18690e-07, val loss: 1.54937e-06, min loss: 4.17521e-07\n",
      "Epoch: 2265300, elapsed: 1.24e+01, train loss: 4.22542e-06, val loss: 6.45244e-06, min loss: 4.17521e-07\n",
      "Epoch: 2265400, elapsed: 1.19e+01, train loss: 4.17410e-07, val loss: 1.54390e-06, min loss: 4.17410e-07\n",
      "Epoch: 2265500, elapsed: 1.22e+01, train loss: 1.01731e-06, val loss: 2.41902e-06, min loss: 4.17410e-07\n",
      "Epoch: 2265600, elapsed: 1.22e+01, train loss: 4.48711e-07, val loss: 1.64013e-06, min loss: 4.17410e-07\n",
      "Epoch: 2265700, elapsed: 1.23e+01, train loss: 4.17533e-07, val loss: 1.55003e-06, min loss: 4.17410e-07\n",
      "Epoch: 2265800, elapsed: 1.20e+01, train loss: 6.37741e-07, val loss: 1.88048e-06, min loss: 4.17410e-07\n",
      "Epoch: 2265900, elapsed: 1.19e+01, train loss: 6.04075e-07, val loss: 1.53069e-06, min loss: 4.17410e-07\n",
      "Epoch: 2266000, elapsed: 1.25e+01, train loss: 4.18994e-07, val loss: 1.54559e-06, min loss: 4.17410e-07\n",
      "Epoch: 2266100, elapsed: 1.22e+01, train loss: 4.18607e-07, val loss: 1.54158e-06, min loss: 4.17410e-07\n",
      "Epoch: 2266200, elapsed: 1.21e+01, train loss: 1.20456e-06, val loss: 1.60085e-06, min loss: 4.17410e-07\n",
      "Epoch: 2266300, elapsed: 1.19e+01, train loss: 4.17151e-07, val loss: 1.54511e-06, min loss: 4.17151e-07\n",
      "Epoch: 2266400, elapsed: 1.22e+01, train loss: 5.11945e-07, val loss: 1.59493e-06, min loss: 4.17151e-07\n",
      "Epoch: 2266500, elapsed: 1.87e+01, train loss: 4.21554e-07, val loss: 1.56028e-06, min loss: 4.17151e-07\n",
      "Epoch: 2266600, elapsed: 1.29e+01, train loss: 4.79232e-07, val loss: 1.61647e-06, min loss: 4.17151e-07\n",
      "Epoch: 2266700, elapsed: 1.27e+01, train loss: 4.16946e-07, val loss: 1.54478e-06, min loss: 4.16946e-07\n",
      "Epoch: 2266800, elapsed: 1.24e+01, train loss: 4.27266e-07, val loss: 1.56167e-06, min loss: 4.16946e-07\n",
      "Epoch: 2266900, elapsed: 1.26e+01, train loss: 4.16875e-07, val loss: 1.54466e-06, min loss: 4.16875e-07\n",
      "Epoch: 2267000, elapsed: 1.25e+01, train loss: 4.21939e-07, val loss: 1.54578e-06, min loss: 4.16875e-07\n",
      "Epoch: 2267100, elapsed: 1.24e+01, train loss: 4.18281e-07, val loss: 1.54108e-06, min loss: 4.16875e-07\n",
      "Epoch: 2267200, elapsed: 1.23e+01, train loss: 6.40446e-07, val loss: 1.81547e-06, min loss: 4.16875e-07\n",
      "Epoch: 2267300, elapsed: 1.28e+01, train loss: 4.48555e-07, val loss: 1.58939e-06, min loss: 4.16875e-07\n",
      "Epoch: 2267400, elapsed: 1.30e+01, train loss: 1.19952e-06, val loss: 2.45015e-06, min loss: 4.16875e-07\n",
      "Epoch: 2267500, elapsed: 1.30e+01, train loss: 4.18187e-07, val loss: 1.54631e-06, min loss: 4.16875e-07\n",
      "Epoch: 2267600, elapsed: 1.38e+01, train loss: 4.17582e-07, val loss: 1.54157e-06, min loss: 4.16875e-07\n",
      "Epoch: 2267700, elapsed: 1.34e+01, train loss: 4.45602e-07, val loss: 1.59339e-06, min loss: 4.16875e-07\n",
      "Epoch: 2267800, elapsed: 1.32e+01, train loss: 5.54412e-07, val loss: 1.81143e-06, min loss: 4.16875e-07\n",
      "Epoch: 2267900, elapsed: 1.22e+01, train loss: 8.41929e-07, val loss: 1.61812e-06, min loss: 4.16875e-07\n",
      "Epoch: 2268000, elapsed: 1.23e+01, train loss: 4.61154e-07, val loss: 1.65328e-06, min loss: 4.16875e-07\n",
      "Epoch: 2268100, elapsed: 1.22e+01, train loss: 7.39693e-07, val loss: 1.62509e-06, min loss: 4.16875e-07\n",
      "Epoch: 2268200, elapsed: 1.20e+01, train loss: 4.16728e-07, val loss: 1.54618e-06, min loss: 4.16728e-07\n",
      "Epoch: 2268300, elapsed: 1.20e+01, train loss: 4.19496e-07, val loss: 1.55488e-06, min loss: 4.16728e-07\n",
      "Epoch: 2268400, elapsed: 1.22e+01, train loss: 2.89836e-06, val loss: 3.88370e-06, min loss: 4.16728e-07\n",
      "Epoch: 2268500, elapsed: 1.21e+01, train loss: 4.16909e-07, val loss: 1.53521e-06, min loss: 4.16728e-07\n",
      "Epoch: 2268600, elapsed: 1.20e+01, train loss: 4.24832e-07, val loss: 1.58108e-06, min loss: 4.16728e-07\n",
      "Epoch: 2268700, elapsed: 1.22e+01, train loss: 4.78911e-07, val loss: 1.55265e-06, min loss: 4.16728e-07\n",
      "Epoch: 2268800, elapsed: 1.22e+01, train loss: 1.06164e-06, val loss: 2.39418e-06, min loss: 4.16728e-07\n",
      "Epoch: 2268900, elapsed: 1.20e+01, train loss: 4.56609e-07, val loss: 1.66458e-06, min loss: 4.16728e-07\n",
      "Epoch: 2269000, elapsed: 1.21e+01, train loss: 4.18328e-07, val loss: 1.53749e-06, min loss: 4.16728e-07\n",
      "Epoch: 2269100, elapsed: 1.22e+01, train loss: 4.20169e-07, val loss: 1.55149e-06, min loss: 4.16728e-07\n",
      "Epoch: 2269200, elapsed: 1.22e+01, train loss: 1.06445e-06, val loss: 2.02494e-06, min loss: 4.16728e-07\n",
      "Epoch: 2269300, elapsed: 1.24e+01, train loss: 4.29442e-07, val loss: 1.57252e-06, min loss: 4.16728e-07\n",
      "Epoch: 2269400, elapsed: 1.20e+01, train loss: 5.61064e-07, val loss: 1.50478e-06, min loss: 4.16728e-07\n",
      "Epoch: 2269500, elapsed: 1.22e+01, train loss: 4.17344e-07, val loss: 1.54181e-06, min loss: 4.16728e-07\n",
      "Epoch: 2269600, elapsed: 1.25e+01, train loss: 5.36617e-07, val loss: 1.75779e-06, min loss: 4.16728e-07\n",
      "Epoch: 2269700, elapsed: 1.23e+01, train loss: 1.01035e-06, val loss: 2.44755e-06, min loss: 4.16728e-07\n",
      "Epoch: 2269800, elapsed: 1.20e+01, train loss: 8.55436e-07, val loss: 1.77073e-06, min loss: 4.16728e-07\n",
      "Epoch: 2269900, elapsed: 1.20e+01, train loss: 4.20356e-07, val loss: 1.52543e-06, min loss: 4.16728e-07\n",
      "Epoch: 2270000, elapsed: 1.19e+01, train loss: 4.37349e-07, val loss: 1.54250e-06, min loss: 4.16728e-07\n",
      "Epoch: 2270100, elapsed: 1.42e+01, train loss: 5.61882e-07, val loss: 1.79720e-06, min loss: 4.16728e-07\n",
      "Epoch: 2270200, elapsed: 1.20e+01, train loss: 1.01933e-06, val loss: 1.61294e-06, min loss: 4.16728e-07\n",
      "Epoch: 2270300, elapsed: 1.21e+01, train loss: 4.22579e-07, val loss: 1.60756e-06, min loss: 4.16728e-07\n",
      "Epoch: 2270400, elapsed: 1.22e+01, train loss: 7.29799e-07, val loss: 1.55553e-06, min loss: 4.16728e-07\n",
      "Epoch: 2270500, elapsed: 1.84e+01, train loss: 1.21440e-06, val loss: 2.23122e-06, min loss: 4.16728e-07\n",
      "Epoch: 2270600, elapsed: 1.26e+01, train loss: 9.31862e-07, val loss: 2.29703e-06, min loss: 4.16728e-07\n",
      "Epoch: 2270700, elapsed: 1.26e+01, train loss: 4.38361e-07, val loss: 1.50155e-06, min loss: 4.16728e-07\n",
      "Epoch: 2270800, elapsed: 1.21e+01, train loss: 8.44645e-07, val loss: 2.26047e-06, min loss: 4.16728e-07\n",
      "Epoch: 2270900, elapsed: 1.23e+01, train loss: 4.23039e-07, val loss: 1.53813e-06, min loss: 4.16728e-07\n",
      "Epoch: 2271000, elapsed: 1.26e+01, train loss: 4.17057e-07, val loss: 1.54257e-06, min loss: 4.16728e-07\n",
      "Epoch: 2271100, elapsed: 1.25e+01, train loss: 5.97848e-07, val loss: 1.62002e-06, min loss: 4.16728e-07\n",
      "Epoch: 2271200, elapsed: 1.25e+01, train loss: 4.63579e-07, val loss: 1.61873e-06, min loss: 4.16728e-07\n",
      "Epoch: 2271300, elapsed: 1.25e+01, train loss: 1.22003e-06, val loss: 1.77022e-06, min loss: 4.16728e-07\n",
      "Epoch: 2271400, elapsed: 1.21e+01, train loss: 4.15977e-07, val loss: 1.54005e-06, min loss: 4.15977e-07\n",
      "Epoch: 2271500, elapsed: 1.22e+01, train loss: 4.21377e-07, val loss: 1.51517e-06, min loss: 4.15977e-07\n",
      "Epoch: 2271600, elapsed: 1.22e+01, train loss: 6.99467e-07, val loss: 2.06634e-06, min loss: 4.15977e-07\n",
      "Epoch: 2271700, elapsed: 1.22e+01, train loss: 4.15876e-07, val loss: 1.54027e-06, min loss: 4.15876e-07\n",
      "Epoch: 2271800, elapsed: 1.22e+01, train loss: 1.02444e-06, val loss: 2.28981e-06, min loss: 4.15876e-07\n",
      "Epoch: 2271900, elapsed: 1.24e+01, train loss: 4.92689e-07, val loss: 1.56747e-06, min loss: 4.15876e-07\n",
      "Epoch: 2272000, elapsed: 1.22e+01, train loss: 4.16044e-07, val loss: 1.53950e-06, min loss: 4.15876e-07\n",
      "Epoch: 2272100, elapsed: 1.25e+01, train loss: 4.18446e-07, val loss: 1.55657e-06, min loss: 4.15876e-07\n",
      "Epoch: 2272200, elapsed: 1.22e+01, train loss: 4.24137e-07, val loss: 1.52289e-06, min loss: 4.15876e-07\n",
      "Epoch: 2272300, elapsed: 1.23e+01, train loss: 8.76863e-07, val loss: 1.93342e-06, min loss: 4.15876e-07\n",
      "Epoch: 2272400, elapsed: 1.22e+01, train loss: 4.95434e-07, val loss: 1.65078e-06, min loss: 4.15876e-07\n",
      "Epoch: 2272500, elapsed: 1.23e+01, train loss: 4.38518e-07, val loss: 1.56061e-06, min loss: 4.15876e-07\n",
      "Epoch: 2272600, elapsed: 1.22e+01, train loss: 5.37103e-07, val loss: 1.90005e-06, min loss: 4.15876e-07\n",
      "Epoch: 2272700, elapsed: 1.22e+01, train loss: 7.71928e-07, val loss: 1.79510e-06, min loss: 4.15876e-07\n",
      "Epoch: 2272800, elapsed: 1.25e+01, train loss: 4.97430e-07, val loss: 1.76155e-06, min loss: 4.15876e-07\n",
      "Epoch: 2272900, elapsed: 1.24e+01, train loss: 6.34982e-07, val loss: 1.82614e-06, min loss: 4.15876e-07\n",
      "Epoch: 2273000, elapsed: 1.25e+01, train loss: 4.17721e-07, val loss: 1.56200e-06, min loss: 4.15876e-07\n",
      "Epoch: 2273100, elapsed: 1.22e+01, train loss: 4.19010e-07, val loss: 1.55995e-06, min loss: 4.15876e-07\n",
      "Epoch: 2273200, elapsed: 1.23e+01, train loss: 4.25874e-07, val loss: 1.52584e-06, min loss: 4.15876e-07\n",
      "Epoch: 2273300, elapsed: 1.20e+01, train loss: 5.21674e-07, val loss: 1.63326e-06, min loss: 4.15876e-07\n",
      "Epoch: 2273400, elapsed: 1.22e+01, train loss: 4.16333e-07, val loss: 1.55684e-06, min loss: 4.15876e-07\n",
      "Epoch: 2273500, elapsed: 1.24e+01, train loss: 4.41131e-07, val loss: 1.54433e-06, min loss: 4.15876e-07\n",
      "Epoch: 2273600, elapsed: 1.26e+01, train loss: 4.53269e-07, val loss: 1.58775e-06, min loss: 4.15876e-07\n",
      "Epoch: 2273700, elapsed: 1.28e+01, train loss: 4.26065e-07, val loss: 1.54356e-06, min loss: 4.15876e-07\n",
      "Epoch: 2273800, elapsed: 1.30e+01, train loss: 4.61783e-07, val loss: 1.54455e-06, min loss: 4.15876e-07\n",
      "Epoch: 2273900, elapsed: 1.37e+01, train loss: 6.64584e-07, val loss: 1.58427e-06, min loss: 4.15876e-07\n",
      "Epoch: 2274000, elapsed: 1.34e+01, train loss: 4.56541e-07, val loss: 1.58776e-06, min loss: 4.15876e-07\n",
      "Epoch: 2274100, elapsed: 1.33e+01, train loss: 5.57266e-07, val loss: 1.66201e-06, min loss: 4.15876e-07\n",
      "Epoch: 2274200, elapsed: 1.39e+01, train loss: 4.15578e-07, val loss: 1.54154e-06, min loss: 4.15578e-07\n",
      "Epoch: 2274300, elapsed: 1.33e+01, train loss: 4.41239e-07, val loss: 1.64290e-06, min loss: 4.15578e-07\n",
      "Epoch: 2274400, elapsed: 1.36e+01, train loss: 5.06007e-07, val loss: 1.66496e-06, min loss: 4.15578e-07\n",
      "Epoch: 2274500, elapsed: 1.39e+01, train loss: 4.15719e-07, val loss: 1.54022e-06, min loss: 4.15578e-07\n",
      "Epoch: 2274600, elapsed: 2.18e+01, train loss: 4.31990e-07, val loss: 1.62409e-06, min loss: 4.15578e-07\n",
      "Epoch: 2274700, elapsed: 1.37e+01, train loss: 7.87196e-07, val loss: 1.69079e-06, min loss: 4.15578e-07\n",
      "Epoch: 2274800, elapsed: 1.37e+01, train loss: 4.15498e-07, val loss: 1.54876e-06, min loss: 4.15498e-07\n",
      "Epoch: 2274900, elapsed: 1.38e+01, train loss: 8.43296e-07, val loss: 1.62409e-06, min loss: 4.15498e-07\n",
      "Epoch: 2275000, elapsed: 1.35e+01, train loss: 4.15137e-07, val loss: 1.54136e-06, min loss: 4.15137e-07\n",
      "Epoch: 2275100, elapsed: 1.47e+01, train loss: 4.25316e-07, val loss: 1.60061e-06, min loss: 4.15137e-07\n",
      "Epoch: 2275200, elapsed: 1.21e+01, train loss: 4.17775e-07, val loss: 1.54239e-06, min loss: 4.15137e-07\n",
      "Epoch: 2275300, elapsed: 1.23e+01, train loss: 4.14972e-07, val loss: 1.54266e-06, min loss: 4.14972e-07\n",
      "Epoch: 2275400, elapsed: 1.25e+01, train loss: 4.53616e-07, val loss: 1.58418e-06, min loss: 4.14972e-07\n",
      "Epoch: 2275500, elapsed: 1.28e+01, train loss: 4.14926e-07, val loss: 1.54232e-06, min loss: 4.14926e-07\n",
      "Epoch: 2275600, elapsed: 1.24e+01, train loss: 4.25597e-07, val loss: 1.54135e-06, min loss: 4.14926e-07\n",
      "Epoch: 2275700, elapsed: 1.22e+01, train loss: 4.16493e-07, val loss: 1.52280e-06, min loss: 4.14926e-07\n",
      "Epoch: 2275800, elapsed: 1.23e+01, train loss: 6.18551e-07, val loss: 1.83488e-06, min loss: 4.14926e-07\n",
      "Epoch: 2275900, elapsed: 1.26e+01, train loss: 4.15699e-07, val loss: 1.55142e-06, min loss: 4.14926e-07\n",
      "Epoch: 2276000, elapsed: 1.25e+01, train loss: 4.16053e-07, val loss: 1.53909e-06, min loss: 4.14926e-07\n",
      "Epoch: 2276100, elapsed: 1.24e+01, train loss: 4.97205e-07, val loss: 1.60135e-06, min loss: 4.14926e-07\n",
      "Epoch: 2276200, elapsed: 1.24e+01, train loss: 4.69258e-07, val loss: 1.47868e-06, min loss: 4.14926e-07\n",
      "Epoch: 2276300, elapsed: 1.22e+01, train loss: 4.16314e-07, val loss: 1.53168e-06, min loss: 4.14926e-07\n",
      "Epoch: 2276400, elapsed: 1.22e+01, train loss: 4.49648e-07, val loss: 1.53522e-06, min loss: 4.14926e-07\n",
      "Epoch: 2276500, elapsed: 1.40e+01, train loss: 4.14862e-07, val loss: 1.54369e-06, min loss: 4.14862e-07\n",
      "Epoch: 2276600, elapsed: 1.37e+01, train loss: 4.18799e-07, val loss: 1.53123e-06, min loss: 4.14862e-07\n",
      "Epoch: 2276700, elapsed: 1.32e+01, train loss: 4.14691e-07, val loss: 1.54296e-06, min loss: 4.14691e-07\n",
      "Epoch: 2276800, elapsed: 1.23e+01, train loss: 4.15060e-07, val loss: 1.54678e-06, min loss: 4.14691e-07\n",
      "Epoch: 2276900, elapsed: 1.30e+01, train loss: 4.15062e-07, val loss: 1.53764e-06, min loss: 4.14691e-07\n",
      "Epoch: 2277000, elapsed: 1.30e+01, train loss: 4.14953e-07, val loss: 1.54435e-06, min loss: 4.14691e-07\n",
      "Epoch: 2277100, elapsed: 1.34e+01, train loss: 5.05860e-07, val loss: 1.63044e-06, min loss: 4.14691e-07\n",
      "Epoch: 2277200, elapsed: 1.25e+01, train loss: 4.69648e-07, val loss: 1.58282e-06, min loss: 4.14691e-07\n",
      "Epoch: 2277300, elapsed: 1.25e+01, train loss: 4.44563e-07, val loss: 1.55638e-06, min loss: 4.14691e-07\n",
      "Epoch: 2277400, elapsed: 1.22e+01, train loss: 4.20252e-07, val loss: 1.56992e-06, min loss: 4.14691e-07\n",
      "Epoch: 2277500, elapsed: 1.24e+01, train loss: 4.92537e-07, val loss: 1.66121e-06, min loss: 4.14691e-07\n",
      "Epoch: 2277600, elapsed: 1.23e+01, train loss: 5.25995e-07, val loss: 1.86604e-06, min loss: 4.14691e-07\n",
      "Epoch: 2277700, elapsed: 1.25e+01, train loss: 4.28657e-07, val loss: 1.56003e-06, min loss: 4.14691e-07\n",
      "Epoch: 2277800, elapsed: 1.32e+01, train loss: 4.27733e-07, val loss: 1.53290e-06, min loss: 4.14691e-07\n",
      "Epoch: 2277900, elapsed: 1.23e+01, train loss: 4.77396e-07, val loss: 1.57754e-06, min loss: 4.14691e-07\n",
      "Epoch: 2278000, elapsed: 1.21e+01, train loss: 4.31167e-07, val loss: 1.51646e-06, min loss: 4.14691e-07\n",
      "Epoch: 2278100, elapsed: 1.19e+01, train loss: 4.14683e-07, val loss: 1.54377e-06, min loss: 4.14683e-07\n",
      "Epoch: 2278200, elapsed: 1.21e+01, train loss: 4.90970e-07, val loss: 1.72425e-06, min loss: 4.14683e-07\n",
      "Epoch: 2278300, elapsed: 1.23e+01, train loss: 4.16508e-07, val loss: 1.53768e-06, min loss: 4.14683e-07\n",
      "Epoch: 2278400, elapsed: 1.27e+01, train loss: 4.20565e-07, val loss: 1.56205e-06, min loss: 4.14683e-07\n",
      "Epoch: 2278500, elapsed: 1.24e+01, train loss: 2.01511e-06, val loss: 2.78103e-06, min loss: 4.14683e-07\n",
      "Epoch: 2278600, elapsed: 2.10e+01, train loss: 4.18550e-07, val loss: 1.54755e-06, min loss: 4.14683e-07\n",
      "Epoch: 2278700, elapsed: 1.39e+01, train loss: 7.00038e-07, val loss: 1.62347e-06, min loss: 4.14683e-07\n",
      "Epoch: 2278800, elapsed: 1.40e+01, train loss: 4.14208e-07, val loss: 1.54202e-06, min loss: 4.14208e-07\n",
      "Epoch: 2278900, elapsed: 1.37e+01, train loss: 4.20184e-07, val loss: 1.54030e-06, min loss: 4.14208e-07\n",
      "Epoch: 2279000, elapsed: 1.40e+01, train loss: 5.44417e-07, val loss: 1.65921e-06, min loss: 4.14208e-07\n",
      "Epoch: 2279100, elapsed: 1.39e+01, train loss: 6.90522e-07, val loss: 1.86302e-06, min loss: 4.14208e-07\n",
      "Epoch: 2279200, elapsed: 1.24e+01, train loss: 4.15642e-07, val loss: 1.54607e-06, min loss: 4.14208e-07\n",
      "Epoch: 2279300, elapsed: 1.24e+01, train loss: 4.16197e-07, val loss: 1.55055e-06, min loss: 4.14208e-07\n",
      "Epoch: 2279400, elapsed: 1.23e+01, train loss: 8.51993e-07, val loss: 1.63870e-06, min loss: 4.14208e-07\n",
      "Epoch: 2279500, elapsed: 1.24e+01, train loss: 4.14686e-07, val loss: 1.54738e-06, min loss: 4.14208e-07\n",
      "Epoch: 2279600, elapsed: 1.25e+01, train loss: 4.29177e-07, val loss: 1.52728e-06, min loss: 4.14208e-07\n",
      "Epoch: 2279700, elapsed: 1.22e+01, train loss: 4.15780e-07, val loss: 1.56653e-06, min loss: 4.14208e-07\n",
      "Epoch: 2279800, elapsed: 1.22e+01, train loss: 4.14845e-07, val loss: 1.52925e-06, min loss: 4.14208e-07\n",
      "Epoch: 2279900, elapsed: 1.23e+01, train loss: 5.34810e-07, val loss: 1.53162e-06, min loss: 4.14208e-07\n",
      "Epoch: 2280000, elapsed: 1.24e+01, train loss: 4.16788e-07, val loss: 1.53838e-06, min loss: 4.14208e-07\n",
      "Epoch: 2280100, elapsed: 1.46e+01, train loss: 4.16956e-07, val loss: 1.54134e-06, min loss: 4.14208e-07\n",
      "Epoch: 2280200, elapsed: 1.24e+01, train loss: 4.23626e-07, val loss: 1.54165e-06, min loss: 4.14208e-07\n",
      "Epoch: 2280300, elapsed: 1.21e+01, train loss: 4.13963e-07, val loss: 1.53274e-06, min loss: 4.13963e-07\n",
      "Epoch: 2280400, elapsed: 1.22e+01, train loss: 4.94433e-07, val loss: 1.67285e-06, min loss: 4.13963e-07\n",
      "Epoch: 2280500, elapsed: 1.23e+01, train loss: 8.41006e-07, val loss: 1.66358e-06, min loss: 4.13963e-07\n",
      "Epoch: 2280600, elapsed: 1.24e+01, train loss: 4.56088e-07, val loss: 1.59552e-06, min loss: 4.13963e-07\n",
      "Epoch: 2280700, elapsed: 1.21e+01, train loss: 6.50079e-07, val loss: 1.92741e-06, min loss: 4.13963e-07\n",
      "Epoch: 2280800, elapsed: 1.23e+01, train loss: 1.84880e-06, val loss: 3.29446e-06, min loss: 4.13963e-07\n",
      "Epoch: 2280900, elapsed: 1.23e+01, train loss: 1.66843e-06, val loss: 2.50854e-06, min loss: 4.13963e-07\n",
      "Epoch: 2281000, elapsed: 1.20e+01, train loss: 7.59845e-07, val loss: 2.00897e-06, min loss: 4.13963e-07\n",
      "Epoch: 2281100, elapsed: 1.25e+01, train loss: 4.13766e-07, val loss: 1.54130e-06, min loss: 4.13766e-07\n",
      "Epoch: 2281200, elapsed: 1.22e+01, train loss: 4.51779e-07, val loss: 1.48730e-06, min loss: 4.13766e-07\n",
      "Epoch: 2281300, elapsed: 1.23e+01, train loss: 4.27291e-07, val loss: 1.54727e-06, min loss: 4.13766e-07\n",
      "Epoch: 2281400, elapsed: 1.24e+01, train loss: 4.34654e-07, val loss: 1.63733e-06, min loss: 4.13766e-07\n",
      "Epoch: 2281500, elapsed: 1.22e+01, train loss: 2.01975e-06, val loss: 3.03103e-06, min loss: 4.13766e-07\n",
      "Epoch: 2281600, elapsed: 1.22e+01, train loss: 7.37099e-07, val loss: 1.85338e-06, min loss: 4.13766e-07\n",
      "Epoch: 2281700, elapsed: 1.22e+01, train loss: 4.27059e-07, val loss: 1.59303e-06, min loss: 4.13766e-07\n",
      "Epoch: 2281800, elapsed: 1.20e+01, train loss: 4.14942e-07, val loss: 1.53162e-06, min loss: 4.13766e-07\n",
      "Epoch: 2281900, elapsed: 1.19e+01, train loss: 4.19558e-07, val loss: 1.54325e-06, min loss: 4.13766e-07\n",
      "Epoch: 2282000, elapsed: 1.24e+01, train loss: 1.06397e-06, val loss: 2.21652e-06, min loss: 4.13766e-07\n",
      "Epoch: 2282100, elapsed: 1.20e+01, train loss: 4.22192e-07, val loss: 1.52840e-06, min loss: 4.13766e-07\n",
      "Epoch: 2282200, elapsed: 1.22e+01, train loss: 4.16394e-07, val loss: 1.53528e-06, min loss: 4.13766e-07\n",
      "Epoch: 2282300, elapsed: 1.25e+01, train loss: 4.70091e-07, val loss: 1.55098e-06, min loss: 4.13766e-07\n",
      "Epoch: 2282400, elapsed: 1.22e+01, train loss: 1.25883e-06, val loss: 2.22380e-06, min loss: 4.13766e-07\n",
      "Epoch: 2282500, elapsed: 1.22e+01, train loss: 4.13589e-07, val loss: 1.53608e-06, min loss: 4.13589e-07\n",
      "Epoch: 2282600, elapsed: 1.84e+01, train loss: 4.35274e-07, val loss: 1.58185e-06, min loss: 4.13589e-07\n",
      "Epoch: 2282700, elapsed: 1.26e+01, train loss: 4.55986e-07, val loss: 1.63731e-06, min loss: 4.13589e-07\n",
      "Epoch: 2282800, elapsed: 1.25e+01, train loss: 4.24548e-07, val loss: 1.51845e-06, min loss: 4.13589e-07\n",
      "Epoch: 2282900, elapsed: 1.22e+01, train loss: 4.66574e-07, val loss: 1.71050e-06, min loss: 4.13589e-07\n",
      "Epoch: 2283000, elapsed: 1.25e+01, train loss: 6.45826e-07, val loss: 1.71353e-06, min loss: 4.13589e-07\n",
      "Epoch: 2283100, elapsed: 1.25e+01, train loss: 1.50280e-06, val loss: 3.13983e-06, min loss: 4.13589e-07\n",
      "Epoch: 2283200, elapsed: 1.25e+01, train loss: 4.28569e-07, val loss: 1.55614e-06, min loss: 4.13589e-07\n",
      "Epoch: 2283300, elapsed: 1.25e+01, train loss: 4.20178e-07, val loss: 1.54061e-06, min loss: 4.13589e-07\n",
      "Epoch: 2283400, elapsed: 1.22e+01, train loss: 4.23977e-07, val loss: 1.53607e-06, min loss: 4.13589e-07\n",
      "Epoch: 2283500, elapsed: 1.27e+01, train loss: 4.52984e-07, val loss: 1.54176e-06, min loss: 4.13589e-07\n",
      "Epoch: 2283600, elapsed: 1.22e+01, train loss: 4.50035e-07, val loss: 1.53967e-06, min loss: 4.13589e-07\n",
      "Epoch: 2283700, elapsed: 1.24e+01, train loss: 5.08492e-07, val loss: 1.64892e-06, min loss: 4.13589e-07\n",
      "Epoch: 2283800, elapsed: 1.24e+01, train loss: 3.03417e-06, val loss: 3.48192e-06, min loss: 4.13589e-07\n",
      "Epoch: 2283900, elapsed: 1.22e+01, train loss: 7.43890e-07, val loss: 1.92316e-06, min loss: 4.13589e-07\n",
      "Epoch: 2284000, elapsed: 1.21e+01, train loss: 4.40735e-07, val loss: 1.52080e-06, min loss: 4.13589e-07\n",
      "Epoch: 2284100, elapsed: 1.25e+01, train loss: 1.40366e-06, val loss: 2.81504e-06, min loss: 4.13589e-07\n",
      "Epoch: 2284200, elapsed: 1.21e+01, train loss: 4.13459e-07, val loss: 1.54604e-06, min loss: 4.13459e-07\n",
      "Epoch: 2284300, elapsed: 1.22e+01, train loss: 4.14409e-07, val loss: 1.54400e-06, min loss: 4.13459e-07\n",
      "Epoch: 2284400, elapsed: 1.22e+01, train loss: 7.51255e-07, val loss: 2.16446e-06, min loss: 4.13459e-07\n",
      "Epoch: 2284500, elapsed: 1.22e+01, train loss: 4.17031e-07, val loss: 1.53909e-06, min loss: 4.13459e-07\n",
      "Epoch: 2284600, elapsed: 1.21e+01, train loss: 4.14777e-07, val loss: 1.52435e-06, min loss: 4.13459e-07\n",
      "Epoch: 2284700, elapsed: 1.22e+01, train loss: 4.15909e-07, val loss: 1.55164e-06, min loss: 4.13459e-07\n",
      "Epoch: 2284800, elapsed: 1.23e+01, train loss: 4.41011e-07, val loss: 1.57716e-06, min loss: 4.13459e-07\n",
      "Epoch: 2284900, elapsed: 1.21e+01, train loss: 1.03322e-06, val loss: 1.81028e-06, min loss: 4.13459e-07\n",
      "Epoch: 2285000, elapsed: 1.23e+01, train loss: 4.34687e-07, val loss: 1.63733e-06, min loss: 4.13459e-07\n",
      "Epoch: 2285100, elapsed: 1.44e+01, train loss: 4.12900e-07, val loss: 1.53475e-06, min loss: 4.12900e-07\n",
      "Epoch: 2285200, elapsed: 1.22e+01, train loss: 4.19949e-07, val loss: 1.55628e-06, min loss: 4.12900e-07\n",
      "Epoch: 2285300, elapsed: 1.19e+01, train loss: 6.64486e-07, val loss: 2.00713e-06, min loss: 4.12900e-07\n",
      "Epoch: 2285400, elapsed: 1.22e+01, train loss: 4.12738e-07, val loss: 1.53864e-06, min loss: 4.12738e-07\n",
      "Epoch: 2285500, elapsed: 1.21e+01, train loss: 6.26366e-07, val loss: 1.54586e-06, min loss: 4.12738e-07\n",
      "Epoch: 2285600, elapsed: 1.22e+01, train loss: 4.12689e-07, val loss: 1.53942e-06, min loss: 4.12689e-07\n",
      "Epoch: 2285700, elapsed: 1.24e+01, train loss: 4.30601e-07, val loss: 1.49968e-06, min loss: 4.12689e-07\n",
      "Epoch: 2285800, elapsed: 1.21e+01, train loss: 4.43624e-07, val loss: 1.62248e-06, min loss: 4.12689e-07\n",
      "Epoch: 2285900, elapsed: 1.24e+01, train loss: 4.26355e-07, val loss: 1.53567e-06, min loss: 4.12689e-07\n",
      "Epoch: 2286000, elapsed: 1.22e+01, train loss: 4.12753e-07, val loss: 1.53741e-06, min loss: 4.12689e-07\n",
      "Epoch: 2286100, elapsed: 1.22e+01, train loss: 4.58056e-07, val loss: 1.59426e-06, min loss: 4.12689e-07\n",
      "Epoch: 2286200, elapsed: 1.22e+01, train loss: 4.12557e-07, val loss: 1.53774e-06, min loss: 4.12557e-07\n",
      "Epoch: 2286300, elapsed: 1.22e+01, train loss: 4.15431e-07, val loss: 1.52244e-06, min loss: 4.12557e-07\n",
      "Epoch: 2286400, elapsed: 1.23e+01, train loss: 9.10687e-07, val loss: 1.71880e-06, min loss: 4.12557e-07\n",
      "Epoch: 2286500, elapsed: 1.21e+01, train loss: 7.85591e-07, val loss: 1.83997e-06, min loss: 4.12557e-07\n",
      "Epoch: 2286600, elapsed: 1.86e+01, train loss: 6.26400e-07, val loss: 1.87009e-06, min loss: 4.12557e-07\n",
      "Epoch: 2286700, elapsed: 1.28e+01, train loss: 7.34269e-07, val loss: 1.77670e-06, min loss: 4.12557e-07\n",
      "Epoch: 2286800, elapsed: 1.24e+01, train loss: 1.64681e-06, val loss: 2.73839e-06, min loss: 4.12557e-07\n",
      "Epoch: 2286900, elapsed: 1.23e+01, train loss: 5.15798e-07, val loss: 1.92600e-06, min loss: 4.12557e-07\n",
      "Epoch: 2287000, elapsed: 1.25e+01, train loss: 4.74437e-07, val loss: 1.57981e-06, min loss: 4.12557e-07\n",
      "Epoch: 2287100, elapsed: 1.27e+01, train loss: 1.54758e-06, val loss: 3.19338e-06, min loss: 4.12557e-07\n",
      "Epoch: 2287200, elapsed: 1.24e+01, train loss: 6.29959e-07, val loss: 1.63928e-06, min loss: 4.12557e-07\n",
      "Epoch: 2287300, elapsed: 1.22e+01, train loss: 7.19310e-07, val loss: 2.16926e-06, min loss: 4.12557e-07\n",
      "Epoch: 2287400, elapsed: 1.26e+01, train loss: 4.90729e-07, val loss: 1.72371e-06, min loss: 4.12557e-07\n",
      "Epoch: 2287500, elapsed: 1.24e+01, train loss: 6.25355e-07, val loss: 1.58256e-06, min loss: 4.12557e-07\n",
      "Epoch: 2287600, elapsed: 1.24e+01, train loss: 1.10807e-06, val loss: 2.27898e-06, min loss: 4.12557e-07\n",
      "Epoch: 2287700, elapsed: 1.26e+01, train loss: 4.17817e-07, val loss: 1.50212e-06, min loss: 4.12557e-07\n",
      "Epoch: 2287800, elapsed: 1.22e+01, train loss: 4.13069e-07, val loss: 1.53230e-06, min loss: 4.12557e-07\n",
      "Epoch: 2287900, elapsed: 1.22e+01, train loss: 4.15346e-07, val loss: 1.53921e-06, min loss: 4.12557e-07\n",
      "Epoch: 2288000, elapsed: 1.24e+01, train loss: 1.34879e-06, val loss: 2.19498e-06, min loss: 4.12557e-07\n",
      "Epoch: 2288100, elapsed: 1.25e+01, train loss: 5.73934e-07, val loss: 1.83747e-06, min loss: 4.12557e-07\n",
      "Epoch: 2288200, elapsed: 1.22e+01, train loss: 7.00189e-07, val loss: 1.87012e-06, min loss: 4.12557e-07\n",
      "Epoch: 2288300, elapsed: 1.23e+01, train loss: 5.36783e-07, val loss: 1.77924e-06, min loss: 4.12557e-07\n",
      "Epoch: 2288400, elapsed: 1.24e+01, train loss: 4.31393e-07, val loss: 1.67287e-06, min loss: 4.12557e-07\n",
      "Epoch: 2288500, elapsed: 1.25e+01, train loss: 4.20094e-07, val loss: 1.54111e-06, min loss: 4.12557e-07\n",
      "Epoch: 2288600, elapsed: 1.24e+01, train loss: 7.40951e-07, val loss: 1.70371e-06, min loss: 4.12557e-07\n",
      "Epoch: 2288700, elapsed: 1.25e+01, train loss: 4.18580e-07, val loss: 1.55146e-06, min loss: 4.12557e-07\n",
      "Epoch: 2288800, elapsed: 1.23e+01, train loss: 4.16833e-07, val loss: 1.57576e-06, min loss: 4.12557e-07\n",
      "Epoch: 2288900, elapsed: 1.24e+01, train loss: 4.12730e-07, val loss: 1.53530e-06, min loss: 4.12557e-07\n",
      "Epoch: 2289000, elapsed: 1.23e+01, train loss: 4.30355e-07, val loss: 1.56309e-06, min loss: 4.12557e-07\n",
      "Epoch: 2289100, elapsed: 1.23e+01, train loss: 1.78338e-06, val loss: 2.75088e-06, min loss: 4.12557e-07\n",
      "Epoch: 2289200, elapsed: 1.20e+01, train loss: 4.70296e-07, val loss: 1.59873e-06, min loss: 4.12557e-07\n",
      "Epoch: 2289300, elapsed: 1.23e+01, train loss: 4.12159e-07, val loss: 1.54340e-06, min loss: 4.12159e-07\n",
      "Epoch: 2289400, elapsed: 1.24e+01, train loss: 7.11325e-07, val loss: 2.66218e-06, min loss: 4.12159e-07\n",
      "Epoch: 2289500, elapsed: 1.27e+01, train loss: 4.11867e-07, val loss: 1.53639e-06, min loss: 4.11867e-07\n",
      "Epoch: 2289600, elapsed: 1.22e+01, train loss: 7.28837e-07, val loss: 2.50549e-06, min loss: 4.11867e-07\n",
      "Epoch: 2289700, elapsed: 1.24e+01, train loss: 7.38449e-07, val loss: 2.05318e-06, min loss: 4.11867e-07\n",
      "Epoch: 2289800, elapsed: 1.21e+01, train loss: 4.25020e-07, val loss: 1.49448e-06, min loss: 4.11867e-07\n",
      "Epoch: 2289900, elapsed: 1.24e+01, train loss: 4.20980e-07, val loss: 1.55875e-06, min loss: 4.11867e-07\n",
      "Epoch: 2290000, elapsed: 1.24e+01, train loss: 4.33686e-07, val loss: 1.52709e-06, min loss: 4.11867e-07\n",
      "Epoch: 2290100, elapsed: 1.46e+01, train loss: 4.13433e-07, val loss: 1.53418e-06, min loss: 4.11867e-07\n",
      "Epoch: 2290200, elapsed: 1.25e+01, train loss: 4.12036e-07, val loss: 1.53648e-06, min loss: 4.11867e-07\n",
      "Epoch: 2290300, elapsed: 1.32e+01, train loss: 4.50184e-07, val loss: 1.55015e-06, min loss: 4.11867e-07\n",
      "Epoch: 2290400, elapsed: 1.39e+01, train loss: 4.13263e-07, val loss: 1.52333e-06, min loss: 4.11867e-07\n",
      "Epoch: 2290500, elapsed: 1.35e+01, train loss: 4.13725e-07, val loss: 1.52018e-06, min loss: 4.11867e-07\n",
      "Epoch: 2290600, elapsed: 1.34e+01, train loss: 4.27136e-07, val loss: 1.53950e-06, min loss: 4.11867e-07\n",
      "Epoch: 2290700, elapsed: 2.15e+01, train loss: 4.16685e-07, val loss: 1.53497e-06, min loss: 4.11867e-07\n",
      "Epoch: 2290800, elapsed: 1.39e+01, train loss: 4.19863e-07, val loss: 1.51985e-06, min loss: 4.11867e-07\n",
      "Epoch: 2290900, elapsed: 1.41e+01, train loss: 4.16623e-07, val loss: 1.54101e-06, min loss: 4.11867e-07\n",
      "Epoch: 2291000, elapsed: 1.42e+01, train loss: 4.15930e-07, val loss: 1.50920e-06, min loss: 4.11867e-07\n",
      "Epoch: 2291100, elapsed: 1.38e+01, train loss: 4.50994e-07, val loss: 1.58434e-06, min loss: 4.11867e-07\n",
      "Epoch: 2291200, elapsed: 1.40e+01, train loss: 1.18538e-06, val loss: 2.14234e-06, min loss: 4.11867e-07\n",
      "Epoch: 2291300, elapsed: 1.39e+01, train loss: 4.16022e-07, val loss: 1.52011e-06, min loss: 4.11867e-07\n",
      "Epoch: 2291400, elapsed: 1.40e+01, train loss: 4.12024e-07, val loss: 1.53379e-06, min loss: 4.11867e-07\n",
      "Epoch: 2291500, elapsed: 1.37e+01, train loss: 4.54039e-07, val loss: 1.57596e-06, min loss: 4.11867e-07\n",
      "Epoch: 2291600, elapsed: 1.38e+01, train loss: 4.25700e-07, val loss: 1.56809e-06, min loss: 4.11867e-07\n",
      "Epoch: 2291700, elapsed: 1.42e+01, train loss: 4.13632e-07, val loss: 1.54433e-06, min loss: 4.11867e-07\n",
      "Epoch: 2291800, elapsed: 1.41e+01, train loss: 4.18100e-07, val loss: 1.57612e-06, min loss: 4.11867e-07\n",
      "Epoch: 2291900, elapsed: 1.39e+01, train loss: 4.13294e-07, val loss: 1.53931e-06, min loss: 4.11867e-07\n",
      "Epoch: 2292000, elapsed: 1.43e+01, train loss: 4.35313e-07, val loss: 1.55766e-06, min loss: 4.11867e-07\n",
      "Epoch: 2292100, elapsed: 1.38e+01, train loss: 4.68916e-07, val loss: 1.67857e-06, min loss: 4.11867e-07\n",
      "Epoch: 2292200, elapsed: 1.41e+01, train loss: 2.22215e-06, val loss: 3.24252e-06, min loss: 4.11867e-07\n",
      "Epoch: 2292300, elapsed: 1.38e+01, train loss: 4.34899e-07, val loss: 1.57929e-06, min loss: 4.11867e-07\n",
      "Epoch: 2292400, elapsed: 1.38e+01, train loss: 1.80912e-06, val loss: 2.62684e-06, min loss: 4.11867e-07\n",
      "Epoch: 2292500, elapsed: 1.37e+01, train loss: 1.65436e-06, val loss: 2.41505e-06, min loss: 4.11867e-07\n",
      "Epoch: 2292600, elapsed: 1.36e+01, train loss: 4.78664e-07, val loss: 1.73807e-06, min loss: 4.11867e-07\n",
      "Epoch: 2292700, elapsed: 1.37e+01, train loss: 8.12630e-07, val loss: 2.20563e-06, min loss: 4.11867e-07\n",
      "Epoch: 2292800, elapsed: 1.37e+01, train loss: 4.11623e-07, val loss: 1.53309e-06, min loss: 4.11623e-07\n",
      "Epoch: 2292900, elapsed: 1.38e+01, train loss: 4.11574e-07, val loss: 1.54013e-06, min loss: 4.11574e-07\n",
      "Epoch: 2293000, elapsed: 1.38e+01, train loss: 4.26094e-07, val loss: 1.57034e-06, min loss: 4.11574e-07\n",
      "Epoch: 2293100, elapsed: 1.28e+01, train loss: 4.74905e-07, val loss: 1.66520e-06, min loss: 4.11574e-07\n",
      "Epoch: 2293200, elapsed: 1.22e+01, train loss: 4.34136e-07, val loss: 1.49018e-06, min loss: 4.11574e-07\n",
      "Epoch: 2293300, elapsed: 1.19e+01, train loss: 8.74170e-07, val loss: 1.87894e-06, min loss: 4.11574e-07\n",
      "Epoch: 2293400, elapsed: 1.22e+01, train loss: 1.70795e-06, val loss: 3.20326e-06, min loss: 4.11574e-07\n",
      "Epoch: 2293500, elapsed: 1.22e+01, train loss: 4.39835e-07, val loss: 1.46544e-06, min loss: 4.11574e-07\n",
      "Epoch: 2293600, elapsed: 1.23e+01, train loss: 4.11357e-07, val loss: 1.53418e-06, min loss: 4.11357e-07\n",
      "Epoch: 2293700, elapsed: 1.24e+01, train loss: 4.36665e-07, val loss: 1.49357e-06, min loss: 4.11357e-07\n",
      "Epoch: 2293800, elapsed: 1.24e+01, train loss: 4.10831e-07, val loss: 1.53641e-06, min loss: 4.10831e-07\n",
      "Epoch: 2293900, elapsed: 1.21e+01, train loss: 4.13064e-07, val loss: 1.54938e-06, min loss: 4.10831e-07\n",
      "Epoch: 2294000, elapsed: 1.21e+01, train loss: 4.11549e-07, val loss: 1.52707e-06, min loss: 4.10831e-07\n",
      "Epoch: 2294100, elapsed: 1.23e+01, train loss: 4.51085e-07, val loss: 1.58975e-06, min loss: 4.10831e-07\n",
      "Epoch: 2294200, elapsed: 1.24e+01, train loss: 6.67726e-07, val loss: 1.81916e-06, min loss: 4.10831e-07\n",
      "Epoch: 2294300, elapsed: 1.21e+01, train loss: 9.02432e-07, val loss: 1.86505e-06, min loss: 4.10831e-07\n",
      "Epoch: 2294400, elapsed: 1.24e+01, train loss: 4.10762e-07, val loss: 1.53488e-06, min loss: 4.10762e-07\n",
      "Epoch: 2294500, elapsed: 1.23e+01, train loss: 4.12125e-07, val loss: 1.55007e-06, min loss: 4.10762e-07\n",
      "Epoch: 2294600, elapsed: 1.23e+01, train loss: 1.90455e-06, val loss: 3.80381e-06, min loss: 4.10762e-07\n",
      "Epoch: 2294700, elapsed: 1.23e+01, train loss: 4.10696e-07, val loss: 1.53736e-06, min loss: 4.10696e-07\n",
      "Epoch: 2294800, elapsed: 1.88e+01, train loss: 4.40841e-07, val loss: 1.51833e-06, min loss: 4.10696e-07\n",
      "Epoch: 2294900, elapsed: 1.24e+01, train loss: 4.51388e-07, val loss: 1.63936e-06, min loss: 4.10696e-07\n",
      "Epoch: 2295000, elapsed: 1.24e+01, train loss: 4.64152e-07, val loss: 1.53333e-06, min loss: 4.10696e-07\n",
      "Epoch: 2295100, elapsed: 1.47e+01, train loss: 1.00402e-06, val loss: 2.36795e-06, min loss: 4.10696e-07\n",
      "Epoch: 2295200, elapsed: 1.24e+01, train loss: 5.86204e-07, val loss: 1.65505e-06, min loss: 4.10696e-07\n",
      "Epoch: 2295300, elapsed: 1.23e+01, train loss: 4.85939e-07, val loss: 1.56780e-06, min loss: 4.10696e-07\n",
      "Epoch: 2295400, elapsed: 1.26e+01, train loss: 4.11117e-07, val loss: 1.52312e-06, min loss: 4.10696e-07\n",
      "Epoch: 2295500, elapsed: 1.25e+01, train loss: 4.11436e-07, val loss: 1.52820e-06, min loss: 4.10696e-07\n",
      "Epoch: 2295600, elapsed: 1.22e+01, train loss: 4.42702e-07, val loss: 1.58217e-06, min loss: 4.10696e-07\n",
      "Epoch: 2295700, elapsed: 1.22e+01, train loss: 4.10855e-07, val loss: 1.53155e-06, min loss: 4.10696e-07\n",
      "Epoch: 2295800, elapsed: 1.25e+01, train loss: 4.10552e-07, val loss: 1.54051e-06, min loss: 4.10552e-07\n",
      "Epoch: 2295900, elapsed: 1.23e+01, train loss: 6.53993e-07, val loss: 1.82547e-06, min loss: 4.10552e-07\n",
      "Epoch: 2296000, elapsed: 1.25e+01, train loss: 4.52757e-07, val loss: 1.57547e-06, min loss: 4.10552e-07\n",
      "Epoch: 2296100, elapsed: 1.22e+01, train loss: 4.19051e-07, val loss: 1.53679e-06, min loss: 4.10552e-07\n",
      "Epoch: 2296200, elapsed: 1.24e+01, train loss: 4.26119e-07, val loss: 1.57777e-06, min loss: 4.10552e-07\n",
      "Epoch: 2296300, elapsed: 1.21e+01, train loss: 4.91786e-07, val loss: 1.68909e-06, min loss: 4.10552e-07\n",
      "Epoch: 2296400, elapsed: 1.23e+01, train loss: 4.34730e-07, val loss: 1.67074e-06, min loss: 4.10552e-07\n",
      "Epoch: 2296500, elapsed: 1.21e+01, train loss: 4.12238e-07, val loss: 1.54516e-06, min loss: 4.10552e-07\n",
      "Epoch: 2296600, elapsed: 1.21e+01, train loss: 4.11625e-07, val loss: 1.55915e-06, min loss: 4.10552e-07\n",
      "Epoch: 2296700, elapsed: 1.23e+01, train loss: 4.29258e-07, val loss: 1.47113e-06, min loss: 4.10552e-07\n",
      "Epoch: 2296800, elapsed: 1.24e+01, train loss: 1.80975e-06, val loss: 2.80885e-06, min loss: 4.10552e-07\n",
      "Epoch: 2296900, elapsed: 1.24e+01, train loss: 4.10287e-07, val loss: 1.52758e-06, min loss: 4.10287e-07\n",
      "Epoch: 2297000, elapsed: 1.22e+01, train loss: 4.33078e-07, val loss: 1.55713e-06, min loss: 4.10287e-07\n",
      "Epoch: 2297100, elapsed: 1.23e+01, train loss: 7.21776e-07, val loss: 1.82624e-06, min loss: 4.10287e-07\n",
      "Epoch: 2297200, elapsed: 1.26e+01, train loss: 5.72084e-07, val loss: 1.63166e-06, min loss: 4.10287e-07\n",
      "Epoch: 2297300, elapsed: 1.22e+01, train loss: 1.21586e-06, val loss: 2.01995e-06, min loss: 4.10287e-07\n",
      "Epoch: 2297400, elapsed: 1.25e+01, train loss: 7.62904e-07, val loss: 1.77773e-06, min loss: 4.10287e-07\n",
      "Epoch: 2297500, elapsed: 1.22e+01, train loss: 7.13513e-07, val loss: 1.76379e-06, min loss: 4.10287e-07\n",
      "Epoch: 2297600, elapsed: 1.22e+01, train loss: 7.11981e-07, val loss: 2.06612e-06, min loss: 4.10287e-07\n",
      "Epoch: 2297700, elapsed: 1.22e+01, train loss: 4.32341e-07, val loss: 1.55133e-06, min loss: 4.10287e-07\n",
      "Epoch: 2297800, elapsed: 1.24e+01, train loss: 4.11226e-07, val loss: 1.52084e-06, min loss: 4.10287e-07\n",
      "Epoch: 2297900, elapsed: 1.23e+01, train loss: 4.11688e-07, val loss: 1.55029e-06, min loss: 4.10287e-07\n",
      "Epoch: 2298000, elapsed: 1.23e+01, train loss: 4.28897e-07, val loss: 1.51819e-06, min loss: 4.10287e-07\n",
      "Epoch: 2298100, elapsed: 1.22e+01, train loss: 1.42491e-06, val loss: 2.50173e-06, min loss: 4.10287e-07\n",
      "Epoch: 2298200, elapsed: 1.22e+01, train loss: 7.23776e-07, val loss: 1.75160e-06, min loss: 4.10287e-07\n",
      "Epoch: 2298300, elapsed: 1.22e+01, train loss: 1.75799e-06, val loss: 2.29778e-06, min loss: 4.10287e-07\n",
      "Epoch: 2298400, elapsed: 1.21e+01, train loss: 2.50326e-06, val loss: 2.75332e-06, min loss: 4.10287e-07\n",
      "Epoch: 2298500, elapsed: 1.24e+01, train loss: 5.18377e-07, val loss: 1.71960e-06, min loss: 4.10287e-07\n",
      "Epoch: 2298600, elapsed: 1.25e+01, train loss: 1.69893e-06, val loss: 2.45557e-06, min loss: 4.10287e-07\n",
      "Epoch: 2298700, elapsed: 1.24e+01, train loss: 7.06383e-07, val loss: 2.06519e-06, min loss: 4.10287e-07\n",
      "Epoch: 2298800, elapsed: 1.84e+01, train loss: 4.10085e-07, val loss: 1.53685e-06, min loss: 4.10085e-07\n",
      "Epoch: 2298900, elapsed: 1.26e+01, train loss: 4.29052e-07, val loss: 1.53475e-06, min loss: 4.10085e-07\n",
      "Epoch: 2299000, elapsed: 1.26e+01, train loss: 4.09940e-07, val loss: 1.53385e-06, min loss: 4.09940e-07\n",
      "Epoch: 2299100, elapsed: 1.25e+01, train loss: 4.37587e-07, val loss: 1.58813e-06, min loss: 4.09940e-07\n",
      "Epoch: 2299200, elapsed: 1.23e+01, train loss: 6.00986e-07, val loss: 1.86096e-06, min loss: 4.09940e-07\n",
      "Epoch: 2299300, elapsed: 1.23e+01, train loss: 6.06320e-07, val loss: 1.61238e-06, min loss: 4.09940e-07\n",
      "Epoch: 2299400, elapsed: 1.23e+01, train loss: 1.00866e-06, val loss: 2.44885e-06, min loss: 4.09940e-07\n",
      "Epoch: 2299500, elapsed: 1.23e+01, train loss: 4.16556e-07, val loss: 1.55772e-06, min loss: 4.09940e-07\n",
      "Epoch: 2299600, elapsed: 1.23e+01, train loss: 4.10036e-07, val loss: 1.52917e-06, min loss: 4.09940e-07\n",
      "Epoch: 2299700, elapsed: 1.23e+01, train loss: 1.23163e-06, val loss: 2.19141e-06, min loss: 4.09940e-07\n",
      "Epoch: 2299800, elapsed: 1.28e+01, train loss: 4.09489e-07, val loss: 1.53373e-06, min loss: 4.09489e-07\n",
      "Epoch: 2299900, elapsed: 1.22e+01, train loss: 4.21854e-07, val loss: 1.55738e-06, min loss: 4.09489e-07\n",
      "Epoch: 2300000, elapsed: 1.21e+01, train loss: 4.16227e-07, val loss: 1.52852e-06, min loss: 4.09489e-07\n",
      "Epoch: 2300100, elapsed: 1.43e+01, train loss: 4.14270e-07, val loss: 1.55261e-06, min loss: 4.09489e-07\n",
      "Epoch: 2300200, elapsed: 1.21e+01, train loss: 5.40216e-07, val loss: 1.63933e-06, min loss: 4.09489e-07\n",
      "Epoch: 2300300, elapsed: 1.23e+01, train loss: 7.10979e-07, val loss: 1.47068e-06, min loss: 4.09489e-07\n",
      "Epoch: 2300400, elapsed: 1.23e+01, train loss: 4.55933e-07, val loss: 1.55105e-06, min loss: 4.09489e-07\n",
      "Epoch: 2300500, elapsed: 1.20e+01, train loss: 4.09681e-07, val loss: 1.53127e-06, min loss: 4.09489e-07\n",
      "Epoch: 2300600, elapsed: 1.23e+01, train loss: 4.30146e-07, val loss: 1.49620e-06, min loss: 4.09489e-07\n",
      "Epoch: 2300700, elapsed: 1.23e+01, train loss: 9.41485e-07, val loss: 1.99287e-06, min loss: 4.09489e-07\n",
      "Epoch: 2300800, elapsed: 1.24e+01, train loss: 4.09340e-07, val loss: 1.53503e-06, min loss: 4.09340e-07\n",
      "Epoch: 2300900, elapsed: 1.21e+01, train loss: 4.14976e-07, val loss: 1.56679e-06, min loss: 4.09340e-07\n",
      "Epoch: 2301000, elapsed: 1.21e+01, train loss: 5.92199e-07, val loss: 1.59828e-06, min loss: 4.09340e-07\n",
      "Epoch: 2301100, elapsed: 1.22e+01, train loss: 4.25913e-07, val loss: 1.61460e-06, min loss: 4.09340e-07\n",
      "Epoch: 2301200, elapsed: 1.21e+01, train loss: 4.09791e-07, val loss: 1.52322e-06, min loss: 4.09340e-07\n",
      "Epoch: 2301300, elapsed: 1.22e+01, train loss: 5.76448e-07, val loss: 1.80566e-06, min loss: 4.09340e-07\n",
      "Epoch: 2301400, elapsed: 1.22e+01, train loss: 4.63847e-07, val loss: 1.49356e-06, min loss: 4.09340e-07\n",
      "Epoch: 2301500, elapsed: 1.23e+01, train loss: 4.17107e-07, val loss: 1.51848e-06, min loss: 4.09340e-07\n",
      "Epoch: 2301600, elapsed: 1.22e+01, train loss: 4.50244e-07, val loss: 1.54744e-06, min loss: 4.09340e-07\n",
      "Epoch: 2301700, elapsed: 1.23e+01, train loss: 4.10567e-07, val loss: 1.52060e-06, min loss: 4.09340e-07\n",
      "Epoch: 2301800, elapsed: 1.22e+01, train loss: 4.61783e-07, val loss: 1.57356e-06, min loss: 4.09340e-07\n",
      "Epoch: 2301900, elapsed: 1.27e+01, train loss: 5.14155e-07, val loss: 1.55729e-06, min loss: 4.09340e-07\n",
      "Epoch: 2302000, elapsed: 1.36e+01, train loss: 9.01485e-07, val loss: 2.23264e-06, min loss: 4.09340e-07\n",
      "Epoch: 2302100, elapsed: 1.38e+01, train loss: 4.09282e-07, val loss: 1.53659e-06, min loss: 4.09282e-07\n",
      "Epoch: 2302200, elapsed: 1.31e+01, train loss: 4.11595e-07, val loss: 1.53886e-06, min loss: 4.09282e-07\n",
      "Epoch: 2302300, elapsed: 1.34e+01, train loss: 8.60788e-07, val loss: 2.13723e-06, min loss: 4.09282e-07\n",
      "Epoch: 2302400, elapsed: 1.32e+01, train loss: 4.09086e-07, val loss: 1.53354e-06, min loss: 4.09086e-07\n",
      "Epoch: 2302500, elapsed: 1.38e+01, train loss: 4.20031e-07, val loss: 1.53066e-06, min loss: 4.09086e-07\n",
      "Epoch: 2302600, elapsed: 1.39e+01, train loss: 4.46854e-07, val loss: 1.52432e-06, min loss: 4.09086e-07\n",
      "Epoch: 2302700, elapsed: 1.33e+01, train loss: 4.31697e-07, val loss: 1.58021e-06, min loss: 4.09086e-07\n",
      "Epoch: 2302800, elapsed: 1.37e+01, train loss: 5.67502e-07, val loss: 1.56051e-06, min loss: 4.09086e-07\n",
      "Epoch: 2302900, elapsed: 2.15e+01, train loss: 5.26489e-07, val loss: 1.62710e-06, min loss: 4.09086e-07\n",
      "Epoch: 2303000, elapsed: 1.40e+01, train loss: 4.32610e-07, val loss: 1.59620e-06, min loss: 4.09086e-07\n",
      "Epoch: 2303100, elapsed: 1.39e+01, train loss: 2.66781e-06, val loss: 4.08326e-06, min loss: 4.09086e-07\n",
      "Epoch: 2303200, elapsed: 1.36e+01, train loss: 4.08882e-07, val loss: 1.52683e-06, min loss: 4.08882e-07\n",
      "Epoch: 2303300, elapsed: 1.35e+01, train loss: 4.20197e-07, val loss: 1.49876e-06, min loss: 4.08882e-07\n",
      "Epoch: 2303400, elapsed: 1.30e+01, train loss: 2.28480e-06, val loss: 4.11471e-06, min loss: 4.08882e-07\n",
      "Epoch: 2303500, elapsed: 1.24e+01, train loss: 4.08681e-07, val loss: 1.53159e-06, min loss: 4.08681e-07\n",
      "Epoch: 2303600, elapsed: 1.24e+01, train loss: 4.13514e-07, val loss: 1.55299e-06, min loss: 4.08681e-07\n",
      "Epoch: 2303700, elapsed: 1.24e+01, train loss: 5.48466e-07, val loss: 1.58418e-06, min loss: 4.08681e-07\n",
      "Epoch: 2303800, elapsed: 1.21e+01, train loss: 4.12721e-07, val loss: 1.52957e-06, min loss: 4.08681e-07\n",
      "Epoch: 2303900, elapsed: 1.23e+01, train loss: 4.18414e-07, val loss: 1.51508e-06, min loss: 4.08681e-07\n",
      "Epoch: 2304000, elapsed: 1.22e+01, train loss: 1.49933e-06, val loss: 2.42487e-06, min loss: 4.08681e-07\n",
      "Epoch: 2304100, elapsed: 1.23e+01, train loss: 4.08884e-07, val loss: 1.53870e-06, min loss: 4.08681e-07\n",
      "Epoch: 2304200, elapsed: 1.22e+01, train loss: 4.32718e-07, val loss: 1.50580e-06, min loss: 4.08681e-07\n",
      "Epoch: 2304300, elapsed: 1.22e+01, train loss: 7.11106e-07, val loss: 2.00990e-06, min loss: 4.08681e-07\n",
      "Epoch: 2304400, elapsed: 1.22e+01, train loss: 6.58530e-07, val loss: 1.85628e-06, min loss: 4.08681e-07\n",
      "Epoch: 2304500, elapsed: 1.26e+01, train loss: 1.63633e-06, val loss: 2.89946e-06, min loss: 4.08681e-07\n",
      "Epoch: 2304600, elapsed: 1.23e+01, train loss: 4.08537e-07, val loss: 1.52800e-06, min loss: 4.08537e-07\n",
      "Epoch: 2304700, elapsed: 1.23e+01, train loss: 4.09785e-07, val loss: 1.52104e-06, min loss: 4.08537e-07\n",
      "Epoch: 2304800, elapsed: 1.20e+01, train loss: 4.15228e-07, val loss: 1.52241e-06, min loss: 4.08537e-07\n",
      "Epoch: 2304900, elapsed: 1.25e+01, train loss: 4.69988e-07, val loss: 1.60199e-06, min loss: 4.08537e-07\n",
      "Epoch: 2305000, elapsed: 1.22e+01, train loss: 4.08863e-07, val loss: 1.52350e-06, min loss: 4.08537e-07\n",
      "Epoch: 2305100, elapsed: 1.46e+01, train loss: 4.22023e-07, val loss: 1.55026e-06, min loss: 4.08537e-07\n",
      "Epoch: 2305200, elapsed: 1.22e+01, train loss: 4.08545e-07, val loss: 1.52830e-06, min loss: 4.08537e-07\n",
      "Epoch: 2305300, elapsed: 1.24e+01, train loss: 4.09899e-07, val loss: 1.53629e-06, min loss: 4.08537e-07\n",
      "Epoch: 2305400, elapsed: 1.21e+01, train loss: 7.01119e-07, val loss: 1.94607e-06, min loss: 4.08537e-07\n",
      "Epoch: 2305500, elapsed: 1.22e+01, train loss: 4.37668e-07, val loss: 1.56775e-06, min loss: 4.08537e-07\n",
      "Epoch: 2305600, elapsed: 1.24e+01, train loss: 1.84433e-06, val loss: 3.01632e-06, min loss: 4.08537e-07\n",
      "Epoch: 2305700, elapsed: 1.20e+01, train loss: 4.08423e-07, val loss: 1.53968e-06, min loss: 4.08423e-07\n",
      "Epoch: 2305800, elapsed: 1.23e+01, train loss: 4.17334e-07, val loss: 1.52634e-06, min loss: 4.08423e-07\n",
      "Epoch: 2305900, elapsed: 1.22e+01, train loss: 4.20218e-07, val loss: 1.55241e-06, min loss: 4.08423e-07\n",
      "Epoch: 2306000, elapsed: 1.21e+01, train loss: 4.26110e-07, val loss: 1.54421e-06, min loss: 4.08423e-07\n",
      "Epoch: 2306100, elapsed: 1.20e+01, train loss: 4.08317e-07, val loss: 1.52923e-06, min loss: 4.08317e-07\n",
      "Epoch: 2306200, elapsed: 1.22e+01, train loss: 8.02727e-07, val loss: 1.58108e-06, min loss: 4.08317e-07\n",
      "Epoch: 2306300, elapsed: 1.22e+01, train loss: 4.08063e-07, val loss: 1.52894e-06, min loss: 4.08063e-07\n",
      "Epoch: 2306400, elapsed: 1.22e+01, train loss: 4.12706e-07, val loss: 1.53592e-06, min loss: 4.08063e-07\n",
      "Epoch: 2306500, elapsed: 1.19e+01, train loss: 5.28204e-07, val loss: 1.75485e-06, min loss: 4.08063e-07\n",
      "Epoch: 2306600, elapsed: 1.19e+01, train loss: 1.11298e-06, val loss: 2.51509e-06, min loss: 4.08063e-07\n",
      "Epoch: 2306700, elapsed: 1.19e+01, train loss: 4.44267e-07, val loss: 1.61337e-06, min loss: 4.08063e-07\n",
      "Epoch: 2306800, elapsed: 1.21e+01, train loss: 4.08326e-07, val loss: 1.53177e-06, min loss: 4.08063e-07\n",
      "Epoch: 2306900, elapsed: 1.83e+01, train loss: 4.16576e-07, val loss: 1.53229e-06, min loss: 4.08063e-07\n",
      "Epoch: 2307000, elapsed: 1.25e+01, train loss: 6.15484e-07, val loss: 1.69681e-06, min loss: 4.08063e-07\n",
      "Epoch: 2307100, elapsed: 1.24e+01, train loss: 4.50862e-07, val loss: 1.55918e-06, min loss: 4.08063e-07\n",
      "Epoch: 2307200, elapsed: 1.26e+01, train loss: 4.14960e-07, val loss: 1.50517e-06, min loss: 4.08063e-07\n",
      "Epoch: 2307300, elapsed: 1.23e+01, train loss: 4.23708e-07, val loss: 1.52689e-06, min loss: 4.08063e-07\n",
      "Epoch: 2307400, elapsed: 1.27e+01, train loss: 4.08214e-07, val loss: 1.53398e-06, min loss: 4.08063e-07\n",
      "Epoch: 2307500, elapsed: 1.23e+01, train loss: 4.10095e-07, val loss: 1.52288e-06, min loss: 4.08063e-07\n",
      "Epoch: 2307600, elapsed: 1.24e+01, train loss: 1.07465e-06, val loss: 2.49676e-06, min loss: 4.08063e-07\n",
      "Epoch: 2307700, elapsed: 1.22e+01, train loss: 4.07764e-07, val loss: 1.53296e-06, min loss: 4.07764e-07\n",
      "Epoch: 2307800, elapsed: 1.24e+01, train loss: 4.27766e-07, val loss: 1.47133e-06, min loss: 4.07764e-07\n",
      "Epoch: 2307900, elapsed: 1.22e+01, train loss: 4.24568e-07, val loss: 1.56153e-06, min loss: 4.07764e-07\n",
      "Epoch: 2308000, elapsed: 1.23e+01, train loss: 1.68471e-06, val loss: 2.32518e-06, min loss: 4.07764e-07\n",
      "Epoch: 2308100, elapsed: 1.21e+01, train loss: 2.55793e-06, val loss: 3.72632e-06, min loss: 4.07764e-07\n",
      "Epoch: 2308200, elapsed: 1.21e+01, train loss: 4.15069e-07, val loss: 1.53572e-06, min loss: 4.07764e-07\n",
      "Epoch: 2308300, elapsed: 1.26e+01, train loss: 4.07938e-07, val loss: 1.52749e-06, min loss: 4.07764e-07\n",
      "Epoch: 2308400, elapsed: 1.23e+01, train loss: 4.18361e-07, val loss: 1.53357e-06, min loss: 4.07764e-07\n",
      "Epoch: 2308500, elapsed: 1.24e+01, train loss: 4.19709e-07, val loss: 1.55783e-06, min loss: 4.07764e-07\n",
      "Epoch: 2308600, elapsed: 1.24e+01, train loss: 4.16878e-07, val loss: 1.50177e-06, min loss: 4.07764e-07\n",
      "Epoch: 2308700, elapsed: 1.24e+01, train loss: 4.07847e-07, val loss: 1.52027e-06, min loss: 4.07764e-07\n",
      "Epoch: 2308800, elapsed: 1.25e+01, train loss: 4.48739e-07, val loss: 1.61810e-06, min loss: 4.07764e-07\n",
      "Epoch: 2308900, elapsed: 1.22e+01, train loss: 7.34807e-07, val loss: 2.06522e-06, min loss: 4.07764e-07\n",
      "Epoch: 2309000, elapsed: 1.22e+01, train loss: 4.08415e-07, val loss: 1.51088e-06, min loss: 4.07764e-07\n",
      "Epoch: 2309100, elapsed: 1.24e+01, train loss: 4.08407e-07, val loss: 1.50628e-06, min loss: 4.07764e-07\n",
      "Epoch: 2309200, elapsed: 1.21e+01, train loss: 2.46508e-06, val loss: 2.85752e-06, min loss: 4.07764e-07\n",
      "Epoch: 2309300, elapsed: 1.22e+01, train loss: 4.07462e-07, val loss: 1.52998e-06, min loss: 4.07462e-07\n",
      "Epoch: 2309400, elapsed: 1.23e+01, train loss: 4.97554e-07, val loss: 1.85447e-06, min loss: 4.07462e-07\n",
      "Epoch: 2309500, elapsed: 1.22e+01, train loss: 4.43586e-07, val loss: 1.46864e-06, min loss: 4.07462e-07\n",
      "Epoch: 2309600, elapsed: 1.23e+01, train loss: 4.08246e-07, val loss: 1.53448e-06, min loss: 4.07462e-07\n",
      "Epoch: 2309700, elapsed: 1.23e+01, train loss: 4.14594e-07, val loss: 1.55739e-06, min loss: 4.07462e-07\n",
      "Epoch: 2309800, elapsed: 1.23e+01, train loss: 4.66381e-07, val loss: 1.54376e-06, min loss: 4.07462e-07\n",
      "Epoch: 2309900, elapsed: 1.22e+01, train loss: 4.53634e-07, val loss: 1.64910e-06, min loss: 4.07462e-07\n",
      "Epoch: 2310000, elapsed: 1.23e+01, train loss: 4.07385e-07, val loss: 1.52704e-06, min loss: 4.07385e-07\n",
      "Epoch: 2310100, elapsed: 1.45e+01, train loss: 4.11291e-07, val loss: 1.56413e-06, min loss: 4.07385e-07\n",
      "Epoch: 2310200, elapsed: 1.22e+01, train loss: 1.50462e-06, val loss: 2.24032e-06, min loss: 4.07385e-07\n",
      "Epoch: 2310300, elapsed: 1.23e+01, train loss: 4.07232e-07, val loss: 1.52580e-06, min loss: 4.07232e-07\n",
      "Epoch: 2310400, elapsed: 1.22e+01, train loss: 4.66072e-07, val loss: 1.64265e-06, min loss: 4.07232e-07\n",
      "Epoch: 2310500, elapsed: 1.25e+01, train loss: 4.07199e-07, val loss: 1.53010e-06, min loss: 4.07199e-07\n",
      "Epoch: 2310600, elapsed: 1.24e+01, train loss: 4.09762e-07, val loss: 1.51613e-06, min loss: 4.07199e-07\n",
      "Epoch: 2310700, elapsed: 1.21e+01, train loss: 4.18683e-07, val loss: 1.53501e-06, min loss: 4.07199e-07\n",
      "Epoch: 2310800, elapsed: 1.22e+01, train loss: 4.07328e-07, val loss: 1.52770e-06, min loss: 4.07199e-07\n",
      "Epoch: 2310900, elapsed: 1.24e+01, train loss: 4.25605e-07, val loss: 1.56657e-06, min loss: 4.07199e-07\n",
      "Epoch: 2311000, elapsed: 1.87e+01, train loss: 5.07500e-07, val loss: 1.53480e-06, min loss: 4.07199e-07\n",
      "Epoch: 2311100, elapsed: 1.26e+01, train loss: 4.82366e-07, val loss: 1.60272e-06, min loss: 4.07199e-07\n",
      "Epoch: 2311200, elapsed: 1.22e+01, train loss: 4.24030e-07, val loss: 1.63447e-06, min loss: 4.07199e-07\n",
      "Epoch: 2311300, elapsed: 1.25e+01, train loss: 4.23784e-07, val loss: 1.57913e-06, min loss: 4.07199e-07\n",
      "Epoch: 2311400, elapsed: 1.24e+01, train loss: 6.36967e-07, val loss: 1.51408e-06, min loss: 4.07199e-07\n",
      "Epoch: 2311500, elapsed: 1.24e+01, train loss: 2.43102e-06, val loss: 3.41704e-06, min loss: 4.07199e-07\n",
      "Epoch: 2311600, elapsed: 1.25e+01, train loss: 4.08522e-07, val loss: 1.51935e-06, min loss: 4.07199e-07\n",
      "Epoch: 2311700, elapsed: 1.25e+01, train loss: 4.09547e-07, val loss: 1.53603e-06, min loss: 4.07199e-07\n",
      "Epoch: 2311800, elapsed: 1.26e+01, train loss: 5.50852e-07, val loss: 1.44032e-06, min loss: 4.07199e-07\n",
      "Epoch: 2311900, elapsed: 1.25e+01, train loss: 6.34079e-07, val loss: 1.73468e-06, min loss: 4.07199e-07\n",
      "Epoch: 2312000, elapsed: 1.21e+01, train loss: 4.07415e-07, val loss: 1.52533e-06, min loss: 4.07199e-07\n",
      "Epoch: 2312100, elapsed: 1.20e+01, train loss: 4.13915e-07, val loss: 1.52688e-06, min loss: 4.07199e-07\n",
      "Epoch: 2312200, elapsed: 1.22e+01, train loss: 1.98486e-06, val loss: 2.35784e-06, min loss: 4.07199e-07\n",
      "Epoch: 2312300, elapsed: 1.23e+01, train loss: 6.15542e-07, val loss: 1.70469e-06, min loss: 4.07199e-07\n",
      "Epoch: 2312400, elapsed: 1.19e+01, train loss: 4.40687e-07, val loss: 1.76611e-06, min loss: 4.07199e-07\n",
      "Epoch: 2312500, elapsed: 1.23e+01, train loss: 4.44799e-07, val loss: 1.58826e-06, min loss: 4.07199e-07\n",
      "Epoch: 2312600, elapsed: 1.23e+01, train loss: 4.42778e-07, val loss: 1.50438e-06, min loss: 4.07199e-07\n",
      "Epoch: 2312700, elapsed: 1.22e+01, train loss: 4.16136e-07, val loss: 1.51985e-06, min loss: 4.07199e-07\n",
      "Epoch: 2312800, elapsed: 1.22e+01, train loss: 4.39335e-07, val loss: 1.62720e-06, min loss: 4.07199e-07\n",
      "Epoch: 2312900, elapsed: 1.21e+01, train loss: 4.10792e-07, val loss: 1.48466e-06, min loss: 4.07199e-07\n",
      "Epoch: 2313000, elapsed: 1.20e+01, train loss: 4.44447e-07, val loss: 1.53533e-06, min loss: 4.07199e-07\n",
      "Epoch: 2313100, elapsed: 1.24e+01, train loss: 4.34993e-07, val loss: 1.58720e-06, min loss: 4.07199e-07\n",
      "Epoch: 2313200, elapsed: 1.21e+01, train loss: 4.07692e-07, val loss: 1.53877e-06, min loss: 4.07199e-07\n",
      "Epoch: 2313300, elapsed: 1.22e+01, train loss: 4.08415e-07, val loss: 1.54571e-06, min loss: 4.07199e-07\n",
      "Epoch: 2313400, elapsed: 1.20e+01, train loss: 4.59988e-07, val loss: 1.60660e-06, min loss: 4.07199e-07\n",
      "Epoch: 2313500, elapsed: 1.25e+01, train loss: 4.21366e-07, val loss: 1.53705e-06, min loss: 4.07199e-07\n",
      "Epoch: 2313600, elapsed: 1.21e+01, train loss: 6.57377e-07, val loss: 1.76630e-06, min loss: 4.07199e-07\n",
      "Epoch: 2313700, elapsed: 1.22e+01, train loss: 4.46132e-07, val loss: 1.48881e-06, min loss: 4.07199e-07\n",
      "Epoch: 2313800, elapsed: 1.22e+01, train loss: 2.04872e-06, val loss: 2.26825e-06, min loss: 4.07199e-07\n",
      "Epoch: 2313900, elapsed: 1.22e+01, train loss: 4.06514e-07, val loss: 1.52828e-06, min loss: 4.06514e-07\n",
      "Epoch: 2314000, elapsed: 1.21e+01, train loss: 4.26584e-07, val loss: 1.54668e-06, min loss: 4.06514e-07\n",
      "Epoch: 2314100, elapsed: 1.22e+01, train loss: 4.07395e-07, val loss: 1.55044e-06, min loss: 4.06514e-07\n",
      "Epoch: 2314200, elapsed: 1.23e+01, train loss: 6.24651e-07, val loss: 1.78788e-06, min loss: 4.06514e-07\n",
      "Epoch: 2314300, elapsed: 1.21e+01, train loss: 4.06379e-07, val loss: 1.52815e-06, min loss: 4.06379e-07\n",
      "Epoch: 2314400, elapsed: 1.23e+01, train loss: 1.47580e-06, val loss: 2.23831e-06, min loss: 4.06379e-07\n",
      "Epoch: 2314500, elapsed: 1.23e+01, train loss: 4.06292e-07, val loss: 1.52739e-06, min loss: 4.06292e-07\n",
      "Epoch: 2314600, elapsed: 1.31e+01, train loss: 4.43952e-07, val loss: 1.55000e-06, min loss: 4.06292e-07\n",
      "Epoch: 2314700, elapsed: 1.34e+01, train loss: 4.39984e-07, val loss: 1.52989e-06, min loss: 4.06292e-07\n",
      "Epoch: 2314800, elapsed: 1.33e+01, train loss: 4.10993e-07, val loss: 1.50716e-06, min loss: 4.06292e-07\n",
      "Epoch: 2314900, elapsed: 1.35e+01, train loss: 4.08611e-07, val loss: 1.51821e-06, min loss: 4.06292e-07\n",
      "Epoch: 2315000, elapsed: 1.36e+01, train loss: 3.75333e-06, val loss: 4.68396e-06, min loss: 4.06292e-07\n",
      "Epoch: 2315100, elapsed: 2.11e+01, train loss: 4.06206e-07, val loss: 1.52437e-06, min loss: 4.06206e-07\n",
      "Epoch: 2315200, elapsed: 1.27e+01, train loss: 7.31255e-07, val loss: 1.83712e-06, min loss: 4.06206e-07\n",
      "Epoch: 2315300, elapsed: 1.27e+01, train loss: 4.71841e-07, val loss: 1.59346e-06, min loss: 4.06206e-07\n",
      "Epoch: 2315400, elapsed: 1.24e+01, train loss: 4.20474e-07, val loss: 1.51784e-06, min loss: 4.06206e-07\n",
      "Epoch: 2315500, elapsed: 1.24e+01, train loss: 5.69069e-07, val loss: 1.76590e-06, min loss: 4.06206e-07\n",
      "Epoch: 2315600, elapsed: 1.23e+01, train loss: 4.06172e-07, val loss: 1.52218e-06, min loss: 4.06172e-07\n",
      "Epoch: 2315700, elapsed: 1.26e+01, train loss: 4.61658e-07, val loss: 1.58390e-06, min loss: 4.06172e-07\n",
      "Epoch: 2315800, elapsed: 1.22e+01, train loss: 2.05158e-06, val loss: 3.82260e-06, min loss: 4.06172e-07\n",
      "Epoch: 2315900, elapsed: 1.24e+01, train loss: 6.36276e-07, val loss: 1.85830e-06, min loss: 4.06172e-07\n",
      "Epoch: 2316000, elapsed: 1.24e+01, train loss: 4.25965e-07, val loss: 1.56566e-06, min loss: 4.06172e-07\n",
      "Epoch: 2316100, elapsed: 1.25e+01, train loss: 4.55430e-07, val loss: 1.64890e-06, min loss: 4.06172e-07\n",
      "Epoch: 2316200, elapsed: 1.23e+01, train loss: 5.12998e-07, val loss: 1.67777e-06, min loss: 4.06172e-07\n",
      "Epoch: 2316300, elapsed: 1.25e+01, train loss: 4.51270e-07, val loss: 1.69651e-06, min loss: 4.06172e-07\n",
      "Epoch: 2316400, elapsed: 1.23e+01, train loss: 4.26503e-07, val loss: 1.48917e-06, min loss: 4.06172e-07\n",
      "Epoch: 2316500, elapsed: 1.25e+01, train loss: 7.92005e-07, val loss: 2.04113e-06, min loss: 4.06172e-07\n",
      "Epoch: 2316600, elapsed: 1.25e+01, train loss: 4.06685e-07, val loss: 1.53593e-06, min loss: 4.06172e-07\n",
      "Epoch: 2316700, elapsed: 1.26e+01, train loss: 4.09355e-07, val loss: 1.51556e-06, min loss: 4.06172e-07\n",
      "Epoch: 2316800, elapsed: 1.27e+01, train loss: 5.65914e-07, val loss: 1.67324e-06, min loss: 4.06172e-07\n",
      "Epoch: 2316900, elapsed: 1.25e+01, train loss: 4.20760e-07, val loss: 1.48960e-06, min loss: 4.06172e-07\n",
      "Epoch: 2317000, elapsed: 1.25e+01, train loss: 4.06919e-07, val loss: 1.52751e-06, min loss: 4.06172e-07\n",
      "Epoch: 2317100, elapsed: 1.25e+01, train loss: 4.10975e-07, val loss: 1.52253e-06, min loss: 4.06172e-07\n",
      "Epoch: 2317200, elapsed: 1.23e+01, train loss: 4.49456e-07, val loss: 1.52541e-06, min loss: 4.06172e-07\n",
      "Epoch: 2317300, elapsed: 1.23e+01, train loss: 4.44172e-07, val loss: 1.53914e-06, min loss: 4.06172e-07\n",
      "Epoch: 2317400, elapsed: 1.27e+01, train loss: 4.06216e-07, val loss: 1.52158e-06, min loss: 4.06172e-07\n",
      "Epoch: 2317500, elapsed: 1.22e+01, train loss: 4.27805e-07, val loss: 1.53566e-06, min loss: 4.06172e-07\n",
      "Epoch: 2317600, elapsed: 1.22e+01, train loss: 6.63410e-07, val loss: 1.90434e-06, min loss: 4.06172e-07\n",
      "Epoch: 2317700, elapsed: 1.25e+01, train loss: 4.16904e-07, val loss: 1.55533e-06, min loss: 4.06172e-07\n",
      "Epoch: 2317800, elapsed: 1.23e+01, train loss: 4.27982e-07, val loss: 1.57368e-06, min loss: 4.06172e-07\n",
      "Epoch: 2317900, elapsed: 1.27e+01, train loss: 4.10975e-07, val loss: 1.54213e-06, min loss: 4.06172e-07\n",
      "Epoch: 2318000, elapsed: 1.21e+01, train loss: 4.10968e-07, val loss: 1.52278e-06, min loss: 4.06172e-07\n",
      "Epoch: 2318100, elapsed: 1.23e+01, train loss: 4.51584e-07, val loss: 1.51915e-06, min loss: 4.06172e-07\n",
      "Epoch: 2318200, elapsed: 1.25e+01, train loss: 4.09054e-07, val loss: 1.55649e-06, min loss: 4.06172e-07\n",
      "Epoch: 2318300, elapsed: 1.23e+01, train loss: 4.06008e-07, val loss: 1.53001e-06, min loss: 4.06008e-07\n",
      "Epoch: 2318400, elapsed: 1.30e+01, train loss: 4.35085e-07, val loss: 1.57336e-06, min loss: 4.06008e-07\n",
      "Epoch: 2318500, elapsed: 1.43e+01, train loss: 4.08053e-07, val loss: 1.52954e-06, min loss: 4.06008e-07\n",
      "Epoch: 2318600, elapsed: 1.35e+01, train loss: 4.07666e-07, val loss: 1.53344e-06, min loss: 4.06008e-07\n",
      "Epoch: 2318700, elapsed: 1.35e+01, train loss: 4.05598e-07, val loss: 1.52038e-06, min loss: 4.05598e-07\n",
      "Epoch: 2318800, elapsed: 1.35e+01, train loss: 4.95844e-07, val loss: 1.69217e-06, min loss: 4.05598e-07\n",
      "Epoch: 2318900, elapsed: 1.37e+01, train loss: 2.88147e-06, val loss: 3.29224e-06, min loss: 4.05598e-07\n",
      "Epoch: 2319000, elapsed: 1.34e+01, train loss: 4.05516e-07, val loss: 1.51991e-06, min loss: 4.05516e-07\n",
      "Epoch: 2319100, elapsed: 1.36e+01, train loss: 4.07853e-07, val loss: 1.53012e-06, min loss: 4.05516e-07\n",
      "Epoch: 2319200, elapsed: 2.13e+01, train loss: 1.22220e-06, val loss: 2.22092e-06, min loss: 4.05516e-07\n",
      "Epoch: 2319300, elapsed: 1.40e+01, train loss: 4.05567e-07, val loss: 1.52528e-06, min loss: 4.05516e-07\n",
      "Epoch: 2319400, elapsed: 1.37e+01, train loss: 4.06735e-07, val loss: 1.53464e-06, min loss: 4.05516e-07\n",
      "Epoch: 2319500, elapsed: 1.38e+01, train loss: 4.26349e-07, val loss: 1.45721e-06, min loss: 4.05516e-07\n",
      "Epoch: 2319600, elapsed: 1.38e+01, train loss: 4.05853e-07, val loss: 1.51627e-06, min loss: 4.05516e-07\n",
      "Epoch: 2319700, elapsed: 1.41e+01, train loss: 4.26445e-06, val loss: 4.77682e-06, min loss: 4.05516e-07\n",
      "Epoch: 2319800, elapsed: 1.37e+01, train loss: 4.05259e-07, val loss: 1.52156e-06, min loss: 4.05259e-07\n",
      "Epoch: 2319900, elapsed: 1.38e+01, train loss: 4.25790e-07, val loss: 1.67004e-06, min loss: 4.05259e-07\n",
      "Epoch: 2320000, elapsed: 1.36e+01, train loss: 4.05085e-07, val loss: 1.52287e-06, min loss: 4.05085e-07\n",
      "Epoch: 2320100, elapsed: 1.64e+01, train loss: 4.63492e-07, val loss: 1.45724e-06, min loss: 4.05085e-07\n",
      "Epoch: 2320200, elapsed: 1.36e+01, train loss: 4.44778e-07, val loss: 1.66081e-06, min loss: 4.05085e-07\n",
      "Epoch: 2320300, elapsed: 1.41e+01, train loss: 1.92020e-06, val loss: 2.38933e-06, min loss: 4.05085e-07\n",
      "Epoch: 2320400, elapsed: 1.36e+01, train loss: 4.88224e-07, val loss: 1.60423e-06, min loss: 4.05085e-07\n",
      "Epoch: 2320500, elapsed: 1.36e+01, train loss: 4.06202e-07, val loss: 1.52625e-06, min loss: 4.05085e-07\n",
      "Epoch: 2320600, elapsed: 1.36e+01, train loss: 5.00380e-07, val loss: 1.60808e-06, min loss: 4.05085e-07\n",
      "Epoch: 2320700, elapsed: 1.41e+01, train loss: 4.05110e-07, val loss: 1.52220e-06, min loss: 4.05085e-07\n",
      "Epoch: 2320800, elapsed: 1.33e+01, train loss: 4.07225e-07, val loss: 1.52743e-06, min loss: 4.05085e-07\n",
      "Epoch: 2320900, elapsed: 1.37e+01, train loss: 4.11162e-07, val loss: 1.54038e-06, min loss: 4.05085e-07\n",
      "Epoch: 2321000, elapsed: 1.30e+01, train loss: 3.79472e-06, val loss: 4.40378e-06, min loss: 4.05085e-07\n",
      "Epoch: 2321100, elapsed: 1.25e+01, train loss: 4.04959e-07, val loss: 1.51673e-06, min loss: 4.04959e-07\n",
      "Epoch: 2321200, elapsed: 1.25e+01, train loss: 5.03580e-07, val loss: 1.50720e-06, min loss: 4.04959e-07\n",
      "Epoch: 2321300, elapsed: 1.26e+01, train loss: 4.04790e-07, val loss: 1.52213e-06, min loss: 4.04790e-07\n",
      "Epoch: 2321400, elapsed: 1.23e+01, train loss: 4.19301e-07, val loss: 1.51870e-06, min loss: 4.04790e-07\n",
      "Epoch: 2321500, elapsed: 1.23e+01, train loss: 4.11542e-07, val loss: 1.59400e-06, min loss: 4.04790e-07\n",
      "Epoch: 2321600, elapsed: 1.21e+01, train loss: 1.73779e-06, val loss: 2.53917e-06, min loss: 4.04790e-07\n",
      "Epoch: 2321700, elapsed: 1.21e+01, train loss: 5.04597e-07, val loss: 1.82788e-06, min loss: 4.04790e-07\n",
      "Epoch: 2321800, elapsed: 1.24e+01, train loss: 1.30402e-06, val loss: 2.15117e-06, min loss: 4.04790e-07\n",
      "Epoch: 2321900, elapsed: 1.21e+01, train loss: 4.04730e-07, val loss: 1.52170e-06, min loss: 4.04730e-07\n",
      "Epoch: 2322000, elapsed: 1.21e+01, train loss: 4.10901e-07, val loss: 1.53205e-06, min loss: 4.04730e-07\n",
      "Epoch: 2322100, elapsed: 1.20e+01, train loss: 4.05887e-07, val loss: 1.52786e-06, min loss: 4.04730e-07\n",
      "Epoch: 2322200, elapsed: 1.20e+01, train loss: 4.08900e-07, val loss: 1.54012e-06, min loss: 4.04730e-07\n",
      "Epoch: 2322300, elapsed: 1.20e+01, train loss: 5.00810e-07, val loss: 1.73422e-06, min loss: 4.04730e-07\n",
      "Epoch: 2322400, elapsed: 1.22e+01, train loss: 4.50586e-07, val loss: 1.61589e-06, min loss: 4.04730e-07\n",
      "Epoch: 2322500, elapsed: 1.22e+01, train loss: 4.04730e-07, val loss: 1.52313e-06, min loss: 4.04730e-07\n",
      "Epoch: 2322600, elapsed: 1.24e+01, train loss: 4.19323e-07, val loss: 1.48860e-06, min loss: 4.04730e-07\n",
      "Epoch: 2322700, elapsed: 1.23e+01, train loss: 5.71704e-07, val loss: 1.62091e-06, min loss: 4.04730e-07\n",
      "Epoch: 2322800, elapsed: 1.21e+01, train loss: 6.24002e-07, val loss: 1.77924e-06, min loss: 4.04730e-07\n",
      "Epoch: 2322900, elapsed: 1.22e+01, train loss: 5.04142e-07, val loss: 1.68426e-06, min loss: 4.04730e-07\n",
      "Epoch: 2323000, elapsed: 1.22e+01, train loss: 1.08752e-06, val loss: 2.32334e-06, min loss: 4.04730e-07\n",
      "Epoch: 2323100, elapsed: 1.24e+01, train loss: 4.97871e-07, val loss: 1.70350e-06, min loss: 4.04730e-07\n",
      "Epoch: 2323200, elapsed: 1.19e+01, train loss: 4.19644e-07, val loss: 1.54401e-06, min loss: 4.04730e-07\n",
      "Epoch: 2323300, elapsed: 1.84e+01, train loss: 4.86530e-07, val loss: 1.69158e-06, min loss: 4.04730e-07\n",
      "Epoch: 2323400, elapsed: 1.27e+01, train loss: 1.51074e-06, val loss: 2.81095e-06, min loss: 4.04730e-07\n",
      "Epoch: 2323500, elapsed: 1.24e+01, train loss: 2.46637e-06, val loss: 3.21068e-06, min loss: 4.04730e-07\n",
      "Epoch: 2323600, elapsed: 1.21e+01, train loss: 4.12176e-07, val loss: 1.50033e-06, min loss: 4.04730e-07\n",
      "Epoch: 2323700, elapsed: 1.24e+01, train loss: 4.04946e-07, val loss: 1.51439e-06, min loss: 4.04730e-07\n",
      "Epoch: 2323800, elapsed: 1.23e+01, train loss: 4.32440e-07, val loss: 1.54494e-06, min loss: 4.04730e-07\n",
      "Epoch: 2323900, elapsed: 1.23e+01, train loss: 4.09937e-07, val loss: 1.57710e-06, min loss: 4.04730e-07\n",
      "Epoch: 2324000, elapsed: 1.23e+01, train loss: 4.04474e-07, val loss: 1.51277e-06, min loss: 4.04474e-07\n",
      "Epoch: 2324100, elapsed: 1.25e+01, train loss: 7.34310e-07, val loss: 1.50343e-06, min loss: 4.04474e-07\n",
      "Epoch: 2324200, elapsed: 1.22e+01, train loss: 4.37190e-07, val loss: 1.52788e-06, min loss: 4.04474e-07\n",
      "Epoch: 2324300, elapsed: 1.23e+01, train loss: 4.11267e-07, val loss: 1.54295e-06, min loss: 4.04474e-07\n",
      "Epoch: 2324400, elapsed: 1.21e+01, train loss: 4.06632e-07, val loss: 1.50824e-06, min loss: 4.04474e-07\n",
      "Epoch: 2324500, elapsed: 1.20e+01, train loss: 4.28863e-07, val loss: 1.54387e-06, min loss: 4.04474e-07\n",
      "Epoch: 2324600, elapsed: 1.20e+01, train loss: 5.61229e-07, val loss: 1.64790e-06, min loss: 4.04474e-07\n",
      "Epoch: 2324700, elapsed: 1.23e+01, train loss: 1.16793e-06, val loss: 2.73324e-06, min loss: 4.04474e-07\n",
      "Epoch: 2324800, elapsed: 1.24e+01, train loss: 4.05356e-07, val loss: 1.51545e-06, min loss: 4.04474e-07\n",
      "Epoch: 2324900, elapsed: 1.24e+01, train loss: 4.06829e-07, val loss: 1.53909e-06, min loss: 4.04474e-07\n",
      "Epoch: 2325000, elapsed: 1.25e+01, train loss: 4.24748e-07, val loss: 1.52439e-06, min loss: 4.04474e-07\n",
      "Epoch: 2325100, elapsed: 1.44e+01, train loss: 4.29721e-07, val loss: 1.53034e-06, min loss: 4.04474e-07\n",
      "Epoch: 2325200, elapsed: 1.22e+01, train loss: 2.01658e-06, val loss: 3.61654e-06, min loss: 4.04474e-07\n",
      "Epoch: 2325300, elapsed: 1.22e+01, train loss: 4.04641e-07, val loss: 1.50275e-06, min loss: 4.04474e-07\n",
      "Epoch: 2325400, elapsed: 1.21e+01, train loss: 2.69159e-06, val loss: 3.57103e-06, min loss: 4.04474e-07\n",
      "Epoch: 2325500, elapsed: 1.22e+01, train loss: 4.21039e-07, val loss: 1.54897e-06, min loss: 4.04474e-07\n",
      "Epoch: 2325600, elapsed: 1.22e+01, train loss: 4.06238e-07, val loss: 1.51163e-06, min loss: 4.04474e-07\n",
      "Epoch: 2325700, elapsed: 1.21e+01, train loss: 4.05775e-07, val loss: 1.51001e-06, min loss: 4.04474e-07\n",
      "Epoch: 2325800, elapsed: 1.19e+01, train loss: 5.21102e-07, val loss: 1.58907e-06, min loss: 4.04474e-07\n",
      "Epoch: 2325900, elapsed: 1.22e+01, train loss: 4.23743e-07, val loss: 1.56642e-06, min loss: 4.04474e-07\n",
      "Epoch: 2326000, elapsed: 1.24e+01, train loss: 4.79658e-07, val loss: 1.63262e-06, min loss: 4.04474e-07\n",
      "Epoch: 2326100, elapsed: 1.24e+01, train loss: 4.82726e-07, val loss: 1.52486e-06, min loss: 4.04474e-07\n",
      "Epoch: 2326200, elapsed: 1.23e+01, train loss: 4.75227e-07, val loss: 1.71870e-06, min loss: 4.04474e-07\n",
      "Epoch: 2326300, elapsed: 1.23e+01, train loss: 1.71520e-06, val loss: 2.38770e-06, min loss: 4.04474e-07\n",
      "Epoch: 2326400, elapsed: 1.32e+01, train loss: 2.61312e-06, val loss: 3.69153e-06, min loss: 4.04474e-07\n",
      "Epoch: 2326500, elapsed: 1.37e+01, train loss: 4.24111e-07, val loss: 1.57513e-06, min loss: 4.04474e-07\n",
      "Epoch: 2326600, elapsed: 1.41e+01, train loss: 4.13387e-07, val loss: 1.55893e-06, min loss: 4.04474e-07\n",
      "Epoch: 2326700, elapsed: 1.36e+01, train loss: 4.05501e-07, val loss: 1.50967e-06, min loss: 4.04474e-07\n",
      "Epoch: 2326800, elapsed: 1.32e+01, train loss: 4.08933e-07, val loss: 1.48252e-06, min loss: 4.04474e-07\n",
      "Epoch: 2326900, elapsed: 1.36e+01, train loss: 1.04692e-06, val loss: 2.11506e-06, min loss: 4.04474e-07\n",
      "Epoch: 2327000, elapsed: 1.35e+01, train loss: 4.04008e-07, val loss: 1.51494e-06, min loss: 4.04008e-07\n",
      "Epoch: 2327100, elapsed: 1.35e+01, train loss: 5.04432e-07, val loss: 1.52009e-06, min loss: 4.04008e-07\n",
      "Epoch: 2327200, elapsed: 1.30e+01, train loss: 7.18955e-07, val loss: 1.75484e-06, min loss: 4.04008e-07\n",
      "Epoch: 2327300, elapsed: 1.26e+01, train loss: 4.43530e-07, val loss: 1.57877e-06, min loss: 4.04008e-07\n",
      "Epoch: 2327400, elapsed: 1.88e+01, train loss: 4.52758e-07, val loss: 1.50777e-06, min loss: 4.04008e-07\n",
      "Epoch: 2327500, elapsed: 1.40e+01, train loss: 4.64121e-07, val loss: 1.59002e-06, min loss: 4.04008e-07\n",
      "Epoch: 2327600, elapsed: 1.42e+01, train loss: 7.59321e-07, val loss: 1.59742e-06, min loss: 4.04008e-07\n",
      "Epoch: 2327700, elapsed: 1.44e+01, train loss: 4.03526e-07, val loss: 1.51600e-06, min loss: 4.03526e-07\n",
      "Epoch: 2327800, elapsed: 1.36e+01, train loss: 4.07811e-07, val loss: 1.52314e-06, min loss: 4.03526e-07\n",
      "Epoch: 2327900, elapsed: 1.40e+01, train loss: 4.90661e-07, val loss: 1.54686e-06, min loss: 4.03526e-07\n",
      "Epoch: 2328000, elapsed: 1.22e+01, train loss: 4.85891e-07, val loss: 1.66933e-06, min loss: 4.03526e-07\n",
      "Epoch: 2328100, elapsed: 1.24e+01, train loss: 4.05757e-07, val loss: 1.52506e-06, min loss: 4.03526e-07\n",
      "Epoch: 2328200, elapsed: 1.21e+01, train loss: 4.04149e-07, val loss: 1.53113e-06, min loss: 4.03526e-07\n",
      "Epoch: 2328300, elapsed: 1.26e+01, train loss: 3.05875e-06, val loss: 3.79738e-06, min loss: 4.03526e-07\n",
      "Epoch: 2328400, elapsed: 1.25e+01, train loss: 4.03279e-07, val loss: 1.51270e-06, min loss: 4.03279e-07\n",
      "Epoch: 2328500, elapsed: 1.23e+01, train loss: 4.10338e-07, val loss: 1.50785e-06, min loss: 4.03279e-07\n",
      "Epoch: 2328600, elapsed: 1.30e+01, train loss: 4.63306e-07, val loss: 1.67147e-06, min loss: 4.03279e-07\n",
      "Epoch: 2328700, elapsed: 1.39e+01, train loss: 6.19231e-07, val loss: 1.56432e-06, min loss: 4.03279e-07\n",
      "Epoch: 2328800, elapsed: 1.32e+01, train loss: 1.17629e-06, val loss: 2.15805e-06, min loss: 4.03279e-07\n",
      "Epoch: 2328900, elapsed: 1.32e+01, train loss: 5.25296e-07, val loss: 1.61074e-06, min loss: 4.03279e-07\n",
      "Epoch: 2329000, elapsed: 1.36e+01, train loss: 1.67520e-06, val loss: 3.16619e-06, min loss: 4.03279e-07\n",
      "Epoch: 2329100, elapsed: 1.28e+01, train loss: 5.53665e-07, val loss: 1.94141e-06, min loss: 4.03279e-07\n",
      "Epoch: 2329200, elapsed: 1.29e+01, train loss: 4.09803e-07, val loss: 1.51832e-06, min loss: 4.03279e-07\n",
      "Epoch: 2329300, elapsed: 1.35e+01, train loss: 6.20006e-07, val loss: 1.72374e-06, min loss: 4.03279e-07\n",
      "Epoch: 2329400, elapsed: 1.36e+01, train loss: 4.03021e-07, val loss: 1.51625e-06, min loss: 4.03021e-07\n",
      "Epoch: 2329500, elapsed: 1.24e+01, train loss: 4.05035e-07, val loss: 1.51236e-06, min loss: 4.03021e-07\n",
      "Epoch: 2329600, elapsed: 1.21e+01, train loss: 1.36376e-06, val loss: 2.80938e-06, min loss: 4.03021e-07\n",
      "Epoch: 2329700, elapsed: 1.20e+01, train loss: 4.03607e-07, val loss: 1.51125e-06, min loss: 4.03021e-07\n",
      "Epoch: 2329800, elapsed: 1.22e+01, train loss: 4.03434e-07, val loss: 1.51564e-06, min loss: 4.03021e-07\n",
      "Epoch: 2329900, elapsed: 1.23e+01, train loss: 3.60594e-06, val loss: 2.42032e-06, min loss: 4.03021e-07\n",
      "Epoch: 2330000, elapsed: 1.23e+01, train loss: 4.03058e-07, val loss: 1.51878e-06, min loss: 4.03021e-07\n",
      "Epoch: 2330100, elapsed: 1.44e+01, train loss: 4.03732e-07, val loss: 1.52791e-06, min loss: 4.03021e-07\n",
      "Epoch: 2330200, elapsed: 1.21e+01, train loss: 4.03295e-07, val loss: 1.51266e-06, min loss: 4.03021e-07\n",
      "Epoch: 2330300, elapsed: 1.20e+01, train loss: 4.30504e-07, val loss: 1.51598e-06, min loss: 4.03021e-07\n",
      "Epoch: 2330400, elapsed: 1.21e+01, train loss: 4.37903e-07, val loss: 1.60658e-06, min loss: 4.03021e-07\n",
      "Epoch: 2330500, elapsed: 1.21e+01, train loss: 6.21208e-07, val loss: 1.84627e-06, min loss: 4.03021e-07\n",
      "Epoch: 2330600, elapsed: 1.21e+01, train loss: 4.16608e-07, val loss: 1.48292e-06, min loss: 4.03021e-07\n",
      "Epoch: 2330700, elapsed: 1.24e+01, train loss: 4.26235e-07, val loss: 1.52318e-06, min loss: 4.03021e-07\n",
      "Epoch: 2330800, elapsed: 1.19e+01, train loss: 4.14249e-07, val loss: 1.50360e-06, min loss: 4.03021e-07\n",
      "Epoch: 2330900, elapsed: 1.20e+01, train loss: 4.34516e-07, val loss: 1.50528e-06, min loss: 4.03021e-07\n",
      "Epoch: 2331000, elapsed: 1.22e+01, train loss: 4.18245e-07, val loss: 1.54125e-06, min loss: 4.03021e-07\n",
      "Epoch: 2331100, elapsed: 1.20e+01, train loss: 4.20875e-07, val loss: 1.57946e-06, min loss: 4.03021e-07\n",
      "Epoch: 2331200, elapsed: 1.23e+01, train loss: 4.09855e-07, val loss: 1.55930e-06, min loss: 4.03021e-07\n",
      "Epoch: 2331300, elapsed: 1.22e+01, train loss: 4.06847e-07, val loss: 1.54684e-06, min loss: 4.03021e-07\n",
      "Epoch: 2331400, elapsed: 1.19e+01, train loss: 5.18034e-07, val loss: 1.57742e-06, min loss: 4.03021e-07\n",
      "Epoch: 2331500, elapsed: 1.84e+01, train loss: 4.15206e-07, val loss: 1.61602e-06, min loss: 4.03021e-07\n",
      "Epoch: 2331600, elapsed: 1.25e+01, train loss: 4.05373e-07, val loss: 1.49288e-06, min loss: 4.03021e-07\n",
      "Epoch: 2331700, elapsed: 1.23e+01, train loss: 4.14335e-07, val loss: 1.52204e-06, min loss: 4.03021e-07\n",
      "Epoch: 2331800, elapsed: 1.26e+01, train loss: 6.69214e-07, val loss: 1.73172e-06, min loss: 4.03021e-07\n",
      "Epoch: 2331900, elapsed: 1.22e+01, train loss: 6.21283e-07, val loss: 1.90500e-06, min loss: 4.03021e-07\n",
      "Epoch: 2332000, elapsed: 1.22e+01, train loss: 6.43010e-07, val loss: 1.65437e-06, min loss: 4.03021e-07\n",
      "Epoch: 2332100, elapsed: 1.23e+01, train loss: 6.50074e-07, val loss: 1.96069e-06, min loss: 4.03021e-07\n",
      "Epoch: 2332200, elapsed: 1.21e+01, train loss: 4.02436e-07, val loss: 1.51138e-06, min loss: 4.02436e-07\n",
      "Epoch: 2332300, elapsed: 1.23e+01, train loss: 4.11890e-07, val loss: 1.57618e-06, min loss: 4.02436e-07\n",
      "Epoch: 2332400, elapsed: 1.25e+01, train loss: 4.02368e-07, val loss: 1.51538e-06, min loss: 4.02368e-07\n",
      "Epoch: 2332500, elapsed: 1.23e+01, train loss: 4.09649e-07, val loss: 1.49447e-06, min loss: 4.02368e-07\n",
      "Epoch: 2332600, elapsed: 1.22e+01, train loss: 3.54109e-06, val loss: 4.42575e-06, min loss: 4.02368e-07\n",
      "Epoch: 2332700, elapsed: 1.25e+01, train loss: 4.02319e-07, val loss: 1.51469e-06, min loss: 4.02319e-07\n",
      "Epoch: 2332800, elapsed: 1.20e+01, train loss: 4.55686e-07, val loss: 1.56458e-06, min loss: 4.02319e-07\n",
      "Epoch: 2332900, elapsed: 1.22e+01, train loss: 4.02422e-07, val loss: 1.51757e-06, min loss: 4.02319e-07\n",
      "Epoch: 2333000, elapsed: 1.23e+01, train loss: 4.03346e-07, val loss: 1.50979e-06, min loss: 4.02319e-07\n",
      "Epoch: 2333100, elapsed: 1.21e+01, train loss: 4.45673e-07, val loss: 1.62470e-06, min loss: 4.02319e-07\n",
      "Epoch: 2333200, elapsed: 1.21e+01, train loss: 4.02563e-07, val loss: 1.52297e-06, min loss: 4.02319e-07\n",
      "Epoch: 2333300, elapsed: 1.23e+01, train loss: 8.89785e-07, val loss: 1.81992e-06, min loss: 4.02319e-07\n",
      "Epoch: 2333400, elapsed: 1.23e+01, train loss: 4.02411e-07, val loss: 1.51522e-06, min loss: 4.02319e-07\n",
      "Epoch: 2333500, elapsed: 1.23e+01, train loss: 4.04958e-07, val loss: 1.51919e-06, min loss: 4.02319e-07\n",
      "Epoch: 2333600, elapsed: 1.21e+01, train loss: 4.21715e-07, val loss: 1.51893e-06, min loss: 4.02319e-07\n",
      "Epoch: 2333700, elapsed: 1.22e+01, train loss: 4.17098e-07, val loss: 1.51936e-06, min loss: 4.02319e-07\n",
      "Epoch: 2333800, elapsed: 1.22e+01, train loss: 4.17659e-07, val loss: 1.48651e-06, min loss: 4.02319e-07\n",
      "Epoch: 2333900, elapsed: 1.24e+01, train loss: 4.02529e-07, val loss: 1.52471e-06, min loss: 4.02319e-07\n",
      "Epoch: 2334000, elapsed: 1.22e+01, train loss: 4.11570e-07, val loss: 1.51955e-06, min loss: 4.02319e-07\n",
      "Epoch: 2334100, elapsed: 1.21e+01, train loss: 4.03481e-07, val loss: 1.50534e-06, min loss: 4.02319e-07\n",
      "Epoch: 2334200, elapsed: 1.22e+01, train loss: 4.03112e-07, val loss: 1.51079e-06, min loss: 4.02319e-07\n",
      "Epoch: 2334300, elapsed: 1.23e+01, train loss: 8.81749e-07, val loss: 1.91356e-06, min loss: 4.02319e-07\n",
      "Epoch: 2334400, elapsed: 1.19e+01, train loss: 4.01999e-07, val loss: 1.51088e-06, min loss: 4.01999e-07\n",
      "Epoch: 2334500, elapsed: 1.23e+01, train loss: 4.12013e-07, val loss: 1.51899e-06, min loss: 4.01999e-07\n",
      "Epoch: 2334600, elapsed: 1.21e+01, train loss: 5.18711e-07, val loss: 1.46856e-06, min loss: 4.01999e-07\n",
      "Epoch: 2334700, elapsed: 1.19e+01, train loss: 4.53248e-07, val loss: 1.50198e-06, min loss: 4.01999e-07\n",
      "Epoch: 2334800, elapsed: 1.22e+01, train loss: 4.05297e-07, val loss: 1.51058e-06, min loss: 4.01999e-07\n",
      "Epoch: 2334900, elapsed: 1.21e+01, train loss: 4.20990e-07, val loss: 1.58665e-06, min loss: 4.01999e-07\n",
      "Epoch: 2335000, elapsed: 1.21e+01, train loss: 4.42987e-07, val loss: 1.51132e-06, min loss: 4.01999e-07\n",
      "Epoch: 2335100, elapsed: 1.42e+01, train loss: 6.19897e-07, val loss: 1.58386e-06, min loss: 4.01999e-07\n",
      "Epoch: 2335200, elapsed: 1.21e+01, train loss: 4.32341e-07, val loss: 1.52551e-06, min loss: 4.01999e-07\n",
      "Epoch: 2335300, elapsed: 1.19e+01, train loss: 5.56127e-07, val loss: 1.67837e-06, min loss: 4.01999e-07\n",
      "Epoch: 2335400, elapsed: 1.20e+01, train loss: 5.08911e-07, val loss: 1.56484e-06, min loss: 4.01999e-07\n",
      "Epoch: 2335500, elapsed: 1.23e+01, train loss: 2.02619e-06, val loss: 3.41138e-06, min loss: 4.01999e-07\n",
      "Epoch: 2335600, elapsed: 1.86e+01, train loss: 1.11552e-06, val loss: 2.43568e-06, min loss: 4.01999e-07\n",
      "Epoch: 2335700, elapsed: 1.27e+01, train loss: 3.29811e-06, val loss: 3.72551e-06, min loss: 4.01999e-07\n",
      "Epoch: 2335800, elapsed: 1.27e+01, train loss: 4.62440e-07, val loss: 1.59711e-06, min loss: 4.01999e-07\n",
      "Epoch: 2335900, elapsed: 1.25e+01, train loss: 4.02754e-07, val loss: 1.50365e-06, min loss: 4.01999e-07\n",
      "Epoch: 2336000, elapsed: 1.22e+01, train loss: 4.20440e-07, val loss: 1.46665e-06, min loss: 4.01999e-07\n",
      "Epoch: 2336100, elapsed: 1.26e+01, train loss: 2.59074e-06, val loss: 2.66202e-06, min loss: 4.01999e-07\n",
      "Epoch: 2336200, elapsed: 1.23e+01, train loss: 4.01859e-07, val loss: 1.51296e-06, min loss: 4.01859e-07\n",
      "Epoch: 2336300, elapsed: 1.22e+01, train loss: 4.07324e-07, val loss: 1.51963e-06, min loss: 4.01859e-07\n",
      "Epoch: 2336400, elapsed: 1.24e+01, train loss: 5.13690e-07, val loss: 1.66658e-06, min loss: 4.01859e-07\n",
      "Epoch: 2336500, elapsed: 1.21e+01, train loss: 4.11577e-07, val loss: 1.54853e-06, min loss: 4.01859e-07\n",
      "Epoch: 2336600, elapsed: 1.21e+01, train loss: 4.40817e-07, val loss: 1.47144e-06, min loss: 4.01859e-07\n",
      "Epoch: 2336700, elapsed: 1.25e+01, train loss: 4.65192e-07, val loss: 1.63231e-06, min loss: 4.01859e-07\n",
      "Epoch: 2336800, elapsed: 1.22e+01, train loss: 4.01543e-07, val loss: 1.50795e-06, min loss: 4.01543e-07\n",
      "Epoch: 2336900, elapsed: 1.23e+01, train loss: 1.07135e-06, val loss: 2.23596e-06, min loss: 4.01543e-07\n",
      "Epoch: 2337000, elapsed: 1.23e+01, train loss: 4.01443e-07, val loss: 1.51019e-06, min loss: 4.01443e-07\n",
      "Epoch: 2337100, elapsed: 1.23e+01, train loss: 5.12998e-07, val loss: 1.52818e-06, min loss: 4.01443e-07\n",
      "Epoch: 2337200, elapsed: 1.23e+01, train loss: 4.14502e-07, val loss: 1.48018e-06, min loss: 4.01443e-07\n",
      "Epoch: 2337300, elapsed: 1.23e+01, train loss: 4.30499e-07, val loss: 1.48979e-06, min loss: 4.01443e-07\n",
      "Epoch: 2337400, elapsed: 1.25e+01, train loss: 6.65927e-07, val loss: 1.63815e-06, min loss: 4.01443e-07\n",
      "Epoch: 2337500, elapsed: 1.22e+01, train loss: 4.18736e-07, val loss: 1.54995e-06, min loss: 4.01443e-07\n",
      "Epoch: 2337600, elapsed: 1.20e+01, train loss: 5.85683e-07, val loss: 1.81190e-06, min loss: 4.01443e-07\n",
      "Epoch: 2337700, elapsed: 1.22e+01, train loss: 5.69326e-07, val loss: 1.59685e-06, min loss: 4.01443e-07\n",
      "Epoch: 2337800, elapsed: 1.22e+01, train loss: 4.57780e-07, val loss: 1.62612e-06, min loss: 4.01443e-07\n",
      "Epoch: 2337900, elapsed: 1.21e+01, train loss: 4.47290e-07, val loss: 1.55791e-06, min loss: 4.01443e-07\n",
      "Epoch: 2338000, elapsed: 1.22e+01, train loss: 8.23688e-07, val loss: 1.82423e-06, min loss: 4.01443e-07\n",
      "Epoch: 2338100, elapsed: 1.21e+01, train loss: 4.01688e-07, val loss: 1.51467e-06, min loss: 4.01443e-07\n",
      "Epoch: 2338200, elapsed: 1.21e+01, train loss: 4.21808e-07, val loss: 1.57403e-06, min loss: 4.01443e-07\n",
      "Epoch: 2338300, elapsed: 1.24e+01, train loss: 6.23842e-07, val loss: 1.87091e-06, min loss: 4.01443e-07\n",
      "Epoch: 2338400, elapsed: 1.20e+01, train loss: 4.11445e-07, val loss: 1.57710e-06, min loss: 4.01443e-07\n",
      "Epoch: 2338500, elapsed: 1.22e+01, train loss: 4.13809e-07, val loss: 1.55907e-06, min loss: 4.01443e-07\n",
      "Epoch: 2338600, elapsed: 1.23e+01, train loss: 4.12518e-07, val loss: 1.53697e-06, min loss: 4.01443e-07\n",
      "Epoch: 2338700, elapsed: 1.22e+01, train loss: 4.20862e-07, val loss: 1.48145e-06, min loss: 4.01443e-07\n",
      "Epoch: 2338800, elapsed: 1.20e+01, train loss: 1.58548e-06, val loss: 1.83621e-06, min loss: 4.01443e-07\n",
      "Epoch: 2338900, elapsed: 1.21e+01, train loss: 4.05721e-07, val loss: 1.56323e-06, min loss: 4.01443e-07\n",
      "Epoch: 2339000, elapsed: 1.22e+01, train loss: 4.02580e-07, val loss: 1.51490e-06, min loss: 4.01443e-07\n",
      "Epoch: 2339100, elapsed: 1.22e+01, train loss: 7.49349e-07, val loss: 1.87552e-06, min loss: 4.01443e-07\n",
      "Epoch: 2339200, elapsed: 1.22e+01, train loss: 4.00990e-07, val loss: 1.51114e-06, min loss: 4.00990e-07\n",
      "Epoch: 2339300, elapsed: 1.22e+01, train loss: 4.06845e-07, val loss: 1.51947e-06, min loss: 4.00990e-07\n",
      "Epoch: 2339400, elapsed: 1.22e+01, train loss: 4.70342e-07, val loss: 1.60672e-06, min loss: 4.00990e-07\n",
      "Epoch: 2339500, elapsed: 1.21e+01, train loss: 4.48508e-07, val loss: 1.56558e-06, min loss: 4.00990e-07\n",
      "Epoch: 2339600, elapsed: 1.21e+01, train loss: 4.01298e-07, val loss: 1.51161e-06, min loss: 4.00990e-07\n",
      "Epoch: 2339700, elapsed: 1.24e+01, train loss: 5.61932e-07, val loss: 1.70640e-06, min loss: 4.00990e-07\n",
      "Epoch: 2339800, elapsed: 1.85e+01, train loss: 5.17551e-07, val loss: 1.72586e-06, min loss: 4.00990e-07\n",
      "Epoch: 2339900, elapsed: 1.22e+01, train loss: 4.04159e-07, val loss: 1.52457e-06, min loss: 4.00990e-07\n",
      "Epoch: 2340000, elapsed: 1.21e+01, train loss: 5.02255e-07, val loss: 1.78216e-06, min loss: 4.00990e-07\n",
      "Epoch: 2340100, elapsed: 1.45e+01, train loss: 4.13039e-07, val loss: 1.54358e-06, min loss: 4.00990e-07\n",
      "Epoch: 2340200, elapsed: 1.20e+01, train loss: 1.30175e-06, val loss: 2.31227e-06, min loss: 4.00990e-07\n",
      "Epoch: 2340300, elapsed: 1.24e+01, train loss: 4.00929e-07, val loss: 1.51156e-06, min loss: 4.00929e-07\n",
      "Epoch: 2340400, elapsed: 1.22e+01, train loss: 4.04861e-07, val loss: 1.51654e-06, min loss: 4.00929e-07\n",
      "Epoch: 2340500, elapsed: 1.24e+01, train loss: 5.35091e-07, val loss: 1.82851e-06, min loss: 4.00929e-07\n",
      "Epoch: 2340600, elapsed: 1.24e+01, train loss: 8.17371e-07, val loss: 2.03612e-06, min loss: 4.00929e-07\n",
      "Epoch: 2340700, elapsed: 1.23e+01, train loss: 2.16242e-06, val loss: 3.14348e-06, min loss: 4.00929e-07\n",
      "Epoch: 2340800, elapsed: 1.24e+01, train loss: 5.09516e-07, val loss: 1.67489e-06, min loss: 4.00929e-07\n",
      "Epoch: 2340900, elapsed: 1.26e+01, train loss: 4.88179e-07, val loss: 1.69877e-06, min loss: 4.00929e-07\n",
      "Epoch: 2341000, elapsed: 1.22e+01, train loss: 4.00696e-07, val loss: 1.50471e-06, min loss: 4.00696e-07\n",
      "Epoch: 2341100, elapsed: 1.22e+01, train loss: 6.91769e-07, val loss: 2.24698e-06, min loss: 4.00696e-07\n",
      "Epoch: 2341200, elapsed: 1.23e+01, train loss: 4.00530e-07, val loss: 1.50741e-06, min loss: 4.00530e-07\n",
      "Epoch: 2341300, elapsed: 1.25e+01, train loss: 1.71572e-06, val loss: 3.00256e-06, min loss: 4.00530e-07\n",
      "Epoch: 2341400, elapsed: 1.24e+01, train loss: 8.15713e-07, val loss: 1.90427e-06, min loss: 4.00530e-07\n",
      "Epoch: 2341500, elapsed: 1.22e+01, train loss: 6.28454e-07, val loss: 1.81870e-06, min loss: 4.00530e-07\n",
      "Epoch: 2341600, elapsed: 1.20e+01, train loss: 4.00707e-07, val loss: 1.50664e-06, min loss: 4.00530e-07\n",
      "Epoch: 2341700, elapsed: 1.21e+01, train loss: 4.04936e-07, val loss: 1.49557e-06, min loss: 4.00530e-07\n",
      "Epoch: 2341800, elapsed: 1.21e+01, train loss: 4.57274e-07, val loss: 1.59764e-06, min loss: 4.00530e-07\n",
      "Epoch: 2341900, elapsed: 1.21e+01, train loss: 5.04352e-07, val loss: 1.49790e-06, min loss: 4.00530e-07\n",
      "Epoch: 2342000, elapsed: 1.21e+01, train loss: 4.00480e-07, val loss: 1.50612e-06, min loss: 4.00480e-07\n",
      "Epoch: 2342100, elapsed: 1.22e+01, train loss: 4.27493e-07, val loss: 1.52431e-06, min loss: 4.00480e-07\n",
      "Epoch: 2342200, elapsed: 1.21e+01, train loss: 4.00283e-07, val loss: 1.50925e-06, min loss: 4.00283e-07\n",
      "Epoch: 2342300, elapsed: 1.21e+01, train loss: 4.07719e-07, val loss: 1.54080e-06, min loss: 4.00283e-07\n",
      "Epoch: 2342400, elapsed: 1.23e+01, train loss: 4.00642e-07, val loss: 1.50996e-06, min loss: 4.00283e-07\n",
      "Epoch: 2342500, elapsed: 1.18e+01, train loss: 4.00540e-07, val loss: 1.51045e-06, min loss: 4.00283e-07\n",
      "Epoch: 2342600, elapsed: 1.22e+01, train loss: 8.94601e-07, val loss: 2.65712e-06, min loss: 4.00283e-07\n",
      "Epoch: 2342700, elapsed: 1.22e+01, train loss: 4.00246e-07, val loss: 1.50647e-06, min loss: 4.00246e-07\n",
      "Epoch: 2342800, elapsed: 1.20e+01, train loss: 4.45836e-07, val loss: 1.59809e-06, min loss: 4.00246e-07\n",
      "Epoch: 2342900, elapsed: 1.21e+01, train loss: 4.17369e-07, val loss: 1.56262e-06, min loss: 4.00246e-07\n",
      "Epoch: 2343000, elapsed: 1.21e+01, train loss: 4.00721e-07, val loss: 1.50768e-06, min loss: 4.00246e-07\n",
      "Epoch: 2343100, elapsed: 1.21e+01, train loss: 4.70522e-07, val loss: 1.90092e-06, min loss: 4.00246e-07\n",
      "Epoch: 2343200, elapsed: 1.22e+01, train loss: 4.01190e-07, val loss: 1.51188e-06, min loss: 4.00246e-07\n",
      "Epoch: 2343300, elapsed: 1.22e+01, train loss: 4.01673e-07, val loss: 1.52901e-06, min loss: 4.00246e-07\n",
      "Epoch: 2343400, elapsed: 1.22e+01, train loss: 4.02012e-07, val loss: 1.51999e-06, min loss: 4.00246e-07\n",
      "Epoch: 2343500, elapsed: 1.22e+01, train loss: 6.62135e-07, val loss: 1.65352e-06, min loss: 4.00246e-07\n",
      "Epoch: 2343600, elapsed: 1.21e+01, train loss: 3.99989e-07, val loss: 1.50865e-06, min loss: 3.99989e-07\n",
      "Epoch: 2343700, elapsed: 1.22e+01, train loss: 4.05216e-07, val loss: 1.52592e-06, min loss: 3.99989e-07\n",
      "Epoch: 2343800, elapsed: 1.23e+01, train loss: 4.03657e-07, val loss: 1.51823e-06, min loss: 3.99989e-07\n",
      "Epoch: 2343900, elapsed: 1.88e+01, train loss: 4.77989e-07, val loss: 1.49385e-06, min loss: 3.99989e-07\n",
      "Epoch: 2344000, elapsed: 1.23e+01, train loss: 4.00864e-07, val loss: 1.50777e-06, min loss: 3.99989e-07\n",
      "Epoch: 2344100, elapsed: 1.22e+01, train loss: 4.03922e-07, val loss: 1.50692e-06, min loss: 3.99989e-07\n",
      "Epoch: 2344200, elapsed: 1.24e+01, train loss: 7.20185e-07, val loss: 2.32908e-06, min loss: 3.99989e-07\n",
      "Epoch: 2344300, elapsed: 1.22e+01, train loss: 3.99881e-07, val loss: 1.50457e-06, min loss: 3.99881e-07\n",
      "Epoch: 2344400, elapsed: 1.22e+01, train loss: 4.17521e-07, val loss: 1.51913e-06, min loss: 3.99881e-07\n",
      "Epoch: 2344500, elapsed: 1.22e+01, train loss: 8.20621e-07, val loss: 2.10073e-06, min loss: 3.99881e-07\n",
      "Epoch: 2344600, elapsed: 1.23e+01, train loss: 4.61525e-07, val loss: 1.57552e-06, min loss: 3.99881e-07\n",
      "Epoch: 2344700, elapsed: 1.23e+01, train loss: 4.12740e-07, val loss: 1.48947e-06, min loss: 3.99881e-07\n",
      "Epoch: 2344800, elapsed: 1.24e+01, train loss: 4.02617e-07, val loss: 1.51037e-06, min loss: 3.99881e-07\n",
      "Epoch: 2344900, elapsed: 1.20e+01, train loss: 4.12790e-07, val loss: 1.46155e-06, min loss: 3.99881e-07\n",
      "Epoch: 2345000, elapsed: 1.23e+01, train loss: 8.07118e-07, val loss: 2.00745e-06, min loss: 3.99881e-07\n",
      "Epoch: 2345100, elapsed: 1.44e+01, train loss: 4.00182e-07, val loss: 1.50579e-06, min loss: 3.99881e-07\n",
      "Epoch: 2345200, elapsed: 1.25e+01, train loss: 4.00134e-07, val loss: 1.50090e-06, min loss: 3.99881e-07\n",
      "Epoch: 2345300, elapsed: 1.20e+01, train loss: 1.20511e-06, val loss: 2.22739e-06, min loss: 3.99881e-07\n",
      "Epoch: 2345400, elapsed: 1.23e+01, train loss: 4.96413e-07, val loss: 1.52461e-06, min loss: 3.99881e-07\n",
      "Epoch: 2345500, elapsed: 1.22e+01, train loss: 4.45072e-07, val loss: 1.47178e-06, min loss: 3.99881e-07\n",
      "Epoch: 2345600, elapsed: 1.24e+01, train loss: 3.99680e-07, val loss: 1.49933e-06, min loss: 3.99680e-07\n",
      "Epoch: 2345700, elapsed: 1.25e+01, train loss: 4.01629e-07, val loss: 1.49923e-06, min loss: 3.99680e-07\n",
      "Epoch: 2345800, elapsed: 1.21e+01, train loss: 4.73892e-07, val loss: 1.70551e-06, min loss: 3.99680e-07\n",
      "Epoch: 2345900, elapsed: 1.24e+01, train loss: 5.92292e-07, val loss: 1.99302e-06, min loss: 3.99680e-07\n",
      "Epoch: 2346000, elapsed: 1.21e+01, train loss: 3.99539e-07, val loss: 1.50969e-06, min loss: 3.99539e-07\n",
      "Epoch: 2346100, elapsed: 1.23e+01, train loss: 4.04610e-07, val loss: 1.51978e-06, min loss: 3.99539e-07\n",
      "Epoch: 2346200, elapsed: 1.21e+01, train loss: 3.57277e-06, val loss: 4.53794e-06, min loss: 3.99539e-07\n",
      "Epoch: 2346300, elapsed: 1.20e+01, train loss: 3.99574e-07, val loss: 1.50403e-06, min loss: 3.99539e-07\n",
      "Epoch: 2346400, elapsed: 1.21e+01, train loss: 4.82115e-07, val loss: 1.50832e-06, min loss: 3.99539e-07\n",
      "Epoch: 2346500, elapsed: 1.19e+01, train loss: 3.99587e-07, val loss: 1.50695e-06, min loss: 3.99539e-07\n",
      "Epoch: 2346600, elapsed: 1.21e+01, train loss: 4.06261e-07, val loss: 1.51136e-06, min loss: 3.99539e-07\n",
      "Epoch: 2346700, elapsed: 1.22e+01, train loss: 3.13955e-06, val loss: 3.57210e-06, min loss: 3.99539e-07\n",
      "Epoch: 2346800, elapsed: 1.22e+01, train loss: 3.99521e-07, val loss: 1.51288e-06, min loss: 3.99521e-07\n",
      "Epoch: 2346900, elapsed: 1.22e+01, train loss: 4.02766e-07, val loss: 1.50570e-06, min loss: 3.99521e-07\n",
      "Epoch: 2347000, elapsed: 1.18e+01, train loss: 4.15734e-07, val loss: 1.51852e-06, min loss: 3.99521e-07\n",
      "Epoch: 2347100, elapsed: 1.21e+01, train loss: 2.46331e-06, val loss: 3.59271e-06, min loss: 3.99521e-07\n",
      "Epoch: 2347200, elapsed: 1.19e+01, train loss: 3.99625e-07, val loss: 1.50060e-06, min loss: 3.99521e-07\n",
      "Epoch: 2347300, elapsed: 1.21e+01, train loss: 4.01631e-07, val loss: 1.50350e-06, min loss: 3.99521e-07\n",
      "Epoch: 2347400, elapsed: 1.20e+01, train loss: 5.33073e-07, val loss: 1.54408e-06, min loss: 3.99521e-07\n",
      "Epoch: 2347500, elapsed: 1.23e+01, train loss: 1.11561e-06, val loss: 1.71291e-06, min loss: 3.99521e-07\n",
      "Epoch: 2347600, elapsed: 1.21e+01, train loss: 3.99240e-07, val loss: 1.50507e-06, min loss: 3.99240e-07\n",
      "Epoch: 2347700, elapsed: 1.21e+01, train loss: 4.40312e-07, val loss: 1.49986e-06, min loss: 3.99240e-07\n",
      "Epoch: 2347800, elapsed: 1.22e+01, train loss: 4.75654e-07, val loss: 1.64701e-06, min loss: 3.99240e-07\n",
      "Epoch: 2347900, elapsed: 1.22e+01, train loss: 5.05070e-07, val loss: 1.56180e-06, min loss: 3.99240e-07\n",
      "Epoch: 2348000, elapsed: 1.82e+01, train loss: 9.25043e-07, val loss: 1.76260e-06, min loss: 3.99240e-07\n",
      "Epoch: 2348100, elapsed: 1.26e+01, train loss: 1.67013e-06, val loss: 2.56345e-06, min loss: 3.99240e-07\n",
      "Epoch: 2348200, elapsed: 1.22e+01, train loss: 4.00250e-07, val loss: 1.51264e-06, min loss: 3.99240e-07\n",
      "Epoch: 2348300, elapsed: 1.22e+01, train loss: 4.00682e-07, val loss: 1.49302e-06, min loss: 3.99240e-07\n",
      "Epoch: 2348400, elapsed: 1.23e+01, train loss: 4.02110e-07, val loss: 1.48749e-06, min loss: 3.99240e-07\n",
      "Epoch: 2348500, elapsed: 1.25e+01, train loss: 5.30455e-07, val loss: 1.63079e-06, min loss: 3.99240e-07\n",
      "Epoch: 2348600, elapsed: 1.25e+01, train loss: 3.99094e-07, val loss: 1.50255e-06, min loss: 3.99094e-07\n",
      "Epoch: 2348700, elapsed: 1.24e+01, train loss: 4.65410e-07, val loss: 1.45485e-06, min loss: 3.99094e-07\n",
      "Epoch: 2348800, elapsed: 1.25e+01, train loss: 3.98908e-07, val loss: 1.50359e-06, min loss: 3.98908e-07\n",
      "Epoch: 2348900, elapsed: 1.22e+01, train loss: 4.04653e-07, val loss: 1.49683e-06, min loss: 3.98908e-07\n",
      "Epoch: 2349000, elapsed: 1.24e+01, train loss: 3.98830e-07, val loss: 1.50154e-06, min loss: 3.98830e-07\n",
      "Epoch: 2349100, elapsed: 1.21e+01, train loss: 4.02790e-07, val loss: 1.49780e-06, min loss: 3.98830e-07\n",
      "Epoch: 2349200, elapsed: 1.23e+01, train loss: 4.20872e-07, val loss: 1.43164e-06, min loss: 3.98830e-07\n",
      "Epoch: 2349300, elapsed: 1.21e+01, train loss: 3.99511e-07, val loss: 1.49722e-06, min loss: 3.98830e-07\n",
      "Epoch: 2349400, elapsed: 1.20e+01, train loss: 1.87635e-06, val loss: 2.30624e-06, min loss: 3.98830e-07\n",
      "Epoch: 2349500, elapsed: 1.24e+01, train loss: 3.98757e-07, val loss: 1.50220e-06, min loss: 3.98757e-07\n",
      "Epoch: 2349600, elapsed: 1.21e+01, train loss: 4.74351e-07, val loss: 1.70840e-06, min loss: 3.98757e-07\n",
      "Epoch: 2349700, elapsed: 1.23e+01, train loss: 4.10954e-07, val loss: 1.54408e-06, min loss: 3.98757e-07\n",
      "Epoch: 2349800, elapsed: 1.22e+01, train loss: 3.99178e-07, val loss: 1.50206e-06, min loss: 3.98757e-07\n",
      "Epoch: 2349900, elapsed: 1.21e+01, train loss: 5.04721e-07, val loss: 1.46807e-06, min loss: 3.98757e-07\n",
      "Epoch: 2350000, elapsed: 1.23e+01, train loss: 6.11468e-07, val loss: 1.73690e-06, min loss: 3.98757e-07\n",
      "Epoch: 2350100, elapsed: 1.43e+01, train loss: 5.11792e-07, val loss: 1.53075e-06, min loss: 3.98757e-07\n",
      "Epoch: 2350200, elapsed: 1.21e+01, train loss: 3.98794e-07, val loss: 1.49926e-06, min loss: 3.98757e-07\n",
      "Epoch: 2350300, elapsed: 1.23e+01, train loss: 2.58896e-06, val loss: 3.18784e-06, min loss: 3.98757e-07\n",
      "Epoch: 2350400, elapsed: 1.21e+01, train loss: 3.99092e-07, val loss: 1.49715e-06, min loss: 3.98757e-07\n",
      "Epoch: 2350500, elapsed: 1.24e+01, train loss: 3.99155e-07, val loss: 1.50224e-06, min loss: 3.98757e-07\n",
      "Epoch: 2350600, elapsed: 1.23e+01, train loss: 1.00991e-06, val loss: 1.63755e-06, min loss: 3.98757e-07\n",
      "Epoch: 2350700, elapsed: 1.20e+01, train loss: 4.12681e-07, val loss: 1.51974e-06, min loss: 3.98757e-07\n",
      "Epoch: 2350800, elapsed: 1.23e+01, train loss: 3.98925e-07, val loss: 1.51738e-06, min loss: 3.98757e-07\n",
      "Epoch: 2350900, elapsed: 1.25e+01, train loss: 4.69971e-07, val loss: 1.45428e-06, min loss: 3.98757e-07\n",
      "Epoch: 2351000, elapsed: 1.24e+01, train loss: 3.99042e-07, val loss: 1.49809e-06, min loss: 3.98757e-07\n",
      "Epoch: 2351100, elapsed: 1.21e+01, train loss: 4.14338e-07, val loss: 1.51064e-06, min loss: 3.98757e-07\n",
      "Epoch: 2351200, elapsed: 1.21e+01, train loss: 6.01844e-07, val loss: 1.63547e-06, min loss: 3.98757e-07\n",
      "Epoch: 2351300, elapsed: 1.22e+01, train loss: 1.38510e-06, val loss: 2.51429e-06, min loss: 3.98757e-07\n",
      "Epoch: 2351400, elapsed: 1.22e+01, train loss: 4.33889e-07, val loss: 1.50540e-06, min loss: 3.98757e-07\n",
      "Epoch: 2351500, elapsed: 1.22e+01, train loss: 3.99449e-07, val loss: 1.49630e-06, min loss: 3.98757e-07\n",
      "Epoch: 2351600, elapsed: 1.22e+01, train loss: 6.15727e-07, val loss: 1.69591e-06, min loss: 3.98757e-07\n",
      "Epoch: 2351700, elapsed: 1.23e+01, train loss: 2.99272e-06, val loss: 4.30766e-06, min loss: 3.98757e-07\n",
      "Epoch: 2351800, elapsed: 1.21e+01, train loss: 4.28393e-07, val loss: 1.65926e-06, min loss: 3.98757e-07\n",
      "Epoch: 2351900, elapsed: 1.24e+01, train loss: 4.09674e-07, val loss: 1.48372e-06, min loss: 3.98757e-07\n",
      "Epoch: 2352000, elapsed: 1.25e+01, train loss: 4.61586e-07, val loss: 1.72001e-06, min loss: 3.98757e-07\n",
      "Epoch: 2352100, elapsed: 1.19e+01, train loss: 4.12452e-07, val loss: 1.58262e-06, min loss: 3.98757e-07\n",
      "Epoch: 2352200, elapsed: 1.88e+01, train loss: 4.06953e-07, val loss: 1.53039e-06, min loss: 3.98757e-07\n",
      "Epoch: 2352300, elapsed: 1.24e+01, train loss: 4.16385e-07, val loss: 1.50288e-06, min loss: 3.98757e-07\n",
      "Epoch: 2352400, elapsed: 1.23e+01, train loss: 1.16433e-06, val loss: 1.63264e-06, min loss: 3.98757e-07\n",
      "Epoch: 2352500, elapsed: 1.25e+01, train loss: 5.28769e-07, val loss: 1.74024e-06, min loss: 3.98757e-07\n",
      "Epoch: 2352600, elapsed: 1.25e+01, train loss: 4.12483e-07, val loss: 1.51305e-06, min loss: 3.98757e-07\n",
      "Epoch: 2352700, elapsed: 1.25e+01, train loss: 4.05522e-07, val loss: 1.48143e-06, min loss: 3.98757e-07\n",
      "Epoch: 2352800, elapsed: 1.24e+01, train loss: 7.19366e-07, val loss: 1.78573e-06, min loss: 3.98757e-07\n",
      "Epoch: 2352900, elapsed: 1.23e+01, train loss: 3.98189e-07, val loss: 1.50076e-06, min loss: 3.98189e-07\n",
      "Epoch: 2353000, elapsed: 1.23e+01, train loss: 4.97856e-07, val loss: 1.65974e-06, min loss: 3.98189e-07\n",
      "Epoch: 2353100, elapsed: 1.26e+01, train loss: 4.00138e-07, val loss: 1.53098e-06, min loss: 3.98189e-07\n",
      "Epoch: 2353200, elapsed: 1.26e+01, train loss: 3.99192e-07, val loss: 1.50956e-06, min loss: 3.98189e-07\n",
      "Epoch: 2353300, elapsed: 1.23e+01, train loss: 4.48919e-07, val loss: 1.55169e-06, min loss: 3.98189e-07\n",
      "Epoch: 2353400, elapsed: 1.25e+01, train loss: 5.90495e-07, val loss: 1.70355e-06, min loss: 3.98189e-07\n",
      "Epoch: 2353500, elapsed: 1.21e+01, train loss: 4.33652e-07, val loss: 1.57087e-06, min loss: 3.98189e-07\n",
      "Epoch: 2353600, elapsed: 1.25e+01, train loss: 4.24465e-07, val loss: 1.46708e-06, min loss: 3.98189e-07\n",
      "Epoch: 2353700, elapsed: 1.23e+01, train loss: 4.05886e-07, val loss: 1.51882e-06, min loss: 3.98189e-07\n",
      "Epoch: 2353800, elapsed: 1.28e+01, train loss: 4.14580e-07, val loss: 1.47276e-06, min loss: 3.98189e-07\n",
      "Epoch: 2353900, elapsed: 1.25e+01, train loss: 4.09694e-07, val loss: 1.51320e-06, min loss: 3.98189e-07\n",
      "Epoch: 2354000, elapsed: 1.25e+01, train loss: 4.04595e-07, val loss: 1.49469e-06, min loss: 3.98189e-07\n",
      "Epoch: 2354100, elapsed: 1.22e+01, train loss: 4.04077e-07, val loss: 1.54179e-06, min loss: 3.98189e-07\n",
      "Epoch: 2354200, elapsed: 1.25e+01, train loss: 4.02300e-07, val loss: 1.52033e-06, min loss: 3.98189e-07\n",
      "Epoch: 2354300, elapsed: 1.23e+01, train loss: 5.61411e-07, val loss: 1.55450e-06, min loss: 3.98189e-07\n",
      "Epoch: 2354400, elapsed: 1.22e+01, train loss: 4.07169e-07, val loss: 1.52831e-06, min loss: 3.98189e-07\n",
      "Epoch: 2354500, elapsed: 1.21e+01, train loss: 3.99242e-07, val loss: 1.49300e-06, min loss: 3.98189e-07\n",
      "Epoch: 2354600, elapsed: 1.26e+01, train loss: 4.01111e-07, val loss: 1.48598e-06, min loss: 3.98189e-07\n",
      "Epoch: 2354700, elapsed: 1.23e+01, train loss: 5.80263e-07, val loss: 1.70200e-06, min loss: 3.98189e-07\n",
      "Epoch: 2354800, elapsed: 1.21e+01, train loss: 3.99580e-07, val loss: 1.48411e-06, min loss: 3.98189e-07\n",
      "Epoch: 2354900, elapsed: 1.24e+01, train loss: 3.98206e-07, val loss: 1.50281e-06, min loss: 3.98189e-07\n",
      "Epoch: 2355000, elapsed: 1.22e+01, train loss: 5.50956e-07, val loss: 1.65956e-06, min loss: 3.98189e-07\n",
      "Epoch: 2355100, elapsed: 1.45e+01, train loss: 3.48954e-06, val loss: 3.06325e-06, min loss: 3.98189e-07\n",
      "Epoch: 2355200, elapsed: 1.24e+01, train loss: 3.97606e-07, val loss: 1.49654e-06, min loss: 3.97606e-07\n",
      "Epoch: 2355300, elapsed: 1.27e+01, train loss: 4.04857e-07, val loss: 1.46141e-06, min loss: 3.97606e-07\n",
      "Epoch: 2355400, elapsed: 1.24e+01, train loss: 4.23800e-07, val loss: 1.57799e-06, min loss: 3.97606e-07\n",
      "Epoch: 2355500, elapsed: 1.21e+01, train loss: 1.44096e-06, val loss: 2.83695e-06, min loss: 3.97606e-07\n",
      "Epoch: 2355600, elapsed: 1.19e+01, train loss: 4.16954e-07, val loss: 1.54416e-06, min loss: 3.97606e-07\n",
      "Epoch: 2355700, elapsed: 1.22e+01, train loss: 4.04149e-07, val loss: 1.51957e-06, min loss: 3.97606e-07\n",
      "Epoch: 2355800, elapsed: 1.23e+01, train loss: 4.01718e-07, val loss: 1.49513e-06, min loss: 3.97606e-07\n",
      "Epoch: 2355900, elapsed: 1.24e+01, train loss: 5.08700e-07, val loss: 1.59423e-06, min loss: 3.97606e-07\n",
      "Epoch: 2356000, elapsed: 1.21e+01, train loss: 4.25201e-07, val loss: 1.42669e-06, min loss: 3.97606e-07\n",
      "Epoch: 2356100, elapsed: 1.23e+01, train loss: 3.98382e-07, val loss: 1.51588e-06, min loss: 3.97606e-07\n",
      "Epoch: 2356200, elapsed: 1.25e+01, train loss: 3.99230e-07, val loss: 1.50164e-06, min loss: 3.97606e-07\n",
      "Epoch: 2356300, elapsed: 2.06e+01, train loss: 4.17356e-07, val loss: 1.56662e-06, min loss: 3.97606e-07\n",
      "Epoch: 2356400, elapsed: 1.42e+01, train loss: 4.10159e-07, val loss: 1.48985e-06, min loss: 3.97606e-07\n",
      "Epoch: 2356500, elapsed: 1.46e+01, train loss: 4.75207e-07, val loss: 1.54135e-06, min loss: 3.97606e-07\n",
      "Epoch: 2356600, elapsed: 1.42e+01, train loss: 3.98511e-07, val loss: 1.50252e-06, min loss: 3.97606e-07\n",
      "Epoch: 2356700, elapsed: 1.40e+01, train loss: 3.99481e-07, val loss: 1.50757e-06, min loss: 3.97606e-07\n",
      "Epoch: 2356800, elapsed: 1.39e+01, train loss: 4.07101e-07, val loss: 1.52175e-06, min loss: 3.97606e-07\n",
      "Epoch: 2356900, elapsed: 1.43e+01, train loss: 8.74997e-07, val loss: 2.06280e-06, min loss: 3.97606e-07\n",
      "Epoch: 2357000, elapsed: 1.36e+01, train loss: 7.98155e-07, val loss: 1.95397e-06, min loss: 3.97606e-07\n",
      "Epoch: 2357100, elapsed: 1.25e+01, train loss: 1.70003e-06, val loss: 1.98964e-06, min loss: 3.97606e-07\n",
      "Epoch: 2357200, elapsed: 1.25e+01, train loss: 3.97364e-07, val loss: 1.49778e-06, min loss: 3.97364e-07\n",
      "Epoch: 2357300, elapsed: 1.25e+01, train loss: 5.00759e-07, val loss: 1.57802e-06, min loss: 3.97364e-07\n",
      "Epoch: 2357400, elapsed: 1.24e+01, train loss: 4.04320e-07, val loss: 1.49592e-06, min loss: 3.97364e-07\n",
      "Epoch: 2357500, elapsed: 1.23e+01, train loss: 3.97629e-07, val loss: 1.49011e-06, min loss: 3.97364e-07\n",
      "Epoch: 2357600, elapsed: 1.23e+01, train loss: 9.93250e-07, val loss: 2.57063e-06, min loss: 3.97364e-07\n",
      "Epoch: 2357700, elapsed: 1.22e+01, train loss: 7.65383e-07, val loss: 1.80239e-06, min loss: 3.97364e-07\n",
      "Epoch: 2357800, elapsed: 1.24e+01, train loss: 3.98057e-07, val loss: 1.49841e-06, min loss: 3.97364e-07\n",
      "Epoch: 2357900, elapsed: 1.22e+01, train loss: 4.01406e-07, val loss: 1.51228e-06, min loss: 3.97364e-07\n",
      "Epoch: 2358000, elapsed: 1.25e+01, train loss: 8.34403e-07, val loss: 1.62700e-06, min loss: 3.97364e-07\n",
      "Epoch: 2358100, elapsed: 1.24e+01, train loss: 3.96952e-07, val loss: 1.49685e-06, min loss: 3.96952e-07\n",
      "Epoch: 2358200, elapsed: 1.26e+01, train loss: 4.05617e-07, val loss: 1.49595e-06, min loss: 3.96952e-07\n",
      "Epoch: 2358300, elapsed: 1.25e+01, train loss: 3.98418e-07, val loss: 1.48678e-06, min loss: 3.96952e-07\n",
      "Epoch: 2358400, elapsed: 1.23e+01, train loss: 3.97259e-07, val loss: 1.49536e-06, min loss: 3.96952e-07\n",
      "Epoch: 2358500, elapsed: 1.23e+01, train loss: 1.59414e-06, val loss: 2.90754e-06, min loss: 3.96952e-07\n",
      "Epoch: 2358600, elapsed: 1.22e+01, train loss: 4.06951e-07, val loss: 1.55070e-06, min loss: 3.96952e-07\n",
      "Epoch: 2358700, elapsed: 1.25e+01, train loss: 1.07731e-06, val loss: 1.75282e-06, min loss: 3.96952e-07\n",
      "Epoch: 2358800, elapsed: 1.24e+01, train loss: 4.07024e-07, val loss: 1.54542e-06, min loss: 3.96952e-07\n",
      "Epoch: 2358900, elapsed: 1.24e+01, train loss: 6.11402e-07, val loss: 1.80227e-06, min loss: 3.96952e-07\n",
      "Epoch: 2359000, elapsed: 1.25e+01, train loss: 4.16865e-07, val loss: 1.51985e-06, min loss: 3.96952e-07\n",
      "Epoch: 2359100, elapsed: 1.20e+01, train loss: 4.11556e-07, val loss: 1.49412e-06, min loss: 3.96952e-07\n",
      "Epoch: 2359200, elapsed: 1.24e+01, train loss: 5.14682e-07, val loss: 1.57661e-06, min loss: 3.96952e-07\n",
      "Epoch: 2359300, elapsed: 1.21e+01, train loss: 1.44491e-06, val loss: 3.04302e-06, min loss: 3.96952e-07\n",
      "Epoch: 2359400, elapsed: 1.23e+01, train loss: 3.99850e-07, val loss: 1.50829e-06, min loss: 3.96952e-07\n",
      "Epoch: 2359500, elapsed: 1.20e+01, train loss: 4.04140e-07, val loss: 1.47074e-06, min loss: 3.96952e-07\n",
      "Epoch: 2359600, elapsed: 1.27e+01, train loss: 4.33052e-07, val loss: 1.45882e-06, min loss: 3.96952e-07\n",
      "Epoch: 2359700, elapsed: 1.29e+01, train loss: 5.18601e-07, val loss: 1.70143e-06, min loss: 3.96952e-07\n",
      "Epoch: 2359800, elapsed: 1.21e+01, train loss: 1.34931e-06, val loss: 1.91777e-06, min loss: 3.96952e-07\n",
      "Epoch: 2359900, elapsed: 1.23e+01, train loss: 3.96559e-07, val loss: 1.49355e-06, min loss: 3.96559e-07\n",
      "Epoch: 2360000, elapsed: 1.21e+01, train loss: 4.00719e-07, val loss: 1.50378e-06, min loss: 3.96559e-07\n",
      "Epoch: 2360100, elapsed: 1.44e+01, train loss: 5.61545e-07, val loss: 1.61215e-06, min loss: 3.96559e-07\n",
      "Epoch: 2360200, elapsed: 1.21e+01, train loss: 1.28659e-06, val loss: 2.51134e-06, min loss: 3.96559e-07\n",
      "Epoch: 2360300, elapsed: 1.22e+01, train loss: 4.73763e-07, val loss: 1.51729e-06, min loss: 3.96559e-07\n",
      "Epoch: 2360400, elapsed: 1.22e+01, train loss: 6.51935e-07, val loss: 1.64196e-06, min loss: 3.96559e-07\n",
      "Epoch: 2360500, elapsed: 1.90e+01, train loss: 4.39278e-07, val loss: 1.51340e-06, min loss: 3.96559e-07\n",
      "Epoch: 2360600, elapsed: 1.27e+01, train loss: 1.41426e-06, val loss: 2.51543e-06, min loss: 3.96559e-07\n",
      "Epoch: 2360700, elapsed: 1.23e+01, train loss: 5.03689e-07, val loss: 1.53904e-06, min loss: 3.96559e-07\n",
      "Epoch: 2360800, elapsed: 1.24e+01, train loss: 2.05551e-06, val loss: 3.49816e-06, min loss: 3.96559e-07\n",
      "Epoch: 2360900, elapsed: 1.25e+01, train loss: 5.81820e-07, val loss: 1.86796e-06, min loss: 3.96559e-07\n",
      "Epoch: 2361000, elapsed: 1.22e+01, train loss: 3.97107e-07, val loss: 1.49149e-06, min loss: 3.96559e-07\n",
      "Epoch: 2361100, elapsed: 1.25e+01, train loss: 3.97229e-07, val loss: 1.49375e-06, min loss: 3.96559e-07\n",
      "Epoch: 2361200, elapsed: 1.23e+01, train loss: 5.13450e-07, val loss: 1.45439e-06, min loss: 3.96559e-07\n",
      "Epoch: 2361300, elapsed: 1.21e+01, train loss: 6.12952e-07, val loss: 1.74794e-06, min loss: 3.96559e-07\n",
      "Epoch: 2361400, elapsed: 1.22e+01, train loss: 3.96458e-07, val loss: 1.48860e-06, min loss: 3.96458e-07\n",
      "Epoch: 2361500, elapsed: 1.21e+01, train loss: 4.87803e-07, val loss: 1.59710e-06, min loss: 3.96458e-07\n",
      "Epoch: 2361600, elapsed: 1.21e+01, train loss: 4.12354e-07, val loss: 1.49556e-06, min loss: 3.96458e-07\n",
      "Epoch: 2361700, elapsed: 1.20e+01, train loss: 4.01289e-07, val loss: 1.51470e-06, min loss: 3.96458e-07\n",
      "Epoch: 2361800, elapsed: 1.22e+01, train loss: 3.98805e-07, val loss: 1.49375e-06, min loss: 3.96458e-07\n",
      "Epoch: 2361900, elapsed: 1.22e+01, train loss: 4.06711e-07, val loss: 1.48046e-06, min loss: 3.96458e-07\n",
      "Epoch: 2362000, elapsed: 1.23e+01, train loss: 4.12107e-07, val loss: 1.52652e-06, min loss: 3.96458e-07\n",
      "Epoch: 2362100, elapsed: 1.23e+01, train loss: 4.19548e-07, val loss: 1.50466e-06, min loss: 3.96458e-07\n",
      "Epoch: 2362200, elapsed: 1.20e+01, train loss: 4.89409e-07, val loss: 1.54040e-06, min loss: 3.96458e-07\n",
      "Epoch: 2362300, elapsed: 1.19e+01, train loss: 3.96059e-07, val loss: 1.48861e-06, min loss: 3.96059e-07\n",
      "Epoch: 2362400, elapsed: 1.21e+01, train loss: 3.97653e-07, val loss: 1.48877e-06, min loss: 3.96059e-07\n",
      "Epoch: 2362500, elapsed: 1.22e+01, train loss: 4.03308e-07, val loss: 1.48105e-06, min loss: 3.96059e-07\n",
      "Epoch: 2362600, elapsed: 1.20e+01, train loss: 3.99267e-07, val loss: 1.48429e-06, min loss: 3.96059e-07\n",
      "Epoch: 2362700, elapsed: 1.21e+01, train loss: 6.41151e-07, val loss: 1.58411e-06, min loss: 3.96059e-07\n",
      "Epoch: 2362800, elapsed: 1.18e+01, train loss: 1.66544e-06, val loss: 2.34850e-06, min loss: 3.96059e-07\n",
      "Epoch: 2362900, elapsed: 1.21e+01, train loss: 1.25783e-06, val loss: 2.78304e-06, min loss: 3.96059e-07\n",
      "Epoch: 2363000, elapsed: 1.23e+01, train loss: 4.00014e-07, val loss: 1.52577e-06, min loss: 3.96059e-07\n",
      "Epoch: 2363100, elapsed: 1.22e+01, train loss: 3.96052e-07, val loss: 1.49199e-06, min loss: 3.96052e-07\n",
      "Epoch: 2363200, elapsed: 1.23e+01, train loss: 4.06868e-07, val loss: 1.51529e-06, min loss: 3.96052e-07\n",
      "Epoch: 2363300, elapsed: 1.22e+01, train loss: 1.30244e-06, val loss: 1.89536e-06, min loss: 3.96052e-07\n",
      "Epoch: 2363400, elapsed: 1.21e+01, train loss: 3.96878e-07, val loss: 1.49788e-06, min loss: 3.96052e-07\n",
      "Epoch: 2363500, elapsed: 1.19e+01, train loss: 3.96640e-07, val loss: 1.49967e-06, min loss: 3.96052e-07\n",
      "Epoch: 2363600, elapsed: 1.22e+01, train loss: 4.31675e-07, val loss: 1.51940e-06, min loss: 3.96052e-07\n",
      "Epoch: 2363700, elapsed: 1.20e+01, train loss: 4.13579e-07, val loss: 1.48214e-06, min loss: 3.96052e-07\n",
      "Epoch: 2363800, elapsed: 1.19e+01, train loss: 4.07664e-07, val loss: 1.51227e-06, min loss: 3.96052e-07\n",
      "Epoch: 2363900, elapsed: 1.23e+01, train loss: 4.01324e-07, val loss: 1.49106e-06, min loss: 3.96052e-07\n",
      "Epoch: 2364000, elapsed: 1.23e+01, train loss: 5.20849e-07, val loss: 1.42082e-06, min loss: 3.96052e-07\n",
      "Epoch: 2364100, elapsed: 1.22e+01, train loss: 4.85782e-07, val loss: 1.60005e-06, min loss: 3.96052e-07\n",
      "Epoch: 2364200, elapsed: 1.21e+01, train loss: 3.96362e-07, val loss: 1.47643e-06, min loss: 3.96052e-07\n",
      "Epoch: 2364300, elapsed: 1.21e+01, train loss: 3.96965e-07, val loss: 1.48965e-06, min loss: 3.96052e-07\n",
      "Epoch: 2364400, elapsed: 1.19e+01, train loss: 4.07533e-07, val loss: 1.47319e-06, min loss: 3.96052e-07\n",
      "Epoch: 2364500, elapsed: 1.20e+01, train loss: 4.38077e-07, val loss: 1.55420e-06, min loss: 3.96052e-07\n",
      "Epoch: 2364600, elapsed: 1.18e+01, train loss: 4.08384e-07, val loss: 1.48286e-06, min loss: 3.96052e-07\n",
      "Epoch: 2364700, elapsed: 1.90e+01, train loss: 4.09994e-07, val loss: 1.45518e-06, min loss: 3.96052e-07\n",
      "Epoch: 2364800, elapsed: 1.27e+01, train loss: 4.41843e-07, val loss: 1.71393e-06, min loss: 3.96052e-07\n",
      "Epoch: 2364900, elapsed: 1.23e+01, train loss: 3.96686e-07, val loss: 1.48582e-06, min loss: 3.96052e-07\n",
      "Epoch: 2365000, elapsed: 1.24e+01, train loss: 3.95711e-07, val loss: 1.48416e-06, min loss: 3.95711e-07\n",
      "Epoch: 2365100, elapsed: 1.46e+01, train loss: 4.27560e-07, val loss: 1.51031e-06, min loss: 3.95711e-07\n",
      "Epoch: 2365200, elapsed: 1.24e+01, train loss: 4.50286e-07, val loss: 1.51703e-06, min loss: 3.95711e-07\n",
      "Epoch: 2365300, elapsed: 1.20e+01, train loss: 5.22757e-07, val loss: 1.68375e-06, min loss: 3.95711e-07\n",
      "Epoch: 2365400, elapsed: 1.22e+01, train loss: 3.96102e-07, val loss: 1.48630e-06, min loss: 3.95711e-07\n",
      "Epoch: 2365500, elapsed: 1.23e+01, train loss: 5.27140e-07, val loss: 1.75733e-06, min loss: 3.95711e-07\n",
      "Epoch: 2365600, elapsed: 1.24e+01, train loss: 3.95471e-07, val loss: 1.49275e-06, min loss: 3.95471e-07\n",
      "Epoch: 2365700, elapsed: 1.25e+01, train loss: 3.97530e-07, val loss: 1.48615e-06, min loss: 3.95471e-07\n",
      "Epoch: 2365800, elapsed: 1.24e+01, train loss: 4.43205e-07, val loss: 1.59309e-06, min loss: 3.95471e-07\n",
      "Epoch: 2365900, elapsed: 1.22e+01, train loss: 3.95614e-07, val loss: 1.49002e-06, min loss: 3.95471e-07\n",
      "Epoch: 2366000, elapsed: 1.23e+01, train loss: 4.62402e-07, val loss: 1.52810e-06, min loss: 3.95471e-07\n",
      "Epoch: 2366100, elapsed: 1.23e+01, train loss: 5.89272e-07, val loss: 2.11724e-06, min loss: 3.95471e-07\n",
      "Epoch: 2366200, elapsed: 1.24e+01, train loss: 6.63006e-07, val loss: 1.79824e-06, min loss: 3.95471e-07\n",
      "Epoch: 2366300, elapsed: 1.24e+01, train loss: 4.21985e-07, val loss: 1.48637e-06, min loss: 3.95471e-07\n",
      "Epoch: 2366400, elapsed: 1.21e+01, train loss: 5.82691e-07, val loss: 1.61501e-06, min loss: 3.95471e-07\n",
      "Epoch: 2366500, elapsed: 1.23e+01, train loss: 3.95248e-07, val loss: 1.48920e-06, min loss: 3.95248e-07\n",
      "Epoch: 2366600, elapsed: 1.20e+01, train loss: 4.13926e-07, val loss: 1.46007e-06, min loss: 3.95248e-07\n",
      "Epoch: 2366700, elapsed: 1.23e+01, train loss: 4.03767e-07, val loss: 1.50375e-06, min loss: 3.95248e-07\n",
      "Epoch: 2366800, elapsed: 1.21e+01, train loss: 4.29286e-07, val loss: 1.48020e-06, min loss: 3.95248e-07\n",
      "Epoch: 2366900, elapsed: 1.22e+01, train loss: 9.56932e-07, val loss: 2.12052e-06, min loss: 3.95248e-07\n",
      "Epoch: 2367000, elapsed: 1.22e+01, train loss: 4.06740e-07, val loss: 1.46130e-06, min loss: 3.95248e-07\n",
      "Epoch: 2367100, elapsed: 1.24e+01, train loss: 4.08458e-07, val loss: 1.45276e-06, min loss: 3.95248e-07\n",
      "Epoch: 2367200, elapsed: 1.21e+01, train loss: 4.27265e-07, val loss: 1.52256e-06, min loss: 3.95248e-07\n",
      "Epoch: 2367300, elapsed: 1.22e+01, train loss: 8.22356e-07, val loss: 1.78498e-06, min loss: 3.95248e-07\n",
      "Epoch: 2367400, elapsed: 1.22e+01, train loss: 3.96569e-07, val loss: 1.49724e-06, min loss: 3.95248e-07\n",
      "Epoch: 2367500, elapsed: 1.20e+01, train loss: 2.54311e-06, val loss: 1.85169e-06, min loss: 3.95248e-07\n",
      "Epoch: 2367600, elapsed: 1.20e+01, train loss: 3.95004e-07, val loss: 1.48807e-06, min loss: 3.95004e-07\n",
      "Epoch: 2367700, elapsed: 1.25e+01, train loss: 3.96480e-07, val loss: 1.49611e-06, min loss: 3.95004e-07\n",
      "Epoch: 2367800, elapsed: 1.20e+01, train loss: 3.95156e-07, val loss: 1.49001e-06, min loss: 3.95004e-07\n",
      "Epoch: 2367900, elapsed: 1.20e+01, train loss: 3.95383e-07, val loss: 1.49498e-06, min loss: 3.95004e-07\n",
      "Epoch: 2368000, elapsed: 1.21e+01, train loss: 3.95152e-07, val loss: 1.47984e-06, min loss: 3.95004e-07\n",
      "Epoch: 2368100, elapsed: 1.21e+01, train loss: 3.99525e-07, val loss: 1.50952e-06, min loss: 3.95004e-07\n",
      "Epoch: 2368200, elapsed: 1.23e+01, train loss: 3.94968e-07, val loss: 1.49145e-06, min loss: 3.94968e-07\n",
      "Epoch: 2368300, elapsed: 1.21e+01, train loss: 3.95735e-07, val loss: 1.48705e-06, min loss: 3.94968e-07\n",
      "Epoch: 2368400, elapsed: 1.22e+01, train loss: 9.89102e-07, val loss: 1.81559e-06, min loss: 3.94968e-07\n",
      "Epoch: 2368500, elapsed: 1.23e+01, train loss: 3.94842e-07, val loss: 1.48234e-06, min loss: 3.94842e-07\n",
      "Epoch: 2368600, elapsed: 1.21e+01, train loss: 3.96458e-07, val loss: 1.47025e-06, min loss: 3.94842e-07\n",
      "Epoch: 2368700, elapsed: 1.20e+01, train loss: 4.04674e-07, val loss: 1.47239e-06, min loss: 3.94842e-07\n",
      "Epoch: 2368800, elapsed: 1.23e+01, train loss: 3.95308e-07, val loss: 1.49150e-06, min loss: 3.94842e-07\n",
      "Epoch: 2368900, elapsed: 1.90e+01, train loss: 4.99481e-07, val loss: 1.55912e-06, min loss: 3.94842e-07\n",
      "Epoch: 2369000, elapsed: 1.38e+01, train loss: 4.14210e-07, val loss: 1.51532e-06, min loss: 3.94842e-07\n",
      "Epoch: 2369100, elapsed: 1.43e+01, train loss: 3.95472e-07, val loss: 1.47695e-06, min loss: 3.94842e-07\n",
      "Epoch: 2369200, elapsed: 1.43e+01, train loss: 3.99508e-07, val loss: 1.47061e-06, min loss: 3.94842e-07\n",
      "Epoch: 2369300, elapsed: 1.24e+01, train loss: 4.09549e-07, val loss: 1.44283e-06, min loss: 3.94842e-07\n",
      "Epoch: 2369400, elapsed: 1.25e+01, train loss: 1.01256e-06, val loss: 1.80205e-06, min loss: 3.94842e-07\n",
      "Epoch: 2369500, elapsed: 1.24e+01, train loss: 7.68164e-07, val loss: 1.96555e-06, min loss: 3.94842e-07\n",
      "Epoch: 2369600, elapsed: 1.22e+01, train loss: 3.96673e-07, val loss: 1.49371e-06, min loss: 3.94842e-07\n",
      "Epoch: 2369700, elapsed: 1.23e+01, train loss: 4.92235e-07, val loss: 1.55580e-06, min loss: 3.94842e-07\n",
      "Epoch: 2369800, elapsed: 1.23e+01, train loss: 4.66252e-07, val loss: 1.62214e-06, min loss: 3.94842e-07\n",
      "Epoch: 2369900, elapsed: 1.24e+01, train loss: 6.66761e-07, val loss: 1.54727e-06, min loss: 3.94842e-07\n",
      "Epoch: 2370000, elapsed: 1.20e+01, train loss: 6.19403e-07, val loss: 1.82365e-06, min loss: 3.94842e-07\n",
      "Epoch: 2370100, elapsed: 1.45e+01, train loss: 4.97235e-07, val loss: 1.60695e-06, min loss: 3.94842e-07\n",
      "Epoch: 2370200, elapsed: 1.22e+01, train loss: 1.04205e-06, val loss: 2.07539e-06, min loss: 3.94842e-07\n",
      "Epoch: 2370300, elapsed: 1.23e+01, train loss: 4.08057e-07, val loss: 1.46673e-06, min loss: 3.94842e-07\n",
      "Epoch: 2370400, elapsed: 1.21e+01, train loss: 3.98051e-07, val loss: 1.49548e-06, min loss: 3.94842e-07\n",
      "Epoch: 2370500, elapsed: 1.22e+01, train loss: 3.95272e-07, val loss: 1.48532e-06, min loss: 3.94842e-07\n",
      "Epoch: 2370600, elapsed: 1.22e+01, train loss: 4.15773e-07, val loss: 1.53339e-06, min loss: 3.94842e-07\n",
      "Epoch: 2370700, elapsed: 1.21e+01, train loss: 6.14206e-07, val loss: 1.75711e-06, min loss: 3.94842e-07\n",
      "Epoch: 2370800, elapsed: 1.21e+01, train loss: 6.45184e-07, val loss: 1.95276e-06, min loss: 3.94842e-07\n",
      "Epoch: 2370900, elapsed: 1.23e+01, train loss: 6.66904e-07, val loss: 1.87947e-06, min loss: 3.94842e-07\n",
      "Epoch: 2371000, elapsed: 1.22e+01, train loss: 9.01753e-07, val loss: 2.44882e-06, min loss: 3.94842e-07\n",
      "Epoch: 2371100, elapsed: 1.22e+01, train loss: 4.67086e-07, val loss: 1.58210e-06, min loss: 3.94842e-07\n",
      "Epoch: 2371200, elapsed: 1.22e+01, train loss: 3.97392e-07, val loss: 1.49707e-06, min loss: 3.94842e-07\n",
      "Epoch: 2371300, elapsed: 1.21e+01, train loss: 3.96571e-07, val loss: 1.49175e-06, min loss: 3.94842e-07\n",
      "Epoch: 2371400, elapsed: 1.21e+01, train loss: 4.13284e-07, val loss: 1.46060e-06, min loss: 3.94842e-07\n",
      "Epoch: 2371500, elapsed: 1.21e+01, train loss: 7.04488e-07, val loss: 1.46736e-06, min loss: 3.94842e-07\n",
      "Epoch: 2371600, elapsed: 1.22e+01, train loss: 3.94156e-07, val loss: 1.48299e-06, min loss: 3.94156e-07\n",
      "Epoch: 2371700, elapsed: 1.22e+01, train loss: 4.05425e-07, val loss: 1.51346e-06, min loss: 3.94156e-07\n",
      "Epoch: 2371800, elapsed: 1.21e+01, train loss: 1.45370e-06, val loss: 3.30988e-06, min loss: 3.94156e-07\n",
      "Epoch: 2371900, elapsed: 1.24e+01, train loss: 3.94283e-07, val loss: 1.48424e-06, min loss: 3.94156e-07\n",
      "Epoch: 2372000, elapsed: 1.21e+01, train loss: 4.13388e-07, val loss: 1.50666e-06, min loss: 3.94156e-07\n",
      "Epoch: 2372100, elapsed: 1.19e+01, train loss: 3.94087e-07, val loss: 1.47599e-06, min loss: 3.94087e-07\n",
      "Epoch: 2372200, elapsed: 1.21e+01, train loss: 4.07527e-07, val loss: 1.52819e-06, min loss: 3.94087e-07\n",
      "Epoch: 2372300, elapsed: 1.21e+01, train loss: 4.03373e-07, val loss: 1.48349e-06, min loss: 3.94087e-07\n",
      "Epoch: 2372400, elapsed: 1.23e+01, train loss: 4.30948e-07, val loss: 1.59051e-06, min loss: 3.94087e-07\n",
      "Epoch: 2372500, elapsed: 1.21e+01, train loss: 7.11939e-07, val loss: 1.76984e-06, min loss: 3.94087e-07\n",
      "Epoch: 2372600, elapsed: 1.24e+01, train loss: 1.90148e-06, val loss: 3.44604e-06, min loss: 3.94087e-07\n",
      "Epoch: 2372700, elapsed: 1.19e+01, train loss: 4.58818e-07, val loss: 1.63756e-06, min loss: 3.94087e-07\n",
      "Epoch: 2372800, elapsed: 1.20e+01, train loss: 1.91079e-06, val loss: 2.92219e-06, min loss: 3.94087e-07\n",
      "Epoch: 2372900, elapsed: 1.22e+01, train loss: 3.94990e-07, val loss: 1.47634e-06, min loss: 3.94087e-07\n",
      "Epoch: 2373000, elapsed: 1.81e+01, train loss: 3.94850e-07, val loss: 1.46672e-06, min loss: 3.94087e-07\n",
      "Epoch: 2373100, elapsed: 1.29e+01, train loss: 4.12182e-07, val loss: 1.47482e-06, min loss: 3.94087e-07\n",
      "Epoch: 2373200, elapsed: 1.25e+01, train loss: 4.38474e-07, val loss: 1.53097e-06, min loss: 3.94087e-07\n",
      "Epoch: 2373300, elapsed: 1.24e+01, train loss: 3.93961e-07, val loss: 1.48432e-06, min loss: 3.93961e-07\n",
      "Epoch: 2373400, elapsed: 1.25e+01, train loss: 4.05589e-07, val loss: 1.49839e-06, min loss: 3.93961e-07\n",
      "Epoch: 2373500, elapsed: 1.25e+01, train loss: 7.70272e-07, val loss: 1.57557e-06, min loss: 3.93961e-07\n",
      "Epoch: 2373600, elapsed: 1.26e+01, train loss: 5.85463e-07, val loss: 1.48442e-06, min loss: 3.93961e-07\n",
      "Epoch: 2373700, elapsed: 1.28e+01, train loss: 4.07565e-07, val loss: 1.48577e-06, min loss: 3.93961e-07\n",
      "Epoch: 2373800, elapsed: 1.25e+01, train loss: 4.81427e-07, val loss: 1.53047e-06, min loss: 3.93961e-07\n",
      "Epoch: 2373900, elapsed: 1.23e+01, train loss: 4.80214e-07, val loss: 1.61636e-06, min loss: 3.93961e-07\n",
      "Epoch: 2374000, elapsed: 1.25e+01, train loss: 4.65552e-07, val loss: 1.57292e-06, min loss: 3.93961e-07\n",
      "Epoch: 2374100, elapsed: 1.23e+01, train loss: 5.60475e-07, val loss: 1.45497e-06, min loss: 3.93961e-07\n",
      "Epoch: 2374200, elapsed: 1.23e+01, train loss: 4.70363e-07, val loss: 1.51077e-06, min loss: 3.93961e-07\n",
      "Epoch: 2374300, elapsed: 1.21e+01, train loss: 3.94760e-07, val loss: 1.47418e-06, min loss: 3.93961e-07\n",
      "Epoch: 2374400, elapsed: 1.22e+01, train loss: 3.94058e-07, val loss: 1.47654e-06, min loss: 3.93961e-07\n",
      "Epoch: 2374500, elapsed: 1.22e+01, train loss: 6.79978e-07, val loss: 1.88826e-06, min loss: 3.93961e-07\n",
      "Epoch: 2374600, elapsed: 1.23e+01, train loss: 4.11127e-07, val loss: 1.57046e-06, min loss: 3.93961e-07\n",
      "Epoch: 2374700, elapsed: 1.21e+01, train loss: 4.25052e-07, val loss: 1.50299e-06, min loss: 3.93961e-07\n",
      "Epoch: 2374800, elapsed: 1.24e+01, train loss: 4.38761e-07, val loss: 1.54574e-06, min loss: 3.93961e-07\n",
      "Epoch: 2374900, elapsed: 1.22e+01, train loss: 4.20948e-07, val loss: 1.54998e-06, min loss: 3.93961e-07\n",
      "Epoch: 2375000, elapsed: 1.24e+01, train loss: 4.48005e-07, val loss: 1.50717e-06, min loss: 3.93961e-07\n",
      "Epoch: 2375100, elapsed: 1.45e+01, train loss: 4.85200e-07, val loss: 1.64918e-06, min loss: 3.93961e-07\n",
      "Epoch: 2375200, elapsed: 1.26e+01, train loss: 3.98005e-07, val loss: 1.48361e-06, min loss: 3.93961e-07\n",
      "Epoch: 2375300, elapsed: 1.25e+01, train loss: 4.08734e-07, val loss: 1.47178e-06, min loss: 3.93961e-07\n",
      "Epoch: 2375400, elapsed: 1.24e+01, train loss: 3.98304e-07, val loss: 1.51929e-06, min loss: 3.93961e-07\n",
      "Epoch: 2375500, elapsed: 1.26e+01, train loss: 1.90753e-06, val loss: 2.91434e-06, min loss: 3.93961e-07\n",
      "Epoch: 2375600, elapsed: 1.27e+01, train loss: 4.07032e-07, val loss: 1.49764e-06, min loss: 3.93961e-07\n",
      "Epoch: 2375700, elapsed: 1.25e+01, train loss: 3.99182e-07, val loss: 1.46257e-06, min loss: 3.93961e-07\n",
      "Epoch: 2375800, elapsed: 1.22e+01, train loss: 3.97205e-07, val loss: 1.45739e-06, min loss: 3.93961e-07\n",
      "Epoch: 2375900, elapsed: 1.24e+01, train loss: 3.97309e-07, val loss: 1.47561e-06, min loss: 3.93961e-07\n",
      "Epoch: 2376000, elapsed: 1.22e+01, train loss: 4.18683e-07, val loss: 1.53250e-06, min loss: 3.93961e-07\n",
      "Epoch: 2376100, elapsed: 1.22e+01, train loss: 4.01776e-07, val loss: 1.48124e-06, min loss: 3.93961e-07\n",
      "Epoch: 2376200, elapsed: 1.20e+01, train loss: 3.96080e-07, val loss: 1.47224e-06, min loss: 3.93961e-07\n",
      "Epoch: 2376300, elapsed: 1.23e+01, train loss: 3.94489e-07, val loss: 1.47780e-06, min loss: 3.93961e-07\n",
      "Epoch: 2376400, elapsed: 1.21e+01, train loss: 3.93767e-07, val loss: 1.47556e-06, min loss: 3.93767e-07\n",
      "Epoch: 2376500, elapsed: 1.24e+01, train loss: 5.04444e-07, val loss: 1.56889e-06, min loss: 3.93767e-07\n",
      "Epoch: 2376600, elapsed: 1.23e+01, train loss: 4.04356e-07, val loss: 1.46589e-06, min loss: 3.93767e-07\n",
      "Epoch: 2376700, elapsed: 1.22e+01, train loss: 3.93087e-07, val loss: 1.47731e-06, min loss: 3.93087e-07\n",
      "Epoch: 2376800, elapsed: 1.24e+01, train loss: 4.14392e-07, val loss: 1.51787e-06, min loss: 3.93087e-07\n",
      "Epoch: 2376900, elapsed: 1.21e+01, train loss: 1.05775e-06, val loss: 1.84897e-06, min loss: 3.93087e-07\n",
      "Epoch: 2377000, elapsed: 1.22e+01, train loss: 3.93236e-07, val loss: 1.47960e-06, min loss: 3.93087e-07\n",
      "Epoch: 2377100, elapsed: 1.23e+01, train loss: 3.95245e-07, val loss: 1.47584e-06, min loss: 3.93087e-07\n",
      "Epoch: 2377200, elapsed: 1.89e+01, train loss: 4.85093e-07, val loss: 1.42447e-06, min loss: 3.93087e-07\n",
      "Epoch: 2377300, elapsed: 1.29e+01, train loss: 4.00366e-07, val loss: 1.47958e-06, min loss: 3.93087e-07\n",
      "Epoch: 2377400, elapsed: 1.27e+01, train loss: 3.93269e-07, val loss: 1.47542e-06, min loss: 3.93087e-07\n",
      "Epoch: 2377500, elapsed: 1.25e+01, train loss: 3.95121e-07, val loss: 1.49871e-06, min loss: 3.93087e-07\n",
      "Epoch: 2377600, elapsed: 1.24e+01, train loss: 6.30125e-07, val loss: 1.65609e-06, min loss: 3.93087e-07\n",
      "Epoch: 2377700, elapsed: 1.24e+01, train loss: 4.51522e-07, val loss: 1.59077e-06, min loss: 3.93087e-07\n",
      "Epoch: 2377800, elapsed: 1.25e+01, train loss: 3.95181e-07, val loss: 1.47217e-06, min loss: 3.93087e-07\n",
      "Epoch: 2377900, elapsed: 1.24e+01, train loss: 5.37157e-07, val loss: 1.47336e-06, min loss: 3.93087e-07\n",
      "Epoch: 2378000, elapsed: 1.26e+01, train loss: 9.33188e-07, val loss: 1.54324e-06, min loss: 3.93087e-07\n",
      "Epoch: 2378100, elapsed: 1.25e+01, train loss: 3.93601e-07, val loss: 1.47741e-06, min loss: 3.93087e-07\n",
      "Epoch: 2378200, elapsed: 1.24e+01, train loss: 3.93349e-07, val loss: 1.47174e-06, min loss: 3.93087e-07\n",
      "Epoch: 2378300, elapsed: 1.26e+01, train loss: 4.07752e-07, val loss: 1.55326e-06, min loss: 3.93087e-07\n",
      "Epoch: 2378400, elapsed: 1.25e+01, train loss: 4.02449e-07, val loss: 1.43603e-06, min loss: 3.93087e-07\n",
      "Epoch: 2378500, elapsed: 1.25e+01, train loss: 3.99746e-07, val loss: 1.45511e-06, min loss: 3.93087e-07\n",
      "Epoch: 2378600, elapsed: 1.25e+01, train loss: 4.37919e-07, val loss: 1.50447e-06, min loss: 3.93087e-07\n",
      "Epoch: 2378700, elapsed: 1.26e+01, train loss: 4.01514e-07, val loss: 1.54704e-06, min loss: 3.93087e-07\n",
      "Epoch: 2378800, elapsed: 1.29e+01, train loss: 3.92763e-07, val loss: 1.47183e-06, min loss: 3.92763e-07\n",
      "Epoch: 2378900, elapsed: 1.24e+01, train loss: 4.01081e-07, val loss: 1.49235e-06, min loss: 3.92763e-07\n",
      "Epoch: 2379000, elapsed: 1.25e+01, train loss: 1.47133e-06, val loss: 2.36788e-06, min loss: 3.92763e-07\n",
      "Epoch: 2379100, elapsed: 1.23e+01, train loss: 3.05181e-06, val loss: 4.12002e-06, min loss: 3.92763e-07\n",
      "Epoch: 2379200, elapsed: 1.22e+01, train loss: 4.03348e-07, val loss: 1.49978e-06, min loss: 3.92763e-07\n",
      "Epoch: 2379300, elapsed: 1.22e+01, train loss: 3.98570e-07, val loss: 1.51721e-06, min loss: 3.92763e-07\n",
      "Epoch: 2379400, elapsed: 1.21e+01, train loss: 6.93368e-07, val loss: 1.84903e-06, min loss: 3.92763e-07\n",
      "Epoch: 2379500, elapsed: 1.23e+01, train loss: 4.28284e-07, val loss: 1.60453e-06, min loss: 3.92763e-07\n",
      "Epoch: 2379600, elapsed: 1.19e+01, train loss: 4.09989e-07, val loss: 1.50803e-06, min loss: 3.92763e-07\n",
      "Epoch: 2379700, elapsed: 1.24e+01, train loss: 4.01586e-07, val loss: 1.47063e-06, min loss: 3.92763e-07\n",
      "Epoch: 2379800, elapsed: 1.24e+01, train loss: 5.14012e-07, val loss: 1.65877e-06, min loss: 3.92763e-07\n",
      "Epoch: 2379900, elapsed: 1.23e+01, train loss: 4.97286e-07, val loss: 1.60786e-06, min loss: 3.92763e-07\n",
      "Epoch: 2380000, elapsed: 1.27e+01, train loss: 3.92936e-07, val loss: 1.47116e-06, min loss: 3.92763e-07\n",
      "Epoch: 2380100, elapsed: 1.43e+01, train loss: 6.88899e-07, val loss: 1.56974e-06, min loss: 3.92763e-07\n",
      "Epoch: 2380200, elapsed: 1.22e+01, train loss: 3.96431e-07, val loss: 1.45427e-06, min loss: 3.92763e-07\n",
      "Epoch: 2380300, elapsed: 1.20e+01, train loss: 3.93410e-07, val loss: 1.47782e-06, min loss: 3.92763e-07\n",
      "Epoch: 2380400, elapsed: 1.21e+01, train loss: 4.01014e-07, val loss: 1.51133e-06, min loss: 3.92763e-07\n",
      "Epoch: 2380500, elapsed: 1.23e+01, train loss: 3.98019e-07, val loss: 1.50598e-06, min loss: 3.92763e-07\n",
      "Epoch: 2380600, elapsed: 1.22e+01, train loss: 6.58625e-07, val loss: 1.82331e-06, min loss: 3.92763e-07\n",
      "Epoch: 2380700, elapsed: 1.23e+01, train loss: 3.92324e-07, val loss: 1.47383e-06, min loss: 3.92324e-07\n",
      "Epoch: 2380800, elapsed: 1.21e+01, train loss: 4.08691e-07, val loss: 1.47493e-06, min loss: 3.92324e-07\n",
      "Epoch: 2380900, elapsed: 1.23e+01, train loss: 4.43374e-07, val loss: 1.42699e-06, min loss: 3.92324e-07\n",
      "Epoch: 2381000, elapsed: 1.22e+01, train loss: 4.16399e-07, val loss: 1.49672e-06, min loss: 3.92324e-07\n",
      "Epoch: 2381100, elapsed: 1.20e+01, train loss: 3.94798e-07, val loss: 1.47249e-06, min loss: 3.92324e-07\n",
      "Epoch: 2381200, elapsed: 1.21e+01, train loss: 3.92417e-07, val loss: 1.47935e-06, min loss: 3.92324e-07\n",
      "Epoch: 2381300, elapsed: 1.21e+01, train loss: 8.50493e-07, val loss: 2.12454e-06, min loss: 3.92324e-07\n",
      "Epoch: 2381400, elapsed: 1.85e+01, train loss: 5.76168e-07, val loss: 1.64129e-06, min loss: 3.92324e-07\n",
      "Epoch: 2381500, elapsed: 1.26e+01, train loss: 6.27687e-07, val loss: 1.72833e-06, min loss: 3.92324e-07\n",
      "Epoch: 2381600, elapsed: 1.24e+01, train loss: 3.92285e-07, val loss: 1.47622e-06, min loss: 3.92285e-07\n",
      "Epoch: 2381700, elapsed: 1.24e+01, train loss: 3.92326e-07, val loss: 1.47220e-06, min loss: 3.92285e-07\n",
      "Epoch: 2381800, elapsed: 1.23e+01, train loss: 3.92753e-07, val loss: 1.46970e-06, min loss: 3.92285e-07\n",
      "Epoch: 2381900, elapsed: 1.25e+01, train loss: 1.75232e-06, val loss: 3.51170e-06, min loss: 3.92285e-07\n",
      "Epoch: 2382000, elapsed: 1.24e+01, train loss: 3.92033e-07, val loss: 1.47365e-06, min loss: 3.92033e-07\n",
      "Epoch: 2382100, elapsed: 1.25e+01, train loss: 1.00407e-06, val loss: 1.65584e-06, min loss: 3.92033e-07\n",
      "Epoch: 2382200, elapsed: 1.25e+01, train loss: 4.00232e-07, val loss: 1.50535e-06, min loss: 3.92033e-07\n",
      "Epoch: 2382300, elapsed: 1.24e+01, train loss: 3.93032e-07, val loss: 1.48204e-06, min loss: 3.92033e-07\n",
      "Epoch: 2382400, elapsed: 1.21e+01, train loss: 4.24507e-07, val loss: 1.46129e-06, min loss: 3.92033e-07\n",
      "Epoch: 2382500, elapsed: 1.25e+01, train loss: 4.05926e-07, val loss: 1.45480e-06, min loss: 3.92033e-07\n",
      "Epoch: 2382600, elapsed: 1.23e+01, train loss: 3.99389e-07, val loss: 1.44677e-06, min loss: 3.92033e-07\n",
      "Epoch: 2382700, elapsed: 1.23e+01, train loss: 4.82638e-07, val loss: 1.66035e-06, min loss: 3.92033e-07\n",
      "Epoch: 2382800, elapsed: 1.24e+01, train loss: 4.54667e-07, val loss: 1.45984e-06, min loss: 3.92033e-07\n",
      "Epoch: 2382900, elapsed: 1.24e+01, train loss: 3.96009e-07, val loss: 1.48303e-06, min loss: 3.92033e-07\n",
      "Epoch: 2383000, elapsed: 1.25e+01, train loss: 5.55101e-07, val loss: 1.73573e-06, min loss: 3.92033e-07\n",
      "Epoch: 2383100, elapsed: 1.25e+01, train loss: 4.66974e-07, val loss: 1.51487e-06, min loss: 3.92033e-07\n",
      "Epoch: 2383200, elapsed: 1.22e+01, train loss: 2.72395e-06, val loss: 3.90678e-06, min loss: 3.92033e-07\n",
      "Epoch: 2383300, elapsed: 1.21e+01, train loss: 3.91862e-07, val loss: 1.47263e-06, min loss: 3.91862e-07\n",
      "Epoch: 2383400, elapsed: 1.24e+01, train loss: 3.98468e-07, val loss: 1.47001e-06, min loss: 3.91862e-07\n",
      "Epoch: 2383500, elapsed: 1.23e+01, train loss: 8.37457e-07, val loss: 1.74802e-06, min loss: 3.91862e-07\n",
      "Epoch: 2383600, elapsed: 1.22e+01, train loss: 3.92091e-07, val loss: 1.46602e-06, min loss: 3.91862e-07\n",
      "Epoch: 2383700, elapsed: 1.25e+01, train loss: 4.24778e-07, val loss: 1.52812e-06, min loss: 3.91862e-07\n",
      "Epoch: 2383800, elapsed: 1.25e+01, train loss: 4.17602e-07, val loss: 1.47210e-06, min loss: 3.91862e-07\n",
      "Epoch: 2383900, elapsed: 1.23e+01, train loss: 4.29443e-07, val loss: 1.47171e-06, min loss: 3.91862e-07\n",
      "Epoch: 2384000, elapsed: 1.24e+01, train loss: 5.52118e-07, val loss: 1.65471e-06, min loss: 3.91862e-07\n",
      "Epoch: 2384100, elapsed: 1.23e+01, train loss: 5.00052e-07, val loss: 1.61551e-06, min loss: 3.91862e-07\n",
      "Epoch: 2384200, elapsed: 1.21e+01, train loss: 3.99525e-07, val loss: 1.49980e-06, min loss: 3.91862e-07\n",
      "Epoch: 2384300, elapsed: 1.19e+01, train loss: 3.92253e-07, val loss: 1.48210e-06, min loss: 3.91862e-07\n",
      "Epoch: 2384400, elapsed: 1.23e+01, train loss: 4.05986e-07, val loss: 1.48427e-06, min loss: 3.91862e-07\n",
      "Epoch: 2384500, elapsed: 1.21e+01, train loss: 7.26610e-07, val loss: 1.75923e-06, min loss: 3.91862e-07\n",
      "Epoch: 2384600, elapsed: 1.21e+01, train loss: 4.16037e-07, val loss: 1.41458e-06, min loss: 3.91862e-07\n",
      "Epoch: 2384700, elapsed: 1.25e+01, train loss: 3.92195e-07, val loss: 1.46806e-06, min loss: 3.91862e-07\n",
      "Epoch: 2384800, elapsed: 1.23e+01, train loss: 8.15916e-07, val loss: 2.05643e-06, min loss: 3.91862e-07\n",
      "Epoch: 2384900, elapsed: 1.25e+01, train loss: 1.14369e-06, val loss: 2.39600e-06, min loss: 3.91862e-07\n",
      "Epoch: 2385000, elapsed: 1.22e+01, train loss: 3.91412e-07, val loss: 1.46834e-06, min loss: 3.91412e-07\n",
      "Epoch: 2385100, elapsed: 1.44e+01, train loss: 4.83711e-07, val loss: 1.48483e-06, min loss: 3.91412e-07\n",
      "Epoch: 2385200, elapsed: 1.21e+01, train loss: 3.91350e-07, val loss: 1.47058e-06, min loss: 3.91350e-07\n",
      "Epoch: 2385300, elapsed: 1.22e+01, train loss: 4.38270e-07, val loss: 1.52722e-06, min loss: 3.91350e-07\n",
      "Epoch: 2385400, elapsed: 1.22e+01, train loss: 3.91324e-07, val loss: 1.47053e-06, min loss: 3.91324e-07\n",
      "Epoch: 2385500, elapsed: 1.22e+01, train loss: 3.96821e-07, val loss: 1.43765e-06, min loss: 3.91324e-07\n",
      "Epoch: 2385600, elapsed: 1.84e+01, train loss: 4.16008e-07, val loss: 1.52970e-06, min loss: 3.91324e-07\n",
      "Epoch: 2385700, elapsed: 1.26e+01, train loss: 3.91695e-07, val loss: 1.47735e-06, min loss: 3.91324e-07\n",
      "Epoch: 2385800, elapsed: 1.23e+01, train loss: 4.54013e-07, val loss: 1.50442e-06, min loss: 3.91324e-07\n",
      "Epoch: 2385900, elapsed: 1.22e+01, train loss: 4.36229e-07, val loss: 1.48482e-06, min loss: 3.91324e-07\n",
      "Epoch: 2386000, elapsed: 1.22e+01, train loss: 4.30735e-07, val loss: 1.67437e-06, min loss: 3.91324e-07\n",
      "Epoch: 2386100, elapsed: 1.24e+01, train loss: 6.02663e-07, val loss: 1.86767e-06, min loss: 3.91324e-07\n",
      "Epoch: 2386200, elapsed: 1.22e+01, train loss: 4.10049e-07, val loss: 1.49034e-06, min loss: 3.91324e-07\n",
      "Epoch: 2386300, elapsed: 1.21e+01, train loss: 4.36933e-07, val loss: 1.54865e-06, min loss: 3.91324e-07\n",
      "Epoch: 2386400, elapsed: 1.21e+01, train loss: 4.16621e-07, val loss: 1.53711e-06, min loss: 3.91324e-07\n",
      "Epoch: 2386500, elapsed: 1.21e+01, train loss: 3.94124e-07, val loss: 1.44657e-06, min loss: 3.91324e-07\n",
      "Epoch: 2386600, elapsed: 1.23e+01, train loss: 3.94235e-07, val loss: 1.45187e-06, min loss: 3.91324e-07\n",
      "Epoch: 2386700, elapsed: 1.23e+01, train loss: 4.05187e-07, val loss: 1.48168e-06, min loss: 3.91324e-07\n",
      "Epoch: 2386800, elapsed: 1.22e+01, train loss: 1.12435e-06, val loss: 2.25604e-06, min loss: 3.91324e-07\n",
      "Epoch: 2386900, elapsed: 1.21e+01, train loss: 3.91290e-07, val loss: 1.46578e-06, min loss: 3.91290e-07\n",
      "Epoch: 2387000, elapsed: 1.23e+01, train loss: 4.45164e-07, val loss: 1.44887e-06, min loss: 3.91290e-07\n",
      "Epoch: 2387100, elapsed: 1.19e+01, train loss: 3.05484e-06, val loss: 3.90326e-06, min loss: 3.91290e-07\n",
      "Epoch: 2387200, elapsed: 1.20e+01, train loss: 5.46253e-07, val loss: 1.67358e-06, min loss: 3.91290e-07\n",
      "Epoch: 2387300, elapsed: 1.20e+01, train loss: 4.00315e-07, val loss: 1.49305e-06, min loss: 3.91290e-07\n",
      "Epoch: 2387400, elapsed: 1.20e+01, train loss: 4.34133e-07, val loss: 1.48056e-06, min loss: 3.91290e-07\n",
      "Epoch: 2387500, elapsed: 1.22e+01, train loss: 4.00131e-07, val loss: 1.49659e-06, min loss: 3.91290e-07\n",
      "Epoch: 2387600, elapsed: 1.19e+01, train loss: 3.95356e-07, val loss: 1.50581e-06, min loss: 3.91290e-07\n",
      "Epoch: 2387700, elapsed: 1.21e+01, train loss: 3.92951e-07, val loss: 1.47679e-06, min loss: 3.91290e-07\n",
      "Epoch: 2387800, elapsed: 1.20e+01, train loss: 4.60761e-07, val loss: 1.43250e-06, min loss: 3.91290e-07\n",
      "Epoch: 2387900, elapsed: 1.18e+01, train loss: 1.19613e-06, val loss: 1.61501e-06, min loss: 3.91290e-07\n",
      "Epoch: 2388000, elapsed: 1.22e+01, train loss: 8.54131e-07, val loss: 1.77812e-06, min loss: 3.91290e-07\n",
      "Epoch: 2388100, elapsed: 1.21e+01, train loss: 4.38338e-07, val loss: 1.46223e-06, min loss: 3.91290e-07\n",
      "Epoch: 2388200, elapsed: 1.24e+01, train loss: 4.11813e-07, val loss: 1.51961e-06, min loss: 3.91290e-07\n",
      "Epoch: 2388300, elapsed: 1.22e+01, train loss: 2.32146e-06, val loss: 2.99007e-06, min loss: 3.91290e-07\n",
      "Epoch: 2388400, elapsed: 1.23e+01, train loss: 4.45588e-07, val loss: 1.48514e-06, min loss: 3.91290e-07\n",
      "Epoch: 2388500, elapsed: 1.20e+01, train loss: 3.91090e-07, val loss: 1.46992e-06, min loss: 3.91090e-07\n",
      "Epoch: 2388600, elapsed: 1.20e+01, train loss: 4.08660e-07, val loss: 1.49312e-06, min loss: 3.91090e-07\n",
      "Epoch: 2388700, elapsed: 1.21e+01, train loss: 3.90656e-07, val loss: 1.46914e-06, min loss: 3.90656e-07\n",
      "Epoch: 2388800, elapsed: 1.19e+01, train loss: 3.92508e-07, val loss: 1.47247e-06, min loss: 3.90656e-07\n",
      "Epoch: 2388900, elapsed: 1.20e+01, train loss: 3.90892e-07, val loss: 1.46619e-06, min loss: 3.90656e-07\n",
      "Epoch: 2389000, elapsed: 1.21e+01, train loss: 3.91671e-07, val loss: 1.46403e-06, min loss: 3.90656e-07\n",
      "Epoch: 2389100, elapsed: 1.21e+01, train loss: 1.67485e-06, val loss: 3.16429e-06, min loss: 3.90656e-07\n",
      "Epoch: 2389200, elapsed: 1.23e+01, train loss: 3.90708e-07, val loss: 1.46252e-06, min loss: 3.90656e-07\n",
      "Epoch: 2389300, elapsed: 1.20e+01, train loss: 6.22922e-07, val loss: 1.80152e-06, min loss: 3.90656e-07\n",
      "Epoch: 2389400, elapsed: 1.19e+01, train loss: 4.09417e-07, val loss: 1.50620e-06, min loss: 3.90656e-07\n",
      "Epoch: 2389500, elapsed: 1.21e+01, train loss: 4.34517e-07, val loss: 1.47507e-06, min loss: 3.90656e-07\n",
      "Epoch: 2389600, elapsed: 1.20e+01, train loss: 5.82868e-07, val loss: 1.58430e-06, min loss: 3.90656e-07\n",
      "Epoch: 2389700, elapsed: 1.22e+01, train loss: 3.94089e-07, val loss: 1.47464e-06, min loss: 3.90656e-07\n",
      "Epoch: 2389800, elapsed: 1.21e+01, train loss: 3.93651e-07, val loss: 1.47784e-06, min loss: 3.90656e-07\n",
      "Epoch: 2389900, elapsed: 1.87e+01, train loss: 3.94092e-07, val loss: 1.47850e-06, min loss: 3.90656e-07\n",
      "Epoch: 2390000, elapsed: 1.28e+01, train loss: 4.00341e-07, val loss: 1.47576e-06, min loss: 3.90656e-07\n",
      "Epoch: 2390100, elapsed: 1.46e+01, train loss: 3.90413e-07, val loss: 1.47006e-06, min loss: 3.90413e-07\n",
      "Epoch: 2390200, elapsed: 1.25e+01, train loss: 4.01987e-07, val loss: 1.45675e-06, min loss: 3.90413e-07\n",
      "Epoch: 2390300, elapsed: 1.24e+01, train loss: 3.93224e-07, val loss: 1.47458e-06, min loss: 3.90413e-07\n",
      "Epoch: 2390400, elapsed: 1.25e+01, train loss: 3.91859e-07, val loss: 1.46159e-06, min loss: 3.90413e-07\n",
      "Epoch: 2390500, elapsed: 1.28e+01, train loss: 3.93297e-07, val loss: 1.45691e-06, min loss: 3.90413e-07\n",
      "Epoch: 2390600, elapsed: 1.25e+01, train loss: 3.92140e-07, val loss: 1.46131e-06, min loss: 3.90413e-07\n",
      "Epoch: 2390700, elapsed: 1.25e+01, train loss: 5.03683e-07, val loss: 1.53184e-06, min loss: 3.90413e-07\n",
      "Epoch: 2390800, elapsed: 1.26e+01, train loss: 2.39584e-06, val loss: 2.70073e-06, min loss: 3.90413e-07\n",
      "Epoch: 2390900, elapsed: 1.24e+01, train loss: 4.31314e-07, val loss: 1.49332e-06, min loss: 3.90413e-07\n",
      "Epoch: 2391000, elapsed: 1.22e+01, train loss: 6.94259e-07, val loss: 2.13565e-06, min loss: 3.90413e-07\n",
      "Epoch: 2391100, elapsed: 1.22e+01, train loss: 6.20433e-07, val loss: 1.57957e-06, min loss: 3.90413e-07\n",
      "Epoch: 2391200, elapsed: 1.19e+01, train loss: 4.36010e-07, val loss: 1.52905e-06, min loss: 3.90413e-07\n",
      "Epoch: 2391300, elapsed: 1.19e+01, train loss: 4.37928e-07, val loss: 1.50459e-06, min loss: 3.90413e-07\n",
      "Epoch: 2391400, elapsed: 1.20e+01, train loss: 1.36473e-06, val loss: 2.06540e-06, min loss: 3.90413e-07\n",
      "Epoch: 2391500, elapsed: 1.20e+01, train loss: 4.83329e-07, val loss: 1.50425e-06, min loss: 3.90413e-07\n",
      "Epoch: 2391600, elapsed: 1.19e+01, train loss: 4.25732e-07, val loss: 1.54220e-06, min loss: 3.90413e-07\n",
      "Epoch: 2391700, elapsed: 1.21e+01, train loss: 4.59033e-07, val loss: 1.40181e-06, min loss: 3.90413e-07\n",
      "Epoch: 2391800, elapsed: 1.21e+01, train loss: 4.14238e-06, val loss: 3.38969e-06, min loss: 3.90413e-07\n",
      "Epoch: 2391900, elapsed: 1.21e+01, train loss: 3.90186e-07, val loss: 1.46258e-06, min loss: 3.90186e-07\n",
      "Epoch: 2392000, elapsed: 1.19e+01, train loss: 4.80673e-07, val loss: 1.61748e-06, min loss: 3.90186e-07\n",
      "Epoch: 2392100, elapsed: 1.23e+01, train loss: 4.00395e-07, val loss: 1.51154e-06, min loss: 3.90186e-07\n",
      "Epoch: 2392200, elapsed: 1.33e+01, train loss: 7.92808e-07, val loss: 1.41260e-06, min loss: 3.90186e-07\n",
      "Epoch: 2392300, elapsed: 1.38e+01, train loss: 3.96314e-07, val loss: 1.51752e-06, min loss: 3.90186e-07\n",
      "Epoch: 2392400, elapsed: 1.24e+01, train loss: 3.91568e-07, val loss: 1.47094e-06, min loss: 3.90186e-07\n",
      "Epoch: 2392500, elapsed: 1.18e+01, train loss: 3.90285e-07, val loss: 1.46663e-06, min loss: 3.90186e-07\n",
      "Epoch: 2392600, elapsed: 1.19e+01, train loss: 9.10163e-07, val loss: 1.52353e-06, min loss: 3.90186e-07\n",
      "Epoch: 2392700, elapsed: 1.19e+01, train loss: 3.89918e-07, val loss: 1.46602e-06, min loss: 3.89918e-07\n",
      "Epoch: 2392800, elapsed: 1.18e+01, train loss: 3.95261e-07, val loss: 1.48571e-06, min loss: 3.89918e-07\n",
      "Epoch: 2392900, elapsed: 1.18e+01, train loss: 1.43940e-06, val loss: 2.27601e-06, min loss: 3.89918e-07\n",
      "Epoch: 2393000, elapsed: 1.21e+01, train loss: 6.16874e-07, val loss: 1.53147e-06, min loss: 3.89918e-07\n",
      "Epoch: 2393100, elapsed: 1.18e+01, train loss: 4.05211e-07, val loss: 1.45074e-06, min loss: 3.89918e-07\n",
      "Epoch: 2393200, elapsed: 1.19e+01, train loss: 4.04473e-07, val loss: 1.44897e-06, min loss: 3.89918e-07\n",
      "Epoch: 2393300, elapsed: 1.18e+01, train loss: 3.97302e-07, val loss: 1.50178e-06, min loss: 3.89918e-07\n",
      "Epoch: 2393400, elapsed: 1.19e+01, train loss: 3.98161e-07, val loss: 1.44512e-06, min loss: 3.89918e-07\n",
      "Epoch: 2393500, elapsed: 1.18e+01, train loss: 4.34132e-07, val loss: 1.53262e-06, min loss: 3.89918e-07\n",
      "Epoch: 2393600, elapsed: 1.15e+01, train loss: 4.37615e-07, val loss: 1.48370e-06, min loss: 3.89918e-07\n",
      "Epoch: 2393700, elapsed: 1.17e+01, train loss: 6.03321e-07, val loss: 1.63856e-06, min loss: 3.89918e-07\n",
      "Epoch: 2393800, elapsed: 1.19e+01, train loss: 4.31201e-07, val loss: 1.55702e-06, min loss: 3.89918e-07\n",
      "Epoch: 2393900, elapsed: 1.21e+01, train loss: 3.90112e-07, val loss: 1.45982e-06, min loss: 3.89918e-07\n",
      "Epoch: 2394000, elapsed: 1.18e+01, train loss: 4.24468e-07, val loss: 1.50452e-06, min loss: 3.89918e-07\n",
      "Epoch: 2394100, elapsed: 1.80e+01, train loss: 2.05321e-06, val loss: 3.36668e-06, min loss: 3.89918e-07\n",
      "Epoch: 2394200, elapsed: 1.21e+01, train loss: 3.89723e-07, val loss: 1.46665e-06, min loss: 3.89723e-07\n",
      "Epoch: 2394300, elapsed: 1.21e+01, train loss: 4.47659e-07, val loss: 1.46906e-06, min loss: 3.89723e-07\n",
      "Epoch: 2394400, elapsed: 1.21e+01, train loss: 3.89517e-07, val loss: 1.46293e-06, min loss: 3.89517e-07\n",
      "Epoch: 2394500, elapsed: 1.19e+01, train loss: 4.83987e-07, val loss: 1.46030e-06, min loss: 3.89517e-07\n",
      "Epoch: 2394600, elapsed: 1.21e+01, train loss: 3.89483e-07, val loss: 1.46200e-06, min loss: 3.89483e-07\n",
      "Epoch: 2394700, elapsed: 1.19e+01, train loss: 4.91502e-07, val loss: 1.47358e-06, min loss: 3.89483e-07\n",
      "Epoch: 2394800, elapsed: 1.20e+01, train loss: 4.25324e-07, val loss: 1.50766e-06, min loss: 3.89483e-07\n",
      "Epoch: 2394900, elapsed: 1.20e+01, train loss: 5.63149e-07, val loss: 1.78901e-06, min loss: 3.89483e-07\n",
      "Epoch: 2395000, elapsed: 1.19e+01, train loss: 3.89730e-07, val loss: 1.46082e-06, min loss: 3.89483e-07\n",
      "Epoch: 2395100, elapsed: 1.39e+01, train loss: 7.51539e-07, val loss: 1.68538e-06, min loss: 3.89483e-07\n",
      "Epoch: 2395200, elapsed: 1.18e+01, train loss: 5.19914e-07, val loss: 1.41613e-06, min loss: 3.89483e-07\n",
      "Epoch: 2395300, elapsed: 1.19e+01, train loss: 1.50772e-06, val loss: 2.57468e-06, min loss: 3.89483e-07\n",
      "Epoch: 2395400, elapsed: 1.19e+01, train loss: 2.45646e-06, val loss: 3.04417e-06, min loss: 3.89483e-07\n",
      "Epoch: 2395500, elapsed: 1.19e+01, train loss: 9.05438e-07, val loss: 2.20618e-06, min loss: 3.89483e-07\n",
      "Epoch: 2395600, elapsed: 1.20e+01, train loss: 8.97926e-07, val loss: 2.22871e-06, min loss: 3.89483e-07\n",
      "Epoch: 2395700, elapsed: 1.19e+01, train loss: 4.28846e-07, val loss: 1.55115e-06, min loss: 3.89483e-07\n",
      "Epoch: 2395800, elapsed: 1.20e+01, train loss: 3.96969e-07, val loss: 1.46633e-06, min loss: 3.89483e-07\n",
      "Epoch: 2395900, elapsed: 1.18e+01, train loss: 3.92581e-07, val loss: 1.47175e-06, min loss: 3.89483e-07\n",
      "Epoch: 2396000, elapsed: 1.18e+01, train loss: 3.90017e-07, val loss: 1.47359e-06, min loss: 3.89483e-07\n",
      "Epoch: 2396100, elapsed: 1.20e+01, train loss: 3.89794e-07, val loss: 1.46310e-06, min loss: 3.89483e-07\n",
      "Epoch: 2396200, elapsed: 1.17e+01, train loss: 4.30183e-07, val loss: 1.52562e-06, min loss: 3.89483e-07\n",
      "Epoch: 2396300, elapsed: 1.19e+01, train loss: 1.84603e-06, val loss: 1.96134e-06, min loss: 3.89483e-07\n",
      "Epoch: 2396400, elapsed: 1.21e+01, train loss: 3.89498e-07, val loss: 1.46940e-06, min loss: 3.89483e-07\n",
      "Epoch: 2396500, elapsed: 1.18e+01, train loss: 3.89388e-07, val loss: 1.46565e-06, min loss: 3.89388e-07\n",
      "Epoch: 2396600, elapsed: 1.16e+01, train loss: 5.14851e-07, val loss: 1.52264e-06, min loss: 3.89388e-07\n",
      "Epoch: 2396700, elapsed: 1.21e+01, train loss: 4.11610e-07, val loss: 1.45902e-06, min loss: 3.89388e-07\n",
      "Epoch: 2396800, elapsed: 1.20e+01, train loss: 3.89415e-07, val loss: 1.45839e-06, min loss: 3.89388e-07\n",
      "Epoch: 2396900, elapsed: 1.18e+01, train loss: 4.00083e-07, val loss: 1.45055e-06, min loss: 3.89388e-07\n",
      "Epoch: 2397000, elapsed: 1.17e+01, train loss: 4.11255e-07, val loss: 1.51327e-06, min loss: 3.89388e-07\n",
      "Epoch: 2397100, elapsed: 1.17e+01, train loss: 4.57210e-07, val loss: 1.48756e-06, min loss: 3.89388e-07\n",
      "Epoch: 2397200, elapsed: 1.19e+01, train loss: 5.27624e-07, val loss: 1.79778e-06, min loss: 3.89388e-07\n",
      "Epoch: 2397300, elapsed: 1.17e+01, train loss: 1.90476e-06, val loss: 3.48625e-06, min loss: 3.89388e-07\n",
      "Epoch: 2397400, elapsed: 1.17e+01, train loss: 4.00459e-07, val loss: 1.45656e-06, min loss: 3.89388e-07\n",
      "Epoch: 2397500, elapsed: 1.17e+01, train loss: 3.95888e-07, val loss: 1.44570e-06, min loss: 3.89388e-07\n",
      "Epoch: 2397600, elapsed: 1.17e+01, train loss: 1.16275e-06, val loss: 2.46379e-06, min loss: 3.89388e-07\n",
      "Epoch: 2397700, elapsed: 1.16e+01, train loss: 3.89097e-07, val loss: 1.46265e-06, min loss: 3.89097e-07\n",
      "Epoch: 2397800, elapsed: 1.16e+01, train loss: 5.48941e-07, val loss: 1.63449e-06, min loss: 3.89097e-07\n",
      "Epoch: 2397900, elapsed: 1.17e+01, train loss: 5.66798e-07, val loss: 1.61724e-06, min loss: 3.89097e-07\n",
      "Epoch: 2398000, elapsed: 1.18e+01, train loss: 3.90844e-07, val loss: 1.46208e-06, min loss: 3.89097e-07\n",
      "Epoch: 2398100, elapsed: 1.18e+01, train loss: 6.21539e-07, val loss: 1.76077e-06, min loss: 3.89097e-07\n",
      "Epoch: 2398200, elapsed: 1.18e+01, train loss: 4.31066e-07, val loss: 1.51491e-06, min loss: 3.89097e-07\n",
      "Epoch: 2398300, elapsed: 1.82e+01, train loss: 4.20792e-07, val loss: 1.55163e-06, min loss: 3.89097e-07\n",
      "Epoch: 2398400, elapsed: 1.22e+01, train loss: 1.27069e-06, val loss: 2.28217e-06, min loss: 3.89097e-07\n",
      "Epoch: 2398500, elapsed: 1.21e+01, train loss: 1.82179e-06, val loss: 2.95530e-06, min loss: 3.89097e-07\n",
      "Epoch: 2398600, elapsed: 1.20e+01, train loss: 3.88808e-07, val loss: 1.45737e-06, min loss: 3.88808e-07\n",
      "Epoch: 2398700, elapsed: 1.20e+01, train loss: 3.89287e-07, val loss: 1.44081e-06, min loss: 3.88808e-07\n",
      "Epoch: 2398800, elapsed: 1.21e+01, train loss: 3.90254e-07, val loss: 1.44110e-06, min loss: 3.88808e-07\n",
      "Epoch: 2398900, elapsed: 1.21e+01, train loss: 4.04942e-07, val loss: 1.44739e-06, min loss: 3.88808e-07\n",
      "Epoch: 2399000, elapsed: 1.20e+01, train loss: 4.45183e-07, val loss: 1.57402e-06, min loss: 3.88808e-07\n",
      "Epoch: 2399100, elapsed: 1.21e+01, train loss: 3.90192e-07, val loss: 1.46948e-06, min loss: 3.88808e-07\n",
      "Epoch: 2399200, elapsed: 1.21e+01, train loss: 3.92568e-07, val loss: 1.45476e-06, min loss: 3.88808e-07\n",
      "Epoch: 2399300, elapsed: 1.18e+01, train loss: 4.29834e-07, val loss: 1.42157e-06, min loss: 3.88808e-07\n",
      "Epoch: 2399400, elapsed: 1.20e+01, train loss: 4.31392e-07, val loss: 1.63817e-06, min loss: 3.88808e-07\n",
      "Epoch: 2399500, elapsed: 1.20e+01, train loss: 4.35223e-07, val loss: 1.47278e-06, min loss: 3.88808e-07\n",
      "Epoch: 2399600, elapsed: 1.20e+01, train loss: 4.25523e-07, val loss: 1.47163e-06, min loss: 3.88808e-07\n",
      "Epoch: 2399700, elapsed: 1.20e+01, train loss: 4.04963e-07, val loss: 1.51737e-06, min loss: 3.88808e-07\n",
      "Epoch: 2399800, elapsed: 1.17e+01, train loss: 3.92044e-07, val loss: 1.50322e-06, min loss: 3.88808e-07\n",
      "Epoch: 2399900, elapsed: 1.19e+01, train loss: 3.97976e-07, val loss: 1.43154e-06, min loss: 3.88808e-07\n",
      "Epoch: 2400000, elapsed: 1.21e+01, train loss: 3.99907e-07, val loss: 1.45427e-06, min loss: 3.88808e-07\n",
      "Epoch: 2400100, elapsed: 1.41e+01, train loss: 4.74683e-07, val loss: 1.38551e-06, min loss: 3.88808e-07\n",
      "Epoch: 2400200, elapsed: 1.21e+01, train loss: 3.89159e-07, val loss: 1.45254e-06, min loss: 3.88808e-07\n",
      "Epoch: 2400300, elapsed: 1.19e+01, train loss: 3.90103e-07, val loss: 1.45400e-06, min loss: 3.88808e-07\n",
      "Epoch: 2400400, elapsed: 1.22e+01, train loss: 4.12139e-07, val loss: 1.46321e-06, min loss: 3.88808e-07\n",
      "Epoch: 2400500, elapsed: 1.20e+01, train loss: 3.88851e-07, val loss: 1.45802e-06, min loss: 3.88808e-07\n",
      "Epoch: 2400600, elapsed: 1.19e+01, train loss: 4.42703e-07, val loss: 1.43909e-06, min loss: 3.88808e-07\n",
      "Epoch: 2400700, elapsed: 1.22e+01, train loss: 4.32733e-07, val loss: 1.46612e-06, min loss: 3.88808e-07\n",
      "Epoch: 2400800, elapsed: 1.19e+01, train loss: 4.30154e-07, val loss: 1.49301e-06, min loss: 3.88808e-07\n",
      "Epoch: 2400900, elapsed: 1.19e+01, train loss: 3.88841e-07, val loss: 1.46111e-06, min loss: 3.88808e-07\n",
      "Epoch: 2401000, elapsed: 1.21e+01, train loss: 4.43285e-07, val loss: 1.46602e-06, min loss: 3.88808e-07\n",
      "Epoch: 2401100, elapsed: 1.17e+01, train loss: 4.70656e-07, val loss: 1.63030e-06, min loss: 3.88808e-07\n",
      "Epoch: 2401200, elapsed: 1.18e+01, train loss: 3.89230e-07, val loss: 1.46179e-06, min loss: 3.88808e-07\n",
      "Epoch: 2401300, elapsed: 1.18e+01, train loss: 3.89105e-07, val loss: 1.46185e-06, min loss: 3.88808e-07\n",
      "Epoch: 2401400, elapsed: 1.17e+01, train loss: 4.13146e-07, val loss: 1.41415e-06, min loss: 3.88808e-07\n",
      "Epoch: 2401500, elapsed: 1.19e+01, train loss: 3.21507e-06, val loss: 2.96709e-06, min loss: 3.88808e-07\n",
      "Epoch: 2401600, elapsed: 1.16e+01, train loss: 3.88259e-07, val loss: 1.45966e-06, min loss: 3.88259e-07\n",
      "Epoch: 2401700, elapsed: 1.18e+01, train loss: 4.07494e-07, val loss: 1.45377e-06, min loss: 3.88259e-07\n",
      "Epoch: 2401800, elapsed: 1.16e+01, train loss: 4.40482e-07, val loss: 1.59071e-06, min loss: 3.88259e-07\n",
      "Epoch: 2401900, elapsed: 1.19e+01, train loss: 5.51836e-07, val loss: 1.73510e-06, min loss: 3.88259e-07\n",
      "Epoch: 2402000, elapsed: 1.17e+01, train loss: 1.42812e-06, val loss: 2.48932e-06, min loss: 3.88259e-07\n",
      "Epoch: 2402100, elapsed: 1.18e+01, train loss: 4.95729e-07, val loss: 1.51713e-06, min loss: 3.88259e-07\n",
      "Epoch: 2402200, elapsed: 1.17e+01, train loss: 3.88351e-07, val loss: 1.45607e-06, min loss: 3.88259e-07\n",
      "Epoch: 2402300, elapsed: 1.17e+01, train loss: 3.96626e-07, val loss: 1.47391e-06, min loss: 3.88259e-07\n",
      "Epoch: 2402400, elapsed: 1.18e+01, train loss: 5.24166e-07, val loss: 1.63333e-06, min loss: 3.88259e-07\n",
      "Epoch: 2402500, elapsed: 1.80e+01, train loss: 5.14872e-07, val loss: 1.76034e-06, min loss: 3.88259e-07\n",
      "Epoch: 2402600, elapsed: 1.23e+01, train loss: 6.01020e-07, val loss: 1.72583e-06, min loss: 3.88259e-07\n",
      "Epoch: 2402700, elapsed: 1.23e+01, train loss: 3.89100e-07, val loss: 1.45059e-06, min loss: 3.88259e-07\n",
      "Epoch: 2402800, elapsed: 1.20e+01, train loss: 8.31711e-07, val loss: 1.48582e-06, min loss: 3.88259e-07\n",
      "Epoch: 2402900, elapsed: 1.21e+01, train loss: 3.91787e-07, val loss: 1.44586e-06, min loss: 3.88259e-07\n",
      "Epoch: 2403000, elapsed: 1.19e+01, train loss: 3.88272e-07, val loss: 1.45679e-06, min loss: 3.88259e-07\n",
      "Epoch: 2403100, elapsed: 1.20e+01, train loss: 3.93621e-07, val loss: 1.46367e-06, min loss: 3.88259e-07\n",
      "Epoch: 2403200, elapsed: 1.21e+01, train loss: 5.05676e-07, val loss: 1.43004e-06, min loss: 3.88259e-07\n",
      "Epoch: 2403300, elapsed: 1.22e+01, train loss: 6.27859e-07, val loss: 1.54871e-06, min loss: 3.88259e-07\n",
      "Epoch: 2403400, elapsed: 1.19e+01, train loss: 4.19030e-07, val loss: 1.52984e-06, min loss: 3.88259e-07\n",
      "Epoch: 2403500, elapsed: 1.20e+01, train loss: 4.82320e-07, val loss: 1.44908e-06, min loss: 3.88259e-07\n",
      "Epoch: 2403600, elapsed: 1.18e+01, train loss: 8.34887e-07, val loss: 1.86919e-06, min loss: 3.88259e-07\n",
      "Epoch: 2403700, elapsed: 1.20e+01, train loss: 8.18997e-07, val loss: 1.80847e-06, min loss: 3.88259e-07\n",
      "Epoch: 2403800, elapsed: 1.23e+01, train loss: 1.20799e-06, val loss: 2.37401e-06, min loss: 3.88259e-07\n",
      "Epoch: 2403900, elapsed: 1.20e+01, train loss: 3.88572e-07, val loss: 1.44799e-06, min loss: 3.88259e-07\n",
      "Epoch: 2404000, elapsed: 1.20e+01, train loss: 3.93146e-07, val loss: 1.49077e-06, min loss: 3.88259e-07\n",
      "Epoch: 2404100, elapsed: 1.17e+01, train loss: 5.83810e-07, val loss: 1.70113e-06, min loss: 3.88259e-07\n",
      "Epoch: 2404200, elapsed: 1.20e+01, train loss: 3.88034e-07, val loss: 1.45613e-06, min loss: 3.88034e-07\n",
      "Epoch: 2404300, elapsed: 1.17e+01, train loss: 4.24029e-07, val loss: 1.49120e-06, min loss: 3.88034e-07\n",
      "Epoch: 2404400, elapsed: 1.19e+01, train loss: 3.96267e-07, val loss: 1.52990e-06, min loss: 3.88034e-07\n",
      "Epoch: 2404500, elapsed: 1.18e+01, train loss: 3.87790e-07, val loss: 1.45036e-06, min loss: 3.87790e-07\n",
      "Epoch: 2404600, elapsed: 1.19e+01, train loss: 1.11627e-06, val loss: 1.99064e-06, min loss: 3.87790e-07\n",
      "Epoch: 2404700, elapsed: 1.19e+01, train loss: 3.92932e-07, val loss: 1.54526e-06, min loss: 3.87790e-07\n",
      "Epoch: 2404800, elapsed: 1.18e+01, train loss: 1.53701e-06, val loss: 2.24928e-06, min loss: 3.87790e-07\n",
      "Epoch: 2404900, elapsed: 1.18e+01, train loss: 4.36778e-07, val loss: 1.60428e-06, min loss: 3.87790e-07\n",
      "Epoch: 2405000, elapsed: 1.17e+01, train loss: 3.87834e-07, val loss: 1.44749e-06, min loss: 3.87790e-07\n",
      "Epoch: 2405100, elapsed: 1.40e+01, train loss: 4.06977e-07, val loss: 1.43909e-06, min loss: 3.87790e-07\n",
      "Epoch: 2405200, elapsed: 1.16e+01, train loss: 9.50571e-07, val loss: 1.90476e-06, min loss: 3.87790e-07\n",
      "Epoch: 2405300, elapsed: 1.18e+01, train loss: 7.27871e-07, val loss: 1.47621e-06, min loss: 3.87790e-07\n",
      "Epoch: 2405400, elapsed: 1.18e+01, train loss: 1.29198e-06, val loss: 2.69256e-06, min loss: 3.87790e-07\n",
      "Epoch: 2405500, elapsed: 1.18e+01, train loss: 6.05689e-07, val loss: 1.61469e-06, min loss: 3.87790e-07\n",
      "Epoch: 2405600, elapsed: 1.18e+01, train loss: 5.36853e-07, val loss: 1.83047e-06, min loss: 3.87790e-07\n",
      "Epoch: 2405700, elapsed: 1.17e+01, train loss: 3.90303e-07, val loss: 1.42471e-06, min loss: 3.87790e-07\n",
      "Epoch: 2405800, elapsed: 1.18e+01, train loss: 4.16815e-07, val loss: 1.48307e-06, min loss: 3.87790e-07\n",
      "Epoch: 2405900, elapsed: 1.18e+01, train loss: 1.04049e-06, val loss: 2.20485e-06, min loss: 3.87790e-07\n",
      "Epoch: 2406000, elapsed: 1.18e+01, train loss: 3.87459e-07, val loss: 1.44957e-06, min loss: 3.87459e-07\n",
      "Epoch: 2406100, elapsed: 1.18e+01, train loss: 4.82320e-07, val loss: 1.41685e-06, min loss: 3.87459e-07\n",
      "Epoch: 2406200, elapsed: 1.16e+01, train loss: 3.87195e-07, val loss: 1.45022e-06, min loss: 3.87195e-07\n",
      "Epoch: 2406300, elapsed: 1.16e+01, train loss: 3.93730e-07, val loss: 1.47572e-06, min loss: 3.87195e-07\n",
      "Epoch: 2406400, elapsed: 1.18e+01, train loss: 3.87138e-07, val loss: 1.44864e-06, min loss: 3.87138e-07\n",
      "Epoch: 2406500, elapsed: 1.16e+01, train loss: 3.90275e-07, val loss: 1.45245e-06, min loss: 3.87138e-07\n",
      "Epoch: 2406600, elapsed: 1.17e+01, train loss: 3.87092e-07, val loss: 1.45050e-06, min loss: 3.87092e-07\n",
      "Epoch: 2406700, elapsed: 1.18e+01, train loss: 3.93776e-07, val loss: 1.43542e-06, min loss: 3.87092e-07\n",
      "Epoch: 2406800, elapsed: 1.86e+01, train loss: 3.87150e-07, val loss: 1.45609e-06, min loss: 3.87092e-07\n",
      "Epoch: 2406900, elapsed: 1.20e+01, train loss: 3.91538e-07, val loss: 1.41946e-06, min loss: 3.87092e-07\n",
      "Epoch: 2407000, elapsed: 1.19e+01, train loss: 6.81076e-07, val loss: 1.71463e-06, min loss: 3.87092e-07\n",
      "Epoch: 2407100, elapsed: 1.19e+01, train loss: 4.00743e-07, val loss: 1.49627e-06, min loss: 3.87092e-07\n",
      "Epoch: 2407200, elapsed: 1.19e+01, train loss: 3.87736e-07, val loss: 1.44728e-06, min loss: 3.87092e-07\n",
      "Epoch: 2407300, elapsed: 1.20e+01, train loss: 4.30581e-07, val loss: 1.54022e-06, min loss: 3.87092e-07\n",
      "Epoch: 2407400, elapsed: 1.18e+01, train loss: 4.31118e-07, val loss: 1.42625e-06, min loss: 3.87092e-07\n",
      "Epoch: 2407500, elapsed: 1.19e+01, train loss: 4.26645e-07, val loss: 1.47417e-06, min loss: 3.87092e-07\n",
      "Epoch: 2407600, elapsed: 1.20e+01, train loss: 3.89962e-07, val loss: 1.45792e-06, min loss: 3.87092e-07\n",
      "Epoch: 2407700, elapsed: 1.19e+01, train loss: 3.86992e-07, val loss: 1.44307e-06, min loss: 3.86992e-07\n",
      "Epoch: 2407800, elapsed: 1.18e+01, train loss: 3.87856e-07, val loss: 1.45501e-06, min loss: 3.86992e-07\n",
      "Epoch: 2407900, elapsed: 1.18e+01, train loss: 3.86862e-07, val loss: 1.44856e-06, min loss: 3.86862e-07\n",
      "Epoch: 2408000, elapsed: 1.21e+01, train loss: 3.88199e-07, val loss: 1.45302e-06, min loss: 3.86862e-07\n",
      "Epoch: 2408100, elapsed: 1.19e+01, train loss: 7.87230e-07, val loss: 1.89495e-06, min loss: 3.86862e-07\n",
      "Epoch: 2408200, elapsed: 1.18e+01, train loss: 3.86926e-07, val loss: 1.44868e-06, min loss: 3.86862e-07\n",
      "Epoch: 2408300, elapsed: 1.20e+01, train loss: 4.20646e-07, val loss: 1.56119e-06, min loss: 3.86862e-07\n",
      "Epoch: 2408400, elapsed: 1.21e+01, train loss: 1.62202e-06, val loss: 2.27036e-06, min loss: 3.86862e-07\n",
      "Epoch: 2408500, elapsed: 1.20e+01, train loss: 3.86850e-07, val loss: 1.44709e-06, min loss: 3.86850e-07\n",
      "Epoch: 2408600, elapsed: 1.21e+01, train loss: 4.49365e-07, val loss: 1.58661e-06, min loss: 3.86850e-07\n",
      "Epoch: 2408700, elapsed: 1.18e+01, train loss: 4.11956e-07, val loss: 1.42893e-06, min loss: 3.86850e-07\n",
      "Epoch: 2408800, elapsed: 1.28e+01, train loss: 4.32111e-07, val loss: 1.50587e-06, min loss: 3.86850e-07\n",
      "Epoch: 2408900, elapsed: 1.33e+01, train loss: 3.58093e-06, val loss: 4.55921e-06, min loss: 3.86850e-07\n",
      "Epoch: 2409000, elapsed: 1.33e+01, train loss: 3.86727e-07, val loss: 1.44954e-06, min loss: 3.86727e-07\n",
      "Epoch: 2409100, elapsed: 1.20e+01, train loss: 4.00987e-07, val loss: 1.47475e-06, min loss: 3.86727e-07\n",
      "Epoch: 2409200, elapsed: 1.19e+01, train loss: 4.03599e-07, val loss: 1.42109e-06, min loss: 3.86727e-07\n",
      "Epoch: 2409300, elapsed: 1.16e+01, train loss: 4.38197e-07, val loss: 1.52740e-06, min loss: 3.86727e-07\n",
      "Epoch: 2409400, elapsed: 1.17e+01, train loss: 4.27643e-07, val loss: 1.44589e-06, min loss: 3.86727e-07\n",
      "Epoch: 2409500, elapsed: 1.16e+01, train loss: 3.87163e-07, val loss: 1.43894e-06, min loss: 3.86727e-07\n",
      "Epoch: 2409600, elapsed: 1.17e+01, train loss: 4.13955e-07, val loss: 1.44799e-06, min loss: 3.86727e-07\n",
      "Epoch: 2409700, elapsed: 1.18e+01, train loss: 8.98011e-07, val loss: 1.77841e-06, min loss: 3.86727e-07\n",
      "Epoch: 2409800, elapsed: 1.16e+01, train loss: 3.86505e-07, val loss: 1.44809e-06, min loss: 3.86505e-07\n",
      "Epoch: 2409900, elapsed: 1.18e+01, train loss: 4.00728e-07, val loss: 1.42349e-06, min loss: 3.86505e-07\n",
      "Epoch: 2410000, elapsed: 1.18e+01, train loss: 4.35672e-07, val loss: 1.47461e-06, min loss: 3.86505e-07\n",
      "Epoch: 2410100, elapsed: 1.41e+01, train loss: 3.86838e-07, val loss: 1.43857e-06, min loss: 3.86505e-07\n",
      "Epoch: 2410200, elapsed: 1.16e+01, train loss: 5.25132e-07, val loss: 1.54076e-06, min loss: 3.86505e-07\n",
      "Epoch: 2410300, elapsed: 1.17e+01, train loss: 3.86783e-07, val loss: 1.44859e-06, min loss: 3.86505e-07\n",
      "Epoch: 2410400, elapsed: 1.17e+01, train loss: 1.56281e-06, val loss: 2.65570e-06, min loss: 3.86505e-07\n",
      "Epoch: 2410500, elapsed: 1.17e+01, train loss: 4.24990e-07, val loss: 1.44680e-06, min loss: 3.86505e-07\n",
      "Epoch: 2410600, elapsed: 1.18e+01, train loss: 3.88371e-07, val loss: 1.46136e-06, min loss: 3.86505e-07\n",
      "Epoch: 2410700, elapsed: 1.17e+01, train loss: 4.15107e-07, val loss: 1.46167e-06, min loss: 3.86505e-07\n",
      "Epoch: 2410800, elapsed: 1.17e+01, train loss: 4.05345e-07, val loss: 1.46271e-06, min loss: 3.86505e-07\n",
      "Epoch: 2410900, elapsed: 1.18e+01, train loss: 4.67497e-07, val loss: 1.48319e-06, min loss: 3.86505e-07\n",
      "Epoch: 2411000, elapsed: 1.82e+01, train loss: 9.27612e-07, val loss: 1.72735e-06, min loss: 3.86505e-07\n",
      "Epoch: 2411100, elapsed: 1.22e+01, train loss: 4.33349e-07, val loss: 1.51485e-06, min loss: 3.86505e-07\n",
      "Epoch: 2411200, elapsed: 1.20e+01, train loss: 3.89326e-07, val loss: 1.44123e-06, min loss: 3.86505e-07\n",
      "Epoch: 2411300, elapsed: 1.20e+01, train loss: 3.96272e-07, val loss: 1.43679e-06, min loss: 3.86505e-07\n",
      "Epoch: 2411400, elapsed: 1.20e+01, train loss: 4.77248e-07, val loss: 1.56871e-06, min loss: 3.86505e-07\n",
      "Epoch: 2411500, elapsed: 1.19e+01, train loss: 4.20661e-07, val loss: 1.44763e-06, min loss: 3.86505e-07\n",
      "Epoch: 2411600, elapsed: 1.19e+01, train loss: 5.82923e-07, val loss: 1.76183e-06, min loss: 3.86505e-07\n",
      "Epoch: 2411700, elapsed: 1.20e+01, train loss: 3.86702e-07, val loss: 1.45358e-06, min loss: 3.86505e-07\n",
      "Epoch: 2411800, elapsed: 1.19e+01, train loss: 2.60783e-06, val loss: 3.83155e-06, min loss: 3.86505e-07\n",
      "Epoch: 2411900, elapsed: 1.20e+01, train loss: 3.92869e-07, val loss: 1.43926e-06, min loss: 3.86505e-07\n",
      "Epoch: 2412000, elapsed: 1.19e+01, train loss: 3.86177e-07, val loss: 1.43968e-06, min loss: 3.86177e-07\n",
      "Epoch: 2412100, elapsed: 1.19e+01, train loss: 3.98031e-07, val loss: 1.42798e-06, min loss: 3.86177e-07\n",
      "Epoch: 2412200, elapsed: 1.18e+01, train loss: 3.86028e-07, val loss: 1.44315e-06, min loss: 3.86028e-07\n",
      "Epoch: 2412300, elapsed: 1.18e+01, train loss: 3.88168e-07, val loss: 1.42465e-06, min loss: 3.86028e-07\n",
      "Epoch: 2412400, elapsed: 1.18e+01, train loss: 7.51129e-07, val loss: 1.76744e-06, min loss: 3.86028e-07\n",
      "Epoch: 2412500, elapsed: 1.18e+01, train loss: 3.95602e-07, val loss: 1.43041e-06, min loss: 3.86028e-07\n",
      "Epoch: 2412600, elapsed: 1.19e+01, train loss: 3.86665e-07, val loss: 1.44058e-06, min loss: 3.86028e-07\n",
      "Epoch: 2412700, elapsed: 1.19e+01, train loss: 3.89945e-07, val loss: 1.46091e-06, min loss: 3.86028e-07\n",
      "Epoch: 2412800, elapsed: 1.19e+01, train loss: 3.91489e-07, val loss: 1.41800e-06, min loss: 3.86028e-07\n",
      "Epoch: 2412900, elapsed: 1.20e+01, train loss: 3.93232e-07, val loss: 1.44740e-06, min loss: 3.86028e-07\n",
      "Epoch: 2413000, elapsed: 1.21e+01, train loss: 4.14964e-07, val loss: 1.58351e-06, min loss: 3.86028e-07\n",
      "Epoch: 2413100, elapsed: 1.20e+01, train loss: 5.13016e-07, val loss: 1.49269e-06, min loss: 3.86028e-07\n",
      "Epoch: 2413200, elapsed: 1.18e+01, train loss: 2.43690e-06, val loss: 3.01653e-06, min loss: 3.86028e-07\n",
      "Epoch: 2413300, elapsed: 1.18e+01, train loss: 7.06108e-07, val loss: 1.48050e-06, min loss: 3.86028e-07\n",
      "Epoch: 2413400, elapsed: 1.20e+01, train loss: 1.88598e-06, val loss: 2.87353e-06, min loss: 3.86028e-07\n",
      "Epoch: 2413500, elapsed: 1.22e+01, train loss: 1.41904e-06, val loss: 2.15380e-06, min loss: 3.86028e-07\n",
      "Epoch: 2413600, elapsed: 1.18e+01, train loss: 4.88199e-07, val loss: 1.47763e-06, min loss: 3.86028e-07\n",
      "Epoch: 2413700, elapsed: 1.19e+01, train loss: 4.59283e-07, val loss: 1.49051e-06, min loss: 3.86028e-07\n",
      "Epoch: 2413800, elapsed: 1.19e+01, train loss: 3.18446e-06, val loss: 3.40163e-06, min loss: 3.86028e-07\n",
      "Epoch: 2413900, elapsed: 1.20e+01, train loss: 3.86151e-07, val loss: 1.42924e-06, min loss: 3.86028e-07\n",
      "Epoch: 2414000, elapsed: 1.17e+01, train loss: 4.44029e-07, val loss: 1.54855e-06, min loss: 3.86028e-07\n",
      "Epoch: 2414100, elapsed: 1.20e+01, train loss: 9.75333e-07, val loss: 2.33961e-06, min loss: 3.86028e-07\n",
      "Epoch: 2414200, elapsed: 1.17e+01, train loss: 1.30314e-06, val loss: 2.61340e-06, min loss: 3.86028e-07\n",
      "Epoch: 2414300, elapsed: 1.18e+01, train loss: 3.98195e-07, val loss: 1.47657e-06, min loss: 3.86028e-07\n",
      "Epoch: 2414400, elapsed: 1.17e+01, train loss: 3.90332e-07, val loss: 1.42476e-06, min loss: 3.86028e-07\n",
      "Epoch: 2414500, elapsed: 1.17e+01, train loss: 3.90759e-07, val loss: 1.46079e-06, min loss: 3.86028e-07\n",
      "Epoch: 2414600, elapsed: 1.20e+01, train loss: 1.08029e-06, val loss: 1.83593e-06, min loss: 3.86028e-07\n",
      "Epoch: 2414700, elapsed: 1.18e+01, train loss: 3.89188e-07, val loss: 1.43163e-06, min loss: 3.86028e-07\n",
      "Epoch: 2414800, elapsed: 1.15e+01, train loss: 3.85676e-07, val loss: 1.44219e-06, min loss: 3.85676e-07\n",
      "Epoch: 2414900, elapsed: 1.19e+01, train loss: 4.90380e-07, val loss: 1.63203e-06, min loss: 3.85676e-07\n",
      "Epoch: 2415000, elapsed: 1.17e+01, train loss: 3.85487e-07, val loss: 1.43917e-06, min loss: 3.85487e-07\n",
      "Epoch: 2415100, elapsed: 1.38e+01, train loss: 5.35433e-07, val loss: 1.70455e-06, min loss: 3.85487e-07\n",
      "Epoch: 2415200, elapsed: 1.20e+01, train loss: 3.85452e-07, val loss: 1.44190e-06, min loss: 3.85452e-07\n",
      "Epoch: 2415300, elapsed: 1.85e+01, train loss: 4.85173e-07, val loss: 1.55639e-06, min loss: 3.85452e-07\n",
      "Epoch: 2415400, elapsed: 1.21e+01, train loss: 5.64143e-07, val loss: 1.41116e-06, min loss: 3.85452e-07\n",
      "Epoch: 2415500, elapsed: 1.20e+01, train loss: 3.86677e-07, val loss: 1.43804e-06, min loss: 3.85452e-07\n",
      "Epoch: 2415600, elapsed: 1.20e+01, train loss: 1.20987e-06, val loss: 2.19680e-06, min loss: 3.85452e-07\n",
      "Epoch: 2415700, elapsed: 1.18e+01, train loss: 3.87572e-07, val loss: 1.44094e-06, min loss: 3.85452e-07\n",
      "Epoch: 2415800, elapsed: 1.21e+01, train loss: 3.89258e-07, val loss: 1.45410e-06, min loss: 3.85452e-07\n",
      "Epoch: 2415900, elapsed: 1.19e+01, train loss: 3.96405e-07, val loss: 1.44004e-06, min loss: 3.85452e-07\n",
      "Epoch: 2416000, elapsed: 1.24e+01, train loss: 4.62278e-07, val loss: 1.50067e-06, min loss: 3.85452e-07\n",
      "Epoch: 2416100, elapsed: 1.19e+01, train loss: 5.37575e-07, val loss: 1.72380e-06, min loss: 3.85452e-07\n",
      "Epoch: 2416200, elapsed: 1.21e+01, train loss: 3.86184e-07, val loss: 1.45087e-06, min loss: 3.85452e-07\n",
      "Epoch: 2416300, elapsed: 1.24e+01, train loss: 3.95652e-07, val loss: 1.49566e-06, min loss: 3.85452e-07\n",
      "Epoch: 2416400, elapsed: 1.28e+01, train loss: 4.56391e-07, val loss: 1.63827e-06, min loss: 3.85452e-07\n",
      "Epoch: 2416500, elapsed: 1.23e+01, train loss: 3.85858e-07, val loss: 1.44206e-06, min loss: 3.85452e-07\n",
      "Epoch: 2416600, elapsed: 1.29e+01, train loss: 3.88824e-07, val loss: 1.44531e-06, min loss: 3.85452e-07\n",
      "Epoch: 2416700, elapsed: 1.28e+01, train loss: 4.90456e-07, val loss: 1.59297e-06, min loss: 3.85452e-07\n",
      "Epoch: 2416800, elapsed: 1.36e+01, train loss: 4.44765e-07, val loss: 1.54018e-06, min loss: 3.85452e-07\n",
      "Epoch: 2416900, elapsed: 1.32e+01, train loss: 3.90194e-07, val loss: 1.43907e-06, min loss: 3.85452e-07\n",
      "Epoch: 2417000, elapsed: 1.32e+01, train loss: 4.90598e-07, val loss: 1.53657e-06, min loss: 3.85452e-07\n",
      "Epoch: 2417100, elapsed: 1.34e+01, train loss: 4.09885e-06, val loss: 4.06753e-06, min loss: 3.85452e-07\n",
      "Epoch: 2417200, elapsed: 1.27e+01, train loss: 3.85203e-07, val loss: 1.44093e-06, min loss: 3.85203e-07\n",
      "Epoch: 2417300, elapsed: 1.18e+01, train loss: 9.11884e-07, val loss: 1.66532e-06, min loss: 3.85203e-07\n",
      "Epoch: 2417400, elapsed: 1.20e+01, train loss: 3.85089e-07, val loss: 1.43847e-06, min loss: 3.85089e-07\n",
      "Epoch: 2417500, elapsed: 1.19e+01, train loss: 3.92254e-07, val loss: 1.42559e-06, min loss: 3.85089e-07\n",
      "Epoch: 2417600, elapsed: 1.15e+01, train loss: 4.29985e-07, val loss: 1.48933e-06, min loss: 3.85089e-07\n",
      "Epoch: 2417700, elapsed: 1.18e+01, train loss: 3.87595e-07, val loss: 1.44069e-06, min loss: 3.85089e-07\n",
      "Epoch: 2417800, elapsed: 1.16e+01, train loss: 4.08235e-07, val loss: 1.44817e-06, min loss: 3.85089e-07\n",
      "Epoch: 2417900, elapsed: 1.18e+01, train loss: 3.87498e-07, val loss: 1.42914e-06, min loss: 3.85089e-07\n",
      "Epoch: 2418000, elapsed: 1.18e+01, train loss: 3.91944e-07, val loss: 1.45463e-06, min loss: 3.85089e-07\n",
      "Epoch: 2418100, elapsed: 1.19e+01, train loss: 4.03555e-07, val loss: 1.44256e-06, min loss: 3.85089e-07\n",
      "Epoch: 2418200, elapsed: 1.19e+01, train loss: 4.03360e-07, val loss: 1.47176e-06, min loss: 3.85089e-07\n",
      "Epoch: 2418300, elapsed: 1.17e+01, train loss: 5.10745e-07, val loss: 1.56399e-06, min loss: 3.85089e-07\n",
      "Epoch: 2418400, elapsed: 1.16e+01, train loss: 9.98226e-07, val loss: 1.76793e-06, min loss: 3.85089e-07\n",
      "Epoch: 2418500, elapsed: 1.17e+01, train loss: 9.04480e-07, val loss: 2.18894e-06, min loss: 3.85089e-07\n",
      "Epoch: 2418600, elapsed: 1.19e+01, train loss: 1.66685e-06, val loss: 2.62103e-06, min loss: 3.85089e-07\n",
      "Epoch: 2418700, elapsed: 1.17e+01, train loss: 1.87995e-06, val loss: 2.86581e-06, min loss: 3.85089e-07\n",
      "Epoch: 2418800, elapsed: 1.17e+01, train loss: 3.96512e-07, val loss: 1.40112e-06, min loss: 3.85089e-07\n",
      "Epoch: 2418900, elapsed: 1.16e+01, train loss: 3.85382e-07, val loss: 1.42821e-06, min loss: 3.85089e-07\n",
      "Epoch: 2419000, elapsed: 1.15e+01, train loss: 6.05925e-07, val loss: 1.60713e-06, min loss: 3.85089e-07\n",
      "Epoch: 2419100, elapsed: 1.16e+01, train loss: 3.84757e-07, val loss: 1.43636e-06, min loss: 3.84757e-07\n",
      "Epoch: 2419200, elapsed: 1.15e+01, train loss: 2.32565e-06, val loss: 2.25218e-06, min loss: 3.84757e-07\n",
      "Epoch: 2419300, elapsed: 1.14e+01, train loss: 3.84768e-07, val loss: 1.43815e-06, min loss: 3.84757e-07\n",
      "Epoch: 2419400, elapsed: 1.14e+01, train loss: 6.59533e-07, val loss: 1.50632e-06, min loss: 3.84757e-07\n",
      "Epoch: 2419500, elapsed: 1.15e+01, train loss: 1.19597e-06, val loss: 2.21132e-06, min loss: 3.84757e-07\n",
      "Epoch: 2419600, elapsed: 1.83e+01, train loss: 4.49167e-07, val loss: 1.47139e-06, min loss: 3.84757e-07\n",
      "Epoch: 2419700, elapsed: 1.21e+01, train loss: 3.85039e-07, val loss: 1.43474e-06, min loss: 3.84757e-07\n",
      "Epoch: 2419800, elapsed: 1.18e+01, train loss: 3.88822e-07, val loss: 1.45946e-06, min loss: 3.84757e-07\n",
      "Epoch: 2419900, elapsed: 1.22e+01, train loss: 5.54481e-07, val loss: 1.86111e-06, min loss: 3.84757e-07\n",
      "Epoch: 2420000, elapsed: 1.23e+01, train loss: 3.84565e-07, val loss: 1.43504e-06, min loss: 3.84565e-07\n",
      "Epoch: 2420100, elapsed: 1.40e+01, train loss: 3.95337e-07, val loss: 1.48754e-06, min loss: 3.84565e-07\n",
      "Epoch: 2420200, elapsed: 1.20e+01, train loss: 3.84600e-07, val loss: 1.43564e-06, min loss: 3.84565e-07\n",
      "Epoch: 2420300, elapsed: 1.21e+01, train loss: 3.85721e-07, val loss: 1.43172e-06, min loss: 3.84565e-07\n",
      "Epoch: 2420400, elapsed: 1.21e+01, train loss: 3.08571e-06, val loss: 5.02847e-06, min loss: 3.84565e-07\n",
      "Epoch: 2420500, elapsed: 1.21e+01, train loss: 4.21645e-07, val loss: 1.49842e-06, min loss: 3.84565e-07\n",
      "Epoch: 2420600, elapsed: 1.17e+01, train loss: 1.32531e-06, val loss: 2.65140e-06, min loss: 3.84565e-07\n",
      "Epoch: 2420700, elapsed: 1.17e+01, train loss: 4.22072e-07, val loss: 1.48390e-06, min loss: 3.84565e-07\n",
      "Epoch: 2420800, elapsed: 1.20e+01, train loss: 4.25388e-07, val loss: 1.47895e-06, min loss: 3.84565e-07\n",
      "Epoch: 2420900, elapsed: 1.20e+01, train loss: 3.84396e-07, val loss: 1.43363e-06, min loss: 3.84396e-07\n",
      "Epoch: 2421000, elapsed: 1.19e+01, train loss: 3.90796e-07, val loss: 1.45426e-06, min loss: 3.84396e-07\n",
      "Epoch: 2421100, elapsed: 1.21e+01, train loss: 4.30281e-07, val loss: 1.41067e-06, min loss: 3.84396e-07\n",
      "Epoch: 2421200, elapsed: 1.21e+01, train loss: 3.86585e-07, val loss: 1.45928e-06, min loss: 3.84396e-07\n",
      "Epoch: 2421300, elapsed: 1.20e+01, train loss: 3.93402e-07, val loss: 1.41792e-06, min loss: 3.84396e-07\n",
      "Epoch: 2421400, elapsed: 1.27e+01, train loss: 3.87057e-07, val loss: 1.42747e-06, min loss: 3.84396e-07\n",
      "Epoch: 2421500, elapsed: 1.30e+01, train loss: 1.00532e-06, val loss: 1.40213e-06, min loss: 3.84396e-07\n",
      "Epoch: 2421600, elapsed: 1.39e+01, train loss: 3.84280e-07, val loss: 1.43203e-06, min loss: 3.84280e-07\n",
      "Epoch: 2421700, elapsed: 1.31e+01, train loss: 3.85084e-07, val loss: 1.43043e-06, min loss: 3.84280e-07\n",
      "Epoch: 2421800, elapsed: 1.24e+01, train loss: 4.18680e-07, val loss: 1.45582e-06, min loss: 3.84280e-07\n",
      "Epoch: 2421900, elapsed: 1.19e+01, train loss: 2.81809e-06, val loss: 4.03461e-06, min loss: 3.84280e-07\n",
      "Epoch: 2422000, elapsed: 1.17e+01, train loss: 3.84244e-07, val loss: 1.42964e-06, min loss: 3.84244e-07\n",
      "Epoch: 2422100, elapsed: 1.19e+01, train loss: 3.92439e-07, val loss: 1.45256e-06, min loss: 3.84244e-07\n",
      "Epoch: 2422200, elapsed: 1.32e+01, train loss: 5.13964e-07, val loss: 1.52909e-06, min loss: 3.84244e-07\n",
      "Epoch: 2422300, elapsed: 1.21e+01, train loss: 3.84948e-07, val loss: 1.43519e-06, min loss: 3.84244e-07\n",
      "Epoch: 2422400, elapsed: 1.18e+01, train loss: 6.64619e-07, val loss: 1.67240e-06, min loss: 3.84244e-07\n",
      "Epoch: 2422500, elapsed: 1.16e+01, train loss: 5.66540e-07, val loss: 1.80845e-06, min loss: 3.84244e-07\n",
      "Epoch: 2422600, elapsed: 1.19e+01, train loss: 3.86742e-07, val loss: 1.42779e-06, min loss: 3.84244e-07\n",
      "Epoch: 2422700, elapsed: 1.18e+01, train loss: 3.84181e-07, val loss: 1.43403e-06, min loss: 3.84181e-07\n",
      "Epoch: 2422800, elapsed: 1.17e+01, train loss: 3.98542e-07, val loss: 1.43985e-06, min loss: 3.84181e-07\n",
      "Epoch: 2422900, elapsed: 1.16e+01, train loss: 4.04785e-07, val loss: 1.42629e-06, min loss: 3.84181e-07\n",
      "Epoch: 2423000, elapsed: 1.17e+01, train loss: 4.50531e-07, val loss: 1.61385e-06, min loss: 3.84181e-07\n",
      "Epoch: 2423100, elapsed: 1.17e+01, train loss: 3.95250e-07, val loss: 1.46885e-06, min loss: 3.84181e-07\n",
      "Epoch: 2423200, elapsed: 1.18e+01, train loss: 3.86138e-07, val loss: 1.43083e-06, min loss: 3.84181e-07\n",
      "Epoch: 2423300, elapsed: 1.16e+01, train loss: 1.11806e-06, val loss: 1.54864e-06, min loss: 3.84181e-07\n",
      "Epoch: 2423400, elapsed: 1.16e+01, train loss: 4.13478e-07, val loss: 1.46723e-06, min loss: 3.84181e-07\n",
      "Epoch: 2423500, elapsed: 1.18e+01, train loss: 4.11419e-07, val loss: 1.43037e-06, min loss: 3.84181e-07\n",
      "Epoch: 2423600, elapsed: 1.15e+01, train loss: 3.85951e-07, val loss: 1.43830e-06, min loss: 3.84181e-07\n",
      "Epoch: 2423700, elapsed: 1.18e+01, train loss: 4.67417e-07, val loss: 1.46717e-06, min loss: 3.84181e-07\n",
      "Epoch: 2423800, elapsed: 1.82e+01, train loss: 8.37245e-07, val loss: 2.11771e-06, min loss: 3.84181e-07\n",
      "Epoch: 2423900, elapsed: 1.22e+01, train loss: 4.22281e-07, val loss: 1.50136e-06, min loss: 3.84181e-07\n",
      "Epoch: 2424000, elapsed: 1.19e+01, train loss: 4.36820e-06, val loss: 5.10905e-06, min loss: 3.84181e-07\n",
      "Epoch: 2424100, elapsed: 1.21e+01, train loss: 3.83782e-07, val loss: 1.43327e-06, min loss: 3.83782e-07\n",
      "Epoch: 2424200, elapsed: 1.20e+01, train loss: 3.31566e-06, val loss: 4.37413e-06, min loss: 3.83782e-07\n",
      "Epoch: 2424300, elapsed: 1.31e+01, train loss: 3.84104e-07, val loss: 1.43764e-06, min loss: 3.83782e-07\n",
      "Epoch: 2424400, elapsed: 1.35e+01, train loss: 3.85193e-07, val loss: 1.46385e-06, min loss: 3.83782e-07\n",
      "Epoch: 2424500, elapsed: 1.35e+01, train loss: 1.01035e-06, val loss: 1.97386e-06, min loss: 3.83782e-07\n",
      "Epoch: 2424600, elapsed: 1.32e+01, train loss: 3.89620e-07, val loss: 1.40432e-06, min loss: 3.83782e-07\n",
      "Epoch: 2424700, elapsed: 1.38e+01, train loss: 3.84686e-07, val loss: 1.43233e-06, min loss: 3.83782e-07\n",
      "Epoch: 2424800, elapsed: 1.27e+01, train loss: 4.91108e-07, val loss: 1.55516e-06, min loss: 3.83782e-07\n",
      "Epoch: 2424900, elapsed: 1.20e+01, train loss: 3.87002e-07, val loss: 1.43068e-06, min loss: 3.83782e-07\n",
      "Epoch: 2425000, elapsed: 1.21e+01, train loss: 3.84505e-07, val loss: 1.42288e-06, min loss: 3.83782e-07\n",
      "Epoch: 2425100, elapsed: 1.41e+01, train loss: 1.42378e-06, val loss: 3.25510e-06, min loss: 3.83782e-07\n",
      "Epoch: 2425200, elapsed: 1.18e+01, train loss: 4.11221e-07, val loss: 1.42488e-06, min loss: 3.83782e-07\n",
      "Epoch: 2425300, elapsed: 1.20e+01, train loss: 5.75231e-07, val loss: 1.39880e-06, min loss: 3.83782e-07\n",
      "Epoch: 2425400, elapsed: 1.20e+01, train loss: 5.28928e-07, val loss: 1.49646e-06, min loss: 3.83782e-07\n",
      "Epoch: 2425500, elapsed: 1.19e+01, train loss: 3.83540e-07, val loss: 1.43005e-06, min loss: 3.83540e-07\n",
      "Epoch: 2425600, elapsed: 1.18e+01, train loss: 3.92572e-07, val loss: 1.43270e-06, min loss: 3.83540e-07\n",
      "Epoch: 2425700, elapsed: 1.17e+01, train loss: 5.07625e-07, val loss: 1.47186e-06, min loss: 3.83540e-07\n",
      "Epoch: 2425800, elapsed: 1.17e+01, train loss: 9.21315e-07, val loss: 2.02128e-06, min loss: 3.83540e-07\n",
      "Epoch: 2425900, elapsed: 1.19e+01, train loss: 3.83424e-07, val loss: 1.42993e-06, min loss: 3.83424e-07\n",
      "Epoch: 2426000, elapsed: 1.17e+01, train loss: 5.07832e-07, val loss: 1.56079e-06, min loss: 3.83424e-07\n",
      "Epoch: 2426100, elapsed: 1.20e+01, train loss: 4.23716e-07, val loss: 1.57534e-06, min loss: 3.83424e-07\n",
      "Epoch: 2426200, elapsed: 1.18e+01, train loss: 3.85089e-07, val loss: 1.42411e-06, min loss: 3.83424e-07\n",
      "Epoch: 2426300, elapsed: 1.19e+01, train loss: 3.93540e-07, val loss: 1.43195e-06, min loss: 3.83424e-07\n",
      "Epoch: 2426400, elapsed: 1.15e+01, train loss: 5.13673e-07, val loss: 1.50095e-06, min loss: 3.83424e-07\n",
      "Epoch: 2426500, elapsed: 1.19e+01, train loss: 3.86493e-07, val loss: 1.42345e-06, min loss: 3.83424e-07\n",
      "Epoch: 2426600, elapsed: 1.17e+01, train loss: 3.83589e-07, val loss: 1.43172e-06, min loss: 3.83424e-07\n",
      "Epoch: 2426700, elapsed: 1.15e+01, train loss: 8.68248e-07, val loss: 2.31069e-06, min loss: 3.83424e-07\n",
      "Epoch: 2426800, elapsed: 1.17e+01, train loss: 3.83491e-07, val loss: 1.43249e-06, min loss: 3.83424e-07\n",
      "Epoch: 2426900, elapsed: 1.17e+01, train loss: 4.56181e-07, val loss: 1.54012e-06, min loss: 3.83424e-07\n",
      "Epoch: 2427000, elapsed: 1.17e+01, train loss: 6.95814e-07, val loss: 1.65816e-06, min loss: 3.83424e-07\n",
      "Epoch: 2427100, elapsed: 1.17e+01, train loss: 6.28712e-07, val loss: 1.66884e-06, min loss: 3.83424e-07\n",
      "Epoch: 2427200, elapsed: 1.18e+01, train loss: 5.01790e-07, val loss: 1.60977e-06, min loss: 3.83424e-07\n",
      "Epoch: 2427300, elapsed: 1.17e+01, train loss: 3.90082e-07, val loss: 1.41507e-06, min loss: 3.83424e-07\n",
      "Epoch: 2427400, elapsed: 1.17e+01, train loss: 1.24851e-06, val loss: 2.04599e-06, min loss: 3.83424e-07\n",
      "Epoch: 2427500, elapsed: 1.19e+01, train loss: 5.77162e-07, val loss: 1.62074e-06, min loss: 3.83424e-07\n",
      "Epoch: 2427600, elapsed: 1.20e+01, train loss: 2.39823e-06, val loss: 3.35872e-06, min loss: 3.83424e-07\n",
      "Epoch: 2427700, elapsed: 1.28e+01, train loss: 3.88336e-07, val loss: 1.40099e-06, min loss: 3.83424e-07\n",
      "Epoch: 2427800, elapsed: 1.24e+01, train loss: 5.66411e-07, val loss: 1.65199e-06, min loss: 3.83424e-07\n",
      "Epoch: 2427900, elapsed: 1.18e+01, train loss: 4.56391e-07, val loss: 1.40691e-06, min loss: 3.83424e-07\n",
      "Epoch: 2428000, elapsed: 1.28e+01, train loss: 3.83076e-07, val loss: 1.42561e-06, min loss: 3.83076e-07\n",
      "Epoch: 2428100, elapsed: 1.87e+01, train loss: 3.83745e-07, val loss: 1.43648e-06, min loss: 3.83076e-07\n",
      "Epoch: 2428200, elapsed: 1.20e+01, train loss: 1.30069e-06, val loss: 2.89806e-06, min loss: 3.83076e-07\n",
      "Epoch: 2428300, elapsed: 1.21e+01, train loss: 3.83120e-07, val loss: 1.43036e-06, min loss: 3.83076e-07\n",
      "Epoch: 2428400, elapsed: 1.22e+01, train loss: 4.44073e-07, val loss: 1.44758e-06, min loss: 3.83076e-07\n",
      "Epoch: 2428500, elapsed: 1.23e+01, train loss: 3.91637e-07, val loss: 1.46514e-06, min loss: 3.83076e-07\n",
      "Epoch: 2428600, elapsed: 1.19e+01, train loss: 3.83553e-07, val loss: 1.42911e-06, min loss: 3.83076e-07\n",
      "Epoch: 2428700, elapsed: 1.20e+01, train loss: 3.95286e-07, val loss: 1.45732e-06, min loss: 3.83076e-07\n",
      "Epoch: 2428800, elapsed: 1.19e+01, train loss: 1.32046e-06, val loss: 2.22738e-06, min loss: 3.83076e-07\n",
      "Epoch: 2428900, elapsed: 1.19e+01, train loss: 5.76742e-07, val loss: 1.39655e-06, min loss: 3.83076e-07\n",
      "Epoch: 2429000, elapsed: 1.19e+01, train loss: 1.61442e-06, val loss: 3.20512e-06, min loss: 3.83076e-07\n",
      "Epoch: 2429100, elapsed: 1.18e+01, train loss: 3.83196e-07, val loss: 1.42252e-06, min loss: 3.83076e-07\n",
      "Epoch: 2429200, elapsed: 1.20e+01, train loss: 3.83481e-07, val loss: 1.41784e-06, min loss: 3.83076e-07\n",
      "Epoch: 2429300, elapsed: 1.17e+01, train loss: 3.94477e-07, val loss: 1.41285e-06, min loss: 3.83076e-07\n",
      "Epoch: 2429400, elapsed: 1.19e+01, train loss: 4.02869e-07, val loss: 1.43190e-06, min loss: 3.83076e-07\n",
      "Epoch: 2429500, elapsed: 1.19e+01, train loss: 3.90113e-07, val loss: 1.40497e-06, min loss: 3.83076e-07\n",
      "Epoch: 2429600, elapsed: 1.19e+01, train loss: 3.83277e-07, val loss: 1.42839e-06, min loss: 3.83076e-07\n",
      "Epoch: 2429700, elapsed: 1.19e+01, train loss: 4.14246e-07, val loss: 1.38223e-06, min loss: 3.83076e-07\n",
      "Epoch: 2429800, elapsed: 1.20e+01, train loss: 5.60512e-07, val loss: 1.76776e-06, min loss: 3.83076e-07\n",
      "Epoch: 2429900, elapsed: 1.20e+01, train loss: 4.84035e-07, val loss: 1.34479e-06, min loss: 3.83076e-07\n",
      "Epoch: 2430000, elapsed: 1.19e+01, train loss: 3.83721e-07, val loss: 1.42756e-06, min loss: 3.83076e-07\n",
      "Epoch: 2430100, elapsed: 1.38e+01, train loss: 4.60522e-07, val loss: 1.49418e-06, min loss: 3.83076e-07\n",
      "Epoch: 2430200, elapsed: 1.20e+01, train loss: 3.82602e-07, val loss: 1.42634e-06, min loss: 3.82602e-07\n",
      "Epoch: 2430300, elapsed: 1.20e+01, train loss: 5.07018e-07, val loss: 1.40646e-06, min loss: 3.82602e-07\n",
      "Epoch: 2430400, elapsed: 1.18e+01, train loss: 3.82661e-07, val loss: 1.42530e-06, min loss: 3.82602e-07\n",
      "Epoch: 2430500, elapsed: 1.19e+01, train loss: 3.86606e-07, val loss: 1.43715e-06, min loss: 3.82602e-07\n",
      "Epoch: 2430600, elapsed: 1.20e+01, train loss: 1.80379e-06, val loss: 3.30782e-06, min loss: 3.82602e-07\n",
      "Epoch: 2430700, elapsed: 1.16e+01, train loss: 3.85777e-07, val loss: 1.41793e-06, min loss: 3.82602e-07\n",
      "Epoch: 2430800, elapsed: 1.18e+01, train loss: 5.55109e-07, val loss: 1.54735e-06, min loss: 3.82602e-07\n",
      "Epoch: 2430900, elapsed: 1.19e+01, train loss: 3.85067e-07, val loss: 1.43465e-06, min loss: 3.82602e-07\n",
      "Epoch: 2431000, elapsed: 1.20e+01, train loss: 1.41116e-06, val loss: 1.79430e-06, min loss: 3.82602e-07\n",
      "Epoch: 2431100, elapsed: 1.19e+01, train loss: 3.85787e-07, val loss: 1.41893e-06, min loss: 3.82602e-07\n",
      "Epoch: 2431200, elapsed: 1.19e+01, train loss: 4.21288e-07, val loss: 1.49415e-06, min loss: 3.82602e-07\n",
      "Epoch: 2431300, elapsed: 1.21e+01, train loss: 4.99671e-07, val loss: 1.52847e-06, min loss: 3.82602e-07\n",
      "Epoch: 2431400, elapsed: 1.17e+01, train loss: 3.84907e-07, val loss: 1.43704e-06, min loss: 3.82602e-07\n",
      "Epoch: 2431500, elapsed: 1.18e+01, train loss: 3.90326e-07, val loss: 1.40067e-06, min loss: 3.82602e-07\n",
      "Epoch: 2431600, elapsed: 1.20e+01, train loss: 3.92675e-07, val loss: 1.46263e-06, min loss: 3.82602e-07\n",
      "Epoch: 2431700, elapsed: 1.16e+01, train loss: 3.91189e-07, val loss: 1.42562e-06, min loss: 3.82602e-07\n",
      "Epoch: 2431800, elapsed: 1.16e+01, train loss: 4.10298e-07, val loss: 1.40128e-06, min loss: 3.82602e-07\n",
      "Epoch: 2431900, elapsed: 1.18e+01, train loss: 6.66736e-07, val loss: 1.66933e-06, min loss: 3.82602e-07\n",
      "Epoch: 2432000, elapsed: 1.16e+01, train loss: 3.85079e-07, val loss: 1.40429e-06, min loss: 3.82602e-07\n",
      "Epoch: 2432100, elapsed: 1.16e+01, train loss: 3.83114e-07, val loss: 1.42836e-06, min loss: 3.82602e-07\n",
      "Epoch: 2432200, elapsed: 1.17e+01, train loss: 7.51493e-07, val loss: 1.45636e-06, min loss: 3.82602e-07\n",
      "Epoch: 2432300, elapsed: 1.18e+01, train loss: 3.82440e-07, val loss: 1.41892e-06, min loss: 3.82440e-07\n",
      "Epoch: 2432400, elapsed: 1.85e+01, train loss: 3.86151e-07, val loss: 1.43635e-06, min loss: 3.82440e-07\n",
      "Epoch: 2432500, elapsed: 1.21e+01, train loss: 1.47187e-06, val loss: 2.44238e-06, min loss: 3.82440e-07\n",
      "Epoch: 2432600, elapsed: 1.20e+01, train loss: 3.96641e-07, val loss: 1.40285e-06, min loss: 3.82440e-07\n",
      "Epoch: 2432700, elapsed: 1.20e+01, train loss: 3.82264e-07, val loss: 1.42435e-06, min loss: 3.82264e-07\n",
      "Epoch: 2432800, elapsed: 1.22e+01, train loss: 1.12384e-06, val loss: 1.73318e-06, min loss: 3.82264e-07\n",
      "Epoch: 2432900, elapsed: 1.21e+01, train loss: 7.43426e-07, val loss: 1.51171e-06, min loss: 3.82264e-07\n",
      "Epoch: 2433000, elapsed: 1.20e+01, train loss: 3.82208e-07, val loss: 1.42296e-06, min loss: 3.82208e-07\n",
      "Epoch: 2433100, elapsed: 1.21e+01, train loss: 3.90016e-07, val loss: 1.42775e-06, min loss: 3.82208e-07\n",
      "Epoch: 2433200, elapsed: 1.20e+01, train loss: 4.26081e-07, val loss: 1.44118e-06, min loss: 3.82208e-07\n",
      "Epoch: 2433300, elapsed: 1.20e+01, train loss: 5.00892e-07, val loss: 1.50967e-06, min loss: 3.82208e-07\n",
      "Epoch: 2433400, elapsed: 1.21e+01, train loss: 3.82249e-07, val loss: 1.42286e-06, min loss: 3.82208e-07\n",
      "Epoch: 2433500, elapsed: 1.19e+01, train loss: 3.83582e-07, val loss: 1.42446e-06, min loss: 3.82208e-07\n",
      "Epoch: 2433600, elapsed: 1.20e+01, train loss: 4.10642e-07, val loss: 1.42388e-06, min loss: 3.82208e-07\n",
      "Epoch: 2433700, elapsed: 1.19e+01, train loss: 3.86539e-07, val loss: 1.40835e-06, min loss: 3.82208e-07\n",
      "Epoch: 2433800, elapsed: 1.22e+01, train loss: 3.99269e-07, val loss: 1.44832e-06, min loss: 3.82208e-07\n",
      "Epoch: 2433900, elapsed: 1.30e+01, train loss: 4.64779e-07, val loss: 1.72841e-06, min loss: 3.82208e-07\n",
      "Epoch: 2434000, elapsed: 1.26e+01, train loss: 3.94987e-07, val loss: 1.43991e-06, min loss: 3.82208e-07\n",
      "Epoch: 2434100, elapsed: 1.26e+01, train loss: 3.88978e-07, val loss: 1.43887e-06, min loss: 3.82208e-07\n",
      "Epoch: 2434200, elapsed: 1.28e+01, train loss: 4.44950e-07, val loss: 1.51076e-06, min loss: 3.82208e-07\n",
      "Epoch: 2434300, elapsed: 1.34e+01, train loss: 4.96025e-07, val loss: 1.67983e-06, min loss: 3.82208e-07\n",
      "Epoch: 2434400, elapsed: 1.29e+01, train loss: 4.01125e-07, val loss: 1.53926e-06, min loss: 3.82208e-07\n",
      "Epoch: 2434500, elapsed: 1.21e+01, train loss: 4.05964e-07, val loss: 1.41789e-06, min loss: 3.82208e-07\n",
      "Epoch: 2434600, elapsed: 1.20e+01, train loss: 2.18115e-06, val loss: 2.53079e-06, min loss: 3.82208e-07\n",
      "Epoch: 2434700, elapsed: 1.20e+01, train loss: 3.81857e-07, val loss: 1.42199e-06, min loss: 3.81857e-07\n",
      "Epoch: 2434800, elapsed: 1.18e+01, train loss: 4.11691e-07, val loss: 1.39505e-06, min loss: 3.81857e-07\n",
      "Epoch: 2434900, elapsed: 1.20e+01, train loss: 4.88221e-07, val loss: 1.51525e-06, min loss: 3.81857e-07\n",
      "Epoch: 2435000, elapsed: 1.20e+01, train loss: 3.81999e-07, val loss: 1.42390e-06, min loss: 3.81857e-07\n",
      "Epoch: 2435100, elapsed: 1.41e+01, train loss: 3.84334e-07, val loss: 1.43222e-06, min loss: 3.81857e-07\n",
      "Epoch: 2435200, elapsed: 1.19e+01, train loss: 5.33766e-07, val loss: 1.38136e-06, min loss: 3.81857e-07\n",
      "Epoch: 2435300, elapsed: 1.24e+01, train loss: 2.85439e-06, val loss: 3.84594e-06, min loss: 3.81857e-07\n",
      "Epoch: 2435400, elapsed: 1.34e+01, train loss: 3.93280e-07, val loss: 1.39468e-06, min loss: 3.81857e-07\n",
      "Epoch: 2435500, elapsed: 1.33e+01, train loss: 3.91306e-07, val loss: 1.44192e-06, min loss: 3.81857e-07\n",
      "Epoch: 2435600, elapsed: 1.30e+01, train loss: 9.68268e-07, val loss: 1.82755e-06, min loss: 3.81857e-07\n",
      "Epoch: 2435700, elapsed: 1.28e+01, train loss: 4.74162e-07, val loss: 1.63695e-06, min loss: 3.81857e-07\n",
      "Epoch: 2435800, elapsed: 1.29e+01, train loss: 8.38130e-07, val loss: 1.66162e-06, min loss: 3.81857e-07\n",
      "Epoch: 2435900, elapsed: 1.28e+01, train loss: 7.46299e-07, val loss: 1.99588e-06, min loss: 3.81857e-07\n",
      "Epoch: 2436000, elapsed: 1.33e+01, train loss: 7.17309e-07, val loss: 1.78241e-06, min loss: 3.81857e-07\n",
      "Epoch: 2436100, elapsed: 1.30e+01, train loss: 3.83603e-07, val loss: 1.41145e-06, min loss: 3.81857e-07\n",
      "Epoch: 2436200, elapsed: 1.33e+01, train loss: 3.83101e-07, val loss: 1.42427e-06, min loss: 3.81857e-07\n",
      "Epoch: 2436300, elapsed: 1.34e+01, train loss: 4.11650e-07, val loss: 1.41579e-06, min loss: 3.81857e-07\n",
      "Epoch: 2436400, elapsed: 1.32e+01, train loss: 7.91705e-07, val loss: 2.03335e-06, min loss: 3.81857e-07\n",
      "Epoch: 2436500, elapsed: 1.31e+01, train loss: 3.81502e-07, val loss: 1.41540e-06, min loss: 3.81502e-07\n",
      "Epoch: 2436600, elapsed: 1.86e+01, train loss: 3.98373e-07, val loss: 1.41113e-06, min loss: 3.81502e-07\n",
      "Epoch: 2436700, elapsed: 1.22e+01, train loss: 4.72205e-07, val loss: 1.44680e-06, min loss: 3.81502e-07\n",
      "Epoch: 2436800, elapsed: 1.20e+01, train loss: 4.29590e-07, val loss: 1.52038e-06, min loss: 3.81502e-07\n",
      "Epoch: 2436900, elapsed: 1.20e+01, train loss: 3.82431e-07, val loss: 1.39604e-06, min loss: 3.81502e-07\n",
      "Epoch: 2437000, elapsed: 1.19e+01, train loss: 3.90985e-07, val loss: 1.40406e-06, min loss: 3.81502e-07\n",
      "Epoch: 2437100, elapsed: 1.19e+01, train loss: 7.40487e-07, val loss: 1.83300e-06, min loss: 3.81502e-07\n",
      "Epoch: 2437200, elapsed: 1.17e+01, train loss: 5.03484e-07, val loss: 1.69973e-06, min loss: 3.81502e-07\n",
      "Epoch: 2437300, elapsed: 1.19e+01, train loss: 8.78026e-07, val loss: 1.92500e-06, min loss: 3.81502e-07\n",
      "Epoch: 2437400, elapsed: 1.21e+01, train loss: 4.27773e-07, val loss: 1.42226e-06, min loss: 3.81502e-07\n",
      "Epoch: 2437500, elapsed: 1.20e+01, train loss: 2.28385e-06, val loss: 2.81538e-06, min loss: 3.81502e-07\n",
      "Epoch: 2437600, elapsed: 1.20e+01, train loss: 4.03410e-07, val loss: 1.44148e-06, min loss: 3.81502e-07\n",
      "Epoch: 2437700, elapsed: 1.19e+01, train loss: 3.82186e-07, val loss: 1.40926e-06, min loss: 3.81502e-07\n",
      "Epoch: 2437800, elapsed: 1.18e+01, train loss: 4.20922e-07, val loss: 1.38229e-06, min loss: 3.81502e-07\n",
      "Epoch: 2437900, elapsed: 1.18e+01, train loss: 4.29267e-07, val loss: 1.48760e-06, min loss: 3.81502e-07\n",
      "Epoch: 2438000, elapsed: 1.18e+01, train loss: 6.18081e-07, val loss: 1.80049e-06, min loss: 3.81502e-07\n",
      "Epoch: 2438100, elapsed: 1.20e+01, train loss: 1.43083e-06, val loss: 1.62321e-06, min loss: 3.81502e-07\n",
      "Epoch: 2438200, elapsed: 1.19e+01, train loss: 4.10049e-07, val loss: 1.40469e-06, min loss: 3.81502e-07\n",
      "Epoch: 2438300, elapsed: 1.18e+01, train loss: 3.85538e-07, val loss: 1.41524e-06, min loss: 3.81502e-07\n",
      "Epoch: 2438400, elapsed: 1.22e+01, train loss: 3.94214e-07, val loss: 1.44455e-06, min loss: 3.81502e-07\n",
      "Epoch: 2438500, elapsed: 1.30e+01, train loss: 4.07758e-07, val loss: 1.48266e-06, min loss: 3.81502e-07\n",
      "Epoch: 2438600, elapsed: 1.28e+01, train loss: 4.01038e-07, val loss: 1.39120e-06, min loss: 3.81502e-07\n",
      "Epoch: 2438700, elapsed: 1.31e+01, train loss: 3.86165e-07, val loss: 1.40993e-06, min loss: 3.81502e-07\n",
      "Epoch: 2438800, elapsed: 1.35e+01, train loss: 3.83078e-07, val loss: 1.42133e-06, min loss: 3.81502e-07\n",
      "Epoch: 2438900, elapsed: 1.35e+01, train loss: 5.56582e-07, val loss: 1.51633e-06, min loss: 3.81502e-07\n",
      "Epoch: 2439000, elapsed: 1.29e+01, train loss: 4.00218e-07, val loss: 1.50038e-06, min loss: 3.81502e-07\n",
      "Epoch: 2439100, elapsed: 1.32e+01, train loss: 1.70633e-06, val loss: 2.24445e-06, min loss: 3.81502e-07\n",
      "Epoch: 2439200, elapsed: 1.26e+01, train loss: 3.90656e-07, val loss: 1.43898e-06, min loss: 3.81502e-07\n",
      "Epoch: 2439300, elapsed: 1.16e+01, train loss: 4.04549e-07, val loss: 1.44732e-06, min loss: 3.81502e-07\n",
      "Epoch: 2439400, elapsed: 1.18e+01, train loss: 5.63036e-07, val loss: 1.51101e-06, min loss: 3.81502e-07\n",
      "Epoch: 2439500, elapsed: 1.17e+01, train loss: 3.81197e-07, val loss: 1.41716e-06, min loss: 3.81197e-07\n",
      "Epoch: 2439600, elapsed: 1.19e+01, train loss: 1.00093e-06, val loss: 1.78493e-06, min loss: 3.81197e-07\n",
      "Epoch: 2439700, elapsed: 1.20e+01, train loss: 3.82209e-07, val loss: 1.42503e-06, min loss: 3.81197e-07\n",
      "Epoch: 2439800, elapsed: 1.17e+01, train loss: 3.82509e-07, val loss: 1.41700e-06, min loss: 3.81197e-07\n",
      "Epoch: 2439900, elapsed: 1.17e+01, train loss: 5.66152e-07, val loss: 1.35735e-06, min loss: 3.81197e-07\n",
      "Epoch: 2440000, elapsed: 1.17e+01, train loss: 3.81446e-07, val loss: 1.41483e-06, min loss: 3.81197e-07\n",
      "Epoch: 2440100, elapsed: 1.37e+01, train loss: 3.95773e-07, val loss: 1.40519e-06, min loss: 3.81197e-07\n",
      "Epoch: 2440200, elapsed: 1.17e+01, train loss: 3.82815e-07, val loss: 1.42109e-06, min loss: 3.81197e-07\n",
      "Epoch: 2440300, elapsed: 1.18e+01, train loss: 3.91918e-07, val loss: 1.37354e-06, min loss: 3.81197e-07\n",
      "Epoch: 2440400, elapsed: 1.29e+01, train loss: 3.90453e-07, val loss: 1.43234e-06, min loss: 3.81197e-07\n",
      "Epoch: 2440500, elapsed: 1.27e+01, train loss: 1.22823e-06, val loss: 2.27756e-06, min loss: 3.81197e-07\n",
      "Epoch: 2440600, elapsed: 1.21e+01, train loss: 5.48810e-07, val loss: 1.69601e-06, min loss: 3.81197e-07\n",
      "Epoch: 2440700, elapsed: 1.19e+01, train loss: 3.84890e-07, val loss: 1.44324e-06, min loss: 3.81197e-07\n",
      "Epoch: 2440800, elapsed: 1.16e+01, train loss: 3.85598e-07, val loss: 1.44647e-06, min loss: 3.81197e-07\n",
      "Epoch: 2440900, elapsed: 1.79e+01, train loss: 2.20991e-06, val loss: 2.84638e-06, min loss: 3.81197e-07\n",
      "Epoch: 2441000, elapsed: 1.23e+01, train loss: 4.05164e-07, val loss: 1.41277e-06, min loss: 3.81197e-07\n",
      "Epoch: 2441100, elapsed: 1.19e+01, train loss: 3.81384e-07, val loss: 1.40542e-06, min loss: 3.81197e-07\n",
      "Epoch: 2441200, elapsed: 1.19e+01, train loss: 3.94617e-07, val loss: 1.44442e-06, min loss: 3.81197e-07\n",
      "Epoch: 2441300, elapsed: 1.21e+01, train loss: 4.40998e-07, val loss: 1.52115e-06, min loss: 3.81197e-07\n",
      "Epoch: 2441400, elapsed: 1.20e+01, train loss: 3.85738e-07, val loss: 1.41434e-06, min loss: 3.81197e-07\n",
      "Epoch: 2441500, elapsed: 1.18e+01, train loss: 3.81991e-07, val loss: 1.40784e-06, min loss: 3.81197e-07\n",
      "Epoch: 2441600, elapsed: 1.19e+01, train loss: 3.88523e-07, val loss: 1.40260e-06, min loss: 3.81197e-07\n",
      "Epoch: 2441700, elapsed: 1.17e+01, train loss: 9.80807e-07, val loss: 1.46128e-06, min loss: 3.81197e-07\n",
      "Epoch: 2441800, elapsed: 1.19e+01, train loss: 3.81319e-07, val loss: 1.40763e-06, min loss: 3.81197e-07\n",
      "Epoch: 2441900, elapsed: 1.19e+01, train loss: 3.80936e-07, val loss: 1.40803e-06, min loss: 3.80936e-07\n",
      "Epoch: 2442000, elapsed: 1.19e+01, train loss: 3.99522e-07, val loss: 1.47573e-06, min loss: 3.80936e-07\n",
      "Epoch: 2442100, elapsed: 1.20e+01, train loss: 5.30851e-07, val loss: 1.64078e-06, min loss: 3.80936e-07\n",
      "Epoch: 2442200, elapsed: 1.19e+01, train loss: 4.31338e-07, val loss: 1.42690e-06, min loss: 3.80936e-07\n",
      "Epoch: 2442300, elapsed: 1.18e+01, train loss: 5.98971e-07, val loss: 1.60012e-06, min loss: 3.80936e-07\n",
      "Epoch: 2442400, elapsed: 1.17e+01, train loss: 3.94928e-07, val loss: 1.35405e-06, min loss: 3.80936e-07\n",
      "Epoch: 2442500, elapsed: 1.20e+01, train loss: 3.80505e-07, val loss: 1.40882e-06, min loss: 3.80505e-07\n",
      "Epoch: 2442600, elapsed: 1.18e+01, train loss: 3.87652e-07, val loss: 1.38908e-06, min loss: 3.80505e-07\n",
      "Epoch: 2442700, elapsed: 1.19e+01, train loss: 3.80263e-07, val loss: 1.41403e-06, min loss: 3.80263e-07\n",
      "Epoch: 2442800, elapsed: 1.16e+01, train loss: 3.83906e-07, val loss: 1.41343e-06, min loss: 3.80263e-07\n",
      "Epoch: 2442900, elapsed: 1.16e+01, train loss: 3.82588e-07, val loss: 1.41200e-06, min loss: 3.80263e-07\n",
      "Epoch: 2443000, elapsed: 1.18e+01, train loss: 3.82043e-07, val loss: 1.41069e-06, min loss: 3.80263e-07\n",
      "Epoch: 2443100, elapsed: 1.20e+01, train loss: 9.87843e-07, val loss: 1.72041e-06, min loss: 3.80263e-07\n",
      "Epoch: 2443200, elapsed: 1.17e+01, train loss: 4.10375e-07, val loss: 1.48079e-06, min loss: 3.80263e-07\n",
      "Epoch: 2443300, elapsed: 1.18e+01, train loss: 3.83358e-07, val loss: 1.42793e-06, min loss: 3.80263e-07\n",
      "Epoch: 2443400, elapsed: 1.22e+01, train loss: 3.81161e-07, val loss: 1.42254e-06, min loss: 3.80263e-07\n",
      "Epoch: 2443500, elapsed: 1.20e+01, train loss: 7.57469e-07, val loss: 1.72598e-06, min loss: 3.80263e-07\n",
      "Epoch: 2443600, elapsed: 1.18e+01, train loss: 5.65417e-07, val loss: 1.50800e-06, min loss: 3.80263e-07\n",
      "Epoch: 2443700, elapsed: 1.18e+01, train loss: 4.12467e-07, val loss: 1.44139e-06, min loss: 3.80263e-07\n",
      "Epoch: 2443800, elapsed: 1.20e+01, train loss: 2.51336e-06, val loss: 4.01885e-06, min loss: 3.80263e-07\n",
      "Epoch: 2443900, elapsed: 1.17e+01, train loss: 3.80050e-07, val loss: 1.41029e-06, min loss: 3.80050e-07\n",
      "Epoch: 2444000, elapsed: 1.16e+01, train loss: 3.82998e-07, val loss: 1.42969e-06, min loss: 3.80050e-07\n",
      "Epoch: 2444100, elapsed: 1.17e+01, train loss: 1.23074e-06, val loss: 2.46396e-06, min loss: 3.80050e-07\n",
      "Epoch: 2444200, elapsed: 1.16e+01, train loss: 3.80192e-07, val loss: 1.40931e-06, min loss: 3.80050e-07\n",
      "Epoch: 2444300, elapsed: 1.16e+01, train loss: 4.26079e-07, val loss: 1.45130e-06, min loss: 3.80050e-07\n",
      "Epoch: 2444400, elapsed: 1.18e+01, train loss: 1.60462e-06, val loss: 2.86058e-06, min loss: 3.80050e-07\n",
      "Epoch: 2444500, elapsed: 1.17e+01, train loss: 3.98593e-07, val loss: 1.40696e-06, min loss: 3.80050e-07\n",
      "Epoch: 2444600, elapsed: 1.16e+01, train loss: 3.80978e-07, val loss: 1.41628e-06, min loss: 3.80050e-07\n",
      "Epoch: 2444700, elapsed: 1.16e+01, train loss: 3.86596e-07, val loss: 1.43259e-06, min loss: 3.80050e-07\n",
      "Epoch: 2444800, elapsed: 1.17e+01, train loss: 4.63987e-07, val loss: 1.66943e-06, min loss: 3.80050e-07\n",
      "Epoch: 2444900, elapsed: 1.17e+01, train loss: 3.80015e-07, val loss: 1.40743e-06, min loss: 3.80015e-07\n",
      "Epoch: 2445000, elapsed: 1.18e+01, train loss: 3.87510e-07, val loss: 1.41270e-06, min loss: 3.80015e-07\n",
      "Epoch: 2445100, elapsed: 1.39e+01, train loss: 2.53134e-06, val loss: 2.67120e-06, min loss: 3.80015e-07\n",
      "Epoch: 2445200, elapsed: 1.80e+01, train loss: 4.39070e-07, val loss: 1.59719e-06, min loss: 3.80015e-07\n",
      "Epoch: 2445300, elapsed: 1.21e+01, train loss: 6.17757e-07, val loss: 1.53616e-06, min loss: 3.80015e-07\n",
      "Epoch: 2445400, elapsed: 1.21e+01, train loss: 4.65901e-07, val loss: 1.43208e-06, min loss: 3.80015e-07\n",
      "Epoch: 2445500, elapsed: 1.21e+01, train loss: 5.29753e-07, val loss: 1.57659e-06, min loss: 3.80015e-07\n",
      "Epoch: 2445600, elapsed: 1.19e+01, train loss: 3.92788e-07, val loss: 1.40399e-06, min loss: 3.80015e-07\n",
      "Epoch: 2445700, elapsed: 1.19e+01, train loss: 3.81003e-07, val loss: 1.41166e-06, min loss: 3.80015e-07\n",
      "Epoch: 2445800, elapsed: 1.19e+01, train loss: 3.92702e-07, val loss: 1.47530e-06, min loss: 3.80015e-07\n",
      "Epoch: 2445900, elapsed: 1.21e+01, train loss: 3.89601e-07, val loss: 1.46381e-06, min loss: 3.80015e-07\n",
      "Epoch: 2446000, elapsed: 1.20e+01, train loss: 4.34229e-07, val loss: 1.40391e-06, min loss: 3.80015e-07\n",
      "Epoch: 2446100, elapsed: 1.20e+01, train loss: 9.26335e-07, val loss: 1.83704e-06, min loss: 3.80015e-07\n",
      "Epoch: 2446200, elapsed: 1.19e+01, train loss: 5.35330e-07, val loss: 1.36834e-06, min loss: 3.80015e-07\n",
      "Epoch: 2446300, elapsed: 1.21e+01, train loss: 1.58624e-06, val loss: 2.29499e-06, min loss: 3.80015e-07\n",
      "Epoch: 2446400, elapsed: 1.24e+01, train loss: 7.87096e-07, val loss: 2.07158e-06, min loss: 3.80015e-07\n",
      "Epoch: 2446500, elapsed: 1.21e+01, train loss: 8.79461e-07, val loss: 1.89210e-06, min loss: 3.80015e-07\n",
      "Epoch: 2446600, elapsed: 1.21e+01, train loss: 3.79786e-07, val loss: 1.40346e-06, min loss: 3.79786e-07\n",
      "Epoch: 2446700, elapsed: 1.20e+01, train loss: 4.29486e-07, val loss: 1.42550e-06, min loss: 3.79786e-07\n",
      "Epoch: 2446800, elapsed: 1.23e+01, train loss: 5.21484e-07, val loss: 1.47514e-06, min loss: 3.79786e-07\n",
      "Epoch: 2446900, elapsed: 1.20e+01, train loss: 5.94289e-07, val loss: 1.37708e-06, min loss: 3.79786e-07\n",
      "Epoch: 2447000, elapsed: 1.20e+01, train loss: 7.57226e-07, val loss: 1.79535e-06, min loss: 3.79786e-07\n",
      "Epoch: 2447100, elapsed: 1.20e+01, train loss: 6.85738e-07, val loss: 1.90504e-06, min loss: 3.79786e-07\n",
      "Epoch: 2447200, elapsed: 1.19e+01, train loss: 5.35808e-07, val loss: 1.75155e-06, min loss: 3.79786e-07\n",
      "Epoch: 2447300, elapsed: 1.19e+01, train loss: 3.79473e-07, val loss: 1.40727e-06, min loss: 3.79473e-07\n",
      "Epoch: 2447400, elapsed: 1.19e+01, train loss: 4.68561e-07, val loss: 1.44548e-06, min loss: 3.79473e-07\n",
      "Epoch: 2447500, elapsed: 1.19e+01, train loss: 6.93391e-07, val loss: 1.61849e-06, min loss: 3.79473e-07\n",
      "Epoch: 2447600, elapsed: 1.18e+01, train loss: 1.12354e-06, val loss: 2.38743e-06, min loss: 3.79473e-07\n",
      "Epoch: 2447700, elapsed: 1.18e+01, train loss: 4.88964e-07, val loss: 1.35896e-06, min loss: 3.79473e-07\n",
      "Epoch: 2447800, elapsed: 1.18e+01, train loss: 3.85130e-07, val loss: 1.43073e-06, min loss: 3.79473e-07\n",
      "Epoch: 2447900, elapsed: 1.18e+01, train loss: 3.82741e-07, val loss: 1.41584e-06, min loss: 3.79473e-07\n",
      "Epoch: 2448000, elapsed: 1.16e+01, train loss: 3.83215e-07, val loss: 1.41718e-06, min loss: 3.79473e-07\n",
      "Epoch: 2448100, elapsed: 1.16e+01, train loss: 3.97831e-07, val loss: 1.44165e-06, min loss: 3.79473e-07\n",
      "Epoch: 2448200, elapsed: 1.18e+01, train loss: 5.31350e-07, val loss: 1.45543e-06, min loss: 3.79473e-07\n",
      "Epoch: 2448300, elapsed: 1.19e+01, train loss: 4.06166e-07, val loss: 1.46083e-06, min loss: 3.79473e-07\n",
      "Epoch: 2448400, elapsed: 1.18e+01, train loss: 3.79511e-07, val loss: 1.41241e-06, min loss: 3.79473e-07\n",
      "Epoch: 2448500, elapsed: 1.16e+01, train loss: 3.80121e-07, val loss: 1.39785e-06, min loss: 3.79473e-07\n",
      "Epoch: 2448600, elapsed: 1.17e+01, train loss: 5.29464e-07, val loss: 1.42052e-06, min loss: 3.79473e-07\n",
      "Epoch: 2448700, elapsed: 1.15e+01, train loss: 3.79475e-07, val loss: 1.40883e-06, min loss: 3.79473e-07\n",
      "Epoch: 2448800, elapsed: 1.18e+01, train loss: 3.81242e-07, val loss: 1.42732e-06, min loss: 3.79473e-07\n",
      "Epoch: 2448900, elapsed: 1.17e+01, train loss: 4.04937e-07, val loss: 1.37091e-06, min loss: 3.79473e-07\n",
      "Epoch: 2449000, elapsed: 1.19e+01, train loss: 3.80867e-07, val loss: 1.40666e-06, min loss: 3.79473e-07\n",
      "Epoch: 2449100, elapsed: 1.18e+01, train loss: 3.80025e-07, val loss: 1.40363e-06, min loss: 3.79473e-07\n",
      "Epoch: 2449200, elapsed: 1.18e+01, train loss: 3.86189e-07, val loss: 1.39499e-06, min loss: 3.79473e-07\n",
      "Epoch: 2449300, elapsed: 1.17e+01, train loss: 7.96372e-07, val loss: 2.02890e-06, min loss: 3.79473e-07\n",
      "Epoch: 2449400, elapsed: 1.19e+01, train loss: 4.19497e-07, val loss: 1.47482e-06, min loss: 3.79473e-07\n",
      "Epoch: 2449500, elapsed: 1.19e+01, train loss: 3.79232e-07, val loss: 1.40662e-06, min loss: 3.79232e-07\n",
      "Epoch: 2449600, elapsed: 1.84e+01, train loss: 4.03060e-07, val loss: 1.42987e-06, min loss: 3.79232e-07\n",
      "Epoch: 2449700, elapsed: 1.21e+01, train loss: 6.89403e-07, val loss: 1.83648e-06, min loss: 3.79232e-07\n",
      "Epoch: 2449800, elapsed: 1.20e+01, train loss: 4.24164e-07, val loss: 1.42209e-06, min loss: 3.79232e-07\n",
      "Epoch: 2449900, elapsed: 1.18e+01, train loss: 5.08479e-07, val loss: 1.61528e-06, min loss: 3.79232e-07\n",
      "Epoch: 2450000, elapsed: 1.18e+01, train loss: 3.82941e-07, val loss: 1.39720e-06, min loss: 3.79232e-07\n",
      "Epoch: 2450100, elapsed: 1.42e+01, train loss: 3.86874e-07, val loss: 1.44703e-06, min loss: 3.79232e-07\n",
      "Epoch: 2450200, elapsed: 1.19e+01, train loss: 4.24191e-07, val loss: 1.50608e-06, min loss: 3.79232e-07\n",
      "Epoch: 2450300, elapsed: 1.20e+01, train loss: 2.71606e-06, val loss: 3.52916e-06, min loss: 3.79232e-07\n",
      "Epoch: 2450400, elapsed: 1.19e+01, train loss: 3.79484e-07, val loss: 1.41175e-06, min loss: 3.79232e-07\n",
      "Epoch: 2450500, elapsed: 1.17e+01, train loss: 3.79389e-07, val loss: 1.39723e-06, min loss: 3.79232e-07\n",
      "Epoch: 2450600, elapsed: 1.18e+01, train loss: 5.23935e-07, val loss: 1.60522e-06, min loss: 3.79232e-07\n",
      "Epoch: 2450700, elapsed: 1.17e+01, train loss: 4.81731e-07, val loss: 1.56960e-06, min loss: 3.79232e-07\n",
      "Epoch: 2450800, elapsed: 1.17e+01, train loss: 5.75589e-07, val loss: 1.76945e-06, min loss: 3.79232e-07\n",
      "Epoch: 2450900, elapsed: 1.23e+01, train loss: 2.05750e-06, val loss: 2.42277e-06, min loss: 3.79232e-07\n",
      "Epoch: 2451000, elapsed: 1.31e+01, train loss: 4.07357e-07, val loss: 1.45298e-06, min loss: 3.79232e-07\n",
      "Epoch: 2451100, elapsed: 1.36e+01, train loss: 3.97387e-07, val loss: 1.33854e-06, min loss: 3.79232e-07\n",
      "Epoch: 2451200, elapsed: 1.28e+01, train loss: 5.59641e-07, val loss: 1.68512e-06, min loss: 3.79232e-07\n",
      "Epoch: 2451300, elapsed: 1.29e+01, train loss: 4.06106e-07, val loss: 1.43472e-06, min loss: 3.79232e-07\n",
      "Epoch: 2451400, elapsed: 1.31e+01, train loss: 5.33124e-07, val loss: 1.40498e-06, min loss: 3.79232e-07\n",
      "Epoch: 2451500, elapsed: 1.35e+01, train loss: 8.26155e-07, val loss: 1.78642e-06, min loss: 3.79232e-07\n",
      "Epoch: 2451600, elapsed: 1.30e+01, train loss: 5.87854e-07, val loss: 1.41058e-06, min loss: 3.79232e-07\n",
      "Epoch: 2451700, elapsed: 1.30e+01, train loss: 3.98221e-07, val loss: 1.48727e-06, min loss: 3.79232e-07\n",
      "Epoch: 2451800, elapsed: 1.31e+01, train loss: 7.49189e-07, val loss: 1.67861e-06, min loss: 3.79232e-07\n",
      "Epoch: 2451900, elapsed: 1.31e+01, train loss: 3.80009e-07, val loss: 1.39709e-06, min loss: 3.79232e-07\n",
      "Epoch: 2452000, elapsed: 1.29e+01, train loss: 3.79128e-07, val loss: 1.40731e-06, min loss: 3.79128e-07\n",
      "Epoch: 2452100, elapsed: 1.32e+01, train loss: 4.70744e-07, val loss: 1.47316e-06, min loss: 3.79128e-07\n",
      "Epoch: 2452200, elapsed: 1.29e+01, train loss: 3.78727e-07, val loss: 1.40450e-06, min loss: 3.78727e-07\n",
      "Epoch: 2452300, elapsed: 1.30e+01, train loss: 3.86592e-07, val loss: 1.41543e-06, min loss: 3.78727e-07\n",
      "Epoch: 2452400, elapsed: 1.18e+01, train loss: 3.84959e-07, val loss: 1.41031e-06, min loss: 3.78727e-07\n",
      "Epoch: 2452500, elapsed: 1.19e+01, train loss: 3.87657e-07, val loss: 1.39852e-06, min loss: 3.78727e-07\n",
      "Epoch: 2452600, elapsed: 1.18e+01, train loss: 4.92593e-07, val loss: 1.99326e-06, min loss: 3.78727e-07\n",
      "Epoch: 2452700, elapsed: 1.17e+01, train loss: 3.78471e-07, val loss: 1.40004e-06, min loss: 3.78471e-07\n",
      "Epoch: 2452800, elapsed: 1.28e+01, train loss: 3.94949e-07, val loss: 1.42238e-06, min loss: 3.78471e-07\n",
      "Epoch: 2452900, elapsed: 1.18e+01, train loss: 3.78390e-07, val loss: 1.40196e-06, min loss: 3.78390e-07\n",
      "Epoch: 2453000, elapsed: 1.16e+01, train loss: 3.92451e-07, val loss: 1.36772e-06, min loss: 3.78390e-07\n",
      "Epoch: 2453100, elapsed: 1.17e+01, train loss: 3.78402e-07, val loss: 1.39974e-06, min loss: 3.78390e-07\n",
      "Epoch: 2453200, elapsed: 1.17e+01, train loss: 3.80925e-07, val loss: 1.39160e-06, min loss: 3.78390e-07\n",
      "Epoch: 2453300, elapsed: 1.16e+01, train loss: 3.78308e-07, val loss: 1.40144e-06, min loss: 3.78308e-07\n",
      "Epoch: 2453400, elapsed: 1.16e+01, train loss: 3.94636e-07, val loss: 1.38950e-06, min loss: 3.78308e-07\n",
      "Epoch: 2453500, elapsed: 1.17e+01, train loss: 3.78297e-07, val loss: 1.40141e-06, min loss: 3.78297e-07\n",
      "Epoch: 2453600, elapsed: 1.17e+01, train loss: 4.24223e-07, val loss: 1.39223e-06, min loss: 3.78297e-07\n",
      "Epoch: 2453700, elapsed: 1.15e+01, train loss: 3.78286e-07, val loss: 1.40004e-06, min loss: 3.78286e-07\n",
      "Epoch: 2453800, elapsed: 1.19e+01, train loss: 3.87935e-07, val loss: 1.41306e-06, min loss: 3.78286e-07\n",
      "Epoch: 2453900, elapsed: 1.87e+01, train loss: 1.42532e-06, val loss: 2.61707e-06, min loss: 3.78286e-07\n",
      "Epoch: 2454000, elapsed: 1.22e+01, train loss: 1.00464e-06, val loss: 2.06620e-06, min loss: 3.78286e-07\n",
      "Epoch: 2454100, elapsed: 1.20e+01, train loss: 3.78457e-07, val loss: 1.39723e-06, min loss: 3.78286e-07\n",
      "Epoch: 2454200, elapsed: 1.20e+01, train loss: 3.84812e-07, val loss: 1.43707e-06, min loss: 3.78286e-07\n",
      "Epoch: 2454300, elapsed: 1.21e+01, train loss: 4.02856e-07, val loss: 1.40530e-06, min loss: 3.78286e-07\n",
      "Epoch: 2454400, elapsed: 1.20e+01, train loss: 3.78328e-07, val loss: 1.39728e-06, min loss: 3.78286e-07\n",
      "Epoch: 2454500, elapsed: 1.18e+01, train loss: 6.76721e-07, val loss: 1.78067e-06, min loss: 3.78286e-07\n",
      "Epoch: 2454600, elapsed: 1.18e+01, train loss: 3.78134e-07, val loss: 1.39947e-06, min loss: 3.78134e-07\n",
      "Epoch: 2454700, elapsed: 1.18e+01, train loss: 4.07125e-07, val loss: 1.44308e-06, min loss: 3.78134e-07\n",
      "Epoch: 2454800, elapsed: 1.18e+01, train loss: 8.96637e-07, val loss: 1.59485e-06, min loss: 3.78134e-07\n",
      "Epoch: 2454900, elapsed: 1.19e+01, train loss: 3.95135e-07, val loss: 1.44612e-06, min loss: 3.78134e-07\n",
      "Epoch: 2455000, elapsed: 1.19e+01, train loss: 3.92154e-07, val loss: 1.41393e-06, min loss: 3.78134e-07\n",
      "Epoch: 2455100, elapsed: 1.40e+01, train loss: 4.91783e-07, val loss: 1.46528e-06, min loss: 3.78134e-07\n",
      "Epoch: 2455200, elapsed: 1.19e+01, train loss: 4.08025e-07, val loss: 1.46339e-06, min loss: 3.78134e-07\n",
      "Epoch: 2455300, elapsed: 1.19e+01, train loss: 1.13663e-06, val loss: 2.39089e-06, min loss: 3.78134e-07\n",
      "Epoch: 2455400, elapsed: 1.19e+01, train loss: 4.56712e-07, val loss: 1.58087e-06, min loss: 3.78134e-07\n",
      "Epoch: 2455500, elapsed: 1.33e+01, train loss: 4.14592e-07, val loss: 1.39480e-06, min loss: 3.78134e-07\n",
      "Epoch: 2455600, elapsed: 1.31e+01, train loss: 4.01505e-07, val loss: 1.46469e-06, min loss: 3.78134e-07\n",
      "Epoch: 2455700, elapsed: 1.31e+01, train loss: 4.23063e-07, val loss: 1.36086e-06, min loss: 3.78134e-07\n",
      "Epoch: 2455800, elapsed: 1.33e+01, train loss: 5.12457e-07, val loss: 1.50468e-06, min loss: 3.78134e-07\n",
      "Epoch: 2455900, elapsed: 1.19e+01, train loss: 3.78960e-07, val loss: 1.40287e-06, min loss: 3.78134e-07\n",
      "Epoch: 2456000, elapsed: 1.18e+01, train loss: 6.13005e-07, val loss: 1.72055e-06, min loss: 3.78134e-07\n",
      "Epoch: 2456100, elapsed: 1.21e+01, train loss: 5.53784e-07, val loss: 1.56097e-06, min loss: 3.78134e-07\n",
      "Epoch: 2456200, elapsed: 1.18e+01, train loss: 3.95397e-07, val loss: 1.54364e-06, min loss: 3.78134e-07\n",
      "Epoch: 2456300, elapsed: 1.18e+01, train loss: 7.36434e-07, val loss: 1.74380e-06, min loss: 3.78134e-07\n",
      "Epoch: 2456400, elapsed: 1.19e+01, train loss: 2.46430e-06, val loss: 3.44389e-06, min loss: 3.78134e-07\n",
      "Epoch: 2456500, elapsed: 1.17e+01, train loss: 3.77944e-07, val loss: 1.40059e-06, min loss: 3.77944e-07\n",
      "Epoch: 2456600, elapsed: 1.16e+01, train loss: 3.88332e-07, val loss: 1.36677e-06, min loss: 3.77944e-07\n",
      "Epoch: 2456700, elapsed: 1.19e+01, train loss: 3.92741e-07, val loss: 1.49163e-06, min loss: 3.77944e-07\n",
      "Epoch: 2456800, elapsed: 1.25e+01, train loss: 2.09571e-06, val loss: 3.68801e-06, min loss: 3.77944e-07\n",
      "Epoch: 2456900, elapsed: 1.17e+01, train loss: 3.77984e-07, val loss: 1.39150e-06, min loss: 3.77944e-07\n",
      "Epoch: 2457000, elapsed: 1.17e+01, train loss: 3.79342e-07, val loss: 1.41790e-06, min loss: 3.77944e-07\n",
      "Epoch: 2457100, elapsed: 1.18e+01, train loss: 3.95298e-07, val loss: 1.45474e-06, min loss: 3.77944e-07\n",
      "Epoch: 2457200, elapsed: 1.18e+01, train loss: 3.78093e-07, val loss: 1.39210e-06, min loss: 3.77944e-07\n",
      "Epoch: 2457300, elapsed: 1.19e+01, train loss: 1.39600e-06, val loss: 2.52683e-06, min loss: 3.77944e-07\n",
      "Epoch: 2457400, elapsed: 1.31e+01, train loss: 3.77740e-07, val loss: 1.39318e-06, min loss: 3.77740e-07\n",
      "Epoch: 2457500, elapsed: 1.33e+01, train loss: 3.78261e-07, val loss: 1.39592e-06, min loss: 3.77740e-07\n",
      "Epoch: 2457600, elapsed: 1.31e+01, train loss: 3.92678e-07, val loss: 1.38268e-06, min loss: 3.77740e-07\n",
      "Epoch: 2457700, elapsed: 1.26e+01, train loss: 6.52592e-07, val loss: 1.73324e-06, min loss: 3.77740e-07\n",
      "Epoch: 2457800, elapsed: 1.33e+01, train loss: 7.01759e-07, val loss: 1.59655e-06, min loss: 3.77740e-07\n",
      "Epoch: 2457900, elapsed: 1.33e+01, train loss: 4.01545e-07, val loss: 1.52996e-06, min loss: 3.77740e-07\n",
      "Epoch: 2458000, elapsed: 1.28e+01, train loss: 8.39699e-07, val loss: 1.90215e-06, min loss: 3.77740e-07\n",
      "Epoch: 2458100, elapsed: 1.32e+01, train loss: 3.77762e-07, val loss: 1.40328e-06, min loss: 3.77740e-07\n",
      "Epoch: 2458200, elapsed: 2.02e+01, train loss: 3.78912e-07, val loss: 1.40577e-06, min loss: 3.77740e-07\n",
      "Epoch: 2458300, elapsed: 1.36e+01, train loss: 1.14005e-06, val loss: 1.80329e-06, min loss: 3.77740e-07\n",
      "Epoch: 2458400, elapsed: 1.36e+01, train loss: 4.44681e-07, val loss: 1.37876e-06, min loss: 3.77740e-07\n",
      "Epoch: 2458500, elapsed: 1.20e+01, train loss: 3.77612e-07, val loss: 1.39795e-06, min loss: 3.77612e-07\n",
      "Epoch: 2458600, elapsed: 1.20e+01, train loss: 4.53193e-07, val loss: 1.47769e-06, min loss: 3.77612e-07\n",
      "Epoch: 2458700, elapsed: 1.21e+01, train loss: 3.78597e-07, val loss: 1.39776e-06, min loss: 3.77612e-07\n",
      "Epoch: 2458800, elapsed: 1.19e+01, train loss: 3.86862e-07, val loss: 1.36627e-06, min loss: 3.77612e-07\n",
      "Epoch: 2458900, elapsed: 1.21e+01, train loss: 2.15028e-06, val loss: 1.86663e-06, min loss: 3.77612e-07\n",
      "Epoch: 2459000, elapsed: 1.19e+01, train loss: 5.03542e-07, val loss: 1.47781e-06, min loss: 3.77612e-07\n",
      "Epoch: 2459100, elapsed: 1.19e+01, train loss: 5.95900e-07, val loss: 1.54912e-06, min loss: 3.77612e-07\n",
      "Epoch: 2459200, elapsed: 1.20e+01, train loss: 3.77558e-07, val loss: 1.39421e-06, min loss: 3.77558e-07\n",
      "Epoch: 2459300, elapsed: 1.19e+01, train loss: 3.82077e-07, val loss: 1.39706e-06, min loss: 3.77558e-07\n",
      "Epoch: 2459400, elapsed: 1.20e+01, train loss: 3.86257e-07, val loss: 1.36788e-06, min loss: 3.77558e-07\n",
      "Epoch: 2459500, elapsed: 1.21e+01, train loss: 4.25553e-07, val loss: 1.48048e-06, min loss: 3.77558e-07\n",
      "Epoch: 2459600, elapsed: 1.17e+01, train loss: 3.86673e-07, val loss: 1.41625e-06, min loss: 3.77558e-07\n",
      "Epoch: 2459700, elapsed: 1.18e+01, train loss: 3.80282e-07, val loss: 1.39003e-06, min loss: 3.77558e-07\n",
      "Epoch: 2459800, elapsed: 1.16e+01, train loss: 4.12799e-07, val loss: 1.46836e-06, min loss: 3.77558e-07\n",
      "Epoch: 2459900, elapsed: 1.19e+01, train loss: 3.91295e-07, val loss: 1.42611e-06, min loss: 3.77558e-07\n",
      "Epoch: 2460000, elapsed: 1.19e+01, train loss: 4.53063e-07, val loss: 1.40063e-06, min loss: 3.77558e-07\n",
      "Epoch: 2460100, elapsed: 1.37e+01, train loss: 4.43760e-07, val loss: 1.45672e-06, min loss: 3.77558e-07\n",
      "Epoch: 2460200, elapsed: 1.18e+01, train loss: 4.08477e-07, val loss: 1.55254e-06, min loss: 3.77558e-07\n",
      "Epoch: 2460300, elapsed: 1.20e+01, train loss: 4.31897e-07, val loss: 1.55368e-06, min loss: 3.77558e-07\n",
      "Epoch: 2460400, elapsed: 1.18e+01, train loss: 3.78488e-07, val loss: 1.40341e-06, min loss: 3.77558e-07\n",
      "Epoch: 2460500, elapsed: 1.16e+01, train loss: 3.86063e-07, val loss: 1.34474e-06, min loss: 3.77558e-07\n",
      "Epoch: 2460600, elapsed: 1.16e+01, train loss: 4.76908e-07, val loss: 1.62564e-06, min loss: 3.77558e-07\n",
      "Epoch: 2460700, elapsed: 1.17e+01, train loss: 4.52752e-07, val loss: 1.43710e-06, min loss: 3.77558e-07\n",
      "Epoch: 2460800, elapsed: 1.17e+01, train loss: 5.43934e-07, val loss: 1.51169e-06, min loss: 3.77558e-07\n",
      "Epoch: 2460900, elapsed: 1.17e+01, train loss: 3.77259e-07, val loss: 1.39721e-06, min loss: 3.77259e-07\n",
      "Epoch: 2461000, elapsed: 1.18e+01, train loss: 4.55682e-07, val loss: 1.36316e-06, min loss: 3.77259e-07\n",
      "Epoch: 2461100, elapsed: 1.19e+01, train loss: 3.76944e-07, val loss: 1.39323e-06, min loss: 3.76944e-07\n",
      "Epoch: 2461200, elapsed: 1.31e+01, train loss: 4.07331e-07, val loss: 1.41829e-06, min loss: 3.76944e-07\n",
      "Epoch: 2461300, elapsed: 1.37e+01, train loss: 3.76910e-07, val loss: 1.39199e-06, min loss: 3.76910e-07\n",
      "Epoch: 2461400, elapsed: 1.36e+01, train loss: 3.87264e-07, val loss: 1.40453e-06, min loss: 3.76910e-07\n",
      "Epoch: 2461500, elapsed: 1.30e+01, train loss: 3.76864e-07, val loss: 1.39203e-06, min loss: 3.76864e-07\n",
      "Epoch: 2461600, elapsed: 1.30e+01, train loss: 4.36638e-07, val loss: 1.42195e-06, min loss: 3.76864e-07\n",
      "Epoch: 2461700, elapsed: 1.26e+01, train loss: 3.76827e-07, val loss: 1.39378e-06, min loss: 3.76827e-07\n",
      "Epoch: 2461800, elapsed: 1.26e+01, train loss: 6.03300e-07, val loss: 1.75800e-06, min loss: 3.76827e-07\n",
      "Epoch: 2461900, elapsed: 1.29e+01, train loss: 5.55244e-07, val loss: 1.53879e-06, min loss: 3.76827e-07\n",
      "Epoch: 2462000, elapsed: 1.19e+01, train loss: 3.77191e-07, val loss: 1.39263e-06, min loss: 3.76827e-07\n",
      "Epoch: 2462100, elapsed: 1.16e+01, train loss: 3.80279e-07, val loss: 1.39251e-06, min loss: 3.76827e-07\n",
      "Epoch: 2462200, elapsed: 1.15e+01, train loss: 4.48033e-07, val loss: 1.39380e-06, min loss: 3.76827e-07\n",
      "Epoch: 2462300, elapsed: 1.16e+01, train loss: 3.81003e-07, val loss: 1.41595e-06, min loss: 3.76827e-07\n",
      "Epoch: 2462400, elapsed: 1.14e+01, train loss: 3.77221e-07, val loss: 1.39532e-06, min loss: 3.76827e-07\n",
      "Epoch: 2462500, elapsed: 1.77e+01, train loss: 2.11134e-06, val loss: 1.79766e-06, min loss: 3.76827e-07\n",
      "Epoch: 2462600, elapsed: 1.22e+01, train loss: 3.76714e-07, val loss: 1.39535e-06, min loss: 3.76714e-07\n",
      "Epoch: 2462700, elapsed: 1.21e+01, train loss: 3.94427e-07, val loss: 1.34928e-06, min loss: 3.76714e-07\n",
      "Epoch: 2462800, elapsed: 1.20e+01, train loss: 4.01688e-07, val loss: 1.37553e-06, min loss: 3.76714e-07\n",
      "Epoch: 2462900, elapsed: 1.19e+01, train loss: 1.34060e-06, val loss: 2.18033e-06, min loss: 3.76714e-07\n",
      "Epoch: 2463000, elapsed: 1.18e+01, train loss: 3.92954e-07, val loss: 1.39871e-06, min loss: 3.76714e-07\n",
      "Epoch: 2463100, elapsed: 1.17e+01, train loss: 3.87794e-07, val loss: 1.38468e-06, min loss: 3.76714e-07\n",
      "Epoch: 2463200, elapsed: 1.17e+01, train loss: 7.39356e-07, val loss: 1.78739e-06, min loss: 3.76714e-07\n",
      "Epoch: 2463300, elapsed: 1.18e+01, train loss: 4.38875e-07, val loss: 1.39985e-06, min loss: 3.76714e-07\n",
      "Epoch: 2463400, elapsed: 1.17e+01, train loss: 6.11447e-07, val loss: 1.48178e-06, min loss: 3.76714e-07\n",
      "Epoch: 2463500, elapsed: 1.17e+01, train loss: 1.19324e-06, val loss: 2.17878e-06, min loss: 3.76714e-07\n",
      "Epoch: 2463600, elapsed: 1.17e+01, train loss: 5.80020e-07, val loss: 1.51161e-06, min loss: 3.76714e-07\n",
      "Epoch: 2463700, elapsed: 1.17e+01, train loss: 4.86779e-07, val loss: 1.52599e-06, min loss: 3.76714e-07\n",
      "Epoch: 2463800, elapsed: 1.17e+01, train loss: 3.79034e-07, val loss: 1.39891e-06, min loss: 3.76714e-07\n",
      "Epoch: 2463900, elapsed: 1.17e+01, train loss: 3.79752e-07, val loss: 1.40143e-06, min loss: 3.76714e-07\n",
      "Epoch: 2464000, elapsed: 1.17e+01, train loss: 4.21160e-07, val loss: 1.37696e-06, min loss: 3.76714e-07\n",
      "Epoch: 2464100, elapsed: 1.19e+01, train loss: 6.32722e-07, val loss: 1.94262e-06, min loss: 3.76714e-07\n",
      "Epoch: 2464200, elapsed: 1.18e+01, train loss: 3.76631e-07, val loss: 1.38729e-06, min loss: 3.76631e-07\n",
      "Epoch: 2464300, elapsed: 1.16e+01, train loss: 4.80754e-07, val loss: 1.41508e-06, min loss: 3.76631e-07\n",
      "Epoch: 2464400, elapsed: 1.17e+01, train loss: 4.26103e-07, val loss: 1.44835e-06, min loss: 3.76631e-07\n",
      "Epoch: 2464500, elapsed: 1.17e+01, train loss: 3.79155e-07, val loss: 1.37708e-06, min loss: 3.76631e-07\n",
      "Epoch: 2464600, elapsed: 1.15e+01, train loss: 4.06025e-07, val loss: 1.37443e-06, min loss: 3.76631e-07\n",
      "Epoch: 2464700, elapsed: 1.18e+01, train loss: 3.95195e-07, val loss: 1.42627e-06, min loss: 3.76631e-07\n",
      "Epoch: 2464800, elapsed: 1.17e+01, train loss: 3.80284e-07, val loss: 1.41567e-06, min loss: 3.76631e-07\n",
      "Epoch: 2464900, elapsed: 1.18e+01, train loss: 3.76696e-07, val loss: 1.40040e-06, min loss: 3.76631e-07\n",
      "Epoch: 2465000, elapsed: 1.16e+01, train loss: 4.35932e-07, val loss: 1.40718e-06, min loss: 3.76631e-07\n",
      "Epoch: 2465100, elapsed: 1.36e+01, train loss: 3.87505e-07, val loss: 1.36231e-06, min loss: 3.76631e-07\n",
      "Epoch: 2465200, elapsed: 1.16e+01, train loss: 3.76478e-07, val loss: 1.38887e-06, min loss: 3.76478e-07\n",
      "Epoch: 2465300, elapsed: 1.17e+01, train loss: 4.22579e-07, val loss: 1.50453e-06, min loss: 3.76478e-07\n",
      "Epoch: 2465400, elapsed: 1.16e+01, train loss: 9.93384e-07, val loss: 2.19294e-06, min loss: 3.76478e-07\n",
      "Epoch: 2465500, elapsed: 1.17e+01, train loss: 5.69518e-07, val loss: 1.95872e-06, min loss: 3.76478e-07\n",
      "Epoch: 2465600, elapsed: 1.16e+01, train loss: 3.76703e-07, val loss: 1.38522e-06, min loss: 3.76478e-07\n",
      "Epoch: 2465700, elapsed: 1.16e+01, train loss: 3.92585e-07, val loss: 1.39272e-06, min loss: 3.76478e-07\n",
      "Epoch: 2465800, elapsed: 1.15e+01, train loss: 1.20883e-06, val loss: 1.75964e-06, min loss: 3.76478e-07\n",
      "Epoch: 2465900, elapsed: 1.17e+01, train loss: 3.76113e-07, val loss: 1.38687e-06, min loss: 3.76113e-07\n",
      "Epoch: 2466000, elapsed: 1.15e+01, train loss: 3.88311e-07, val loss: 1.40768e-06, min loss: 3.76113e-07\n",
      "Epoch: 2466100, elapsed: 1.23e+01, train loss: 4.72788e-07, val loss: 1.43344e-06, min loss: 3.76113e-07\n",
      "Epoch: 2466200, elapsed: 1.32e+01, train loss: 3.82862e-07, val loss: 1.37427e-06, min loss: 3.76113e-07\n",
      "Epoch: 2466300, elapsed: 1.26e+01, train loss: 1.35750e-06, val loss: 2.58034e-06, min loss: 3.76113e-07\n",
      "Epoch: 2466400, elapsed: 1.29e+01, train loss: 3.76465e-07, val loss: 1.38324e-06, min loss: 3.76113e-07\n",
      "Epoch: 2466500, elapsed: 1.27e+01, train loss: 3.76659e-07, val loss: 1.39071e-06, min loss: 3.76113e-07\n",
      "Epoch: 2466600, elapsed: 1.31e+01, train loss: 1.02823e-06, val loss: 1.61661e-06, min loss: 3.76113e-07\n",
      "Epoch: 2466700, elapsed: 1.24e+01, train loss: 4.18254e-07, val loss: 1.44907e-06, min loss: 3.76113e-07\n",
      "Epoch: 2466800, elapsed: 1.33e+01, train loss: 3.84741e-07, val loss: 1.40927e-06, min loss: 3.76113e-07\n",
      "Epoch: 2466900, elapsed: 2.26e+01, train loss: 4.67201e-07, val loss: 1.37898e-06, min loss: 3.76113e-07\n",
      "Epoch: 2467000, elapsed: 1.53e+01, train loss: 4.22226e-07, val loss: 1.35933e-06, min loss: 3.76113e-07\n",
      "Epoch: 2467100, elapsed: 1.35e+01, train loss: 4.50810e-07, val loss: 1.36057e-06, min loss: 3.76113e-07\n",
      "Epoch: 2467200, elapsed: 1.34e+01, train loss: 4.94786e-07, val loss: 1.37068e-06, min loss: 3.76113e-07\n",
      "Epoch: 2467300, elapsed: 1.27e+01, train loss: 3.76285e-07, val loss: 1.37812e-06, min loss: 3.76113e-07\n",
      "Epoch: 2467400, elapsed: 1.37e+01, train loss: 3.79692e-07, val loss: 1.37695e-06, min loss: 3.76113e-07\n",
      "Epoch: 2467500, elapsed: 1.30e+01, train loss: 1.14657e-06, val loss: 2.65901e-06, min loss: 3.76113e-07\n",
      "Epoch: 2467600, elapsed: 1.29e+01, train loss: 3.79491e-07, val loss: 1.40162e-06, min loss: 3.76113e-07\n",
      "Epoch: 2467700, elapsed: 1.35e+01, train loss: 3.83756e-07, val loss: 1.39681e-06, min loss: 3.76113e-07\n",
      "Epoch: 2467800, elapsed: 1.41e+01, train loss: 3.88468e-07, val loss: 1.37235e-06, min loss: 3.76113e-07\n",
      "Epoch: 2467900, elapsed: 1.54e+01, train loss: 3.83636e-07, val loss: 1.41735e-06, min loss: 3.76113e-07\n",
      "Epoch: 2468000, elapsed: 1.41e+01, train loss: 3.97701e-07, val loss: 1.38238e-06, min loss: 3.76113e-07\n",
      "Epoch: 2468100, elapsed: 1.27e+01, train loss: 4.21904e-06, val loss: 4.40759e-06, min loss: 3.76113e-07\n",
      "Epoch: 2468200, elapsed: 1.30e+01, train loss: 3.75807e-07, val loss: 1.38260e-06, min loss: 3.75807e-07\n",
      "Epoch: 2468300, elapsed: 1.28e+01, train loss: 6.36389e-07, val loss: 1.72018e-06, min loss: 3.75807e-07\n",
      "Epoch: 2468400, elapsed: 1.24e+01, train loss: 3.75671e-07, val loss: 1.38453e-06, min loss: 3.75671e-07\n",
      "Epoch: 2468500, elapsed: 1.17e+01, train loss: 7.42402e-07, val loss: 1.92150e-06, min loss: 3.75671e-07\n",
      "Epoch: 2468600, elapsed: 1.16e+01, train loss: 2.11930e-06, val loss: 2.79582e-06, min loss: 3.75671e-07\n",
      "Epoch: 2468700, elapsed: 1.16e+01, train loss: 4.38256e-07, val loss: 1.42887e-06, min loss: 3.75671e-07\n",
      "Epoch: 2468800, elapsed: 1.16e+01, train loss: 4.11012e-07, val loss: 1.36321e-06, min loss: 3.75671e-07\n",
      "Epoch: 2468900, elapsed: 1.18e+01, train loss: 5.72848e-07, val loss: 1.64943e-06, min loss: 3.75671e-07\n",
      "Epoch: 2469000, elapsed: 1.16e+01, train loss: 3.92823e-07, val loss: 1.37796e-06, min loss: 3.75671e-07\n",
      "Epoch: 2469100, elapsed: 1.16e+01, train loss: 4.59833e-07, val loss: 1.40573e-06, min loss: 3.75671e-07\n",
      "Epoch: 2469200, elapsed: 1.16e+01, train loss: 4.41945e-07, val loss: 1.33072e-06, min loss: 3.75671e-07\n",
      "Epoch: 2469300, elapsed: 1.14e+01, train loss: 3.75872e-07, val loss: 1.37781e-06, min loss: 3.75671e-07\n",
      "Epoch: 2469400, elapsed: 1.15e+01, train loss: 3.96340e-07, val loss: 1.38248e-06, min loss: 3.75671e-07\n",
      "Epoch: 2469500, elapsed: 1.16e+01, train loss: 3.80199e-07, val loss: 1.43554e-06, min loss: 3.75671e-07\n",
      "Epoch: 2469600, elapsed: 1.15e+01, train loss: 4.40259e-07, val loss: 1.52735e-06, min loss: 3.75671e-07\n",
      "Epoch: 2469700, elapsed: 1.17e+01, train loss: 6.99102e-07, val loss: 1.59152e-06, min loss: 3.75671e-07\n",
      "Epoch: 2469800, elapsed: 1.15e+01, train loss: 3.75392e-07, val loss: 1.38359e-06, min loss: 3.75392e-07\n",
      "Epoch: 2469900, elapsed: 1.15e+01, train loss: 3.79827e-07, val loss: 1.36689e-06, min loss: 3.75392e-07\n",
      "Epoch: 2470000, elapsed: 1.14e+01, train loss: 9.52159e-07, val loss: 1.74755e-06, min loss: 3.75392e-07\n",
      "Epoch: 2470100, elapsed: 1.38e+01, train loss: 4.02189e-07, val loss: 1.35704e-06, min loss: 3.75392e-07\n",
      "Epoch: 2470200, elapsed: 1.15e+01, train loss: 1.31631e-06, val loss: 1.87346e-06, min loss: 3.75392e-07\n",
      "Epoch: 2470300, elapsed: 1.14e+01, train loss: 7.08079e-07, val loss: 1.75302e-06, min loss: 3.75392e-07\n",
      "Epoch: 2470400, elapsed: 1.14e+01, train loss: 6.15722e-07, val loss: 1.77983e-06, min loss: 3.75392e-07\n",
      "Epoch: 2470500, elapsed: 1.14e+01, train loss: 5.69115e-07, val loss: 1.71933e-06, min loss: 3.75392e-07\n",
      "Epoch: 2470600, elapsed: 1.15e+01, train loss: 7.79550e-07, val loss: 1.45566e-06, min loss: 3.75392e-07\n",
      "Epoch: 2470700, elapsed: 1.15e+01, train loss: 3.75368e-07, val loss: 1.38481e-06, min loss: 3.75368e-07\n",
      "Epoch: 2470800, elapsed: 1.15e+01, train loss: 3.75804e-07, val loss: 1.38208e-06, min loss: 3.75368e-07\n",
      "Epoch: 2470900, elapsed: 1.14e+01, train loss: 6.34725e-07, val loss: 1.61172e-06, min loss: 3.75368e-07\n",
      "Epoch: 2471000, elapsed: 1.16e+01, train loss: 4.08170e-07, val loss: 1.36868e-06, min loss: 3.75368e-07\n",
      "Epoch: 2471100, elapsed: 1.15e+01, train loss: 5.26844e-07, val loss: 1.53163e-06, min loss: 3.75368e-07\n",
      "Epoch: 2471200, elapsed: 1.79e+01, train loss: 9.08425e-07, val loss: 1.55149e-06, min loss: 3.75368e-07\n",
      "Epoch: 2471300, elapsed: 1.18e+01, train loss: 3.91882e-07, val loss: 1.42528e-06, min loss: 3.75368e-07\n",
      "Epoch: 2471400, elapsed: 1.16e+01, train loss: 4.00562e-07, val loss: 1.40088e-06, min loss: 3.75368e-07\n",
      "Epoch: 2471500, elapsed: 1.19e+01, train loss: 3.80743e-07, val loss: 1.38506e-06, min loss: 3.75368e-07\n",
      "Epoch: 2471600, elapsed: 1.17e+01, train loss: 1.09188e-06, val loss: 1.95668e-06, min loss: 3.75368e-07\n",
      "Epoch: 2471700, elapsed: 1.18e+01, train loss: 5.74154e-07, val loss: 1.39666e-06, min loss: 3.75368e-07\n",
      "Epoch: 2471800, elapsed: 1.17e+01, train loss: 6.95203e-07, val loss: 1.68553e-06, min loss: 3.75368e-07\n",
      "Epoch: 2471900, elapsed: 1.17e+01, train loss: 3.94635e-07, val loss: 1.38521e-06, min loss: 3.75368e-07\n",
      "Epoch: 2472000, elapsed: 1.18e+01, train loss: 3.75842e-07, val loss: 1.38154e-06, min loss: 3.75368e-07\n",
      "Epoch: 2472100, elapsed: 1.18e+01, train loss: 3.75309e-07, val loss: 1.38004e-06, min loss: 3.75309e-07\n",
      "Epoch: 2472200, elapsed: 1.17e+01, train loss: 4.80362e-07, val loss: 1.70050e-06, min loss: 3.75309e-07\n",
      "Epoch: 2472300, elapsed: 1.18e+01, train loss: 3.76849e-07, val loss: 1.37537e-06, min loss: 3.75309e-07\n",
      "Epoch: 2472400, elapsed: 1.17e+01, train loss: 4.01034e-07, val loss: 1.38292e-06, min loss: 3.75309e-07\n",
      "Epoch: 2472500, elapsed: 1.18e+01, train loss: 7.52895e-07, val loss: 1.83331e-06, min loss: 3.75309e-07\n",
      "Epoch: 2472600, elapsed: 1.18e+01, train loss: 9.15350e-07, val loss: 2.04070e-06, min loss: 3.75309e-07\n",
      "Epoch: 2472700, elapsed: 1.18e+01, train loss: 5.57323e-07, val loss: 1.72269e-06, min loss: 3.75309e-07\n",
      "Epoch: 2472800, elapsed: 1.21e+01, train loss: 3.75685e-07, val loss: 1.36407e-06, min loss: 3.75309e-07\n",
      "Epoch: 2472900, elapsed: 1.26e+01, train loss: 4.14690e-07, val loss: 1.46777e-06, min loss: 3.75309e-07\n",
      "Epoch: 2473000, elapsed: 1.33e+01, train loss: 1.26644e-06, val loss: 1.82413e-06, min loss: 3.75309e-07\n",
      "Epoch: 2473100, elapsed: 1.23e+01, train loss: 4.22131e-07, val loss: 1.52729e-06, min loss: 3.75309e-07\n",
      "Epoch: 2473200, elapsed: 1.16e+01, train loss: 3.75036e-07, val loss: 1.38501e-06, min loss: 3.75036e-07\n",
      "Epoch: 2473300, elapsed: 1.19e+01, train loss: 4.49452e-07, val loss: 1.47711e-06, min loss: 3.75036e-07\n",
      "Epoch: 2473400, elapsed: 1.17e+01, train loss: 3.75394e-07, val loss: 1.38165e-06, min loss: 3.75036e-07\n",
      "Epoch: 2473500, elapsed: 1.16e+01, train loss: 3.76062e-07, val loss: 1.36975e-06, min loss: 3.75036e-07\n",
      "Epoch: 2473600, elapsed: 1.17e+01, train loss: 1.53617e-06, val loss: 2.14538e-06, min loss: 3.75036e-07\n",
      "Epoch: 2473700, elapsed: 1.14e+01, train loss: 4.37812e-07, val loss: 1.36007e-06, min loss: 3.75036e-07\n",
      "Epoch: 2473800, elapsed: 1.14e+01, train loss: 3.84821e-07, val loss: 1.37848e-06, min loss: 3.75036e-07\n",
      "Epoch: 2473900, elapsed: 1.17e+01, train loss: 3.80557e-07, val loss: 1.38346e-06, min loss: 3.75036e-07\n",
      "Epoch: 2474000, elapsed: 1.16e+01, train loss: 3.75454e-07, val loss: 1.38321e-06, min loss: 3.75036e-07\n",
      "Epoch: 2474100, elapsed: 1.17e+01, train loss: 5.22378e-07, val loss: 1.45765e-06, min loss: 3.75036e-07\n",
      "Epoch: 2474200, elapsed: 1.16e+01, train loss: 3.79513e-07, val loss: 1.37497e-06, min loss: 3.75036e-07\n",
      "Epoch: 2474300, elapsed: 1.15e+01, train loss: 3.74998e-07, val loss: 1.37582e-06, min loss: 3.74998e-07\n",
      "Epoch: 2474400, elapsed: 1.14e+01, train loss: 1.76568e-06, val loss: 2.93562e-06, min loss: 3.74998e-07\n",
      "Epoch: 2474500, elapsed: 1.15e+01, train loss: 3.74625e-07, val loss: 1.37680e-06, min loss: 3.74625e-07\n",
      "Epoch: 2474600, elapsed: 1.14e+01, train loss: 3.77996e-07, val loss: 1.39248e-06, min loss: 3.74625e-07\n",
      "Epoch: 2474700, elapsed: 1.15e+01, train loss: 4.77104e-07, val loss: 1.71852e-06, min loss: 3.74625e-07\n",
      "Epoch: 2474800, elapsed: 1.13e+01, train loss: 3.74604e-07, val loss: 1.37620e-06, min loss: 3.74604e-07\n",
      "Epoch: 2474900, elapsed: 1.16e+01, train loss: 6.17491e-07, val loss: 1.51845e-06, min loss: 3.74604e-07\n",
      "Epoch: 2475000, elapsed: 1.15e+01, train loss: 3.74523e-07, val loss: 1.37843e-06, min loss: 3.74523e-07\n",
      "Epoch: 2475100, elapsed: 1.38e+01, train loss: 4.11170e-07, val loss: 1.45035e-06, min loss: 3.74523e-07\n",
      "Epoch: 2475200, elapsed: 1.14e+01, train loss: 3.74437e-07, val loss: 1.37754e-06, min loss: 3.74437e-07\n",
      "Epoch: 2475300, elapsed: 1.15e+01, train loss: 3.77338e-07, val loss: 1.36014e-06, min loss: 3.74437e-07\n",
      "Epoch: 2475400, elapsed: 1.17e+01, train loss: 3.74412e-07, val loss: 1.37877e-06, min loss: 3.74412e-07\n",
      "Epoch: 2475500, elapsed: 1.15e+01, train loss: 3.76377e-07, val loss: 1.38736e-06, min loss: 3.74412e-07\n",
      "Epoch: 2475600, elapsed: 1.83e+01, train loss: 4.18256e-07, val loss: 1.33351e-06, min loss: 3.74412e-07\n",
      "Epoch: 2475700, elapsed: 1.17e+01, train loss: 4.24327e-07, val loss: 1.59906e-06, min loss: 3.74412e-07\n",
      "Epoch: 2475800, elapsed: 1.20e+01, train loss: 3.82624e-07, val loss: 1.40244e-06, min loss: 3.74412e-07\n",
      "Epoch: 2475900, elapsed: 1.17e+01, train loss: 3.99842e-07, val loss: 1.37489e-06, min loss: 3.74412e-07\n",
      "Epoch: 2476000, elapsed: 1.16e+01, train loss: 3.81741e-07, val loss: 1.42633e-06, min loss: 3.74412e-07\n",
      "Epoch: 2476100, elapsed: 1.18e+01, train loss: 4.16206e-07, val loss: 1.36970e-06, min loss: 3.74412e-07\n",
      "Epoch: 2476200, elapsed: 1.16e+01, train loss: 3.74307e-07, val loss: 1.37274e-06, min loss: 3.74307e-07\n",
      "Epoch: 2476300, elapsed: 1.19e+01, train loss: 3.77229e-07, val loss: 1.36973e-06, min loss: 3.74307e-07\n",
      "Epoch: 2476400, elapsed: 1.16e+01, train loss: 5.09539e-07, val loss: 1.45857e-06, min loss: 3.74307e-07\n",
      "Epoch: 2476500, elapsed: 1.15e+01, train loss: 3.75031e-07, val loss: 1.37248e-06, min loss: 3.74307e-07\n",
      "Epoch: 2476600, elapsed: 1.17e+01, train loss: 3.82314e-07, val loss: 1.39673e-06, min loss: 3.74307e-07\n",
      "Epoch: 2476700, elapsed: 1.18e+01, train loss: 4.08419e-07, val loss: 1.37633e-06, min loss: 3.74307e-07\n",
      "Epoch: 2476800, elapsed: 1.18e+01, train loss: 4.74024e-07, val loss: 1.56833e-06, min loss: 3.74307e-07\n",
      "Epoch: 2476900, elapsed: 1.16e+01, train loss: 8.76704e-07, val loss: 2.13586e-06, min loss: 3.74307e-07\n",
      "Epoch: 2477000, elapsed: 1.18e+01, train loss: 3.74479e-07, val loss: 1.37550e-06, min loss: 3.74307e-07\n",
      "Epoch: 2477100, elapsed: 1.16e+01, train loss: 3.95535e-07, val loss: 1.41001e-06, min loss: 3.74307e-07\n",
      "Epoch: 2477200, elapsed: 1.18e+01, train loss: 3.74582e-07, val loss: 1.37276e-06, min loss: 3.74307e-07\n",
      "Epoch: 2477300, elapsed: 1.13e+01, train loss: 3.74625e-07, val loss: 1.37249e-06, min loss: 3.74307e-07\n",
      "Epoch: 2477400, elapsed: 1.16e+01, train loss: 8.74851e-07, val loss: 1.95113e-06, min loss: 3.74307e-07\n",
      "Epoch: 2477500, elapsed: 1.15e+01, train loss: 3.74215e-07, val loss: 1.37537e-06, min loss: 3.74215e-07\n",
      "Epoch: 2477600, elapsed: 1.15e+01, train loss: 5.37964e-07, val loss: 1.48790e-06, min loss: 3.74215e-07\n",
      "Epoch: 2477700, elapsed: 1.16e+01, train loss: 4.36776e-07, val loss: 1.56844e-06, min loss: 3.74215e-07\n",
      "Epoch: 2477800, elapsed: 1.16e+01, train loss: 1.06197e-06, val loss: 2.38834e-06, min loss: 3.74215e-07\n",
      "Epoch: 2477900, elapsed: 1.15e+01, train loss: 3.89592e-07, val loss: 1.44474e-06, min loss: 3.74215e-07\n",
      "Epoch: 2478000, elapsed: 1.16e+01, train loss: 3.78835e-07, val loss: 1.33837e-06, min loss: 3.74215e-07\n",
      "Epoch: 2478100, elapsed: 1.16e+01, train loss: 3.75170e-07, val loss: 1.37031e-06, min loss: 3.74215e-07\n",
      "Epoch: 2478200, elapsed: 1.14e+01, train loss: 3.82436e-07, val loss: 1.36149e-06, min loss: 3.74215e-07\n",
      "Epoch: 2478300, elapsed: 1.16e+01, train loss: 3.95248e-07, val loss: 1.32055e-06, min loss: 3.74215e-07\n",
      "Epoch: 2478400, elapsed: 1.15e+01, train loss: 3.83519e-07, val loss: 1.39136e-06, min loss: 3.74215e-07\n",
      "Epoch: 2478500, elapsed: 1.15e+01, train loss: 3.74821e-07, val loss: 1.36748e-06, min loss: 3.74215e-07\n",
      "Epoch: 2478600, elapsed: 1.16e+01, train loss: 3.92803e-07, val loss: 1.37238e-06, min loss: 3.74215e-07\n",
      "Epoch: 2478700, elapsed: 1.14e+01, train loss: 3.91167e-07, val loss: 1.45516e-06, min loss: 3.74215e-07\n",
      "Epoch: 2478800, elapsed: 1.15e+01, train loss: 3.93986e-07, val loss: 1.48763e-06, min loss: 3.74215e-07\n",
      "Epoch: 2478900, elapsed: 1.15e+01, train loss: 3.92921e-07, val loss: 1.39123e-06, min loss: 3.74215e-07\n",
      "Epoch: 2479000, elapsed: 1.15e+01, train loss: 8.74198e-07, val loss: 1.53845e-06, min loss: 3.74215e-07\n",
      "Epoch: 2479100, elapsed: 1.15e+01, train loss: 7.11021e-07, val loss: 1.66652e-06, min loss: 3.74215e-07\n",
      "Epoch: 2479200, elapsed: 1.15e+01, train loss: 5.29274e-07, val loss: 1.50447e-06, min loss: 3.74215e-07\n",
      "Epoch: 2479300, elapsed: 1.14e+01, train loss: 4.15631e-07, val loss: 1.40566e-06, min loss: 3.74215e-07\n",
      "Epoch: 2479400, elapsed: 1.15e+01, train loss: 4.22938e-07, val loss: 1.45457e-06, min loss: 3.74215e-07\n",
      "Epoch: 2479500, elapsed: 1.15e+01, train loss: 3.89651e-07, val loss: 1.50924e-06, min loss: 3.74215e-07\n",
      "Epoch: 2479600, elapsed: 1.16e+01, train loss: 7.99265e-07, val loss: 2.11428e-06, min loss: 3.74215e-07\n",
      "Epoch: 2479700, elapsed: 1.14e+01, train loss: 3.74819e-07, val loss: 1.36683e-06, min loss: 3.74215e-07\n",
      "Epoch: 2479800, elapsed: 1.15e+01, train loss: 3.74148e-07, val loss: 1.37234e-06, min loss: 3.74148e-07\n",
      "Epoch: 2479900, elapsed: 1.14e+01, train loss: 5.57690e-07, val loss: 1.45590e-06, min loss: 3.74148e-07\n",
      "Epoch: 2480000, elapsed: 1.84e+01, train loss: 5.21937e-07, val loss: 1.64529e-06, min loss: 3.74148e-07\n",
      "Epoch: 2480100, elapsed: 1.42e+01, train loss: 3.73916e-07, val loss: 1.38242e-06, min loss: 3.73916e-07\n",
      "Epoch: 2480200, elapsed: 1.18e+01, train loss: 4.55074e-07, val loss: 1.46639e-06, min loss: 3.73916e-07\n",
      "Epoch: 2480300, elapsed: 1.17e+01, train loss: 2.40251e-06, val loss: 3.17819e-06, min loss: 3.73916e-07\n",
      "Epoch: 2480400, elapsed: 1.19e+01, train loss: 3.77221e-07, val loss: 1.38870e-06, min loss: 3.73916e-07\n",
      "Epoch: 2480500, elapsed: 1.18e+01, train loss: 3.97101e-07, val loss: 1.41273e-06, min loss: 3.73916e-07\n",
      "Epoch: 2480600, elapsed: 1.16e+01, train loss: 2.99666e-06, val loss: 2.22243e-06, min loss: 3.73916e-07\n",
      "Epoch: 2480700, elapsed: 1.19e+01, train loss: 3.73637e-07, val loss: 1.37368e-06, min loss: 3.73637e-07\n",
      "Epoch: 2480800, elapsed: 1.18e+01, train loss: 3.77411e-07, val loss: 1.37770e-06, min loss: 3.73637e-07\n",
      "Epoch: 2480900, elapsed: 1.16e+01, train loss: 1.17907e-06, val loss: 2.30774e-06, min loss: 3.73637e-07\n",
      "Epoch: 2481000, elapsed: 1.16e+01, train loss: 4.55011e-07, val loss: 1.52205e-06, min loss: 3.73637e-07\n",
      "Epoch: 2481100, elapsed: 1.17e+01, train loss: 4.25633e-07, val loss: 1.39127e-06, min loss: 3.73637e-07\n",
      "Epoch: 2481200, elapsed: 1.14e+01, train loss: 9.57974e-07, val loss: 1.64251e-06, min loss: 3.73637e-07\n",
      "Epoch: 2481300, elapsed: 1.19e+01, train loss: 5.11544e-07, val loss: 1.37056e-06, min loss: 3.73637e-07\n",
      "Epoch: 2481400, elapsed: 1.17e+01, train loss: 3.98976e-07, val loss: 1.38306e-06, min loss: 3.73637e-07\n",
      "Epoch: 2481500, elapsed: 1.18e+01, train loss: 3.73713e-07, val loss: 1.37153e-06, min loss: 3.73637e-07\n",
      "Epoch: 2481600, elapsed: 1.17e+01, train loss: 3.80074e-07, val loss: 1.36013e-06, min loss: 3.73637e-07\n",
      "Epoch: 2481700, elapsed: 1.19e+01, train loss: 3.75957e-07, val loss: 1.35649e-06, min loss: 3.73637e-07\n",
      "Epoch: 2481800, elapsed: 1.16e+01, train loss: 3.78934e-07, val loss: 1.34915e-06, min loss: 3.73637e-07\n",
      "Epoch: 2481900, elapsed: 1.16e+01, train loss: 3.74265e-07, val loss: 1.37848e-06, min loss: 3.73637e-07\n",
      "Epoch: 2482000, elapsed: 1.18e+01, train loss: 3.73933e-07, val loss: 1.36969e-06, min loss: 3.73637e-07\n",
      "Epoch: 2482100, elapsed: 1.17e+01, train loss: 7.92538e-07, val loss: 1.77330e-06, min loss: 3.73637e-07\n",
      "Epoch: 2482200, elapsed: 1.16e+01, train loss: 1.08432e-06, val loss: 1.95005e-06, min loss: 3.73637e-07\n",
      "Epoch: 2482300, elapsed: 1.16e+01, train loss: 3.74438e-07, val loss: 1.38207e-06, min loss: 3.73637e-07\n",
      "Epoch: 2482400, elapsed: 1.16e+01, train loss: 3.75371e-07, val loss: 1.36468e-06, min loss: 3.73637e-07\n",
      "Epoch: 2482500, elapsed: 1.17e+01, train loss: 3.99582e-07, val loss: 1.39216e-06, min loss: 3.73637e-07\n",
      "Epoch: 2482600, elapsed: 1.18e+01, train loss: 3.89911e-07, val loss: 1.42897e-06, min loss: 3.73637e-07\n",
      "Epoch: 2482700, elapsed: 1.20e+01, train loss: 3.73645e-07, val loss: 1.38415e-06, min loss: 3.73637e-07\n",
      "Epoch: 2482800, elapsed: 1.16e+01, train loss: 4.18541e-07, val loss: 1.43413e-06, min loss: 3.73637e-07\n",
      "Epoch: 2482900, elapsed: 1.16e+01, train loss: 4.68697e-07, val loss: 1.42450e-06, min loss: 3.73637e-07\n",
      "Epoch: 2483000, elapsed: 1.15e+01, train loss: 6.24549e-07, val loss: 1.83502e-06, min loss: 3.73637e-07\n",
      "Epoch: 2483100, elapsed: 1.16e+01, train loss: 3.84906e-07, val loss: 1.38128e-06, min loss: 3.73637e-07\n",
      "Epoch: 2483200, elapsed: 1.16e+01, train loss: 3.77732e-07, val loss: 1.37576e-06, min loss: 3.73637e-07\n",
      "Epoch: 2483300, elapsed: 1.16e+01, train loss: 3.90999e-07, val loss: 1.35986e-06, min loss: 3.73637e-07\n",
      "Epoch: 2483400, elapsed: 1.17e+01, train loss: 3.75699e-07, val loss: 1.36725e-06, min loss: 3.73637e-07\n",
      "Epoch: 2483500, elapsed: 1.15e+01, train loss: 5.53006e-07, val loss: 1.42711e-06, min loss: 3.73637e-07\n",
      "Epoch: 2483600, elapsed: 1.15e+01, train loss: 3.73171e-07, val loss: 1.37122e-06, min loss: 3.73171e-07\n",
      "Epoch: 2483700, elapsed: 1.16e+01, train loss: 3.86790e-07, val loss: 1.37696e-06, min loss: 3.73171e-07\n",
      "Epoch: 2483800, elapsed: 1.17e+01, train loss: 1.74798e-06, val loss: 2.95966e-06, min loss: 3.73171e-07\n",
      "Epoch: 2483900, elapsed: 1.14e+01, train loss: 3.73045e-07, val loss: 1.36587e-06, min loss: 3.73045e-07\n",
      "Epoch: 2484000, elapsed: 1.16e+01, train loss: 4.52499e-07, val loss: 1.51765e-06, min loss: 3.73045e-07\n",
      "Epoch: 2484100, elapsed: 1.13e+01, train loss: 5.27381e-07, val loss: 1.33023e-06, min loss: 3.73045e-07\n",
      "Epoch: 2484200, elapsed: 1.17e+01, train loss: 3.78626e-07, val loss: 1.37430e-06, min loss: 3.73045e-07\n",
      "Epoch: 2484300, elapsed: 1.81e+01, train loss: 3.73667e-07, val loss: 1.36337e-06, min loss: 3.73045e-07\n",
      "Epoch: 2484400, elapsed: 1.19e+01, train loss: 3.75856e-07, val loss: 1.39944e-06, min loss: 3.73045e-07\n",
      "Epoch: 2484500, elapsed: 1.19e+01, train loss: 2.08876e-06, val loss: 3.15024e-06, min loss: 3.73045e-07\n",
      "Epoch: 2484600, elapsed: 1.20e+01, train loss: 3.73008e-07, val loss: 1.36476e-06, min loss: 3.73008e-07\n",
      "Epoch: 2484700, elapsed: 1.17e+01, train loss: 3.80102e-07, val loss: 1.37033e-06, min loss: 3.73008e-07\n",
      "Epoch: 2484800, elapsed: 1.20e+01, train loss: 3.91857e-07, val loss: 1.43370e-06, min loss: 3.73008e-07\n",
      "Epoch: 2484900, elapsed: 1.19e+01, train loss: 4.27935e-07, val loss: 1.34737e-06, min loss: 3.73008e-07\n",
      "Epoch: 2485000, elapsed: 1.17e+01, train loss: 3.74729e-07, val loss: 1.39256e-06, min loss: 3.73008e-07\n",
      "Epoch: 2485100, elapsed: 1.37e+01, train loss: 8.13031e-07, val loss: 1.96915e-06, min loss: 3.73008e-07\n",
      "Epoch: 2485200, elapsed: 1.17e+01, train loss: 6.50831e-07, val loss: 1.67312e-06, min loss: 3.73008e-07\n",
      "Epoch: 2485300, elapsed: 1.17e+01, train loss: 3.98871e-07, val loss: 1.33648e-06, min loss: 3.73008e-07\n",
      "Epoch: 2485400, elapsed: 1.17e+01, train loss: 4.01636e-07, val loss: 1.40162e-06, min loss: 3.73008e-07\n",
      "Epoch: 2485500, elapsed: 1.18e+01, train loss: 3.86543e-07, val loss: 1.40832e-06, min loss: 3.73008e-07\n",
      "Epoch: 2485600, elapsed: 1.17e+01, train loss: 3.95186e-07, val loss: 1.41020e-06, min loss: 3.73008e-07\n",
      "Epoch: 2485700, elapsed: 1.17e+01, train loss: 3.73190e-07, val loss: 1.38045e-06, min loss: 3.73008e-07\n",
      "Epoch: 2485800, elapsed: 1.16e+01, train loss: 4.43842e-07, val loss: 1.48440e-06, min loss: 3.73008e-07\n",
      "Epoch: 2485900, elapsed: 1.18e+01, train loss: 3.78107e-07, val loss: 1.37052e-06, min loss: 3.73008e-07\n",
      "Epoch: 2486000, elapsed: 1.17e+01, train loss: 3.73252e-07, val loss: 1.37115e-06, min loss: 3.73008e-07\n",
      "Epoch: 2486100, elapsed: 1.17e+01, train loss: 4.20827e-07, val loss: 1.36118e-06, min loss: 3.73008e-07\n",
      "Epoch: 2486200, elapsed: 1.16e+01, train loss: 3.72814e-07, val loss: 1.36865e-06, min loss: 3.72814e-07\n",
      "Epoch: 2486300, elapsed: 1.15e+01, train loss: 3.73234e-07, val loss: 1.36939e-06, min loss: 3.72814e-07\n",
      "Epoch: 2486400, elapsed: 1.16e+01, train loss: 3.77189e-07, val loss: 1.33745e-06, min loss: 3.72814e-07\n",
      "Epoch: 2486500, elapsed: 1.14e+01, train loss: 4.51396e-07, val loss: 1.49859e-06, min loss: 3.72814e-07\n",
      "Epoch: 2486600, elapsed: 1.17e+01, train loss: 3.72630e-07, val loss: 1.36711e-06, min loss: 3.72630e-07\n",
      "Epoch: 2486700, elapsed: 1.16e+01, train loss: 5.16801e-07, val loss: 1.52902e-06, min loss: 3.72630e-07\n",
      "Epoch: 2486800, elapsed: 1.17e+01, train loss: 6.42511e-07, val loss: 1.53025e-06, min loss: 3.72630e-07\n",
      "Epoch: 2486900, elapsed: 1.16e+01, train loss: 3.73780e-07, val loss: 1.38137e-06, min loss: 3.72630e-07\n",
      "Epoch: 2487000, elapsed: 1.15e+01, train loss: 4.01139e-07, val loss: 1.43048e-06, min loss: 3.72630e-07\n",
      "Epoch: 2487100, elapsed: 1.16e+01, train loss: 3.77525e-07, val loss: 1.35431e-06, min loss: 3.72630e-07\n",
      "Epoch: 2487200, elapsed: 1.18e+01, train loss: 3.73127e-07, val loss: 1.36374e-06, min loss: 3.72630e-07\n",
      "Epoch: 2487300, elapsed: 1.15e+01, train loss: 4.97076e-07, val loss: 1.49613e-06, min loss: 3.72630e-07\n",
      "Epoch: 2487400, elapsed: 1.16e+01, train loss: 3.87510e-07, val loss: 1.35850e-06, min loss: 3.72630e-07\n",
      "Epoch: 2487500, elapsed: 1.19e+01, train loss: 3.87208e-07, val loss: 1.39526e-06, min loss: 3.72630e-07\n",
      "Epoch: 2487600, elapsed: 1.29e+01, train loss: 3.85169e-07, val loss: 1.40416e-06, min loss: 3.72630e-07\n",
      "Epoch: 2487700, elapsed: 1.26e+01, train loss: 5.87511e-07, val loss: 1.45288e-06, min loss: 3.72630e-07\n",
      "Epoch: 2487800, elapsed: 1.17e+01, train loss: 4.61188e-07, val loss: 1.40520e-06, min loss: 3.72630e-07\n",
      "Epoch: 2487900, elapsed: 1.17e+01, train loss: 1.44402e-06, val loss: 2.65013e-06, min loss: 3.72630e-07\n",
      "Epoch: 2488000, elapsed: 1.16e+01, train loss: 3.80359e-07, val loss: 1.39301e-06, min loss: 3.72630e-07\n",
      "Epoch: 2488100, elapsed: 1.15e+01, train loss: 3.72951e-07, val loss: 1.36066e-06, min loss: 3.72630e-07\n",
      "Epoch: 2488200, elapsed: 1.15e+01, train loss: 3.76797e-07, val loss: 1.38172e-06, min loss: 3.72630e-07\n",
      "Epoch: 2488300, elapsed: 1.15e+01, train loss: 4.12118e-07, val loss: 1.32717e-06, min loss: 3.72630e-07\n",
      "Epoch: 2488400, elapsed: 1.17e+01, train loss: 3.72171e-07, val loss: 1.36424e-06, min loss: 3.72171e-07\n",
      "Epoch: 2488500, elapsed: 1.17e+01, train loss: 3.78190e-07, val loss: 1.42752e-06, min loss: 3.72171e-07\n",
      "Epoch: 2488600, elapsed: 1.15e+01, train loss: 3.72256e-07, val loss: 1.36764e-06, min loss: 3.72171e-07\n",
      "Epoch: 2488700, elapsed: 1.82e+01, train loss: 3.73004e-07, val loss: 1.36847e-06, min loss: 3.72171e-07\n",
      "Epoch: 2488800, elapsed: 1.19e+01, train loss: 3.77842e-07, val loss: 1.37567e-06, min loss: 3.72171e-07\n",
      "Epoch: 2488900, elapsed: 1.20e+01, train loss: 3.72352e-07, val loss: 1.35384e-06, min loss: 3.72171e-07\n",
      "Epoch: 2489000, elapsed: 1.19e+01, train loss: 7.77122e-07, val loss: 1.35287e-06, min loss: 3.72171e-07\n",
      "Epoch: 2489100, elapsed: 1.20e+01, train loss: 3.73063e-07, val loss: 1.35887e-06, min loss: 3.72171e-07\n",
      "Epoch: 2489200, elapsed: 1.23e+01, train loss: 5.55711e-07, val loss: 1.62317e-06, min loss: 3.72171e-07\n",
      "Epoch: 2489300, elapsed: 1.31e+01, train loss: 3.72070e-07, val loss: 1.36619e-06, min loss: 3.72070e-07\n",
      "Epoch: 2489400, elapsed: 1.22e+01, train loss: 4.97562e-07, val loss: 1.50680e-06, min loss: 3.72070e-07\n",
      "Epoch: 2489500, elapsed: 1.24e+01, train loss: 3.71978e-07, val loss: 1.36302e-06, min loss: 3.71978e-07\n",
      "Epoch: 2489600, elapsed: 1.26e+01, train loss: 3.83982e-07, val loss: 1.32984e-06, min loss: 3.71978e-07\n",
      "Epoch: 2489700, elapsed: 1.23e+01, train loss: 3.73153e-07, val loss: 1.35535e-06, min loss: 3.71978e-07\n",
      "Epoch: 2489800, elapsed: 1.17e+01, train loss: 3.72248e-07, val loss: 1.36003e-06, min loss: 3.71978e-07\n",
      "Epoch: 2489900, elapsed: 1.16e+01, train loss: 3.96356e-06, val loss: 5.43139e-06, min loss: 3.71978e-07\n",
      "Epoch: 2490000, elapsed: 1.16e+01, train loss: 3.72068e-07, val loss: 1.36416e-06, min loss: 3.71978e-07\n",
      "Epoch: 2490100, elapsed: 1.37e+01, train loss: 4.15037e-07, val loss: 1.42213e-06, min loss: 3.71978e-07\n",
      "Epoch: 2490200, elapsed: 1.17e+01, train loss: 3.73820e-07, val loss: 1.38515e-06, min loss: 3.71978e-07\n",
      "Epoch: 2490300, elapsed: 1.14e+01, train loss: 3.97081e-07, val loss: 1.38186e-06, min loss: 3.71978e-07\n",
      "Epoch: 2490400, elapsed: 1.15e+01, train loss: 3.71932e-07, val loss: 1.36822e-06, min loss: 3.71932e-07\n",
      "Epoch: 2490500, elapsed: 1.15e+01, train loss: 3.72894e-07, val loss: 1.34454e-06, min loss: 3.71932e-07\n",
      "Epoch: 2490600, elapsed: 1.17e+01, train loss: 1.11459e-06, val loss: 2.06033e-06, min loss: 3.71932e-07\n",
      "Epoch: 2490700, elapsed: 1.15e+01, train loss: 3.72095e-07, val loss: 1.36271e-06, min loss: 3.71932e-07\n",
      "Epoch: 2490800, elapsed: 1.15e+01, train loss: 6.92344e-07, val loss: 1.70639e-06, min loss: 3.71932e-07\n",
      "Epoch: 2490900, elapsed: 1.17e+01, train loss: 4.98467e-07, val loss: 1.52868e-06, min loss: 3.71932e-07\n",
      "Epoch: 2491000, elapsed: 1.15e+01, train loss: 4.12197e-07, val loss: 1.40966e-06, min loss: 3.71932e-07\n",
      "Epoch: 2491100, elapsed: 1.17e+01, train loss: 1.36551e-06, val loss: 2.15453e-06, min loss: 3.71932e-07\n",
      "Epoch: 2491200, elapsed: 1.15e+01, train loss: 4.71813e-07, val loss: 1.57548e-06, min loss: 3.71932e-07\n",
      "Epoch: 2491300, elapsed: 1.16e+01, train loss: 6.05787e-07, val loss: 1.64346e-06, min loss: 3.71932e-07\n",
      "Epoch: 2491400, elapsed: 1.15e+01, train loss: 3.78649e-07, val loss: 1.40741e-06, min loss: 3.71932e-07\n",
      "Epoch: 2491500, elapsed: 1.14e+01, train loss: 4.28488e-07, val loss: 1.30892e-06, min loss: 3.71932e-07\n",
      "Epoch: 2491600, elapsed: 1.15e+01, train loss: 3.73001e-07, val loss: 1.35797e-06, min loss: 3.71932e-07\n",
      "Epoch: 2491700, elapsed: 1.14e+01, train loss: 3.73778e-07, val loss: 1.34846e-06, min loss: 3.71932e-07\n",
      "Epoch: 2491800, elapsed: 1.17e+01, train loss: 6.79440e-07, val loss: 1.90684e-06, min loss: 3.71932e-07\n",
      "Epoch: 2491900, elapsed: 1.16e+01, train loss: 4.80197e-07, val loss: 1.32991e-06, min loss: 3.71932e-07\n",
      "Epoch: 2492000, elapsed: 1.14e+01, train loss: 4.21464e-07, val loss: 1.30634e-06, min loss: 3.71932e-07\n",
      "Epoch: 2492100, elapsed: 1.16e+01, train loss: 4.13295e-07, val loss: 1.31519e-06, min loss: 3.71932e-07\n",
      "Epoch: 2492200, elapsed: 1.14e+01, train loss: 3.71950e-07, val loss: 1.35327e-06, min loss: 3.71932e-07\n",
      "Epoch: 2492300, elapsed: 1.14e+01, train loss: 3.72849e-07, val loss: 1.35250e-06, min loss: 3.71932e-07\n",
      "Epoch: 2492400, elapsed: 1.15e+01, train loss: 5.07863e-07, val loss: 1.47710e-06, min loss: 3.71932e-07\n",
      "Epoch: 2492500, elapsed: 1.16e+01, train loss: 3.71583e-07, val loss: 1.35773e-06, min loss: 3.71583e-07\n",
      "Epoch: 2492600, elapsed: 1.23e+01, train loss: 4.10858e-07, val loss: 1.36244e-06, min loss: 3.71583e-07\n",
      "Epoch: 2492700, elapsed: 1.22e+01, train loss: 4.03270e-07, val loss: 1.40560e-06, min loss: 3.71583e-07\n",
      "Epoch: 2492800, elapsed: 1.24e+01, train loss: 3.89442e-07, val loss: 1.38574e-06, min loss: 3.71583e-07\n",
      "Epoch: 2492900, elapsed: 1.24e+01, train loss: 3.71961e-07, val loss: 1.35944e-06, min loss: 3.71583e-07\n",
      "Epoch: 2493000, elapsed: 1.16e+01, train loss: 4.82556e-07, val loss: 1.49264e-06, min loss: 3.71583e-07\n",
      "Epoch: 2493100, elapsed: 1.81e+01, train loss: 5.22326e-07, val loss: 1.62252e-06, min loss: 3.71583e-07\n",
      "Epoch: 2493200, elapsed: 1.20e+01, train loss: 3.81396e-07, val loss: 1.41853e-06, min loss: 3.71583e-07\n",
      "Epoch: 2493300, elapsed: 1.19e+01, train loss: 4.37151e-07, val loss: 1.48996e-06, min loss: 3.71583e-07\n",
      "Epoch: 2493400, elapsed: 1.16e+01, train loss: 4.98527e-07, val loss: 1.43461e-06, min loss: 3.71583e-07\n",
      "Epoch: 2493500, elapsed: 1.17e+01, train loss: 3.81347e-07, val loss: 1.35119e-06, min loss: 3.71583e-07\n",
      "Epoch: 2493600, elapsed: 1.16e+01, train loss: 3.82887e-07, val loss: 1.42322e-06, min loss: 3.71583e-07\n",
      "Epoch: 2493700, elapsed: 1.16e+01, train loss: 3.72039e-07, val loss: 1.34830e-06, min loss: 3.71583e-07\n",
      "Epoch: 2493800, elapsed: 1.16e+01, train loss: 3.81942e-07, val loss: 1.40119e-06, min loss: 3.71583e-07\n",
      "Epoch: 2493900, elapsed: 1.16e+01, train loss: 1.08886e-06, val loss: 2.02517e-06, min loss: 3.71583e-07\n",
      "Epoch: 2494000, elapsed: 1.17e+01, train loss: 3.72007e-07, val loss: 1.34855e-06, min loss: 3.71583e-07\n",
      "Epoch: 2494100, elapsed: 1.17e+01, train loss: 3.92306e-07, val loss: 1.34574e-06, min loss: 3.71583e-07\n",
      "Epoch: 2494200, elapsed: 1.17e+01, train loss: 9.26330e-07, val loss: 1.46164e-06, min loss: 3.71583e-07\n",
      "Epoch: 2494300, elapsed: 1.16e+01, train loss: 4.13367e-07, val loss: 1.50545e-06, min loss: 3.71583e-07\n",
      "Epoch: 2494400, elapsed: 1.16e+01, train loss: 5.28396e-07, val loss: 1.62556e-06, min loss: 3.71583e-07\n",
      "Epoch: 2494500, elapsed: 1.15e+01, train loss: 6.37422e-07, val loss: 1.56851e-06, min loss: 3.71583e-07\n",
      "Epoch: 2494600, elapsed: 1.16e+01, train loss: 3.71227e-07, val loss: 1.35690e-06, min loss: 3.71227e-07\n",
      "Epoch: 2494700, elapsed: 1.17e+01, train loss: 3.73767e-07, val loss: 1.35381e-06, min loss: 3.71227e-07\n",
      "Epoch: 2494800, elapsed: 1.15e+01, train loss: 4.70953e-07, val loss: 1.49208e-06, min loss: 3.71227e-07\n",
      "Epoch: 2494900, elapsed: 1.15e+01, train loss: 8.73699e-07, val loss: 1.79637e-06, min loss: 3.71227e-07\n",
      "Epoch: 2495000, elapsed: 1.17e+01, train loss: 4.09021e-07, val loss: 1.37873e-06, min loss: 3.71227e-07\n",
      "Epoch: 2495100, elapsed: 1.37e+01, train loss: 1.25877e-06, val loss: 2.67364e-06, min loss: 3.71227e-07\n",
      "Epoch: 2495200, elapsed: 1.14e+01, train loss: 6.51173e-07, val loss: 1.75114e-06, min loss: 3.71227e-07\n",
      "Epoch: 2495300, elapsed: 1.14e+01, train loss: 3.72666e-07, val loss: 1.37656e-06, min loss: 3.71227e-07\n",
      "Epoch: 2495400, elapsed: 1.15e+01, train loss: 4.03239e-07, val loss: 1.37915e-06, min loss: 3.71227e-07\n",
      "Epoch: 2495500, elapsed: 1.18e+01, train loss: 5.29710e-07, val loss: 1.55092e-06, min loss: 3.71227e-07\n",
      "Epoch: 2495600, elapsed: 1.13e+01, train loss: 8.13357e-07, val loss: 1.45743e-06, min loss: 3.71227e-07\n",
      "Epoch: 2495700, elapsed: 1.15e+01, train loss: 1.84362e-06, val loss: 2.62496e-06, min loss: 3.71227e-07\n",
      "Epoch: 2495800, elapsed: 1.14e+01, train loss: 6.53191e-07, val loss: 1.83762e-06, min loss: 3.71227e-07\n",
      "Epoch: 2495900, elapsed: 1.13e+01, train loss: 3.71997e-07, val loss: 1.36270e-06, min loss: 3.71227e-07\n",
      "Epoch: 2496000, elapsed: 1.13e+01, train loss: 3.72222e-07, val loss: 1.36083e-06, min loss: 3.71227e-07\n",
      "Epoch: 2496100, elapsed: 1.14e+01, train loss: 4.23284e-07, val loss: 1.31434e-06, min loss: 3.71227e-07\n",
      "Epoch: 2496200, elapsed: 1.13e+01, train loss: 4.14234e-07, val loss: 1.36501e-06, min loss: 3.71227e-07\n",
      "Epoch: 2496300, elapsed: 1.15e+01, train loss: 8.87701e-07, val loss: 1.54830e-06, min loss: 3.71227e-07\n",
      "Epoch: 2496400, elapsed: 1.15e+01, train loss: 5.25605e-07, val loss: 1.48599e-06, min loss: 3.71227e-07\n",
      "Epoch: 2496500, elapsed: 1.14e+01, train loss: 3.71692e-07, val loss: 1.35296e-06, min loss: 3.71227e-07\n",
      "Epoch: 2496600, elapsed: 1.14e+01, train loss: 3.77698e-07, val loss: 1.35712e-06, min loss: 3.71227e-07\n",
      "Epoch: 2496700, elapsed: 1.15e+01, train loss: 3.74242e-07, val loss: 1.35288e-06, min loss: 3.71227e-07\n",
      "Epoch: 2496800, elapsed: 1.13e+01, train loss: 4.00779e-07, val loss: 1.38955e-06, min loss: 3.71227e-07\n",
      "Epoch: 2496900, elapsed: 1.14e+01, train loss: 4.60720e-07, val loss: 1.31709e-06, min loss: 3.71227e-07\n",
      "Epoch: 2497000, elapsed: 1.15e+01, train loss: 2.15086e-06, val loss: 2.94267e-06, min loss: 3.71227e-07\n",
      "Epoch: 2497100, elapsed: 1.14e+01, train loss: 3.92614e-07, val loss: 1.31243e-06, min loss: 3.71227e-07\n",
      "Epoch: 2497200, elapsed: 1.13e+01, train loss: 3.70867e-07, val loss: 1.35087e-06, min loss: 3.70867e-07\n",
      "Epoch: 2497300, elapsed: 1.13e+01, train loss: 3.85404e-07, val loss: 1.38788e-06, min loss: 3.70867e-07\n",
      "Epoch: 2497400, elapsed: 1.10e+01, train loss: 5.14583e-07, val loss: 1.54152e-06, min loss: 3.70867e-07\n",
      "Epoch: 2497500, elapsed: 1.85e+01, train loss: 4.96331e-07, val loss: 1.51321e-06, min loss: 3.70867e-07\n",
      "Epoch: 2497600, elapsed: 1.20e+01, train loss: 4.04355e-07, val loss: 1.36625e-06, min loss: 3.70867e-07\n",
      "Epoch: 2497700, elapsed: 1.16e+01, train loss: 3.70718e-07, val loss: 1.35476e-06, min loss: 3.70718e-07\n",
      "Epoch: 2497800, elapsed: 1.18e+01, train loss: 7.41460e-07, val loss: 1.85380e-06, min loss: 3.70718e-07\n",
      "Epoch: 2497900, elapsed: 1.18e+01, train loss: 3.70589e-07, val loss: 1.35250e-06, min loss: 3.70589e-07\n",
      "Epoch: 2498000, elapsed: 1.19e+01, train loss: 4.04209e-07, val loss: 1.34807e-06, min loss: 3.70589e-07\n",
      "Epoch: 2498100, elapsed: 1.17e+01, train loss: 1.09237e-06, val loss: 2.54704e-06, min loss: 3.70589e-07\n",
      "Epoch: 2498200, elapsed: 1.19e+01, train loss: 5.11520e-07, val loss: 1.54839e-06, min loss: 3.70589e-07\n",
      "Epoch: 2498300, elapsed: 1.19e+01, train loss: 6.01297e-07, val loss: 1.48098e-06, min loss: 3.70589e-07\n",
      "Epoch: 2498400, elapsed: 1.19e+01, train loss: 3.70573e-07, val loss: 1.35282e-06, min loss: 3.70573e-07\n",
      "Epoch: 2498500, elapsed: 1.28e+01, train loss: 3.89800e-07, val loss: 1.34281e-06, min loss: 3.70573e-07\n",
      "Epoch: 2498600, elapsed: 1.29e+01, train loss: 4.23453e-07, val loss: 1.42644e-06, min loss: 3.70573e-07\n",
      "Epoch: 2498700, elapsed: 1.29e+01, train loss: 3.74087e-07, val loss: 1.36450e-06, min loss: 3.70573e-07\n",
      "Epoch: 2498800, elapsed: 1.32e+01, train loss: 3.71182e-07, val loss: 1.35172e-06, min loss: 3.70573e-07\n",
      "Epoch: 2498900, elapsed: 1.27e+01, train loss: 4.89571e-07, val loss: 1.62043e-06, min loss: 3.70573e-07\n",
      "Epoch: 2499000, elapsed: 1.26e+01, train loss: 3.73294e-07, val loss: 1.35143e-06, min loss: 3.70573e-07\n",
      "Epoch: 2499100, elapsed: 1.29e+01, train loss: 3.80171e-07, val loss: 1.35849e-06, min loss: 3.70573e-07\n",
      "Epoch: 2499200, elapsed: 1.27e+01, train loss: 3.74001e-07, val loss: 1.36292e-06, min loss: 3.70573e-07\n",
      "Epoch: 2499300, elapsed: 1.28e+01, train loss: 3.99386e-07, val loss: 1.36295e-06, min loss: 3.70573e-07\n",
      "Epoch: 2499400, elapsed: 1.23e+01, train loss: 3.70291e-07, val loss: 1.35214e-06, min loss: 3.70291e-07\n",
      "Epoch: 2499500, elapsed: 1.18e+01, train loss: 3.71047e-07, val loss: 1.34695e-06, min loss: 3.70291e-07\n",
      "Epoch: 2499600, elapsed: 1.15e+01, train loss: 3.70312e-07, val loss: 1.34978e-06, min loss: 3.70291e-07\n",
      "Epoch: 2499700, elapsed: 1.17e+01, train loss: 3.73814e-07, val loss: 1.34666e-06, min loss: 3.70291e-07\n",
      "Epoch: 2499800, elapsed: 1.16e+01, train loss: 1.31228e-06, val loss: 2.29308e-06, min loss: 3.70291e-07\n",
      "Epoch: 2499900, elapsed: 1.18e+01, train loss: 3.70307e-07, val loss: 1.35341e-06, min loss: 3.70291e-07\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "# tf.keras.backend.set_floatx('float64')\n",
    "# tf.config.experimental_run_functions_eagerly(True)\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model, losses\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Multiply\n",
    "import time\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "# import types\n",
    "import tempfile\n",
    "import keras.models\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "activation = tf.keras.activations.swish\n",
    "# define two sets of inputs\n",
    "inputA = Input(shape=(7,))\n",
    "inputB = Input(shape=(1,))\n",
    "# the first branch operates on the first input\n",
    "x = Dense(32, activation=activation)(inputA)\n",
    "x = Dense(32, activation=activation)(x)\n",
    "x = Dense(32, activation=activation)(x)\n",
    "x = Dense(32, activation=activation)(x)\n",
    "x = Dense(32, activation=activation)(x)\n",
    "x = Dense(32, activation=activation)(x)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "# the second branch opreates on the second input\n",
    "y = Dense(32, activation=activation)(inputB)\n",
    "y = Dense(32, activation=activation)(y)\n",
    "y = Dense(32, activation=activation)(y)\n",
    "y = Dense(32, activation=activation)(y)\n",
    "y = Dense(32, activation=activation)(y)\n",
    "y = Dense(32, activation=activation)(y)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "\n",
    "\n",
    "combined = Multiply()([x.output, y.output])\n",
    "out = tf.math.reduce_sum(combined, axis = 1)\n",
    "out = tf.keras.backend.reshape(out, (-1, 1))\n",
    "\n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = Model(inputs=[x.input, y.input], outputs=out)\n",
    "\n",
    "trainEpochVector = []\n",
    "trainLossVector = []\n",
    "testLossVector = []\n",
    "def plotLossHistory(trainEpochVector, trainLossVector, testLossVector):        \n",
    "    font_size = 12\n",
    "    fig_loss, ax_loss = plt.subplots()\n",
    "    plt.rcParams.update({'font.size': font_size})\n",
    "    plt.subplots_adjust(left=0.25, bottom=0.15, right=0.9, top=0.9, wspace=0.1, hspace=0.1)\n",
    "    ax_loss.plot(trainEpochVector, np.log10(trainLossVector), 'b-.')\n",
    "    ax_loss.plot(trainEpochVector, np.log10(testLossVector), 'r-*')\n",
    "    ax_loss.legend(['Train loss', 'Test loss'])\n",
    "    ax_loss.set_xlabel('Epoch')\n",
    "    ax_loss.set_ylabel(\"Log loss\")\n",
    "    ax_loss.title.set_text('Total loss history')\n",
    "    fig_loss_history = fig_fpath + \"LossHistory.png\"\n",
    "    plt.savefig(fig_loss_history)\n",
    "    plt.close()\n",
    "\n",
    "def test_and_save(model, epoch):        \n",
    "    font_size = 40\n",
    "    fig = plt.figure(figsize=(40, 12), dpi = 100)\n",
    "    plt.rcParams.update({'font.size': font_size})\n",
    "    fig.suptitle('DeepONet on test set', y=0.99)\n",
    "    plt.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.85, wspace=0.25, hspace=0.1)\n",
    "    num_subplot = 1\n",
    "    idx = np.random.choice(u_test.shape[0] // unit_size)\n",
    "    for i in range(num_subplot):            \n",
    "        u_validate = u_test[idx * unit_size: (idx+1)*unit_size, :]\n",
    "        y_validate = y_test[idx * unit_size: (idx+1)*unit_size, :]\n",
    "        s_validate = s_test[idx * unit_size: (idx+1)*unit_size, :]\n",
    "\n",
    "        s_pred = model.predict([u_validate, y_validate])\n",
    "        axs = fig.add_subplot(131)\n",
    "        im = axs.scatter(y_validate[:, 0], s_validate, cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"Ground Truth\")\n",
    "        plt.xlim(0, 0.02)\n",
    "#             plt.colorbar(im)\n",
    "\n",
    "        axs = fig.add_subplot(132)\n",
    "        im = axs.scatter(y_validate[:, 0], s_pred, cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"DeepONet Prediction\")\n",
    "        plt.xlim(0, 0.02)\n",
    "#             plt.colorbar(im)\n",
    "\n",
    "        axs = fig.add_subplot(133)\n",
    "        im = axs.scatter(y_validate[:, 0], np.abs(s_pred - s_validate), cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"DeepONet Error\")\n",
    "        plt.xlim(0, 0.02)\n",
    "#             plt.colorbar(im)\n",
    "\n",
    "    path = f\"./DeepONet_v0{trial}/fig/\"\n",
    "    fig_test_name = path + \"Test_tf_pred_epoch_{0}.png\".format(epoch)\n",
    "    plt.savefig(fig_test_name)\n",
    "    plt.close()\n",
    "        \n",
    "def validate_and_save(model, epoch):        \n",
    "    font_size = 40\n",
    "    fig = plt.figure(figsize=(40, 12), dpi = 100)\n",
    "    plt.rcParams.update({'font.size': font_size})\n",
    "\n",
    "    fig.suptitle('DeepONet on training set', y=0.99)\n",
    "    plt.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.85, wspace=0.25, hspace=0.1)\n",
    "\n",
    "    num_subplot = 1\n",
    "    idx = np.random.choice(u_train.shape[0] // unit_size)\n",
    "\n",
    "    for i in range(num_subplot):   \n",
    "        u_validate = u_train[idx * unit_size: (idx+1)*unit_size, :]\n",
    "        y_validate = y_train[idx * unit_size: (idx+1)*unit_size, :]\n",
    "        s_validate = s_train[idx * unit_size: (idx+1)*unit_size, :]\n",
    "\n",
    "#         tf_dict = {self.X_func_tf: u_validate, self.X_loc_tf: y_validate} \n",
    "#         s_pred = self.sess.run(self.y_pred, tf_dict)\n",
    "        s_pred = model.predict([u_validate, y_validate])\n",
    "\n",
    "        axs = fig.add_subplot(131)\n",
    "        im = axs.scatter(y_validate[:, 0], s_validate, cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"Ground Truth\")\n",
    "        plt.xlim(0, 0.02)\n",
    "#             plt.colorbar(im)\n",
    "\n",
    "        axs = fig.add_subplot(132)\n",
    "        im = axs.scatter(y_validate[:, 0], s_pred, cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"DeepONet Prediction\")\n",
    "        plt.xlim(0, 0.02)\n",
    "#             plt.colorbar(im)\n",
    "\n",
    "        axs = fig.add_subplot(133)\n",
    "        im = axs.scatter(y_validate[:, 0],np.abs(s_pred - s_validate), cmap = 'jet', alpha = 0.9)\n",
    "        axs.set_title(\"DeepONet Error\")\n",
    "        plt.xlim(0, 0.02)\n",
    "\n",
    "    path = f\"./DeepONet_v0{trial}/fig/\"\n",
    "    fig_test_name = path + \"Validation_tf_pred_epoch_{0}.png\".format(epoch)\n",
    "    plt.savefig(fig_test_name)\n",
    "    plt.close()\n",
    "    \n",
    "with tf.device('/device:GPU:1'):\n",
    "    model.compile(optimizer=Adam(learning_rate=5e-4), loss=\"mse\")\n",
    "    epoch = 2500000\n",
    "    start_time = time.time()\n",
    "    min_loss = 10**3\n",
    "    for i in range(epoch):\n",
    "        hist = model.fit([u_train, y_train], s_train, batch_size=u_train.shape[0],\\\n",
    "                         validation_data=([u_test, y_test], s_test), epochs=1, verbose=0)\n",
    "        if i % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            loss = hist.history['loss'][0]\n",
    "            val_loss = hist.history['val_loss'][0]\n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "                ckpt_fpath = path + f\"/DeepONet_v0{trial}/Checkpoint/model.h5\"\n",
    "                model.save(ckpt_fpath)\n",
    "            print(f'Epoch: {i}, elapsed: {elapsed:.2e}, train loss: {loss:.5e}, val loss: {val_loss:.5e}, min loss: {min_loss:.5e}')\n",
    "            start_time = time.time()\n",
    "            trainLossVector.append(loss)  \n",
    "            testLossVector.append(val_loss)\n",
    "            trainEpochVector.append(i)\n",
    "        if i % 5000 == 0:\n",
    "            test_and_save(model, i)\n",
    "            validate_and_save(model, i)\n",
    "    plotLossHistory(trainEpochVector, trainLossVector, testLossVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(model.inputs[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(ckpt_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ckpt_fpath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b752a4c127f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Convert the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ckpt_fpath' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert the model.\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(ckpt_fpath)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(ckpt_fpath[:-8] + 'model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0333605]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=ckpt_fpath[:-8] + \"model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape_1 = input_details[0]['shape']\n",
    "input_data_1 = np.array(np.random.random_sample(input_shape_1), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data_1)\n",
    "\n",
    "input_shape_2 = input_details[1]['shape']\n",
    "input_data_2 = np.array(np.random.random_sample(input_shape_2), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[1]['index'], input_data_2)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(540900, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=ckpt_fpath[:-8] + \"model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "pred = np.empty((0, 1))\n",
    "for i in range(u_train.shape[0]):\n",
    "    u_tr = u_train[i:i+1, :]\n",
    "    y_tr = y_train[i:i+1, :]\n",
    "    # Test the model on random input data.\n",
    "    input_shape_1 = input_details[0]['shape']\n",
    "    input_data_1 = np.array(u_tr, dtype=np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data_1)\n",
    "\n",
    "    input_shape_2 = input_details[1]['shape']\n",
    "    input_data_2 = np.array(y_tr, dtype=np.float32)\n",
    "    interpreter.set_tensor(input_details[1]['index'], input_data_2)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # The function `get_tensor()` returns a copy of the tensor data.\n",
    "    # Use `tensor()` in order to get a pointer to the tensor.\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    pred = np.vstack((pred, output_data))\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540900, 7)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
